{"meta":{"version":1,"warehouse":"2.2.0"},"models":{"Asset":[{"_id":"source/favicon.ico","path":"favicon.ico","modified":1,"renderable":0},{"_id":"themes/next/source/css/main.styl","path":"css/main.styl","modified":1,"renderable":1},{"_id":"themes/next/source/images/algolia_logo.svg","path":"images/algolia_logo.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/avatar.gif","path":"images/avatar.gif","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","path":"images/cc-by-nc-nd.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","path":"images/cc-by-nc-sa.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc.svg","path":"images/cc-by-nc.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-nd.svg","path":"images/cc-by-nd.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-sa.svg","path":"images/cc-by-sa.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by.svg","path":"images/cc-by.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-zero.svg","path":"images/cc-zero.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/loading.gif","path":"images/loading.gif","modified":1,"renderable":1},{"_id":"themes/next/source/images/placeholder.gif","path":"images/placeholder.gif","modified":1,"renderable":1},{"_id":"themes/next/source/images/quote-l.svg","path":"images/quote-l.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/quote-r.svg","path":"images/quote-r.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/searchicon.png","path":"images/searchicon.png","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/algolia-search.js","path":"js/src/algolia-search.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/affix.js","path":"js/src/affix.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/exturl.js","path":"js/src/exturl.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/hook-duoshuo.js","path":"js/src/hook-duoshuo.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/motion.js","path":"js/src/motion.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/js.cookie.js","path":"js/src/js.cookie.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/post-details.js","path":"js/src/post-details.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/scroll-cookie.js","path":"js/src/scroll-cookie.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/scrollspy.js","path":"js/src/scrollspy.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/utils.js","path":"js/src/utils.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.css","path":"lib/algolia-instant-search/instantsearch.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/canvas-nest/canvas-nest.min.js","path":"lib/canvas-nest/canvas-nest.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/canvas-ribbon/canvas-ribbon.js","path":"lib/canvas-ribbon/canvas-ribbon.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fastclick/LICENSE","path":"lib/fastclick/LICENSE","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fastclick/bower.json","path":"lib/fastclick/bower.json","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fastclick/README.md","path":"lib/fastclick/README.md","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/HELP-US-OUT.txt","path":"lib/font-awesome/HELP-US-OUT.txt","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/bower.json","path":"lib/font-awesome/bower.json","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/README.md","path":"lib/jquery_lazyload/README.md","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/bower.json","path":"lib/jquery_lazyload/bower.json","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.lazyload.js","path":"lib/jquery_lazyload/jquery.lazyload.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.scrollstop.js","path":"lib/jquery_lazyload/jquery.scrollstop.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-barber-shop.min.css","path":"lib/pace/pace-theme-barber-shop.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/CONTRIBUTING.md","path":"lib/jquery_lazyload/CONTRIBUTING.md","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-big-counter.min.css","path":"lib/pace/pace-theme-big-counter.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-bounce.min.css","path":"lib/pace/pace-theme-bounce.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-center-atom.min.css","path":"lib/pace/pace-theme-center-atom.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-center-circle.min.css","path":"lib/pace/pace-theme-center-circle.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-center-radar.min.css","path":"lib/pace/pace-theme-center-radar.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-fill-left.min.css","path":"lib/pace/pace-theme-fill-left.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-flash.min.css","path":"lib/pace/pace-theme-flash.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-center-simple.min.css","path":"lib/pace/pace-theme-center-simple.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-corner-indicator.min.css","path":"lib/pace/pace-theme-corner-indicator.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-minimal.min.css","path":"lib/pace/pace-theme-minimal.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-mac-osx.min.css","path":"lib/pace/pace-theme-mac-osx.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace.min.js","path":"lib/pace/pace.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-loading-bar.min.css","path":"lib/pace/pace-theme-loading-bar.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/three/canvas_lines.min.js","path":"lib/three/canvas_lines.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/three/canvas_sphere.min.js","path":"lib/three/canvas_sphere.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/three/three-waves.min.js","path":"lib/three/three-waves.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/bower.json","path":"lib/velocity/bower.json","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.min.js","path":"lib/velocity/velocity.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.ui.js","path":"lib/velocity/velocity.ui.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.ui.min.js","path":"lib/velocity/velocity.ui.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery/index.js","path":"lib/jquery/index.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/bootstrap.js","path":"js/src/bootstrap.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/schemes/pisces.js","path":"js/src/schemes/pisces.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/han.css","path":"lib/Han/dist/han.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/han.min.css","path":"lib/Han/dist/han.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/han.min.js","path":"lib/Han/dist/han.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/blank.gif","path":"lib/fancybox/source/blank.gif","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading.gif","path":"lib/fancybox/source/fancybox_loading.gif","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading@2x.gif","path":"lib/fancybox/source/fancybox_loading@2x.gif","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_overlay.png","path":"lib/fancybox/source/fancybox_overlay.png","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite.png","path":"lib/fancybox/source/fancybox_sprite.png","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite@2x.png","path":"lib/fancybox/source/fancybox_sprite@2x.png","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.js","path":"lib/fancybox/source/jquery.fancybox.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.css","path":"lib/fancybox/source/jquery.fancybox.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.pack.js","path":"lib/fancybox/source/jquery.fancybox.pack.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.js","path":"lib/fastclick/lib/fastclick.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.min.js","path":"lib/fastclick/lib/fastclick.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css","path":"lib/font-awesome/css/font-awesome.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css.map","path":"lib/font-awesome/css/font-awesome.css.map","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.min.css","path":"lib/font-awesome/css/font-awesome.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.min.js","path":"lib/ua-parser-js/dist/ua-parser.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.pack.js","path":"lib/ua-parser-js/dist/ua-parser.pack.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/han.js","path":"lib/Han/dist/han.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff","path":"lib/font-awesome/fonts/fontawesome-webfont.woff","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff2","path":"lib/font-awesome/fonts/fontawesome-webfont.woff2","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.js","path":"lib/velocity/velocity.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han-space.otf","path":"lib/Han/dist/font/han-space.otf","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han.woff","path":"lib/Han/dist/font/han.woff","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/fancybox_buttons.png","path":"lib/fancybox/source/helpers/fancybox_buttons.png","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.css","path":"lib/fancybox/source/helpers/jquery.fancybox-buttons.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.js","path":"lib/fancybox/source/helpers/jquery.fancybox-buttons.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-media.js","path":"lib/fancybox/source/helpers/jquery.fancybox-media.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","path":"lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/FontAwesome.otf","path":"lib/font-awesome/fonts/FontAwesome.otf","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.eot","path":"lib/font-awesome/fonts/fontawesome-webfont.eot","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.ttf","path":"lib/font-awesome/fonts/fontawesome-webfont.ttf","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","path":"lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.js","path":"lib/algolia-instant-search/instantsearch.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han-space.woff","path":"lib/Han/dist/font/han-space.woff","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han.otf","path":"lib/Han/dist/font/han.otf","modified":1,"renderable":1},{"_id":"themes/next/source/lib/three/three.min.js","path":"lib/three/three.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.svg","path":"lib/font-awesome/fonts/fontawesome-webfont.svg","modified":1,"renderable":1}],"Cache":[{"_id":"themes/next/.git","hash":"042ff34da0707513a5681580b37513c890c671ef","modified":1504001401995},{"_id":"themes/next/.gitattributes","hash":"44bd4729c74ccb88110804f41746fec07bf487d4","modified":1504001401999},{"_id":"themes/next/.gitignore","hash":"32ea93f21d8693d5d8fa4eef1c51a21ad0670047","modified":1504001401999},{"_id":"source/favicon.ico","hash":"7be62720671a143fb8b79a1a196730df57fee81d","modified":1501552287379},{"_id":"themes/next/.javascript_ignore","hash":"8a224b381155f10e6eb132a4d815c5b52962a9d1","modified":1504001401999},{"_id":"themes/next/.bowerrc","hash":"3228a58ed0ece9f85e1e3136352094080b8dece1","modified":1504001401999},{"_id":"themes/next/.editorconfig","hash":"792fd2bd8174ece1a75d5fd24ab16594886f3a7f","modified":1504001401999},{"_id":"themes/next/.hound.yml","hash":"b76daa84c9ca3ad292c78412603370a367cc2bc3","modified":1504001401999},{"_id":"themes/next/.jshintrc","hash":"9928f81bd822f6a8d67fdbc909b517178533bca9","modified":1504001401999},{"_id":"themes/next/.stylintrc","hash":"b28e24704a5d8de08346c45286574c8e76cc109f","modified":1504001401999},{"_id":"themes/next/.travis.yml","hash":"d60d4a5375fea23d53b2156b764a99b2e56fa660","modified":1504001401999},{"_id":"themes/next/_config.yml","hash":"687bfa956ddee9f82559cadaee2190320ed452d2","modified":1507628212695},{"_id":"themes/next/LICENSE","hash":"f293bcfcdc06c0b77ba13570bb8af55eb5c059fd","modified":1504001401999},{"_id":"themes/next/README.cn.md","hash":"6d9177e7dad87e6129760e4b559bd3f7a15429d7","modified":1504001401999},{"_id":"themes/next/README.md","hash":"950ca6e9c0fa607d290a5b1fd883df44725b36b2","modified":1504001401999},{"_id":"themes/next/bower.json","hash":"7d7938f9da896fe710aa0e9120140e528bf058df","modified":1504001401999},{"_id":"themes/next/gulpfile.coffee","hash":"031bffc483e417b20e90eceb6cf358e7596d2e69","modified":1504001401999},{"_id":"themes/next/package.json","hash":"193dad6f59a588908fac082cc46fe067dac1b84d","modified":1504001402003},{"_id":"source/_posts/course-deep-learning-course2-week1.md","hash":"6c54f29a87e9730c6222b47529c758990d0ef2e9","modified":1506567652139},{"_id":"source/_posts/course-deep-learning-course2-week3.md","hash":"874676afc60bd0fc787d02f74264e4ef9715d8da","modified":1506950348404},{"_id":"source/_posts/hello.md","hash":"42c9e2fc129719066d04a28f169563cf2c898cdf","modified":1501552287379},{"_id":"source/_posts/gd-and-nm.md","hash":"6d7ad0d9af51dede1d06ab282aea39094f6ad83b","modified":1505287817667},{"_id":"source/_posts/paper-imbalance.md","hash":"9eff4f0fa6b63e1a8472acfd31f89b7a4b34aaef","modified":1505091331135},{"_id":"source/_posts/paper-facebook.md","hash":"1eee845e7367f3e8048643c2eadbedc01c026e99","modified":1504842727030},{"_id":"source/_posts/course-deep-learning-course2-week2.md","hash":"dbc352bf24d350ad39bd1c0becbd6b212fa5b863","modified":1506757251685},{"_id":"source/categories/index.md","hash":"479f410ff06047057aa727660ca0be19cd2ce350","modified":1501552287379},{"_id":"source/tags/index.md","hash":"9c20663285d74dac8c2d444ac95dbb25a838a6b2","modified":1501552287379},{"_id":"themes/next/.github/CONTRIBUTING.md","hash":"3b5eafd32abb718e56ccf8d1cee0607ad8ce611d","modified":1504001401999},{"_id":"themes/next/.github/ISSUE_TEMPLATE.md","hash":"679cdcc33eda5b33375206b2add1de84cea1615e","modified":1504001401999},{"_id":"themes/next/.github/PULL_REQUEST_TEMPLATE.md","hash":"e9169b65a7e3392c27562f9e11061a3ab76bb600","modified":1504001401999},{"_id":"themes/next/.github/browserstack_logo.png","hash":"a6c43887f64a7f48a2814e3714eaa1215e542037","modified":1504001401999},{"_id":"source/about/index.md","hash":"b91af98cc15092bbf8e68c7efeab10bca017701c","modified":1501817852991},{"_id":"themes/next/languages/de.yml","hash":"98aa551443b2a61a74b6f2a218635da6d2f6cf57","modified":1504001401999},{"_id":"themes/next/languages/default.yml","hash":"c0b90d66772e79585cd26a81694ad69c16312d6b","modified":1504001401999},{"_id":"themes/next/languages/en.yml","hash":"c0b90d66772e79585cd26a81694ad69c16312d6b","modified":1504001401999},{"_id":"themes/next/languages/fr-FR.yml","hash":"a14d051bbec26cfcae358bdcf1acf62a35fb1a45","modified":1504001401999},{"_id":"themes/next/languages/id.yml","hash":"f8b57daac2e50ace9a6d5051b17208af8139c2ae","modified":1504001401999},{"_id":"themes/next/languages/ja.yml","hash":"0c99ba4ba7d36c43d002342611d2c656ef498582","modified":1504001401999},{"_id":"themes/next/languages/ko.yml","hash":"043951e82997131dd8be40ff2093ef36849ba725","modified":1504001401999},{"_id":"themes/next/languages/pt-BR.yml","hash":"91584764104ef29293117375fc010b1bdbe9aff6","modified":1504001401999},{"_id":"source/_posts/ridge-lasso.md","hash":"ed958fa27cba733d09076a4b768381ededa5d66b","modified":1503913453916},{"_id":"themes/next/languages/pt.yml","hash":"dfd0b8574177346b78cab29db055fbc44ac309dc","modified":1504001401999},{"_id":"themes/next/languages/ru.yml","hash":"98dd9b6ddd88400a7b02cd7e8adb41e7b842bf57","modified":1504001401999},{"_id":"themes/next/languages/zh-Hans.yml","hash":"c1255b722fc5fdecf1852c3b592edfea9dbb554c","modified":1504001401999},{"_id":"themes/next/languages/zh-hk.yml","hash":"e8072846fd43beadbae394e30a49aa5c92a0a53b","modified":1504001401999},{"_id":"themes/next/layout/archive.swig","hash":"f0a8225feafd971419837cdb4bcfec98a4a59b2f","modified":1504001401999},{"_id":"themes/next/languages/zh-tw.yml","hash":"562141bfe450432131af012baa262a3de79a50bc","modified":1504001401999},{"_id":"themes/next/layout/category.swig","hash":"4472255f4a3e3dd6d79201523a9526dcabdfbf18","modified":1504001401999},{"_id":"themes/next/layout/_layout.swig","hash":"ada7ffc71cf05e7236a19e0648bce6d6d6cbc7dc","modified":1504001401999},{"_id":"themes/next/layout/index.swig","hash":"783611349c941848a0e26ee2f1dc44dd14879bd1","modified":1504001402003},{"_id":"themes/next/layout/post.swig","hash":"2d5f8d7f0a96b611e2d5a5e4d111fc17726a990f","modified":1504001402003},{"_id":"themes/next/layout/schedule.swig","hash":"d86f8de4e118f8c4d778b285c140474084a271db","modified":1504001402003},{"_id":"themes/next/scripts/merge-configs.js","hash":"13c8b3a2d9fce06c2488820d9248d190c8100e0a","modified":1504001402003},{"_id":"themes/next/scripts/merge.js","hash":"9130dabe6a674c54b535f322b17d75fe6081472f","modified":1504001402003},{"_id":"themes/next/layout/tag.swig","hash":"7e0a7d7d832883eddb1297483ad22c184e4368de","modified":1504001402003},{"_id":"source/_posts/convex-opt.md","hash":"fe2668f101a14669340b172528c31ff38a5fa4bb","modified":1502596607696},{"_id":"themes/next/test/.jshintrc","hash":"19f93d13d1689fe033c82eb2d5f3ce30b6543cc0","modified":1504001402023},{"_id":"themes/next/test/intern.js","hash":"11fa8a4f5c3b4119a179ae0a2584c8187f907a73","modified":1504001402023},{"_id":"themes/next/source/fonts/.gitkeep","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1504001402003},{"_id":"themes/next/test/helpers.js","hash":"a1f5de25154c3724ffc24a91ddc576cdbd60864f","modified":1504001402023},{"_id":"themes/next/layout/_custom/header.swig","hash":"adc83b19e793491b1c6ea0fd8b46cd9f32e592fc","modified":1504001401999},{"_id":"themes/next/layout/page.swig","hash":"37c874cd720acf0eda8d26e063278f2b6ae8d3a6","modified":1504001402003},{"_id":"themes/next/layout/_macro/post.swig","hash":"767e1d5503ecce85f577c8fb673a3503b65484ce","modified":1504001401999},{"_id":"themes/next/layout/_macro/reward.swig","hash":"5d5f70deb6074cb4dd0438463e14ccf89213c282","modified":1504001401999},{"_id":"themes/next/layout/_macro/sidebar.swig","hash":"4201551e0499bfcedde01bd663207b4d4126c18e","modified":1504001401999},{"_id":"themes/next/layout/_macro/wechat-subscriber.swig","hash":"39852700e4084ecccffa6d4669168e5cc0514c9e","modified":1504001401999},{"_id":"themes/next/layout/_partials/comments.swig","hash":"010ef8c42d2e1a95abc60caf757293ca8eb4a68b","modified":1504001401999},{"_id":"themes/next/layout/_partials/footer.swig","hash":"fb02c81273d5897ebb98b50f4c10f7edc34f9240","modified":1504001401999},{"_id":"themes/next/layout/_partials/head.swig","hash":"2cbeae795c9929ec1966b8a1fb9c058a0b547fa9","modified":1504001401999},{"_id":"themes/next/layout/_partials/header.swig","hash":"ed042be6252848058c90109236ec988e392d91d4","modified":1504001401999},{"_id":"themes/next/layout/_partials/page-header.swig","hash":"1efd925d34a5d4ba2dc0838d9c86ba911e705fc9","modified":1504001401999},{"_id":"themes/next/layout/_partials/pagination.swig","hash":"9e8e21d194ef44d271b1cca0bc1448c14d7edf4f","modified":1504001401999},{"_id":"themes/next/layout/_partials/search.swig","hash":"9dbd378e94abfcb3f864a5b8dbbf18d212ca2ee0","modified":1504001401999},{"_id":"themes/next/layout/_scripts/boostrap.swig","hash":"03aaebe9d50f6acb007ec38cc04acd1cfceb404d","modified":1504001401999},{"_id":"themes/next/layout/_scripts/commons.swig","hash":"766b2bdda29523ed6cd8d7aa197f996022f8fd94","modified":1504001401999},{"_id":"themes/next/layout/_scripts/vendors.swig","hash":"9baf90f7c40b3b10f288e9268c3191e895890cea","modified":1504001401999},{"_id":"themes/next/layout/_third-party/duoshuo-hot-articles.swig","hash":"5d4638c46aef65bf32a01681495b62416ccc98db","modified":1504001401999},{"_id":"themes/next/layout/_third-party/exturl.swig","hash":"7c04a42319d728be356746363aff8ea247791d24","modified":1504001401999},{"_id":"themes/next/layout/_third-party/mathjax.swig","hash":"6d25596d6a7c57700d37b607f8d9a62d89708683","modified":1504001401999},{"_id":"themes/next/layout/_third-party/rating.swig","hash":"fc93b1a7e6aed0dddb1f3910142b48d8ab61174e","modified":1504001401999},{"_id":"themes/next/layout/_third-party/schedule.swig","hash":"22369026c87fc23893c35a7f250b42f3bb1b60f1","modified":1504001401999},{"_id":"themes/next/layout/_third-party/scroll-cookie.swig","hash":"1ddb2336a1a19b47af3017047012c01ec5f54529","modified":1504001401999},{"_id":"themes/next/scripts/tags/button.js","hash":"62e6dbeb53d07627a048132c79630b45d9a8f2cc","modified":1504001402003},{"_id":"themes/next/scripts/tags/center-quote.js","hash":"535fc542781021c4326dec24d8495cbb1387634a","modified":1504001402003},{"_id":"themes/next/scripts/tags/exturl.js","hash":"8d7e60f60779bde050d20fd76f6fdc36fc85e06d","modified":1504001402003},{"_id":"themes/next/scripts/tags/full-image.js","hash":"8eeb3fb89540299bdbb799edfdfdac3743b50596","modified":1504001402003},{"_id":"themes/next/scripts/tags/group-pictures.js","hash":"49252824cd53184dc9b97b2f2d87ff28e1b3ef27","modified":1504001402003},{"_id":"themes/next/scripts/tags/label.js","hash":"2f8f41a7316372f0d1ed6b51190dc4acd3e16fff","modified":1504001402003},{"_id":"themes/next/scripts/tags/lazy-image.js","hash":"eeeabede68cf263de9e6593ecf682f620da16f0a","modified":1504001402003},{"_id":"themes/next/scripts/tags/note.js","hash":"64de4e9d01cf3b491ffc7d53afdf148ee5ad9779","modified":1504001402003},{"_id":"themes/next/scripts/tags/tabs.js","hash":"5786545d51c38e8ca38d1bfc7dd9e946fc70a316","modified":1504001402003},{"_id":"themes/next/source/css/main.styl","hash":"20702c48d6053c92c5bcdbc68e8d0ef1369848a0","modified":1504001402003},{"_id":"themes/next/source/images/algolia_logo.svg","hash":"ec119560b382b2624e00144ae01c137186e91621","modified":1504001402003},{"_id":"themes/next/source/images/avatar.gif","hash":"264082bb3a1af70d5499c7d22b0902cb454b6d12","modified":1504001402003},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","hash":"c6524ece3f8039a5f612feaf865d21ec8a794564","modified":1504001402003},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","hash":"3031be41e8753c70508aa88e84ed8f4f653f157e","modified":1504001402003},{"_id":"themes/next/source/images/cc-by-nc.svg","hash":"8d39b39d88f8501c0d27f8df9aae47136ebc59b7","modified":1504001402003},{"_id":"themes/next/source/images/cc-by-nd.svg","hash":"c563508ce9ced1e66948024ba1153400ac0e0621","modified":1504001402003},{"_id":"themes/next/source/images/cc-by-sa.svg","hash":"aa4742d733c8af8d38d4c183b8adbdcab045872e","modified":1504001402003},{"_id":"themes/next/source/images/cc-by.svg","hash":"28a0a4fe355a974a5e42f68031652b76798d4f7e","modified":1504001402003},{"_id":"themes/next/source/images/cc-zero.svg","hash":"87669bf8ac268a91d027a0a4802c92a1473e9030","modified":1504001402003},{"_id":"themes/next/source/images/loading.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1504001402003},{"_id":"themes/next/source/images/placeholder.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1504001402003},{"_id":"themes/next/source/images/quote-l.svg","hash":"94e870b4c8c48da61d09522196d4dd40e277a98f","modified":1504001402003},{"_id":"themes/next/source/images/quote-r.svg","hash":"e60ae504f9d99b712c793c3740c6b100d057d4ec","modified":1504001402003},{"_id":"themes/next/source/images/searchicon.png","hash":"67727a6a969be0b2659b908518fa6706eed307b8","modified":1504001402003},{"_id":"themes/next/layout/_scripts/schemes/mist.swig","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1504001401999},{"_id":"themes/next/layout/_scripts/schemes/muse.swig","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1504001401999},{"_id":"themes/next/source/css/_mixins/Mist.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1504001402003},{"_id":"themes/next/source/css/_mixins/Muse.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1504001402003},{"_id":"themes/next/source/css/_mixins/custom.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1504001402003},{"_id":"themes/next/source/css/_variables/Muse.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1504001402003},{"_id":"themes/next/source/css/_variables/custom.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1504001402003},{"_id":"themes/next/layout/_custom/sidebar.swig","hash":"adc83b19e793491b1c6ea0fd8b46cd9f32e592fc","modified":1504001401999},{"_id":"themes/next/layout/_macro/post-collapse.swig","hash":"31322a7f57936cf2dc62e824af5490da5354cf02","modified":1504001401999},{"_id":"themes/next/layout/_macro/post-copyright.swig","hash":"53d4f83b2b7fb4387dfc9fe81519abd56fbce4ae","modified":1504001401999},{"_id":"themes/next/layout/_partials/head/custom-head.swig","hash":"9e1b9666efa77f4cf8d8261bcfa445a9ac608e53","modified":1504001401999},{"_id":"themes/next/layout/_partials/head/external-fonts.swig","hash":"7ce76358411184482bb0934e70037949dd0da8ca","modified":1504001401999},{"_id":"themes/next/layout/_partials/search/localsearch.swig","hash":"957701729b85fb0c5bfcf2fb99c19d54582f91ed","modified":1504001401999},{"_id":"themes/next/layout/_partials/search/swiftype.swig","hash":"959b7e04a96a5596056e4009b73b6489c117597e","modified":1504001401999},{"_id":"themes/next/layout/_partials/share/duoshuo_share.swig","hash":"89c5a5240ecb223acfe1d12377df5562a943fd5d","modified":1504001401999},{"_id":"themes/next/layout/_partials/share/jiathis.swig","hash":"fd65b0d38d4a8b8306de815c48caad20b84ba4cb","modified":1504001401999},{"_id":"themes/next/layout/_scripts/pages/post-details.swig","hash":"069d1357c717572256e5cdee09574ebce529cbae","modified":1504001401999},{"_id":"themes/next/layout/_scripts/schemes/gemini.swig","hash":"a44acf9b0d0f44ef3dfc767376a95c984cc127de","modified":1504001401999},{"_id":"themes/next/layout/_scripts/schemes/pisces.swig","hash":"a44acf9b0d0f44ef3dfc767376a95c984cc127de","modified":1504001401999},{"_id":"themes/next/layout/_third-party/comments/changyan.swig","hash":"0e3378f7c39b2b0f69638290873ede6b6b6825c0","modified":1504001401999},{"_id":"themes/next/layout/_third-party/comments/disqus.swig","hash":"c316758546dc9ba6c60cb4d852c17ca6bb6d6724","modified":1504001401999},{"_id":"themes/next/layout/_third-party/comments/duoshuo.swig","hash":"a356b2185d40914447fde817eb3d358ab6b3e4c3","modified":1504001401999},{"_id":"themes/next/layout/_third-party/comments/hypercomments.swig","hash":"3e8dc5c6c912628a37e3b5f886bec7b2e5ed14ea","modified":1504001401999},{"_id":"themes/next/layout/_third-party/comments/index.swig","hash":"576e716893153a855eaf6d136fad7cb6d4065e09","modified":1504001401999},{"_id":"themes/next/layout/_third-party/comments/livere.swig","hash":"8a2e393d2e49f7bf560766d8a07cd461bf3fce4f","modified":1504001401999},{"_id":"themes/next/layout/_third-party/comments/valine.swig","hash":"9f4ed36c73e890909b8ebbe601fb60e13d048288","modified":1504001401999},{"_id":"themes/next/layout/_third-party/comments/youyan.swig","hash":"8b6650f77fe0a824c8075b2659e0403e0c78a705","modified":1504001401999},{"_id":"themes/next/layout/_third-party/search/index.swig","hash":"c747fb5c6b1f500e8f0c583e44195878b66e4e29","modified":1504001401999},{"_id":"themes/next/layout/_third-party/search/localsearch.swig","hash":"385c066af96bee30be2459dbec8aae1f15d382f5","modified":1504001401999},{"_id":"themes/next/layout/_third-party/search/tinysou.swig","hash":"cb3a5d36dbe1630bab84e03a52733a46df7c219b","modified":1504001401999},{"_id":"themes/next/layout/_third-party/seo/baidu-push.swig","hash":"c057b17f79e8261680fbae8dc4e81317a127c799","modified":1504001401999},{"_id":"themes/next/source/css/_custom/custom.styl","hash":"328d9a9696cc2ccf59c67d3c26000d569f46344c","modified":1504001402003},{"_id":"themes/next/source/css/_mixins/Gemini.styl","hash":"2aa5b7166a85a8aa34b17792ae4f58a5a96df6cc","modified":1504001402003},{"_id":"themes/next/source/css/_mixins/Pisces.styl","hash":"9ab65361ba0a12a986edd103e56492644c2db0b8","modified":1504001402003},{"_id":"themes/next/source/css/_mixins/base.styl","hash":"82f9055955920ed88a2ab6a20ab02169abb2c634","modified":1504001402003},{"_id":"themes/next/source/css/_variables/Gemini.styl","hash":"6359c84aaa02c90be60b22abe638b737ddd69c9c","modified":1504001402003},{"_id":"themes/next/source/css/_variables/Mist.styl","hash":"c8d35a6b9e3bff6d8fdb66de853065af9d37562d","modified":1504001402003},{"_id":"themes/next/source/css/_variables/Pisces.styl","hash":"cfee25d790e4f9b7d57f0dc7e2ea9c1649f08f11","modified":1504001402003},{"_id":"themes/next/source/css/_variables/base.styl","hash":"d477196c5699c8261b08e993a77ef67054d86166","modified":1504001402003},{"_id":"themes/next/layout/_partials/search/tinysou.swig","hash":"eefe2388ff3d424694045eda21346989b123977c","modified":1504001401999},{"_id":"themes/next/layout/_partials/share/add-this.swig","hash":"23e23dc0f76ef3c631f24c65277adf7ea517b383","modified":1504001401999},{"_id":"themes/next/layout/_partials/share/baidushare.swig","hash":"1f1107468aaf03f7d0dcd7eb2b653e2813a675b4","modified":1504001401999},{"_id":"themes/next/source/js/src/algolia-search.js","hash":"b172f697ed339a24b1e80261075232978d164c35","modified":1504001402003},{"_id":"themes/next/source/js/src/affix.js","hash":"978e0422b5bf1b560236d8d10ebc1adcf66392e3","modified":1504001402003},{"_id":"themes/next/source/js/src/exturl.js","hash":"e42e2aaab7bf4c19a0c8e779140e079c6aa5c0b1","modified":1504001402003},{"_id":"themes/next/source/js/src/hook-duoshuo.js","hash":"a6119070c0119f33e08b29da7d2cce2635eb40a0","modified":1504001402007},{"_id":"themes/next/source/js/src/motion.js","hash":"da146caf488078a634d961debf2a71ce4106018c","modified":1504001402007},{"_id":"themes/next/source/js/src/js.cookie.js","hash":"9b37973a90fd50e71ea91682265715e45ae82c75","modified":1504001402007},{"_id":"themes/next/source/js/src/post-details.js","hash":"0693695a9512641daff63d99da772625a058ab18","modified":1504001402007},{"_id":"themes/next/source/js/src/scroll-cookie.js","hash":"09dc828cbf5f31158ff6250d2bf7c3cde6365c67","modified":1504001402007},{"_id":"themes/next/source/js/src/scrollspy.js","hash":"fe4da1b9fe73518226446f5f27d2831e4426fc35","modified":1504001402007},{"_id":"themes/next/source/js/src/utils.js","hash":"2917c39c75b14b6dab7e1c46ab4d87b4df9fcd5d","modified":1504001402007},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.css","hash":"90ef19edc982645b118b095615838d9c5eaba0de","modified":1504001402007},{"_id":"themes/next/source/lib/canvas-nest/canvas-nest.min.js","hash":"0387e75e23b1db108a755073fe52a0d03eb391a7","modified":1504001402007},{"_id":"themes/next/source/lib/canvas-ribbon/canvas-ribbon.js","hash":"7fd2f3e2773555392ef40df40cae3bedb884f17a","modified":1504001402007},{"_id":"themes/next/source/lib/fancybox/.bower.json","hash":"cc40a9b11e52348e554c84e4a5c058056f6b7aeb","modified":1504001402007},{"_id":"themes/next/source/lib/fancybox/.gitattributes","hash":"2db21acfbd457452462f71cc4048a943ee61b8e0","modified":1504001402007},{"_id":"themes/next/source/lib/fastclick/.bower.json","hash":"93ebd5b35e632f714dcf1753e1f6db77ec74449b","modified":1504001402011},{"_id":"themes/next/source/lib/fastclick/LICENSE","hash":"dcd5b6b43095d9e90353a28b09cb269de8d4838e","modified":1504001402011},{"_id":"themes/next/source/lib/fastclick/bower.json","hash":"13379463c7463b4b96d13556b46faa4cc38d81e6","modified":1504001402011},{"_id":"themes/next/source/lib/font-awesome/.gitignore","hash":"69d152fa46b517141ec3b1114dd6134724494d83","modified":1504001402011},{"_id":"themes/next/source/lib/fastclick/README.md","hash":"1decd8e1adad2cd6db0ab50cf56de6035156f4ea","modified":1504001402011},{"_id":"themes/next/source/lib/font-awesome/.bower.json","hash":"a2aaaf12378db56bd10596ba3daae30950eac051","modified":1504001402011},{"_id":"themes/next/source/lib/font-awesome/.npmignore","hash":"dcf470ab3a358103bb896a539cc03caeda10fa8b","modified":1504001402011},{"_id":"themes/next/source/lib/font-awesome/HELP-US-OUT.txt","hash":"4f7bf961f1bed448f6ba99aeb9219fabf930ba96","modified":1504001402011},{"_id":"themes/next/source/lib/font-awesome/bower.json","hash":"279a8a718ab6c930a67c41237f0aac166c1b9440","modified":1504001402011},{"_id":"themes/next/source/lib/jquery/.bower.json","hash":"91745c2cc6c946c7275f952b2b0760b880cea69e","modified":1504001402019},{"_id":"themes/next/source/lib/jquery_lazyload/.bower.json","hash":"b7638afc93e9cd350d0783565ee9a7da6805ad8e","modified":1504001402019},{"_id":"themes/next/source/lib/jquery_lazyload/README.md","hash":"895d50fa29759af7835256522e9dd7dac597765c","modified":1504001402019},{"_id":"themes/next/source/lib/jquery_lazyload/bower.json","hash":"65bc85d12197e71c40a55c0cd7f6823995a05222","modified":1504001402019},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.lazyload.js","hash":"481fd478650e12b67c201a0ea41e92743f8b45a3","modified":1504001402019},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.scrollstop.js","hash":"0e9a81785a011c98be5ea821a8ed7d411818cfd1","modified":1504001402019},{"_id":"themes/next/source/lib/pace/pace-theme-barber-shop.min.css","hash":"ee0d51446cb4ffe1bb96bd7bc8c8e046dddfcf46","modified":1504001402019},{"_id":"themes/next/source/lib/jquery_lazyload/CONTRIBUTING.md","hash":"4891864c24c28efecd81a6a8d3f261145190f901","modified":1504001402019},{"_id":"themes/next/source/lib/pace/pace-theme-big-counter.min.css","hash":"5b561dc328af4c4d512e20a76fe964d113a32ba8","modified":1504001402019},{"_id":"themes/next/source/lib/pace/pace-theme-bounce.min.css","hash":"f6bdb9a785b7979dd8ec5c60e278af955ef1e585","modified":1504001402019},{"_id":"themes/next/source/lib/pace/pace-theme-center-atom.min.css","hash":"dcf79c24fe5350fb73d8038573a104e73639e9d3","modified":1504001402019},{"_id":"themes/next/source/lib/pace/pace-theme-center-circle.min.css","hash":"a4066769c78affbfbc5e30a600e2c7862cd532e0","modified":1504001402019},{"_id":"themes/next/source/lib/pace/pace-theme-center-radar.min.css","hash":"ab7cba998bf4c03b13df342bf43647fa4f419783","modified":1504001402019},{"_id":"themes/next/source/lib/pace/pace-theme-fill-left.min.css","hash":"0bec1e235a4a2cccda3f993b205424e1441a44ae","modified":1504001402019},{"_id":"themes/next/source/lib/pace/pace-theme-flash.min.css","hash":"13ace22c40312d7bbd8d9c1e50eff897a7a497d8","modified":1504001402019},{"_id":"themes/next/source/lib/pace/pace-theme-center-simple.min.css","hash":"67f44c947548bd4d77e7590d3f59e236cbf9e98a","modified":1504001402019},{"_id":"themes/next/source/lib/pace/pace-theme-corner-indicator.min.css","hash":"b3c64c973f31884e3d8145989476707333406b9a","modified":1504001402019},{"_id":"themes/next/source/lib/pace/pace-theme-minimal.min.css","hash":"9cd783cceb8a191f3c8b5d81f7a430ecc3e489d3","modified":1504001402019},{"_id":"themes/next/source/lib/pace/pace-theme-mac-osx.min.css","hash":"9f2e7b51b084da407863826b25265b31150b3821","modified":1504001402019},{"_id":"themes/next/source/lib/pace/pace.min.js","hash":"9944dfb7814b911090e96446cea4d36e2b487234","modified":1504001402019},{"_id":"themes/next/source/lib/pace/pace-theme-loading-bar.min.css","hash":"7ee28875dfc1230d76c537f6605766e8d4011e9f","modified":1504001402019},{"_id":"themes/next/source/lib/three/canvas_lines.min.js","hash":"dce4a3b65f8bf958f973690caa7ec4952f353b0c","modified":1504001402019},{"_id":"themes/next/source/lib/three/canvas_sphere.min.js","hash":"d8ea241a53c135a650f7335d2b6982b899fd58a9","modified":1504001402019},{"_id":"themes/next/source/lib/three/three-waves.min.js","hash":"d968cba6b3a50b3626a02d67b544f349d83b147c","modified":1504001402019},{"_id":"themes/next/source/lib/velocity/.bower.json","hash":"05f960846f1c7a93dab1d3f9a1121e86812e8c88","modified":1504001402023},{"_id":"themes/next/source/lib/velocity/bower.json","hash":"2ec99573e84c7117368beccb9e94b6bf35d2db03","modified":1504001402023},{"_id":"themes/next/source/lib/velocity/velocity.min.js","hash":"2f1afadc12e4cf59ef3b405308d21baa97e739c6","modified":1504001402023},{"_id":"themes/next/source/lib/velocity/velocity.ui.js","hash":"6a1d101eab3de87527bb54fcc8c7b36b79d8f0df","modified":1504001402023},{"_id":"themes/next/layout/_third-party/analytics/analytics-with-widget.swig","hash":"98df9d72e37dd071e882f2d5623c9d817815b139","modified":1504001401999},{"_id":"themes/next/source/lib/velocity/velocity.ui.min.js","hash":"ed5e534cd680a25d8d14429af824f38a2c7d9908","modified":1504001402023},{"_id":"themes/next/layout/_third-party/analytics/application-insights.swig","hash":"60426bf73f8a89ba61fb1be2df3ad5398e32c4ef","modified":1504001401999},{"_id":"themes/next/layout/_third-party/analytics/baidu-analytics.swig","hash":"deda6a814ed48debc694c4e0c466f06c127163d0","modified":1504001401999},{"_id":"themes/next/layout/_third-party/analytics/busuanzi-counter.swig","hash":"18e7bef8923d83ea42df6c97405e515a876cede4","modified":1504001401999},{"_id":"themes/next/layout/_third-party/analytics/cnzz-analytics.swig","hash":"8160b27bee0aa372c7dc7c8476c05bae57f58d0f","modified":1504001401999},{"_id":"themes/next/layout/_third-party/analytics/facebook-sdk.swig","hash":"a234c5cd1f75ca5731e814d0dbb92fdcf9240d1b","modified":1504001401999},{"_id":"themes/next/layout/_third-party/analytics/google-analytics.swig","hash":"5d9943d74cc2e0a91badcf4f755c6de77eab193a","modified":1504001401999},{"_id":"themes/next/layout/_third-party/analytics/index.swig","hash":"5e9bb24c750b49513d9a65799e832f07410002ac","modified":1504001401999},{"_id":"themes/next/layout/_third-party/analytics/lean-analytics.swig","hash":"fc65b9c98a0a8ab43a5e7aabff6c5f03838e09c8","modified":1504001401999},{"_id":"themes/next/layout/_third-party/analytics/tencent-analytics.swig","hash":"3658414379e0e8a34c45c40feadc3edc8dc55f88","modified":1504001401999},{"_id":"themes/next/layout/_third-party/analytics/tencent-mta.swig","hash":"0ddc94ed4ba0c19627765fdf1abc4d8efbe53d5a","modified":1504001401999},{"_id":"themes/next/source/lib/jquery/index.js","hash":"41b4bfbaa96be6d1440db6e78004ade1c134e276","modified":1504001402019},{"_id":"themes/next/layout/_third-party/analytics/vkontakte-api.swig","hash":"c3971fd154d781088e1cc665035f8561a4098f4c","modified":1504001401999},{"_id":"themes/next/layout/_third-party/search/algolia-search/assets.swig","hash":"28ff4ed6714c59124569ffcbd10f1173d53ca923","modified":1504001401999},{"_id":"themes/next/layout/_third-party/search/algolia-search/dom.swig","hash":"ba698f49dd3a868c95b240d802f5b1b24ff287e4","modified":1504001401999},{"_id":"themes/next/source/css/_common/components/back-to-top-sidebar.styl","hash":"d026c8489f66ab6c12ad04bd37f1d5b6f2f3f0d1","modified":1504001402003},{"_id":"themes/next/source/css/_common/components/back-to-top.styl","hash":"31050fc7a25784805b4843550151c93bfa55c9c8","modified":1504001402003},{"_id":"themes/next/source/css/_common/components/buttons.styl","hash":"0dfb4b3ba3180d7285e66f270e1d3fa0f132c3d2","modified":1504001402003},{"_id":"themes/next/source/css/_common/components/comments.styl","hash":"471f1627891aca5c0e1973e09fbcb01e1510d193","modified":1504001402003},{"_id":"themes/next/source/css/_common/components/components.styl","hash":"a6bb5256be6195e76addbda12f4ed7c662d65e7a","modified":1504001402003},{"_id":"themes/next/source/css/_common/components/tag-cloud.styl","hash":"dd8a3b22fc2f222ac6e6c05bd8a773fb039169c0","modified":1504001402003},{"_id":"themes/next/source/css/_common/outline/outline.styl","hash":"f2030fa436c47791d1a42358cc0ef6f9809f212c","modified":1504001402003},{"_id":"themes/next/source/css/_common/scaffolding/base.styl","hash":"f7c44b0ee46cf2cf82a4c9455ba8d8b55299976f","modified":1504001402003},{"_id":"themes/next/source/css/_common/scaffolding/helpers.styl","hash":"9c25c75311e1bd4d68df031d3f2ae6d141a90766","modified":1504001402003},{"_id":"themes/next/source/css/_common/scaffolding/mobile.styl","hash":"86b6fd7f1b1be3ae98f8af6b23a6b1299c670ce9","modified":1504001402003},{"_id":"themes/next/source/css/_common/scaffolding/normalize.styl","hash":"ece571f38180febaf02ace8187ead8318a300ea7","modified":1504001402003},{"_id":"themes/next/source/css/_common/scaffolding/scaffolding.styl","hash":"a280a583b7615e939aaddbf778f5c108ef8a2a6c","modified":1504001402003},{"_id":"themes/next/source/css/_common/scaffolding/tables.styl","hash":"64f5d56c08d74a338813df1265580ca0cbf0190b","modified":1504001402003},{"_id":"themes/next/source/css/_schemes/Gemini/index.styl","hash":"bc8c388553bbcf95897459a466ba35bffd5ec5f0","modified":1504001402003},{"_id":"themes/next/source/css/_schemes/Mist/_base.styl","hash":"c2d079788d6fc2e9a191ccdae94e50d55bf849dc","modified":1504001402003},{"_id":"themes/next/source/css/_schemes/Mist/_header.styl","hash":"5ae7906dc7c1d9468c7f4b4a6feddddc555797a1","modified":1504001402003},{"_id":"themes/next/source/css/_schemes/Mist/_logo.styl","hash":"38e5df90c8689a71c978fd83ba74af3d4e4e5386","modified":1504001402003},{"_id":"themes/next/source/css/_schemes/Mist/_menu.styl","hash":"b0dcca862cd0cc6e732e33d975b476d744911742","modified":1504001402003},{"_id":"themes/next/source/css/_schemes/Mist/_posts-expanded.styl","hash":"fda14bc35be2e1b332809b55b3d07155a833dbf4","modified":1504001402003},{"_id":"themes/next/source/css/_schemes/Mist/_search.styl","hash":"1452cbe674cc1d008e1e9640eb4283841058fc64","modified":1504001402003},{"_id":"themes/next/source/css/_schemes/Mist/index.styl","hash":"9a5581a770af8964064fef7afd3e16963e45547f","modified":1504001402003},{"_id":"themes/next/source/css/_common/components/pagination.styl","hash":"711c8830886619d4f4a0598b0cde5499dce50c62","modified":1504001402003},{"_id":"themes/next/source/css/_schemes/Muse/_layout.styl","hash":"0efa036a15c18f5abb058b7c0fad1dd9ac5eed4c","modified":1504001402003},{"_id":"themes/next/source/js/src/bootstrap.js","hash":"034bc8113e0966fe2096ba5b56061bbf10ef0512","modified":1504001402003},{"_id":"themes/next/source/css/_schemes/Muse/_logo.styl","hash":"8829bc556ca38bfec4add4f15a2f028092ac6d46","modified":1504001402003},{"_id":"themes/next/source/css/_schemes/Muse/_menu.styl","hash":"82bbaa6322764779a1ac2e2c8390ce901c7972e2","modified":1504001402003},{"_id":"themes/next/source/css/_schemes/Muse/index.styl","hash":"a0e2030a606c934fb2c5c7373aaae04a1caac4c5","modified":1504001402003},{"_id":"themes/next/source/css/_schemes/Pisces/_layout.styl","hash":"0af5a9322156c4c21d3c7d38f5ee48de5286f523","modified":1504001402003},{"_id":"themes/next/source/css/_schemes/Pisces/_menu.styl","hash":"215de948be49bcf14f06d500cef9f7035e406a43","modified":1504001402003},{"_id":"themes/next/source/css/_schemes/Pisces/_posts.styl","hash":"2f878213cb24c5ddc18877f6d15ec5c5f57745ac","modified":1504001402003},{"_id":"themes/next/source/css/_schemes/Pisces/_sidebar.styl","hash":"f00d0a9ff02f6814011e0b613a2d9020911b5c58","modified":1504001402003},{"_id":"themes/next/source/css/_schemes/Pisces/index.styl","hash":"69ecd6c97e7cdfd822ac8102b45ad0ede85050db","modified":1504001402003},{"_id":"themes/next/source/js/src/schemes/pisces.js","hash":"79da92119bc246fe05d1626ac98426a83ec90a94","modified":1504001402007},{"_id":"themes/next/source/lib/Han/dist/han.css","hash":"bd40da3fba8735df5850956814e312bd7b3193d7","modified":1504001402007},{"_id":"themes/next/source/lib/Han/dist/han.min.css","hash":"a0c9e32549a8b8cf327ab9227b037f323cdb60ee","modified":1504001402007},{"_id":"themes/next/source/lib/Han/dist/han.min.js","hash":"f559c68a25065a14f47da954a7617d87263e409d","modified":1504001402007},{"_id":"themes/next/source/lib/fancybox/source/blank.gif","hash":"2daeaa8b5f19f0bc209d976c02bd6acb51b00b0a","modified":1504001402007},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading.gif","hash":"1a755fb2599f3a313cc6cfdb14df043f8c14a99c","modified":1504001402007},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading@2x.gif","hash":"273b123496a42ba45c3416adb027cd99745058b0","modified":1504001402007},{"_id":"themes/next/source/lib/fancybox/source/fancybox_overlay.png","hash":"b3a4ee645ba494f52840ef8412015ba0f465dbe0","modified":1504001402007},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite.png","hash":"17df19f97628e77be09c352bf27425faea248251","modified":1504001402011},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite@2x.png","hash":"30c58913f327e28f466a00f4c1ac8001b560aed8","modified":1504001402011},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.js","hash":"1cf3d47b5ccb7cb6e9019c64f2a88d03a64853e4","modified":1504001402011},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.css","hash":"5f163444617b6cf267342f06ac166a237bb62df9","modified":1504001402011},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.pack.js","hash":"53360764b429c212f424399384417ccc233bb3be","modified":1504001402011},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.js","hash":"06cef196733a710e77ad7e386ced6963f092dc55","modified":1504001402011},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.min.js","hash":"2cae0f5a6c5d6f3cb993015e6863f9483fc4de18","modified":1504001402011},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css","hash":"0140952c64e3f2b74ef64e050f2fe86eab6624c8","modified":1504001402011},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css.map","hash":"0189d278706509412bac4745f96c83984e1d59f4","modified":1504001402011},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.min.css","hash":"512c7d79033e3028a9be61b540cf1a6870c896f8","modified":1504001402011},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.min.js","hash":"38628e75e4412cc6f11074e03e1c6d257aae495b","modified":1504001402019},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.pack.js","hash":"214dad442a92d36af77ed0ca1d9092b16687f02f","modified":1504001402023},{"_id":"themes/next/source/css/_schemes/Muse/_search.styl","hash":"1452cbe674cc1d008e1e9640eb4283841058fc64","modified":1504001402003},{"_id":"themes/next/source/css/_schemes/Pisces/_brand.styl","hash":"c4ed249798296f60bda02351fe6404fb3ef2126f","modified":1504001402003},{"_id":"themes/next/source/lib/Han/dist/han.js","hash":"e345397e0585c9fed1449e614ec13e0224acf2ab","modified":1504001402007},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff","hash":"28b782240b3e76db824e12c02754a9731a167527","modified":1504001402019},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff2","hash":"d6f48cba7d076fb6f2fd6ba993a75b9dc1ecbf0c","modified":1504001402019},{"_id":"themes/next/source/lib/velocity/velocity.js","hash":"9f08181baea0cc0e906703b7e5df9111b9ef3373","modified":1504001402023},{"_id":"themes/next/source/css/_common/components/footer/footer.styl","hash":"7905a7f625702b45645d8be1268cb8af3f698c70","modified":1504001402003},{"_id":"themes/next/source/css/_common/components/header/header.styl","hash":"ae1ca14e51de67b07dba8f61ec79ee0e2e344574","modified":1504001402003},{"_id":"themes/next/source/css/_common/components/header/headerband.styl","hash":"d27448f199fc2f9980b601bc22b87f08b5d64dd1","modified":1504001402003},{"_id":"themes/next/source/css/_common/components/header/site-nav.styl","hash":"49c2b2c14a1e7fcc810c6be4b632975d0204c281","modified":1504001402003},{"_id":"themes/next/source/css/_common/components/highlight/diff.styl","hash":"96f32ea6c3265a3889e6abe57587f6e2a2a40dfb","modified":1504001402003},{"_id":"themes/next/source/css/_common/components/highlight/highlight.styl","hash":"740d37f428b8f4574a76fc95cc25e50e0565f45e","modified":1504001402003},{"_id":"themes/next/source/css/_common/components/highlight/theme.styl","hash":"b76387934fb6bb75212b23c1a194486892cc495e","modified":1504001402003},{"_id":"themes/next/source/css/_common/components/pages/archive.styl","hash":"f5aa2ba3bfffc15475e7e72a55b5c9d18609fdf5","modified":1504001402003},{"_id":"themes/next/source/css/_common/components/pages/categories.styl","hash":"4eff5b252d7b614e500fc7d52c97ce325e57d3ab","modified":1504001402003},{"_id":"themes/next/source/css/_common/components/pages/pages.styl","hash":"2039590632bba3943c39319d80ef630af7928185","modified":1504001402003},{"_id":"themes/next/source/css/_common/components/pages/post-detail.styl","hash":"9bf4362a4d0ae151ada84b219d39fbe5bb8c790e","modified":1504001402003},{"_id":"themes/next/source/css/_common/components/pages/schedule.styl","hash":"a82afbb72d83ee394aedc7b37ac0008a9823b4f4","modified":1504001402003},{"_id":"themes/next/source/css/_common/components/post/post-button.styl","hash":"beccb53dcd658136fb91a0c5678dea8f37d6e0b6","modified":1504001402003},{"_id":"themes/next/source/css/_common/components/post/post-collapse.styl","hash":"0f7f522cc6bfb3401d5afd62b0fcdf48bb2d604b","modified":1504001402003},{"_id":"themes/next/source/css/_common/components/post/post-copyright.styl","hash":"f54367c0feda6986c030cc4d15a0ca6ceea14bcb","modified":1504001402003},{"_id":"themes/next/source/css/_common/components/post/post-eof.styl","hash":"2cdc094ecf907a02fce25ad4a607cd5c40da0f2b","modified":1504001402003},{"_id":"themes/next/source/css/_common/components/post/post-expand.styl","hash":"88c7d75646b66b168213190ee4cd874609afd5e3","modified":1504001402003},{"_id":"themes/next/source/css/_common/components/post/post-gallery.styl","hash":"387ce23bba52b22a586b2dfb4ec618fe1ffd3926","modified":1504001402003},{"_id":"themes/next/source/css/_common/components/post/post-meta.styl","hash":"aea21141015ca8c409d8b33e3e34ec505f464e93","modified":1504001402003},{"_id":"themes/next/source/css/_common/components/post/post-nav.styl","hash":"a5d8617a24d7cb6c5ad91ea621183ca2c0917331","modified":1504001402003},{"_id":"themes/next/source/css/_common/components/post/post-reward.styl","hash":"ee554b1031ef0070a5916477939021800e3c9d27","modified":1504001402003},{"_id":"themes/next/source/css/_common/components/post/post-rtl.styl","hash":"017074ef58166e2d69c53bb7590a0e7a8947a1ed","modified":1504001402003},{"_id":"themes/next/source/css/_common/components/post/post-tags.styl","hash":"a352ae5b1f8857393bf770d2e638bf15f0c9585d","modified":1504001402003},{"_id":"themes/next/source/css/_common/components/post/post-title.styl","hash":"963105a531403d7aad6d9e5e23e3bfabb8ec065a","modified":1504001402003},{"_id":"themes/next/source/css/_common/components/post/post-type.styl","hash":"10251257aceecb117233c9554dcf8ecfef8e2104","modified":1504001402003},{"_id":"themes/next/source/css/_common/components/post/post-widgets.styl","hash":"08a500b2984f109b751f3697ca33172d1340591a","modified":1504001402003},{"_id":"themes/next/source/css/_common/components/post/post.styl","hash":"67c357ddc16b31e7dfd8f956a77f984662c06fc2","modified":1504001402003},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-author-links.styl","hash":"65a64d5662637b66e2f039a5f58217afe7a6e800","modified":1504001402003},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-author.styl","hash":"920343e41c124221a17f050bbb989494d44f7a24","modified":1504001402003},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-blogroll.styl","hash":"89dd4f8b1f1cce3ad46cf2256038472712387d02","modified":1504001402003},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-dimmer.styl","hash":"efa5e5022e205b52786ce495d4879f5e7b8f84b2","modified":1504001402003},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-feed-link.styl","hash":"9486ddd2cb255227db102d09a7df4cae0fabad72","modified":1504001402003},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-nav.styl","hash":"45fa7193435a8eae9960267438750b4c9fa9587f","modified":1504001402003},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-toc.styl","hash":"77c92a449ce84d558d26d052681f2e0dd77c70c9","modified":1504001402003},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-toggle.styl","hash":"f7784aba0c1cd20d824c918c120012d57a5eaa2a","modified":1504001402003},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar.styl","hash":"57d2c8a060f5e4e1a0aef9aae11a0016cf7ac5ba","modified":1504001402003},{"_id":"themes/next/source/css/_common/components/sidebar/site-state.styl","hash":"3623e7fa4324ec1307370f33d8f287a9e20a5578","modified":1504001402003},{"_id":"themes/next/source/css/_common/components/tags/blockquote-center.styl","hash":"c2abe4d87148e23e15d49ee225bc650de60baf46","modified":1504001402003},{"_id":"themes/next/source/css/_common/components/tags/exturl.styl","hash":"1b3cc9f4e5a7f6e05b4100e9990b37b20d4a2005","modified":1504001402003},{"_id":"themes/next/source/css/_common/components/tags/full-image.styl","hash":"b8969e1654eec89a0fd10d88b337fee9cb03cd44","modified":1504001402003},{"_id":"themes/next/source/css/_common/components/tags/group-pictures.styl","hash":"4851b981020c5cbc354a1af9b831a2dcb3cf9d39","modified":1504001402003},{"_id":"themes/next/source/css/_common/components/tags/label.styl","hash":"4a457d265d62f287c63d48764ce45d9bcfc9ec5a","modified":1504001402003},{"_id":"themes/next/source/css/_common/components/tags/note-modern.styl","hash":"45df0cf4c97b47e05573bcd41028ee50f3fdf432","modified":1504001402003},{"_id":"themes/next/source/css/_common/components/tags/note.styl","hash":"32c9156bea5bac9e9ad0b4c08ffbca8b3d9aac4b","modified":1504001402003},{"_id":"themes/next/source/css/_common/components/tags/tabs.styl","hash":"4ab5deed8c3b0c338212380f678f8382672e1bcb","modified":1504001402003},{"_id":"themes/next/source/css/_common/components/tags/tags.styl","hash":"ead0d0f2321dc71505788c7f689f92257cf14947","modified":1504001402003},{"_id":"themes/next/source/css/_common/components/third-party/algolia-search.styl","hash":"fd42777b9125fd8969dc39d4f15473e2b91b4142","modified":1504001402003},{"_id":"themes/next/source/css/_common/components/third-party/baidushare.styl","hash":"93b08815c4d17e2b96fef8530ec1f1064dede6ef","modified":1504001402003},{"_id":"themes/next/source/css/_common/components/third-party/busuanzi-counter.styl","hash":"d4e6d8d7b34dc69994593c208f875ae8f7e8a3ae","modified":1504001402003},{"_id":"themes/next/source/css/_common/components/third-party/duoshuo.styl","hash":"2340dd9b3202c61d73cc708b790fac5adddbfc7f","modified":1504001402003},{"_id":"themes/next/source/css/_common/components/third-party/han.styl","hash":"cce6772e2cdb4db85d35486ae4c6c59367fbdd40","modified":1504001402003},{"_id":"themes/next/source/css/_common/components/third-party/jiathis.styl","hash":"327b5f63d55ec26f7663185c1a778440588d9803","modified":1504001402003},{"_id":"themes/next/source/css/_common/components/third-party/localsearch.styl","hash":"d89c4b562b528e4746696b2ad8935764d133bdae","modified":1504001402003},{"_id":"themes/next/source/css/_common/components/third-party/third-party.styl","hash":"aeff0e6e23725e8baea27c890ccbbf466024f767","modified":1504001402003},{"_id":"themes/next/source/css/_schemes/Mist/outline/outline.styl","hash":"5dc4859c66305f871e56cba78f64bfe3bf1b5f01","modified":1504001402003},{"_id":"themes/next/source/css/_schemes/Mist/sidebar/sidebar-blogroll.styl","hash":"817587e46df49e819858c8ecbafa08b53d5ff040","modified":1504001402003},{"_id":"themes/next/source/css/_schemes/Muse/sidebar/sidebar-blogroll.styl","hash":"817587e46df49e819858c8ecbafa08b53d5ff040","modified":1504001402003},{"_id":"themes/next/source/css/_common/components/header/menu.styl","hash":"8a2421cb9005352905fae9d41a847ae56957247e","modified":1504001402003},{"_id":"themes/next/source/css/_common/components/header/site-meta.styl","hash":"6c00f6e0978f4d8f9a846a15579963728aaa6a17","modified":1504001402003},{"_id":"themes/next/source/lib/Han/dist/font/han-space.otf","hash":"07436f011b44051f61b8329c99de4bec64e86f4b","modified":1504001402007},{"_id":"themes/next/source/lib/Han/dist/font/han.woff","hash":"f38ff9b2eecaa17b50b66aa2dae87e9e7436d195","modified":1504001402007},{"_id":"themes/next/source/lib/fancybox/source/helpers/fancybox_buttons.png","hash":"e385b139516c6813dcd64b8fc431c364ceafe5f3","modified":1504001402011},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.css","hash":"1a9d8e5c22b371fcc69d4dbbb823d9c39f04c0c8","modified":1504001402011},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.js","hash":"91e41741c2e93f732c82aaacec4cfc6e3f3ec876","modified":1504001402011},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-media.js","hash":"3bdf69ed2469e4fb57f5a95f17300eef891ff90d","modified":1504001402011},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","hash":"4ac329c16a5277592fc12a37cca3d72ca4ec292f","modified":1504001402011},{"_id":"themes/next/source/lib/font-awesome/fonts/FontAwesome.otf","hash":"048707bc52ac4b6563aaa383bfe8660a0ddc908c","modified":1504001402011},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.eot","hash":"d980c2ce873dc43af460d4d572d441304499f400","modified":1504001402015},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.ttf","hash":"13b1eab65a983c7a73bc7997c479d66943f7c6cb","modified":1504001402015},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","hash":"53e194f4a72e649c04fb586dd57762b8c022800b","modified":1504001402011},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.js","hash":"9ccc6f8144f54e86df9a3fd33a18368d81cf3a4f","modified":1504001402007},{"_id":"themes/next/source/lib/Han/dist/font/han-space.woff","hash":"7a635062b10bf5662ae1d218ba0980171005d060","modified":1504001402007},{"_id":"themes/next/source/lib/Han/dist/font/han.otf","hash":"f1f6bb8f461f5672e000380195d3d2358a28494c","modified":1504001402007},{"_id":"themes/next/source/lib/three/three.min.js","hash":"73f4cdc17e51a72b9bf5b9291f65386d615c483b","modified":1504001402019},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.svg","hash":"98a8aa5cf7d62c2eff5f07ede8d844b874ef06ed","modified":1504001402015},{"_id":"public/categories/index.html","hash":"7403fca05aa86eede70f22698828584e8f2504a0","modified":1507628322596},{"_id":"public/tags/index.html","hash":"bd99d5076d8bfd36a0b6d40e967cbeb564423932","modified":1507628322596},{"_id":"public/about/index.html","hash":"5804ad03788441105609971cae448266853597ad","modified":1507628322596},{"_id":"public/archives/2017/07/index.html","hash":"a09ec6f42f8ec1fb04764188882a285b05307ab3","modified":1507628322596},{"_id":"public/archives/2017/08/index.html","hash":"b4ead8af248fc9a95152f9246c87153e4e9ff331","modified":1507628322596},{"_id":"public/archives/2017/09/index.html","hash":"9c0aa70648d7c11041a95f4f28ec0bdd6ef45b21","modified":1507628322596},{"_id":"public/categories/learning-notes/index.html","hash":"6c7fea8daee2e7e756f80c717db8cef2e7d5f558","modified":1507628322596},{"_id":"public/categories/杂/index.html","hash":"d2f5372f062147eacad65ad639bc42bdd8e47e78","modified":1507628322596},{"_id":"public/categories/machine-learning/index.html","hash":"82dc586431042402ddbd852adfd91f0e17297f52","modified":1507628322596},{"_id":"public/categories/reading-notes/index.html","hash":"712ca77789e2747a382f9fbf8285908a4e730ae8","modified":1507628322597},{"_id":"public/tags/regularization/index.html","hash":"451afb3696512172f64ff043e69f3412b43aaf74","modified":1507628322597},{"_id":"public/tags/gradient-descent/index.html","hash":"fc0c85693c2809da730ed3cb4321d3dcf544ad15","modified":1507628322597},{"_id":"public/tags/hyperparameter/index.html","hash":"d42ca2ef0fb060801f0c49242797029120dd021d","modified":1507628322597},{"_id":"public/tags/batch-norm/index.html","hash":"97429e187ba116bd8ff85a319c7869ca581fefcd","modified":1507628322597},{"_id":"public/tags/covariate-shift/index.html","hash":"cc4a2a96e1c7cab432a3e891bc82b80f4a2b89a1","modified":1507628322597},{"_id":"public/tags/抒情一把/index.html","hash":"6695e9cb3937429055a1f37ca997dfe706b635f3","modified":1507628322597},{"_id":"public/tags/unconstrained-optimization/index.html","hash":"45e3c3ea4572e9af538f92699cb71a9e4c8df5db","modified":1507628322597},{"_id":"public/tags/newton-s-method/index.html","hash":"9744d560d2b22b032f47f977d3f854db7f7cbe1e","modified":1507628322597},{"_id":"public/tags/imbalanced-data/index.html","hash":"d2188defd9a8a08b9f6eac13aa0643ae5666ac1a","modified":1507628322597},{"_id":"public/tags/undersampling/index.html","hash":"85145dd4fd6e19bfc4ea888fff0f5cf3497f183b","modified":1507628322597},{"_id":"public/tags/bagging/index.html","hash":"c5aac17f409ad214ae132632730da24f650fd0d9","modified":1507628322597},{"_id":"public/tags/gbt/index.html","hash":"e04517ee946cbbe48a67c21a0e078f9754b1d161","modified":1507628322597},{"_id":"public/tags/logistic-regression/index.html","hash":"56ead0c178ec4c21f1cd38647b3cda0a70f83184","modified":1507628322597},{"_id":"public/tags/moving-averages/index.html","hash":"44bb3412c5b73b92129de200ef47aab0cf72fe22","modified":1507628322597},{"_id":"public/tags/MAP/index.html","hash":"9f093899285d678a667948380c680b9fd65dc477","modified":1507628322597},{"_id":"public/tags/ridge-regression/index.html","hash":"12d4995c4a977cf476dfa0e4ad1fdebb300f3607","modified":1507628322597},{"_id":"public/tags/lasso-regression/index.html","hash":"d845578a2239077d9d5cbb9aa4a0bb672efaf415","modified":1507628322597},{"_id":"public/tags/convex-optimization/index.html","hash":"220536fd18ba95672f4e3908979bc23c085473ab","modified":1507628322597},{"_id":"public/2017/09/30/course-deep-learning-course2-week3/index.html","hash":"266ac1a2f2841674819cd810f50ac55ec1ab78ec","modified":1507628322597},{"_id":"public/2017/09/27/course-deep-learning-course2-week2/index.html","hash":"72a2536fbd2679c646058a6c6425d15f573348b6","modified":1507628322597},{"_id":"public/2017/09/24/course-deep-learning-course2-week1/index.html","hash":"6d1c39e7ef7bbae1ec6abd4bb8a7abb6713140ed","modified":1507628322597},{"_id":"public/2017/09/10/paper-imbalance/index.html","hash":"544996f57cdf318c4cdd25e03ce7fb863a283669","modified":1507628322597},{"_id":"public/2017/08/27/ridge-lasso/index.html","hash":"e32895f3f8d75abfad6fea4cc302d764848ae7b5","modified":1507628322597},{"_id":"public/2017/08/23/paper-facebook/index.html","hash":"b9e56e2a2d04db41ec0c03b9c3781f85eb69fd56","modified":1507628322598},{"_id":"public/2017/08/11/gd-and-nm/index.html","hash":"e1836901c42682a5d54a774b80819a7b28b4b5b4","modified":1507628322598},{"_id":"public/2017/08/02/convex-opt/index.html","hash":"0e1d2bce864876c2b680b49e1a63b6767a404b17","modified":1507628322598},{"_id":"public/2017/07/26/hello/index.html","hash":"ef18575158ddff98d667302b913680c61c1aea8a","modified":1507628322598},{"_id":"public/archives/index.html","hash":"87fc983eaafbc94beae1a1ccfb430da7d01bc048","modified":1507628322598},{"_id":"public/archives/2017/index.html","hash":"063a6e3de2f0d7b77efda9002bd11036606ff01f","modified":1507628322598},{"_id":"public/index.html","hash":"b57d771bfb955467916d4945322ddd563cf464f5","modified":1507628322598},{"_id":"public/favicon.ico","hash":"7be62720671a143fb8b79a1a196730df57fee81d","modified":1507628322610},{"_id":"public/images/algolia_logo.svg","hash":"ec119560b382b2624e00144ae01c137186e91621","modified":1507628322610},{"_id":"public/images/avatar.gif","hash":"264082bb3a1af70d5499c7d22b0902cb454b6d12","modified":1507628322610},{"_id":"public/images/cc-by-nc-nd.svg","hash":"c6524ece3f8039a5f612feaf865d21ec8a794564","modified":1507628322610},{"_id":"public/images/cc-by-nc-sa.svg","hash":"3031be41e8753c70508aa88e84ed8f4f653f157e","modified":1507628322610},{"_id":"public/images/cc-by-nc.svg","hash":"8d39b39d88f8501c0d27f8df9aae47136ebc59b7","modified":1507628322610},{"_id":"public/images/cc-by-nd.svg","hash":"c563508ce9ced1e66948024ba1153400ac0e0621","modified":1507628322610},{"_id":"public/images/cc-by-sa.svg","hash":"aa4742d733c8af8d38d4c183b8adbdcab045872e","modified":1507628322610},{"_id":"public/images/cc-by.svg","hash":"28a0a4fe355a974a5e42f68031652b76798d4f7e","modified":1507628322610},{"_id":"public/images/cc-zero.svg","hash":"87669bf8ac268a91d027a0a4802c92a1473e9030","modified":1507628322610},{"_id":"public/images/loading.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1507628322610},{"_id":"public/images/placeholder.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1507628322610},{"_id":"public/images/quote-l.svg","hash":"94e870b4c8c48da61d09522196d4dd40e277a98f","modified":1507628322610},{"_id":"public/images/quote-r.svg","hash":"e60ae504f9d99b712c793c3740c6b100d057d4ec","modified":1507628322610},{"_id":"public/images/searchicon.png","hash":"67727a6a969be0b2659b908518fa6706eed307b8","modified":1507628322610},{"_id":"public/lib/fastclick/LICENSE","hash":"dcd5b6b43095d9e90353a28b09cb269de8d4838e","modified":1507628322610},{"_id":"public/lib/font-awesome/HELP-US-OUT.txt","hash":"4f7bf961f1bed448f6ba99aeb9219fabf930ba96","modified":1507628322610},{"_id":"public/lib/fancybox/source/blank.gif","hash":"2daeaa8b5f19f0bc209d976c02bd6acb51b00b0a","modified":1507628322611},{"_id":"public/lib/fancybox/source/fancybox_loading.gif","hash":"1a755fb2599f3a313cc6cfdb14df043f8c14a99c","modified":1507628322611},{"_id":"public/lib/fancybox/source/fancybox_loading@2x.gif","hash":"273b123496a42ba45c3416adb027cd99745058b0","modified":1507628322611},{"_id":"public/lib/fancybox/source/fancybox_overlay.png","hash":"b3a4ee645ba494f52840ef8412015ba0f465dbe0","modified":1507628322611},{"_id":"public/lib/fancybox/source/fancybox_sprite.png","hash":"17df19f97628e77be09c352bf27425faea248251","modified":1507628322611},{"_id":"public/lib/fancybox/source/fancybox_sprite@2x.png","hash":"30c58913f327e28f466a00f4c1ac8001b560aed8","modified":1507628322611},{"_id":"public/lib/font-awesome/css/font-awesome.css.map","hash":"0189d278706509412bac4745f96c83984e1d59f4","modified":1507628322611},{"_id":"public/lib/Han/dist/font/han-space.otf","hash":"07436f011b44051f61b8329c99de4bec64e86f4b","modified":1507628322611},{"_id":"public/lib/Han/dist/font/han.woff","hash":"f38ff9b2eecaa17b50b66aa2dae87e9e7436d195","modified":1507628322611},{"_id":"public/lib/fancybox/source/helpers/fancybox_buttons.png","hash":"e385b139516c6813dcd64b8fc431c364ceafe5f3","modified":1507628322611},{"_id":"public/lib/Han/dist/font/han-space.woff","hash":"7a635062b10bf5662ae1d218ba0980171005d060","modified":1507628322611},{"_id":"public/lib/Han/dist/font/han.otf","hash":"f1f6bb8f461f5672e000380195d3d2358a28494c","modified":1507628322611},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.woff","hash":"28b782240b3e76db824e12c02754a9731a167527","modified":1507628323172},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.woff2","hash":"d6f48cba7d076fb6f2fd6ba993a75b9dc1ecbf0c","modified":1507628323178},{"_id":"public/js/src/algolia-search.js","hash":"b172f697ed339a24b1e80261075232978d164c35","modified":1507628323186},{"_id":"public/js/src/affix.js","hash":"978e0422b5bf1b560236d8d10ebc1adcf66392e3","modified":1507628323186},{"_id":"public/js/src/exturl.js","hash":"e42e2aaab7bf4c19a0c8e779140e079c6aa5c0b1","modified":1507628323186},{"_id":"public/js/src/motion.js","hash":"da146caf488078a634d961debf2a71ce4106018c","modified":1507628323186},{"_id":"public/js/src/hook-duoshuo.js","hash":"a6119070c0119f33e08b29da7d2cce2635eb40a0","modified":1507628323186},{"_id":"public/js/src/post-details.js","hash":"0693695a9512641daff63d99da772625a058ab18","modified":1507628323187},{"_id":"public/js/src/js.cookie.js","hash":"9b37973a90fd50e71ea91682265715e45ae82c75","modified":1507628323187},{"_id":"public/js/src/scroll-cookie.js","hash":"09dc828cbf5f31158ff6250d2bf7c3cde6365c67","modified":1507628323187},{"_id":"public/js/src/scrollspy.js","hash":"fe4da1b9fe73518226446f5f27d2831e4426fc35","modified":1507628323187},{"_id":"public/js/src/utils.js","hash":"2917c39c75b14b6dab7e1c46ab4d87b4df9fcd5d","modified":1507628323187},{"_id":"public/lib/canvas-nest/canvas-nest.min.js","hash":"0387e75e23b1db108a755073fe52a0d03eb391a7","modified":1507628323187},{"_id":"public/lib/algolia-instant-search/instantsearch.min.css","hash":"90ef19edc982645b118b095615838d9c5eaba0de","modified":1507628323187},{"_id":"public/lib/canvas-ribbon/canvas-ribbon.js","hash":"7fd2f3e2773555392ef40df40cae3bedb884f17a","modified":1507628323187},{"_id":"public/lib/fastclick/bower.json","hash":"4dcecf83afddba148464d5339c93f6d0aa9f42e9","modified":1507628323187},{"_id":"public/lib/fastclick/README.html","hash":"c07b353b4efa132290ec4479102a55d80ac6d300","modified":1507628323187},{"_id":"public/lib/font-awesome/bower.json","hash":"64394a2a9aa00f8e321d8daa5e51a420f0e96dad","modified":1507628323187},{"_id":"public/lib/jquery_lazyload/bower.json","hash":"ae3c3b61e6e7f9e1d7e3585ad854380ecc04cf53","modified":1507628323187},{"_id":"public/lib/jquery_lazyload/README.html","hash":"a08fccd381c8fdb70ba8974b208254c5ba23a95f","modified":1507628323187},{"_id":"public/lib/jquery_lazyload/jquery.lazyload.js","hash":"481fd478650e12b67c201a0ea41e92743f8b45a3","modified":1507628323187},{"_id":"public/lib/jquery_lazyload/jquery.scrollstop.js","hash":"0e9a81785a011c98be5ea821a8ed7d411818cfd1","modified":1507628323187},{"_id":"public/lib/pace/pace-theme-barber-shop.min.css","hash":"ee0d51446cb4ffe1bb96bd7bc8c8e046dddfcf46","modified":1507628323187},{"_id":"public/lib/jquery_lazyload/CONTRIBUTING.html","hash":"06811ca2f722dead021493457f27cdc264ef928d","modified":1507628323187},{"_id":"public/lib/pace/pace-theme-big-counter.min.css","hash":"5b561dc328af4c4d512e20a76fe964d113a32ba8","modified":1507628323187},{"_id":"public/lib/pace/pace-theme-bounce.min.css","hash":"f6bdb9a785b7979dd8ec5c60e278af955ef1e585","modified":1507628323187},{"_id":"public/lib/pace/pace-theme-center-atom.min.css","hash":"dcf79c24fe5350fb73d8038573a104e73639e9d3","modified":1507628323187},{"_id":"public/lib/pace/pace-theme-center-circle.min.css","hash":"a4066769c78affbfbc5e30a600e2c7862cd532e0","modified":1507628323187},{"_id":"public/lib/pace/pace-theme-center-radar.min.css","hash":"ab7cba998bf4c03b13df342bf43647fa4f419783","modified":1507628323187},{"_id":"public/lib/pace/pace-theme-fill-left.min.css","hash":"0bec1e235a4a2cccda3f993b205424e1441a44ae","modified":1507628323187},{"_id":"public/lib/pace/pace-theme-flash.min.css","hash":"13ace22c40312d7bbd8d9c1e50eff897a7a497d8","modified":1507628323187},{"_id":"public/lib/pace/pace-theme-center-simple.min.css","hash":"67f44c947548bd4d77e7590d3f59e236cbf9e98a","modified":1507628323187},{"_id":"public/lib/pace/pace-theme-corner-indicator.min.css","hash":"b3c64c973f31884e3d8145989476707333406b9a","modified":1507628323187},{"_id":"public/lib/pace/pace-theme-minimal.min.css","hash":"9cd783cceb8a191f3c8b5d81f7a430ecc3e489d3","modified":1507628323187},{"_id":"public/lib/pace/pace-theme-mac-osx.min.css","hash":"9f2e7b51b084da407863826b25265b31150b3821","modified":1507628323187},{"_id":"public/lib/pace/pace.min.js","hash":"9944dfb7814b911090e96446cea4d36e2b487234","modified":1507628323187},{"_id":"public/lib/pace/pace-theme-loading-bar.min.css","hash":"7ee28875dfc1230d76c537f6605766e8d4011e9f","modified":1507628323187},{"_id":"public/lib/velocity/bower.json","hash":"0ef14e7ccdfba5db6eb3f8fc6aa3b47282c36409","modified":1507628323187},{"_id":"public/lib/velocity/velocity.ui.min.js","hash":"ed5e534cd680a25d8d14429af824f38a2c7d9908","modified":1507628323187},{"_id":"public/js/src/bootstrap.js","hash":"034bc8113e0966fe2096ba5b56061bbf10ef0512","modified":1507628323187},{"_id":"public/js/src/schemes/pisces.js","hash":"79da92119bc246fe05d1626ac98426a83ec90a94","modified":1507628323187},{"_id":"public/lib/fancybox/source/jquery.fancybox.css","hash":"5f163444617b6cf267342f06ac166a237bb62df9","modified":1507628323187},{"_id":"public/lib/fastclick/lib/fastclick.min.js","hash":"2cae0f5a6c5d6f3cb993015e6863f9483fc4de18","modified":1507628323187},{"_id":"public/lib/ua-parser-js/dist/ua-parser.min.js","hash":"38628e75e4412cc6f11074e03e1c6d257aae495b","modified":1507628323187},{"_id":"public/lib/ua-parser-js/dist/ua-parser.pack.js","hash":"214dad442a92d36af77ed0ca1d9092b16687f02f","modified":1507628323187},{"_id":"public/lib/fancybox/source/helpers/jquery.fancybox-buttons.css","hash":"1a9d8e5c22b371fcc69d4dbbb823d9c39f04c0c8","modified":1507628323187},{"_id":"public/lib/fancybox/source/helpers/jquery.fancybox-buttons.js","hash":"91e41741c2e93f732c82aaacec4cfc6e3f3ec876","modified":1507628323187},{"_id":"public/lib/fancybox/source/helpers/jquery.fancybox-media.js","hash":"3bdf69ed2469e4fb57f5a95f17300eef891ff90d","modified":1507628323187},{"_id":"public/lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","hash":"4ac329c16a5277592fc12a37cca3d72ca4ec292f","modified":1507628323187},{"_id":"public/lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","hash":"53e194f4a72e649c04fb586dd57762b8c022800b","modified":1507628323187},{"_id":"public/css/main.css","hash":"c03e467356abea84e096d3e2de08332958452b64","modified":1507628323188},{"_id":"public/lib/three/canvas_lines.min.js","hash":"dce4a3b65f8bf958f973690caa7ec4952f353b0c","modified":1507628323188},{"_id":"public/lib/three/canvas_sphere.min.js","hash":"d8ea241a53c135a650f7335d2b6982b899fd58a9","modified":1507628323188},{"_id":"public/lib/three/three-waves.min.js","hash":"d968cba6b3a50b3626a02d67b544f349d83b147c","modified":1507628323188},{"_id":"public/lib/velocity/velocity.min.js","hash":"2f1afadc12e4cf59ef3b405308d21baa97e739c6","modified":1507628323188},{"_id":"public/lib/velocity/velocity.ui.js","hash":"6a1d101eab3de87527bb54fcc8c7b36b79d8f0df","modified":1507628323188},{"_id":"public/lib/Han/dist/han.css","hash":"bd40da3fba8735df5850956814e312bd7b3193d7","modified":1507628323188},{"_id":"public/lib/jquery/index.js","hash":"41b4bfbaa96be6d1440db6e78004ade1c134e276","modified":1507628323188},{"_id":"public/lib/Han/dist/han.min.js","hash":"f559c68a25065a14f47da954a7617d87263e409d","modified":1507628323188},{"_id":"public/lib/Han/dist/han.min.css","hash":"a0c9e32549a8b8cf327ab9227b037f323cdb60ee","modified":1507628323188},{"_id":"public/lib/fancybox/source/jquery.fancybox.pack.js","hash":"53360764b429c212f424399384417ccc233bb3be","modified":1507628323188},{"_id":"public/lib/fancybox/source/jquery.fancybox.js","hash":"1cf3d47b5ccb7cb6e9019c64f2a88d03a64853e4","modified":1507628323188},{"_id":"public/lib/fastclick/lib/fastclick.js","hash":"06cef196733a710e77ad7e386ced6963f092dc55","modified":1507628323188},{"_id":"public/lib/font-awesome/css/font-awesome.css","hash":"0140952c64e3f2b74ef64e050f2fe86eab6624c8","modified":1507628323188},{"_id":"public/lib/font-awesome/css/font-awesome.min.css","hash":"512c7d79033e3028a9be61b540cf1a6870c896f8","modified":1507628323188},{"_id":"public/lib/Han/dist/han.js","hash":"e345397e0585c9fed1449e614ec13e0224acf2ab","modified":1507628323188},{"_id":"public/lib/velocity/velocity.js","hash":"9f08181baea0cc0e906703b7e5df9111b9ef3373","modified":1507628323189},{"_id":"public/lib/algolia-instant-search/instantsearch.min.js","hash":"9ccc6f8144f54e86df9a3fd33a18368d81cf3a4f","modified":1507628323189},{"_id":"public/lib/three/three.min.js","hash":"73f4cdc17e51a72b9bf5b9291f65386d615c483b","modified":1507628323189},{"_id":"public/lib/font-awesome/fonts/FontAwesome.otf","hash":"048707bc52ac4b6563aaa383bfe8660a0ddc908c","modified":1507628323189},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.ttf","hash":"13b1eab65a983c7a73bc7997c479d66943f7c6cb","modified":1507628323189},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.eot","hash":"d980c2ce873dc43af460d4d572d441304499f400","modified":1507628323189},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.svg","hash":"98a8aa5cf7d62c2eff5f07ede8d844b874ef06ed","modified":1507628323226}],"Category":[{"name":"learning notes","_id":"cj8lez5350004w2kzpxd9gm3h"},{"name":"杂","_id":"cj8lez53j000fw2kzm63nnwuo"},{"name":"machine learning","_id":"cj8lez53m000kw2kzuzvi66kk"},{"name":"reading notes","_id":"cj8lez53n000ow2kziuub29yp"}],"Data":[],"Page":[{"title":"分类","date":"2017-07-25T23:25:10.000Z","type":"categories","comments":0,"_content":"","source":"categories/index.md","raw":"---\ntitle: 分类\ndate: 2017-07-26 07:25:10\ntype: categories\ncomments: false\n---\n","updated":"2017-08-01T01:51:27.379Z","path":"categories/index.html","layout":"page","_id":"cj8lez52y0001w2kzx13z5ki5","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"标签","date":"2017-07-25T23:25:21.000Z","type":"tags","comments":0,"_content":"","source":"tags/index.md","raw":"---\ntitle: 标签\ndate: 2017-07-26 07:25:21\ntype: \"tags\"\ncomments: false\n---\n","updated":"2017-08-01T01:51:27.379Z","path":"tags/index.html","layout":"page","_id":"cj8lez5330003w2kzio2va3sj","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"关于","date":"2017-07-25T23:25:29.000Z","type":"about","comments":0,"_content":"我是**阿瑟**，英文是**Asir**\n\n平时喜欢自称superAsir或者JoeAsir，至于为什么叫**ASir**，因为他会出现在好友通讯录的第一个位置，yeah！\n\n本硕7年一直在[**北京航空航天大学**](http://www.buaa.edu.cn/)蹉跎度过\n\n毕业后有幸加入[**JD.COM**](https://www.jd.com/)，不得不说是一次狗屎运\n\n方向是**Machine learning算法工程师**，喜欢研究算法原理却发现工作中基本用不上，只能在这里独自默默把玩一番\n\n还有，我喜欢**Arsenal**，喜欢**Rock&Roll**\n\nBTW，我也喜欢喝酒，**欢迎约酒!!**\n","source":"about/index.md","raw":"---\ntitle: 关于\ndate: 2017-07-26 07:25:29\ntype: \"about\"\ncomments: false\n---\n我是**阿瑟**，英文是**Asir**\n\n平时喜欢自称superAsir或者JoeAsir，至于为什么叫**ASir**，因为他会出现在好友通讯录的第一个位置，yeah！\n\n本硕7年一直在[**北京航空航天大学**](http://www.buaa.edu.cn/)蹉跎度过\n\n毕业后有幸加入[**JD.COM**](https://www.jd.com/)，不得不说是一次狗屎运\n\n方向是**Machine learning算法工程师**，喜欢研究算法原理却发现工作中基本用不上，只能在这里独自默默把玩一番\n\n还有，我喜欢**Arsenal**，喜欢**Rock&Roll**\n\nBTW，我也喜欢喝酒，**欢迎约酒!!**\n","updated":"2017-08-04T03:37:32.991Z","path":"about/index.html","layout":"page","_id":"cj8lez53b0007w2kz3w9rfkdm","content":"<p>我是<strong>阿瑟</strong>，英文是<strong>Asir</strong></p>\n<p>平时喜欢自称superAsir或者JoeAsir，至于为什么叫<strong>ASir</strong>，因为他会出现在好友通讯录的第一个位置，yeah！</p>\n<p>本硕7年一直在<a href=\"http://www.buaa.edu.cn/\" target=\"_blank\" rel=\"external\"><strong>北京航空航天大学</strong></a>蹉跎度过</p>\n<p>毕业后有幸加入<a href=\"https://www.jd.com/\" target=\"_blank\" rel=\"external\"><strong>JD.COM</strong></a>，不得不说是一次狗屎运</p>\n<p>方向是<strong>Machine learning算法工程师</strong>，喜欢研究算法原理却发现工作中基本用不上，只能在这里独自默默把玩一番</p>\n<p>还有，我喜欢<strong>Arsenal</strong>，喜欢<strong>Rock&amp;Roll</strong></p>\n<p>BTW，我也喜欢喝酒，<strong>欢迎约酒!!</strong></p>\n","site":{"data":{}},"excerpt":"","more":"<p>我是<strong>阿瑟</strong>，英文是<strong>Asir</strong></p>\n<p>平时喜欢自称superAsir或者JoeAsir，至于为什么叫<strong>ASir</strong>，因为他会出现在好友通讯录的第一个位置，yeah！</p>\n<p>本硕7年一直在<a href=\"http://www.buaa.edu.cn/\" target=\"_blank\" rel=\"external\"><strong>北京航空航天大学</strong></a>蹉跎度过</p>\n<p>毕业后有幸加入<a href=\"https://www.jd.com/\" target=\"_blank\" rel=\"external\"><strong>JD.COM</strong></a>，不得不说是一次狗屎运</p>\n<p>方向是<strong>Machine learning算法工程师</strong>，喜欢研究算法原理却发现工作中基本用不上，只能在这里独自默默把玩一番</p>\n<p>还有，我喜欢<strong>Arsenal</strong>，喜欢<strong>Rock&amp;Roll</strong></p>\n<p>BTW，我也喜欢喝酒，<strong>欢迎约酒!!</strong></p>\n"}],"Post":[{"title":"Learning notes-Deep Learning, course2, week1","date":"2017-09-24T06:06:08.000Z","_content":"大家好，最近在学习Andrew Ng的Deep learning课程，于是决定写一些learning notes来recap和mark一下学到的知识，避免遗忘。由于该课程的course1比较基础，我个人认为没有mark的必要，所以从course2开始，按照week来mark.\n<!--more-->\n## Data set\n在machine learning中，data set可以说是最重要的部分，区别于传统machine learning，deep learning中的data set分布更侧重于training，Ng建议我们讲data set分为三部分：\n* training set——训练数据集\n* dev/validation set——模型选择和参数调整，泛化能力测试\n* testing set——模型效果测试\n一定有很多人对于dev和testing set有一些疑问，最开始我也是懵逼的，来看看下面这段话\n> Dev/Validation Set: this data set is used to minimize overfitting. You're not adjusting the weights of the network with this data set, you're just verifying that any increase in accuracy over the training data set actually yields an increase in accuracy over a data set that has not been shown to the network before, or at least the network hasn't trained on it (i.e. validation data set). If the accuracy over the training data set increases, but the accuracy over then validation data set stays the same or decreases, then you're overfitting your neural network and you should stop training.\nTesting Set: this data set is used only for testing the final solution in order to confirm the actual predictive power of the network.\n\n这三者的比例则是/99.5%/2.5%/2.5%/，这样的原因是因为deep learning中，数据量足够大而且deep learning的学习能力很强，大家一定注意这一点。当然，如果实在没有test set，但是有dev set也是可以接受的。\n## Bias and variance\n### 什么是bias和variance\nbias & variance是machine learning 领域一个经典的辩证问题，在Ng经典的CS229中就重点的讲述过，具体的定义我不太想给出了，后续有时间可以专门写一篇，后面会给出一些资料链接。我们简单的看一幅图\n![](http://otmy7guvn.bkt.clouddn.com/blog/6/6-5.png) \n左图就是一个典型的high bias situation，模型没有办法很好的拟合数据，这也就是我们常说的under fitting，右图则是典型的high variance situation，模型过分的拟合了training set，这就是我们最需要防范的over fitting.当然，中间的则是比较理想的状况。\n### Solution\n在实际的工作中，我们应该怎么分析自己模型的bias和variance情况呢，Ng给了我们一个流程图，如下：\n![](http://otmy7guvn.bkt.clouddn.com/blog/6/6-2.png) \n首先检验是否存在high bias 情况，具体方法是在training set 和 dev set上计算error，对比training error和dev error，如果两者都很高，那么就是high bias，如果training error很小而dev error很高，那么一定是high variance，如果两者都很大，那么就是最差的情况了既high bias又high variance\n\n对于high bias，我们可以通过更复杂的神经网络、更长的训练时间，更强的网络结构来解决这个问题；\n对于high variance，我们可以通过更多的数据，regularization的方法来解决。\n## Regularization\n### L1&L2 regularization\n这部分内容我就不多说了，我之前专门详细深入的讲述过L1和L2 regularization，大家可以去看一看。\n\n唯一需要明确的一点是，在加入L1或者L2 regularization之后，在观测cost function convergence 的时候，**一定要带上regularization item**，否则结果是很难看到convergence的，这和regularization性质有很大的关系。 \n### Dropout\nDropout是neural network中一种经典的regularization方法，经典到什么程度呢，我当年毕设课题中都用到了这个方法，而且效果超赞\n\nDropout方法的实质是**按比例随机隐藏**掉neural network中layer里的某些units，也就是说，再一次epoch中，只有一部分的units对应的weights和bias会得到更新，而下一次epoch中，则是另一部分units对应的weights和bias得到更新，如下图\n![](http://otmy7guvn.bkt.clouddn.com/blog/6/6-3.png) \n那么为什么Dropout可以实现regularization效果呢，Ng告诉我们：\n> Intuition:Can't rely on any one feature, so have to spread out weights\n\n如何理解呢？加入dropout后，每个unit对应的weights和bias不能完全依赖上层units，因为他并不是每一次epoch都可以work on，因此在学习的过程中，见笑了over fitting的风险。实际上，dropout可以产生shrink weights的效果，和L2 regularization相似，因此也是一种regularization方法。\n\n但是，dropout和L2 regularization唯一的区别在于，他很难给出一个regularization item，所以你没有办法画出cost function convergence的轨迹。\n### Other methods\n除了经典的L1、L2 regularization和dropout方法，还有一些防止over fitting的方法，例如图像处理中，我们可以用data augmentation，旋转，翻转，加噪声等方法。\n\n还有一个early stopping方法，我们都知道，随着training 的epoch增多，模型对training set拟合会越来越好，随之带来的问题就是可能over fitting，我们可以通过early stopping，让模型在没有产生over fitting的时候停下来，效果可能会更好。\n## Exploding/vannishing gradient\n### 什么是exploding/vanishing gradient\n对于deep learning，曾经最为棘手的问题就是exploding/vanishing gradient，甚至是限制deep learning发展的瓶颈，我们来一起看看。\n 假设我们有一个**比较深**的neural network，假设一共有\\\\(l\\\\)层，对应的weights是\\\\(W^{[1]}\\\\)到\\\\(W^{[l]}\\\\)，bias是\\\\(b^[1]\\\\)到\\\\(b^{[l]}\\\\)，我们为了计算方便，假设bias均为0，active function为\\\\(g(z)=z\\\\)，那么，\\\\(y\\\\)就等于\n$$\\hat{y}=W^{[l]}W^{[l-1]}W^{l-2} \\cdots W^{[3]}W^{[2]}W^{[1]}X$$\n大家感兴趣的话可以验证一下，很简单的。\n\n那么现在问题来了，当\\\\(l\\\\)很大的情况下，如果\\\\(W\\\\)元素都大于1，那么最后的结果就会非常非常大，甚至到无限大，这种情况叫exploding gradient；相应的，如果\\\\(W\\\\)元素都小于1，那么最后的结果就会特别小，甚至为零，这就是vanishing gradient.\n### Solution\n对于上面的问题，我们一般在weights初始化的时候做一些工作来解决可能出现的exploding or vanishing gradient。我们可以直观的理解一下，对于active function\\\\(g(z)\\\\)，假设bias为零，\n$$z=w_{1}x_1 +w_{2}x_2+ \\cdots +w_{n}x_n$$\n我们要我们可以看到，\\\\(n\\\\)的增大，\\\\(w\\\\)会变小，我们让\\\\(w\\\\)始终保持以0为mean，1为varance的Gaussian distribution下，就可以很好的控制\\\\(w\\\\)的大小，那么我们可以看到，\\\\(var(w)= \\frac{1}{n^{l-1}}\\\\)\n\n在Ng的建议中，如果active function是sigmoid，我们一般取\\\\(var(w)= \\frac{1}{n^{l-1}}\\\\)，如果是reLu，我们取\\\\(var(w)= \\frac{2}{n^{l-1}}\\\\)，对于tanh，\\\\(var(w)= \\frac{1}{n^{l-1}}\\\\)或者\\\\(var(w)= \\frac{2}{n^{l-1}+n^{l}}\\\\)，这样可以很好的避免exploding or vanishing gradient.\n## Gradient checking\n### Gradient approximation\n在调试neural network的时候，我们会经常做gradient check的工作，以确定整个network正常的运行，Ng在这里建议我们使用双边逼近的方法去做gradient check，这里我不做太多描述，主要上一张图：\n![](http://otmy7guvn.bkt.clouddn.com/blog/6/6-6.png) \n通常来说，双边逼近的方法获得结果更加准确。\n### Gradient checking notes\n> 1. Don't use in training-only to debug(too slow)\n2. If algorithm fails grad check, look at components to try to identify bug\n3. Remeber regularization\n4. Dosen't wrok with dropout\n5. Run at random initialzation; perhaps again after some training.\n\n## Reference\n* [Deep learning-Coursera Andrew Ng](https://www.coursera.org/specializations/deep-learning)\n* [Deep learning-网易云课堂 Andrew Ng](https://mooc.study.163.com/course/deeplearning_ai-2001281003#/info)\n* [Bias and variance](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff)\n","source":"_posts/course-deep-learning-course2-week1.md","raw":"---\ntitle: Learning notes-Deep Learning, course2, week1\ndate: 2017-09-24 14:06:08\ntags: \n\t- regularization\n\t- gradient descent\ncategories: learning notes\n---\n大家好，最近在学习Andrew Ng的Deep learning课程，于是决定写一些learning notes来recap和mark一下学到的知识，避免遗忘。由于该课程的course1比较基础，我个人认为没有mark的必要，所以从course2开始，按照week来mark.\n<!--more-->\n## Data set\n在machine learning中，data set可以说是最重要的部分，区别于传统machine learning，deep learning中的data set分布更侧重于training，Ng建议我们讲data set分为三部分：\n* training set——训练数据集\n* dev/validation set——模型选择和参数调整，泛化能力测试\n* testing set——模型效果测试\n一定有很多人对于dev和testing set有一些疑问，最开始我也是懵逼的，来看看下面这段话\n> Dev/Validation Set: this data set is used to minimize overfitting. You're not adjusting the weights of the network with this data set, you're just verifying that any increase in accuracy over the training data set actually yields an increase in accuracy over a data set that has not been shown to the network before, or at least the network hasn't trained on it (i.e. validation data set). If the accuracy over the training data set increases, but the accuracy over then validation data set stays the same or decreases, then you're overfitting your neural network and you should stop training.\nTesting Set: this data set is used only for testing the final solution in order to confirm the actual predictive power of the network.\n\n这三者的比例则是/99.5%/2.5%/2.5%/，这样的原因是因为deep learning中，数据量足够大而且deep learning的学习能力很强，大家一定注意这一点。当然，如果实在没有test set，但是有dev set也是可以接受的。\n## Bias and variance\n### 什么是bias和variance\nbias & variance是machine learning 领域一个经典的辩证问题，在Ng经典的CS229中就重点的讲述过，具体的定义我不太想给出了，后续有时间可以专门写一篇，后面会给出一些资料链接。我们简单的看一幅图\n![](http://otmy7guvn.bkt.clouddn.com/blog/6/6-5.png) \n左图就是一个典型的high bias situation，模型没有办法很好的拟合数据，这也就是我们常说的under fitting，右图则是典型的high variance situation，模型过分的拟合了training set，这就是我们最需要防范的over fitting.当然，中间的则是比较理想的状况。\n### Solution\n在实际的工作中，我们应该怎么分析自己模型的bias和variance情况呢，Ng给了我们一个流程图，如下：\n![](http://otmy7guvn.bkt.clouddn.com/blog/6/6-2.png) \n首先检验是否存在high bias 情况，具体方法是在training set 和 dev set上计算error，对比training error和dev error，如果两者都很高，那么就是high bias，如果training error很小而dev error很高，那么一定是high variance，如果两者都很大，那么就是最差的情况了既high bias又high variance\n\n对于high bias，我们可以通过更复杂的神经网络、更长的训练时间，更强的网络结构来解决这个问题；\n对于high variance，我们可以通过更多的数据，regularization的方法来解决。\n## Regularization\n### L1&L2 regularization\n这部分内容我就不多说了，我之前专门详细深入的讲述过L1和L2 regularization，大家可以去看一看。\n\n唯一需要明确的一点是，在加入L1或者L2 regularization之后，在观测cost function convergence 的时候，**一定要带上regularization item**，否则结果是很难看到convergence的，这和regularization性质有很大的关系。 \n### Dropout\nDropout是neural network中一种经典的regularization方法，经典到什么程度呢，我当年毕设课题中都用到了这个方法，而且效果超赞\n\nDropout方法的实质是**按比例随机隐藏**掉neural network中layer里的某些units，也就是说，再一次epoch中，只有一部分的units对应的weights和bias会得到更新，而下一次epoch中，则是另一部分units对应的weights和bias得到更新，如下图\n![](http://otmy7guvn.bkt.clouddn.com/blog/6/6-3.png) \n那么为什么Dropout可以实现regularization效果呢，Ng告诉我们：\n> Intuition:Can't rely on any one feature, so have to spread out weights\n\n如何理解呢？加入dropout后，每个unit对应的weights和bias不能完全依赖上层units，因为他并不是每一次epoch都可以work on，因此在学习的过程中，见笑了over fitting的风险。实际上，dropout可以产生shrink weights的效果，和L2 regularization相似，因此也是一种regularization方法。\n\n但是，dropout和L2 regularization唯一的区别在于，他很难给出一个regularization item，所以你没有办法画出cost function convergence的轨迹。\n### Other methods\n除了经典的L1、L2 regularization和dropout方法，还有一些防止over fitting的方法，例如图像处理中，我们可以用data augmentation，旋转，翻转，加噪声等方法。\n\n还有一个early stopping方法，我们都知道，随着training 的epoch增多，模型对training set拟合会越来越好，随之带来的问题就是可能over fitting，我们可以通过early stopping，让模型在没有产生over fitting的时候停下来，效果可能会更好。\n## Exploding/vannishing gradient\n### 什么是exploding/vanishing gradient\n对于deep learning，曾经最为棘手的问题就是exploding/vanishing gradient，甚至是限制deep learning发展的瓶颈，我们来一起看看。\n 假设我们有一个**比较深**的neural network，假设一共有\\\\(l\\\\)层，对应的weights是\\\\(W^{[1]}\\\\)到\\\\(W^{[l]}\\\\)，bias是\\\\(b^[1]\\\\)到\\\\(b^{[l]}\\\\)，我们为了计算方便，假设bias均为0，active function为\\\\(g(z)=z\\\\)，那么，\\\\(y\\\\)就等于\n$$\\hat{y}=W^{[l]}W^{[l-1]}W^{l-2} \\cdots W^{[3]}W^{[2]}W^{[1]}X$$\n大家感兴趣的话可以验证一下，很简单的。\n\n那么现在问题来了，当\\\\(l\\\\)很大的情况下，如果\\\\(W\\\\)元素都大于1，那么最后的结果就会非常非常大，甚至到无限大，这种情况叫exploding gradient；相应的，如果\\\\(W\\\\)元素都小于1，那么最后的结果就会特别小，甚至为零，这就是vanishing gradient.\n### Solution\n对于上面的问题，我们一般在weights初始化的时候做一些工作来解决可能出现的exploding or vanishing gradient。我们可以直观的理解一下，对于active function\\\\(g(z)\\\\)，假设bias为零，\n$$z=w_{1}x_1 +w_{2}x_2+ \\cdots +w_{n}x_n$$\n我们要我们可以看到，\\\\(n\\\\)的增大，\\\\(w\\\\)会变小，我们让\\\\(w\\\\)始终保持以0为mean，1为varance的Gaussian distribution下，就可以很好的控制\\\\(w\\\\)的大小，那么我们可以看到，\\\\(var(w)= \\frac{1}{n^{l-1}}\\\\)\n\n在Ng的建议中，如果active function是sigmoid，我们一般取\\\\(var(w)= \\frac{1}{n^{l-1}}\\\\)，如果是reLu，我们取\\\\(var(w)= \\frac{2}{n^{l-1}}\\\\)，对于tanh，\\\\(var(w)= \\frac{1}{n^{l-1}}\\\\)或者\\\\(var(w)= \\frac{2}{n^{l-1}+n^{l}}\\\\)，这样可以很好的避免exploding or vanishing gradient.\n## Gradient checking\n### Gradient approximation\n在调试neural network的时候，我们会经常做gradient check的工作，以确定整个network正常的运行，Ng在这里建议我们使用双边逼近的方法去做gradient check，这里我不做太多描述，主要上一张图：\n![](http://otmy7guvn.bkt.clouddn.com/blog/6/6-6.png) \n通常来说，双边逼近的方法获得结果更加准确。\n### Gradient checking notes\n> 1. Don't use in training-only to debug(too slow)\n2. If algorithm fails grad check, look at components to try to identify bug\n3. Remeber regularization\n4. Dosen't wrok with dropout\n5. Run at random initialzation; perhaps again after some training.\n\n## Reference\n* [Deep learning-Coursera Andrew Ng](https://www.coursera.org/specializations/deep-learning)\n* [Deep learning-网易云课堂 Andrew Ng](https://mooc.study.163.com/course/deeplearning_ai-2001281003#/info)\n* [Bias and variance](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff)\n","slug":"course-deep-learning-course2-week1","published":1,"updated":"2017-09-28T03:00:52.139Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cj8lez52t0000w2kzcmr3yrfe","content":"<p>大家好，最近在学习Andrew Ng的Deep learning课程，于是决定写一些learning notes来recap和mark一下学到的知识，避免遗忘。由于该课程的course1比较基础，我个人认为没有mark的必要，所以从course2开始，按照week来mark.<br><a id=\"more\"></a></p>\n<h2 id=\"Data-set\"><a href=\"#Data-set\" class=\"headerlink\" title=\"Data set\"></a>Data set</h2><p>在machine learning中，data set可以说是最重要的部分，区别于传统machine learning，deep learning中的data set分布更侧重于training，Ng建议我们讲data set分为三部分：</p>\n<ul>\n<li>training set——训练数据集</li>\n<li>dev/validation set——模型选择和参数调整，泛化能力测试</li>\n<li>testing set——模型效果测试<br>一定有很多人对于dev和testing set有一些疑问，最开始我也是懵逼的，来看看下面这段话<blockquote>\n<p>Dev/Validation Set: this data set is used to minimize overfitting. You’re not adjusting the weights of the network with this data set, you’re just verifying that any increase in accuracy over the training data set actually yields an increase in accuracy over a data set that has not been shown to the network before, or at least the network hasn’t trained on it (i.e. validation data set). If the accuracy over the training data set increases, but the accuracy over then validation data set stays the same or decreases, then you’re overfitting your neural network and you should stop training.<br>Testing Set: this data set is used only for testing the final solution in order to confirm the actual predictive power of the network.</p>\n</blockquote>\n</li>\n</ul>\n<p>这三者的比例则是/99.5%/2.5%/2.5%/，这样的原因是因为deep learning中，数据量足够大而且deep learning的学习能力很强，大家一定注意这一点。当然，如果实在没有test set，但是有dev set也是可以接受的。</p>\n<h2 id=\"Bias-and-variance\"><a href=\"#Bias-and-variance\" class=\"headerlink\" title=\"Bias and variance\"></a>Bias and variance</h2><h3 id=\"什么是bias和variance\"><a href=\"#什么是bias和variance\" class=\"headerlink\" title=\"什么是bias和variance\"></a>什么是bias和variance</h3><p>bias &amp; variance是machine learning 领域一个经典的辩证问题，在Ng经典的CS229中就重点的讲述过，具体的定义我不太想给出了，后续有时间可以专门写一篇，后面会给出一些资料链接。我们简单的看一幅图<br><img src=\"http://otmy7guvn.bkt.clouddn.com/blog/6/6-5.png\" alt=\"\"><br>左图就是一个典型的high bias situation，模型没有办法很好的拟合数据，这也就是我们常说的under fitting，右图则是典型的high variance situation，模型过分的拟合了training set，这就是我们最需要防范的over fitting.当然，中间的则是比较理想的状况。</p>\n<h3 id=\"Solution\"><a href=\"#Solution\" class=\"headerlink\" title=\"Solution\"></a>Solution</h3><p>在实际的工作中，我们应该怎么分析自己模型的bias和variance情况呢，Ng给了我们一个流程图，如下：<br><img src=\"http://otmy7guvn.bkt.clouddn.com/blog/6/6-2.png\" alt=\"\"><br>首先检验是否存在high bias 情况，具体方法是在training set 和 dev set上计算error，对比training error和dev error，如果两者都很高，那么就是high bias，如果training error很小而dev error很高，那么一定是high variance，如果两者都很大，那么就是最差的情况了既high bias又high variance</p>\n<p>对于high bias，我们可以通过更复杂的神经网络、更长的训练时间，更强的网络结构来解决这个问题；<br>对于high variance，我们可以通过更多的数据，regularization的方法来解决。</p>\n<h2 id=\"Regularization\"><a href=\"#Regularization\" class=\"headerlink\" title=\"Regularization\"></a>Regularization</h2><h3 id=\"L1-amp-L2-regularization\"><a href=\"#L1-amp-L2-regularization\" class=\"headerlink\" title=\"L1&amp;L2 regularization\"></a>L1&amp;L2 regularization</h3><p>这部分内容我就不多说了，我之前专门详细深入的讲述过L1和L2 regularization，大家可以去看一看。</p>\n<p>唯一需要明确的一点是，在加入L1或者L2 regularization之后，在观测cost function convergence 的时候，<strong>一定要带上regularization item</strong>，否则结果是很难看到convergence的，这和regularization性质有很大的关系。 </p>\n<h3 id=\"Dropout\"><a href=\"#Dropout\" class=\"headerlink\" title=\"Dropout\"></a>Dropout</h3><p>Dropout是neural network中一种经典的regularization方法，经典到什么程度呢，我当年毕设课题中都用到了这个方法，而且效果超赞</p>\n<p>Dropout方法的实质是<strong>按比例随机隐藏</strong>掉neural network中layer里的某些units，也就是说，再一次epoch中，只有一部分的units对应的weights和bias会得到更新，而下一次epoch中，则是另一部分units对应的weights和bias得到更新，如下图<br><img src=\"http://otmy7guvn.bkt.clouddn.com/blog/6/6-3.png\" alt=\"\"><br>那么为什么Dropout可以实现regularization效果呢，Ng告诉我们：</p>\n<blockquote>\n<p>Intuition:Can’t rely on any one feature, so have to spread out weights</p>\n</blockquote>\n<p>如何理解呢？加入dropout后，每个unit对应的weights和bias不能完全依赖上层units，因为他并不是每一次epoch都可以work on，因此在学习的过程中，见笑了over fitting的风险。实际上，dropout可以产生shrink weights的效果，和L2 regularization相似，因此也是一种regularization方法。</p>\n<p>但是，dropout和L2 regularization唯一的区别在于，他很难给出一个regularization item，所以你没有办法画出cost function convergence的轨迹。</p>\n<h3 id=\"Other-methods\"><a href=\"#Other-methods\" class=\"headerlink\" title=\"Other methods\"></a>Other methods</h3><p>除了经典的L1、L2 regularization和dropout方法，还有一些防止over fitting的方法，例如图像处理中，我们可以用data augmentation，旋转，翻转，加噪声等方法。</p>\n<p>还有一个early stopping方法，我们都知道，随着training 的epoch增多，模型对training set拟合会越来越好，随之带来的问题就是可能over fitting，我们可以通过early stopping，让模型在没有产生over fitting的时候停下来，效果可能会更好。</p>\n<h2 id=\"Exploding-vannishing-gradient\"><a href=\"#Exploding-vannishing-gradient\" class=\"headerlink\" title=\"Exploding/vannishing gradient\"></a>Exploding/vannishing gradient</h2><h3 id=\"什么是exploding-vanishing-gradient\"><a href=\"#什么是exploding-vanishing-gradient\" class=\"headerlink\" title=\"什么是exploding/vanishing gradient\"></a>什么是exploding/vanishing gradient</h3><p>对于deep learning，曾经最为棘手的问题就是exploding/vanishing gradient，甚至是限制deep learning发展的瓶颈，我们来一起看看。<br> 假设我们有一个<strong>比较深</strong>的neural network，假设一共有\\(l\\)层，对应的weights是\\(W^{[1]}\\)到\\(W^{[l]}\\)，bias是\\(b^[1]\\)到\\(b^{[l]}\\)，我们为了计算方便，假设bias均为0，active function为\\(g(z)=z\\)，那么，\\(y\\)就等于</p>\n<script type=\"math/tex; mode=display\">\\hat{y}=W^{[l]}W^{[l-1]}W^{l-2} \\cdots W^{[3]}W^{[2]}W^{[1]}X</script><p>大家感兴趣的话可以验证一下，很简单的。</p>\n<p>那么现在问题来了，当\\(l\\)很大的情况下，如果\\(W\\)元素都大于1，那么最后的结果就会非常非常大，甚至到无限大，这种情况叫exploding gradient；相应的，如果\\(W\\)元素都小于1，那么最后的结果就会特别小，甚至为零，这就是vanishing gradient.</p>\n<h3 id=\"Solution-1\"><a href=\"#Solution-1\" class=\"headerlink\" title=\"Solution\"></a>Solution</h3><p>对于上面的问题，我们一般在weights初始化的时候做一些工作来解决可能出现的exploding or vanishing gradient。我们可以直观的理解一下，对于active function\\(g(z)\\)，假设bias为零，</p>\n<script type=\"math/tex; mode=display\">z=w_{1}x_1 +w_{2}x_2+ \\cdots +w_{n}x_n</script><p>我们要我们可以看到，\\(n\\)的增大，\\(w\\)会变小，我们让\\(w\\)始终保持以0为mean，1为varance的Gaussian distribution下，就可以很好的控制\\(w\\)的大小，那么我们可以看到，\\(var(w)= \\frac{1}{n^{l-1}}\\)</p>\n<p>在Ng的建议中，如果active function是sigmoid，我们一般取\\(var(w)= \\frac{1}{n^{l-1}}\\)，如果是reLu，我们取\\(var(w)= \\frac{2}{n^{l-1}}\\)，对于tanh，\\(var(w)= \\frac{1}{n^{l-1}}\\)或者\\(var(w)= \\frac{2}{n^{l-1}+n^{l}}\\)，这样可以很好的避免exploding or vanishing gradient.</p>\n<h2 id=\"Gradient-checking\"><a href=\"#Gradient-checking\" class=\"headerlink\" title=\"Gradient checking\"></a>Gradient checking</h2><h3 id=\"Gradient-approximation\"><a href=\"#Gradient-approximation\" class=\"headerlink\" title=\"Gradient approximation\"></a>Gradient approximation</h3><p>在调试neural network的时候，我们会经常做gradient check的工作，以确定整个network正常的运行，Ng在这里建议我们使用双边逼近的方法去做gradient check，这里我不做太多描述，主要上一张图：<br><img src=\"http://otmy7guvn.bkt.clouddn.com/blog/6/6-6.png\" alt=\"\"><br>通常来说，双边逼近的方法获得结果更加准确。</p>\n<h3 id=\"Gradient-checking-notes\"><a href=\"#Gradient-checking-notes\" class=\"headerlink\" title=\"Gradient checking notes\"></a>Gradient checking notes</h3><blockquote>\n<ol>\n<li>Don’t use in training-only to debug(too slow)</li>\n<li>If algorithm fails grad check, look at components to try to identify bug</li>\n<li>Remeber regularization</li>\n<li>Dosen’t wrok with dropout</li>\n<li>Run at random initialzation; perhaps again after some training.</li>\n</ol>\n</blockquote>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ul>\n<li><a href=\"https://www.coursera.org/specializations/deep-learning\" target=\"_blank\" rel=\"external\">Deep learning-Coursera Andrew Ng</a></li>\n<li><a href=\"https://mooc.study.163.com/course/deeplearning_ai-2001281003#/info\" target=\"_blank\" rel=\"external\">Deep learning-网易云课堂 Andrew Ng</a></li>\n<li><a href=\"https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff\" target=\"_blank\" rel=\"external\">Bias and variance</a></li>\n</ul>\n","site":{"data":{}},"excerpt":"<p>大家好，最近在学习Andrew Ng的Deep learning课程，于是决定写一些learning notes来recap和mark一下学到的知识，避免遗忘。由于该课程的course1比较基础，我个人认为没有mark的必要，所以从course2开始，按照week来mark.<br>","more":"</p>\n<h2 id=\"Data-set\"><a href=\"#Data-set\" class=\"headerlink\" title=\"Data set\"></a>Data set</h2><p>在machine learning中，data set可以说是最重要的部分，区别于传统machine learning，deep learning中的data set分布更侧重于training，Ng建议我们讲data set分为三部分：</p>\n<ul>\n<li>training set——训练数据集</li>\n<li>dev/validation set——模型选择和参数调整，泛化能力测试</li>\n<li>testing set——模型效果测试<br>一定有很多人对于dev和testing set有一些疑问，最开始我也是懵逼的，来看看下面这段话<blockquote>\n<p>Dev/Validation Set: this data set is used to minimize overfitting. You’re not adjusting the weights of the network with this data set, you’re just verifying that any increase in accuracy over the training data set actually yields an increase in accuracy over a data set that has not been shown to the network before, or at least the network hasn’t trained on it (i.e. validation data set). If the accuracy over the training data set increases, but the accuracy over then validation data set stays the same or decreases, then you’re overfitting your neural network and you should stop training.<br>Testing Set: this data set is used only for testing the final solution in order to confirm the actual predictive power of the network.</p>\n</blockquote>\n</li>\n</ul>\n<p>这三者的比例则是/99.5%/2.5%/2.5%/，这样的原因是因为deep learning中，数据量足够大而且deep learning的学习能力很强，大家一定注意这一点。当然，如果实在没有test set，但是有dev set也是可以接受的。</p>\n<h2 id=\"Bias-and-variance\"><a href=\"#Bias-and-variance\" class=\"headerlink\" title=\"Bias and variance\"></a>Bias and variance</h2><h3 id=\"什么是bias和variance\"><a href=\"#什么是bias和variance\" class=\"headerlink\" title=\"什么是bias和variance\"></a>什么是bias和variance</h3><p>bias &amp; variance是machine learning 领域一个经典的辩证问题，在Ng经典的CS229中就重点的讲述过，具体的定义我不太想给出了，后续有时间可以专门写一篇，后面会给出一些资料链接。我们简单的看一幅图<br><img src=\"http://otmy7guvn.bkt.clouddn.com/blog/6/6-5.png\" alt=\"\"><br>左图就是一个典型的high bias situation，模型没有办法很好的拟合数据，这也就是我们常说的under fitting，右图则是典型的high variance situation，模型过分的拟合了training set，这就是我们最需要防范的over fitting.当然，中间的则是比较理想的状况。</p>\n<h3 id=\"Solution\"><a href=\"#Solution\" class=\"headerlink\" title=\"Solution\"></a>Solution</h3><p>在实际的工作中，我们应该怎么分析自己模型的bias和variance情况呢，Ng给了我们一个流程图，如下：<br><img src=\"http://otmy7guvn.bkt.clouddn.com/blog/6/6-2.png\" alt=\"\"><br>首先检验是否存在high bias 情况，具体方法是在training set 和 dev set上计算error，对比training error和dev error，如果两者都很高，那么就是high bias，如果training error很小而dev error很高，那么一定是high variance，如果两者都很大，那么就是最差的情况了既high bias又high variance</p>\n<p>对于high bias，我们可以通过更复杂的神经网络、更长的训练时间，更强的网络结构来解决这个问题；<br>对于high variance，我们可以通过更多的数据，regularization的方法来解决。</p>\n<h2 id=\"Regularization\"><a href=\"#Regularization\" class=\"headerlink\" title=\"Regularization\"></a>Regularization</h2><h3 id=\"L1-amp-L2-regularization\"><a href=\"#L1-amp-L2-regularization\" class=\"headerlink\" title=\"L1&amp;L2 regularization\"></a>L1&amp;L2 regularization</h3><p>这部分内容我就不多说了，我之前专门详细深入的讲述过L1和L2 regularization，大家可以去看一看。</p>\n<p>唯一需要明确的一点是，在加入L1或者L2 regularization之后，在观测cost function convergence 的时候，<strong>一定要带上regularization item</strong>，否则结果是很难看到convergence的，这和regularization性质有很大的关系。 </p>\n<h3 id=\"Dropout\"><a href=\"#Dropout\" class=\"headerlink\" title=\"Dropout\"></a>Dropout</h3><p>Dropout是neural network中一种经典的regularization方法，经典到什么程度呢，我当年毕设课题中都用到了这个方法，而且效果超赞</p>\n<p>Dropout方法的实质是<strong>按比例随机隐藏</strong>掉neural network中layer里的某些units，也就是说，再一次epoch中，只有一部分的units对应的weights和bias会得到更新，而下一次epoch中，则是另一部分units对应的weights和bias得到更新，如下图<br><img src=\"http://otmy7guvn.bkt.clouddn.com/blog/6/6-3.png\" alt=\"\"><br>那么为什么Dropout可以实现regularization效果呢，Ng告诉我们：</p>\n<blockquote>\n<p>Intuition:Can’t rely on any one feature, so have to spread out weights</p>\n</blockquote>\n<p>如何理解呢？加入dropout后，每个unit对应的weights和bias不能完全依赖上层units，因为他并不是每一次epoch都可以work on，因此在学习的过程中，见笑了over fitting的风险。实际上，dropout可以产生shrink weights的效果，和L2 regularization相似，因此也是一种regularization方法。</p>\n<p>但是，dropout和L2 regularization唯一的区别在于，他很难给出一个regularization item，所以你没有办法画出cost function convergence的轨迹。</p>\n<h3 id=\"Other-methods\"><a href=\"#Other-methods\" class=\"headerlink\" title=\"Other methods\"></a>Other methods</h3><p>除了经典的L1、L2 regularization和dropout方法，还有一些防止over fitting的方法，例如图像处理中，我们可以用data augmentation，旋转，翻转，加噪声等方法。</p>\n<p>还有一个early stopping方法，我们都知道，随着training 的epoch增多，模型对training set拟合会越来越好，随之带来的问题就是可能over fitting，我们可以通过early stopping，让模型在没有产生over fitting的时候停下来，效果可能会更好。</p>\n<h2 id=\"Exploding-vannishing-gradient\"><a href=\"#Exploding-vannishing-gradient\" class=\"headerlink\" title=\"Exploding/vannishing gradient\"></a>Exploding/vannishing gradient</h2><h3 id=\"什么是exploding-vanishing-gradient\"><a href=\"#什么是exploding-vanishing-gradient\" class=\"headerlink\" title=\"什么是exploding/vanishing gradient\"></a>什么是exploding/vanishing gradient</h3><p>对于deep learning，曾经最为棘手的问题就是exploding/vanishing gradient，甚至是限制deep learning发展的瓶颈，我们来一起看看。<br> 假设我们有一个<strong>比较深</strong>的neural network，假设一共有\\(l\\)层，对应的weights是\\(W^{[1]}\\)到\\(W^{[l]}\\)，bias是\\(b^[1]\\)到\\(b^{[l]}\\)，我们为了计算方便，假设bias均为0，active function为\\(g(z)=z\\)，那么，\\(y\\)就等于</p>\n<script type=\"math/tex; mode=display\">\\hat{y}=W^{[l]}W^{[l-1]}W^{l-2} \\cdots W^{[3]}W^{[2]}W^{[1]}X</script><p>大家感兴趣的话可以验证一下，很简单的。</p>\n<p>那么现在问题来了，当\\(l\\)很大的情况下，如果\\(W\\)元素都大于1，那么最后的结果就会非常非常大，甚至到无限大，这种情况叫exploding gradient；相应的，如果\\(W\\)元素都小于1，那么最后的结果就会特别小，甚至为零，这就是vanishing gradient.</p>\n<h3 id=\"Solution-1\"><a href=\"#Solution-1\" class=\"headerlink\" title=\"Solution\"></a>Solution</h3><p>对于上面的问题，我们一般在weights初始化的时候做一些工作来解决可能出现的exploding or vanishing gradient。我们可以直观的理解一下，对于active function\\(g(z)\\)，假设bias为零，</p>\n<script type=\"math/tex; mode=display\">z=w_{1}x_1 +w_{2}x_2+ \\cdots +w_{n}x_n</script><p>我们要我们可以看到，\\(n\\)的增大，\\(w\\)会变小，我们让\\(w\\)始终保持以0为mean，1为varance的Gaussian distribution下，就可以很好的控制\\(w\\)的大小，那么我们可以看到，\\(var(w)= \\frac{1}{n^{l-1}}\\)</p>\n<p>在Ng的建议中，如果active function是sigmoid，我们一般取\\(var(w)= \\frac{1}{n^{l-1}}\\)，如果是reLu，我们取\\(var(w)= \\frac{2}{n^{l-1}}\\)，对于tanh，\\(var(w)= \\frac{1}{n^{l-1}}\\)或者\\(var(w)= \\frac{2}{n^{l-1}+n^{l}}\\)，这样可以很好的避免exploding or vanishing gradient.</p>\n<h2 id=\"Gradient-checking\"><a href=\"#Gradient-checking\" class=\"headerlink\" title=\"Gradient checking\"></a>Gradient checking</h2><h3 id=\"Gradient-approximation\"><a href=\"#Gradient-approximation\" class=\"headerlink\" title=\"Gradient approximation\"></a>Gradient approximation</h3><p>在调试neural network的时候，我们会经常做gradient check的工作，以确定整个network正常的运行，Ng在这里建议我们使用双边逼近的方法去做gradient check，这里我不做太多描述，主要上一张图：<br><img src=\"http://otmy7guvn.bkt.clouddn.com/blog/6/6-6.png\" alt=\"\"><br>通常来说，双边逼近的方法获得结果更加准确。</p>\n<h3 id=\"Gradient-checking-notes\"><a href=\"#Gradient-checking-notes\" class=\"headerlink\" title=\"Gradient checking notes\"></a>Gradient checking notes</h3><blockquote>\n<ol>\n<li>Don’t use in training-only to debug(too slow)</li>\n<li>If algorithm fails grad check, look at components to try to identify bug</li>\n<li>Remeber regularization</li>\n<li>Dosen’t wrok with dropout</li>\n<li>Run at random initialzation; perhaps again after some training.</li>\n</ol>\n</blockquote>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ul>\n<li><a href=\"https://www.coursera.org/specializations/deep-learning\" target=\"_blank\" rel=\"external\">Deep learning-Coursera Andrew Ng</a></li>\n<li><a href=\"https://mooc.study.163.com/course/deeplearning_ai-2001281003#/info\" target=\"_blank\" rel=\"external\">Deep learning-网易云课堂 Andrew Ng</a></li>\n<li><a href=\"https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff\" target=\"_blank\" rel=\"external\">Bias and variance</a></li>\n</ul>"},{"title":"Learning notes-Deep Learning, course2, week3","date":"2017-09-30T07:44:25.000Z","_content":"不知不觉来到第三周的课程了，大家加油！这周的主要内容是hyperparameter selection和batch normal的问题，我们一起来看看这一周的内容！\n<!--more-->\n## Hyperparameter selection\nHyperparameter selection在machine learning中是一个非常重要的优化过程，例如gradient descent中的learning rate \\\\(\\alpha\\\\)就是关乎算法结果的重要hyperparameter，那么我们应该怎么去选择呢？Ng给出了两个建议：\n* 构建多个hyperparameter交叉，随机选择大小，选择效果较好的范围，继续随机选择hyperparameter大小，观察结果。\n* 选择参数的时候分段选择，并且使用log分段，例如在0.0001到1之间选择，将数轴分成0.0001，0.001，0.01，0.1和1，这样选择出的结果更好\n\n对于整体模型的hyperparameter selection，Ng也出了建议，那就是babysitting和parallel方法，一种是对一个模型多次调整，一种是同时启动多个不同hyperparameter的模型，最后取效果最好的。\n\n两种方法殊途同归，可以根据自己的具体情况做出选择。\n## Batch norm\n### Normalization\n相信大家都听说过大名鼎鼎的normalization吧，这是一种很棒的数据预处理的方法，它可以很好的提升数据处理（例如gradient descent）的速度和效果，在引入batch norm之前，我也稍微提一下normalization，下面上公式：\n\n对于输入数据来说，我们可以按以下方法来normalize\n$$ \\mu = \\frac{1}{m} \\sum_i x^{(i)}$$\n$$X = X- \\mu$$\n$$ \\sigma^2 = \\frac{1}{m} \\sum_i (x^{(i)})^2 $$\n$$ X = X/ \\sigma ^2$$\n这样，我们就把输入数据转化成了符合期望为0，方差为1的Gaussian distribution的数据。\n\n当然，这只是normaliztion中的一种方法，也是被称作z-score方法。\n### Batch norm\n上面说的normalization方法可以推广到neural networks中，对于nerual networks中的某一个layer来说，可以看做是一个孤立的计算过程，在这个过程中，我们可以引入normalization，对于\\\\(z^{(i)}\\\\)来说：\n$$ \\mu = \\frac{1}{m} \\sum_i z^{(i)}$$\n$$ \\sigma ^2= \\frac{1}{m} \\sum_i (z^{(i)}- \\mu)^2 $$\n$$z^{(i)}_{norm}= \\frac{z^{(i)}- \\mu}{ \\sqrt{ \\sigma^2 + \\epsilon}}$$\n$$z^{N(i)}= \\gamma z^{(i)}_{norm} + \\beta$$\n然后我们用最终的\\\\(z^{N\\[l](i)}\\\\)来替换\\\\(z^{\\[l](i)}\\\\) 就可以，其中\\\\( \\gamma\\\\)和\\\\(\\beta\\\\)是两个parameter，可以通过gradient descent来更新，这两个parameter存在的意义，就是可以调整normalization映射的Gaussian distribution，而不是统统映射到Normal distribution，值得注意的是，\\\\(\\epsilon\\\\)是一个很小的数，用来避免分母分0的情况。\n\n如果\\\\(\\gamma = \\sqrt{ \\sigma^2 + \\epsilon}\\\\)且\\\\( \\beta = \\mu\\\\)的话，那么其实\\\\(z^{N(i)}=z^(i)\\\\)的，大家可以算算，这种情况下，就是相当于没做normalization.\n### Batch norm on neural networks\n对于neural networks，输入\\\\(X\\\\)通过parameter\\\\(w^{[1]}\\\\)和\\\\(b^{[1]}\\\\)得到\\\\(z^{[1]}\\\\)，通过\\\\(\\beta\\\\)和\\\\(\\gamma\\\\)获得\\\\(z^{N[1]}\\\\)，经过active function后获得\\\\(a^{[1]}\\\\)，通过\\\\(w^{[2]}\\\\)和\\\\(b^{[2]}\\\\)获得\\\\(z^{[2]}\\\\)，如此下去，一直到最后的输出层，完成forward propagation.\n\n在整个过程中，一共有四个parameters，分别是\\\\(w^{[l]}\\\\)，\\\\(b^{[l]}\\\\)，\\\\( \\beta^{[l]}\\\\)，\\\\( \\gamma^{[l]}\\\\)，我们都知道：\n$$z^{[l]}=w^{[l]}a^{[l-1]}+b^{[l]}$$\n但是，我们在做batch normal的时候，首先会把\\\\(z^{[l]}\\\\)映射到期望为1方差为0的Gaussian distribution上，这就意味着\\\\(b^{[l]}\\\\)是可以忽略掉的，因为即使保留，在batch normal的时候也会被减去，因此，我们的parameter只有三个，即：\\\\(w^{[l]}\\\\)，\\\\( \\beta^{[l]}\\\\)，\\\\( \\gamma^{[l]}\\\\)\n\n在backforward的时候，我们和普通的neural networks一样，只是可以不用再去计算\\\\(db\\\\)\n### Solve covariate shift\n什么是covariate shift？简单的理解，就是模型需要随着样本的变化而变化，Ng举的例子就很直观，在猫脸试验中，假设training set里都是黑猫，这样获得的模型，对于花猫识别就是不适用的，这就叫covariate shift. 其实，batch norm可以改善neural networks效果的原因，就可以理解为solve covariate shift的过程。\n\nOK，我们来详细看看原因，假设我们有一个如图的neural networks：\n![](http://otmy7guvn.bkt.clouddn.com/blog/8/8-1.png) \n在标示出的位置，有parameter\\\\(w^{[3]}\\\\)和\\\\(b^{[3]}\\\\)，如果我们盖住前面的部分，那么我们将获得如图的neural networks\n![](http://otmy7guvn.bkt.clouddn.com/blog/8/8-2.png) \n对于neural networks来说，相当于获得了黑箱输出的\\\\(a^{[2]}\\\\)，而\\\\(a^{[2]}\\\\)的值其实并不是是固定的，每一次iteration后都有不一样的\\\\(a^{[2]}\\\\)，这就产生了covariate shift问题。\n\n但是，batch norm可以将\\\\(a^{[2]}\\\\)的期望和方差限制到\\\\(\\beta\\\\)和\\\\(gamma\\\\)控制的范围内，以此**极大限度**的缓解了covariate shift现象。\n\n另外，batch norm还可以有一些regularization的作用，由于每次mini-batch gradient descent中batch norm作用的sample不一样，类似于dropout的效果，会给对应layer加入一些噪声，以此产生一些regularization的效果。但是，我们一般不会把batch norm列入regularization范畴内。\n\n## Reference\n* [Deep learning-Coursera Andrew Ng](https://www.coursera.org/specializations/deep-learning)\n* [Deep learning-网易云课堂 Andrew Ng](https://mooc.study.163.com/course/deeplearning_ai-2001281003#/info)","source":"_posts/course-deep-learning-course2-week3.md","raw":"---\ntitle: Learning notes-Deep Learning, course2, week3\ndate: 2017-09-30 15:44:25\ntags: \n\t- hyperparameter\n\t- batch norm\n\t- covariate shift\ncategories: learning notes\n---\n不知不觉来到第三周的课程了，大家加油！这周的主要内容是hyperparameter selection和batch normal的问题，我们一起来看看这一周的内容！\n<!--more-->\n## Hyperparameter selection\nHyperparameter selection在machine learning中是一个非常重要的优化过程，例如gradient descent中的learning rate \\\\(\\alpha\\\\)就是关乎算法结果的重要hyperparameter，那么我们应该怎么去选择呢？Ng给出了两个建议：\n* 构建多个hyperparameter交叉，随机选择大小，选择效果较好的范围，继续随机选择hyperparameter大小，观察结果。\n* 选择参数的时候分段选择，并且使用log分段，例如在0.0001到1之间选择，将数轴分成0.0001，0.001，0.01，0.1和1，这样选择出的结果更好\n\n对于整体模型的hyperparameter selection，Ng也出了建议，那就是babysitting和parallel方法，一种是对一个模型多次调整，一种是同时启动多个不同hyperparameter的模型，最后取效果最好的。\n\n两种方法殊途同归，可以根据自己的具体情况做出选择。\n## Batch norm\n### Normalization\n相信大家都听说过大名鼎鼎的normalization吧，这是一种很棒的数据预处理的方法，它可以很好的提升数据处理（例如gradient descent）的速度和效果，在引入batch norm之前，我也稍微提一下normalization，下面上公式：\n\n对于输入数据来说，我们可以按以下方法来normalize\n$$ \\mu = \\frac{1}{m} \\sum_i x^{(i)}$$\n$$X = X- \\mu$$\n$$ \\sigma^2 = \\frac{1}{m} \\sum_i (x^{(i)})^2 $$\n$$ X = X/ \\sigma ^2$$\n这样，我们就把输入数据转化成了符合期望为0，方差为1的Gaussian distribution的数据。\n\n当然，这只是normaliztion中的一种方法，也是被称作z-score方法。\n### Batch norm\n上面说的normalization方法可以推广到neural networks中，对于nerual networks中的某一个layer来说，可以看做是一个孤立的计算过程，在这个过程中，我们可以引入normalization，对于\\\\(z^{(i)}\\\\)来说：\n$$ \\mu = \\frac{1}{m} \\sum_i z^{(i)}$$\n$$ \\sigma ^2= \\frac{1}{m} \\sum_i (z^{(i)}- \\mu)^2 $$\n$$z^{(i)}_{norm}= \\frac{z^{(i)}- \\mu}{ \\sqrt{ \\sigma^2 + \\epsilon}}$$\n$$z^{N(i)}= \\gamma z^{(i)}_{norm} + \\beta$$\n然后我们用最终的\\\\(z^{N\\[l](i)}\\\\)来替换\\\\(z^{\\[l](i)}\\\\) 就可以，其中\\\\( \\gamma\\\\)和\\\\(\\beta\\\\)是两个parameter，可以通过gradient descent来更新，这两个parameter存在的意义，就是可以调整normalization映射的Gaussian distribution，而不是统统映射到Normal distribution，值得注意的是，\\\\(\\epsilon\\\\)是一个很小的数，用来避免分母分0的情况。\n\n如果\\\\(\\gamma = \\sqrt{ \\sigma^2 + \\epsilon}\\\\)且\\\\( \\beta = \\mu\\\\)的话，那么其实\\\\(z^{N(i)}=z^(i)\\\\)的，大家可以算算，这种情况下，就是相当于没做normalization.\n### Batch norm on neural networks\n对于neural networks，输入\\\\(X\\\\)通过parameter\\\\(w^{[1]}\\\\)和\\\\(b^{[1]}\\\\)得到\\\\(z^{[1]}\\\\)，通过\\\\(\\beta\\\\)和\\\\(\\gamma\\\\)获得\\\\(z^{N[1]}\\\\)，经过active function后获得\\\\(a^{[1]}\\\\)，通过\\\\(w^{[2]}\\\\)和\\\\(b^{[2]}\\\\)获得\\\\(z^{[2]}\\\\)，如此下去，一直到最后的输出层，完成forward propagation.\n\n在整个过程中，一共有四个parameters，分别是\\\\(w^{[l]}\\\\)，\\\\(b^{[l]}\\\\)，\\\\( \\beta^{[l]}\\\\)，\\\\( \\gamma^{[l]}\\\\)，我们都知道：\n$$z^{[l]}=w^{[l]}a^{[l-1]}+b^{[l]}$$\n但是，我们在做batch normal的时候，首先会把\\\\(z^{[l]}\\\\)映射到期望为1方差为0的Gaussian distribution上，这就意味着\\\\(b^{[l]}\\\\)是可以忽略掉的，因为即使保留，在batch normal的时候也会被减去，因此，我们的parameter只有三个，即：\\\\(w^{[l]}\\\\)，\\\\( \\beta^{[l]}\\\\)，\\\\( \\gamma^{[l]}\\\\)\n\n在backforward的时候，我们和普通的neural networks一样，只是可以不用再去计算\\\\(db\\\\)\n### Solve covariate shift\n什么是covariate shift？简单的理解，就是模型需要随着样本的变化而变化，Ng举的例子就很直观，在猫脸试验中，假设training set里都是黑猫，这样获得的模型，对于花猫识别就是不适用的，这就叫covariate shift. 其实，batch norm可以改善neural networks效果的原因，就可以理解为solve covariate shift的过程。\n\nOK，我们来详细看看原因，假设我们有一个如图的neural networks：\n![](http://otmy7guvn.bkt.clouddn.com/blog/8/8-1.png) \n在标示出的位置，有parameter\\\\(w^{[3]}\\\\)和\\\\(b^{[3]}\\\\)，如果我们盖住前面的部分，那么我们将获得如图的neural networks\n![](http://otmy7guvn.bkt.clouddn.com/blog/8/8-2.png) \n对于neural networks来说，相当于获得了黑箱输出的\\\\(a^{[2]}\\\\)，而\\\\(a^{[2]}\\\\)的值其实并不是是固定的，每一次iteration后都有不一样的\\\\(a^{[2]}\\\\)，这就产生了covariate shift问题。\n\n但是，batch norm可以将\\\\(a^{[2]}\\\\)的期望和方差限制到\\\\(\\beta\\\\)和\\\\(gamma\\\\)控制的范围内，以此**极大限度**的缓解了covariate shift现象。\n\n另外，batch norm还可以有一些regularization的作用，由于每次mini-batch gradient descent中batch norm作用的sample不一样，类似于dropout的效果，会给对应layer加入一些噪声，以此产生一些regularization的效果。但是，我们一般不会把batch norm列入regularization范畴内。\n\n## Reference\n* [Deep learning-Coursera Andrew Ng](https://www.coursera.org/specializations/deep-learning)\n* [Deep learning-网易云课堂 Andrew Ng](https://mooc.study.163.com/course/deeplearning_ai-2001281003#/info)","slug":"course-deep-learning-course2-week3","published":1,"updated":"2017-10-02T13:19:08.404Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cj8lez5310002w2kzkfv6p2ny","content":"<p>不知不觉来到第三周的课程了，大家加油！这周的主要内容是hyperparameter selection和batch normal的问题，我们一起来看看这一周的内容！<br><a id=\"more\"></a></p>\n<h2 id=\"Hyperparameter-selection\"><a href=\"#Hyperparameter-selection\" class=\"headerlink\" title=\"Hyperparameter selection\"></a>Hyperparameter selection</h2><p>Hyperparameter selection在machine learning中是一个非常重要的优化过程，例如gradient descent中的learning rate \\(\\alpha\\)就是关乎算法结果的重要hyperparameter，那么我们应该怎么去选择呢？Ng给出了两个建议：</p>\n<ul>\n<li>构建多个hyperparameter交叉，随机选择大小，选择效果较好的范围，继续随机选择hyperparameter大小，观察结果。</li>\n<li>选择参数的时候分段选择，并且使用log分段，例如在0.0001到1之间选择，将数轴分成0.0001，0.001，0.01，0.1和1，这样选择出的结果更好</li>\n</ul>\n<p>对于整体模型的hyperparameter selection，Ng也出了建议，那就是babysitting和parallel方法，一种是对一个模型多次调整，一种是同时启动多个不同hyperparameter的模型，最后取效果最好的。</p>\n<p>两种方法殊途同归，可以根据自己的具体情况做出选择。</p>\n<h2 id=\"Batch-norm\"><a href=\"#Batch-norm\" class=\"headerlink\" title=\"Batch norm\"></a>Batch norm</h2><h3 id=\"Normalization\"><a href=\"#Normalization\" class=\"headerlink\" title=\"Normalization\"></a>Normalization</h3><p>相信大家都听说过大名鼎鼎的normalization吧，这是一种很棒的数据预处理的方法，它可以很好的提升数据处理（例如gradient descent）的速度和效果，在引入batch norm之前，我也稍微提一下normalization，下面上公式：</p>\n<p>对于输入数据来说，我们可以按以下方法来normalize</p>\n<script type=\"math/tex; mode=display\">\\mu = \\frac{1}{m} \\sum_i x^{(i)}</script><script type=\"math/tex; mode=display\">X = X- \\mu</script><script type=\"math/tex; mode=display\">\\sigma^2 = \\frac{1}{m} \\sum_i (x^{(i)})^2</script><script type=\"math/tex; mode=display\">X = X/ \\sigma ^2</script><p>这样，我们就把输入数据转化成了符合期望为0，方差为1的Gaussian distribution的数据。</p>\n<p>当然，这只是normaliztion中的一种方法，也是被称作z-score方法。</p>\n<h3 id=\"Batch-norm-1\"><a href=\"#Batch-norm-1\" class=\"headerlink\" title=\"Batch norm\"></a>Batch norm</h3><p>上面说的normalization方法可以推广到neural networks中，对于nerual networks中的某一个layer来说，可以看做是一个孤立的计算过程，在这个过程中，我们可以引入normalization，对于\\(z^{(i)}\\)来说：</p>\n<script type=\"math/tex; mode=display\">\\mu = \\frac{1}{m} \\sum_i z^{(i)}</script><script type=\"math/tex; mode=display\">\\sigma ^2= \\frac{1}{m} \\sum_i (z^{(i)}- \\mu)^2</script><script type=\"math/tex; mode=display\">z^{(i)}_{norm}= \\frac{z^{(i)}- \\mu}{ \\sqrt{ \\sigma^2 + \\epsilon}}</script><script type=\"math/tex; mode=display\">z^{N(i)}= \\gamma z^{(i)}_{norm} + \\beta</script><p>然后我们用最终的\\(z^{N[l](i)}\\)来替换\\(z^{[l](i)}\\) 就可以，其中\\( \\gamma\\)和\\(\\beta\\)是两个parameter，可以通过gradient descent来更新，这两个parameter存在的意义，就是可以调整normalization映射的Gaussian distribution，而不是统统映射到Normal distribution，值得注意的是，\\(\\epsilon\\)是一个很小的数，用来避免分母分0的情况。</p>\n<p>如果\\(\\gamma = \\sqrt{ \\sigma^2 + \\epsilon}\\)且\\( \\beta = \\mu\\)的话，那么其实\\(z^{N(i)}=z^(i)\\)的，大家可以算算，这种情况下，就是相当于没做normalization.</p>\n<h3 id=\"Batch-norm-on-neural-networks\"><a href=\"#Batch-norm-on-neural-networks\" class=\"headerlink\" title=\"Batch norm on neural networks\"></a>Batch norm on neural networks</h3><p>对于neural networks，输入\\(X\\)通过parameter\\(w^{[1]}\\)和\\(b^{[1]}\\)得到\\(z^{[1]}\\)，通过\\(\\beta\\)和\\(\\gamma\\)获得\\(z^{N[1]}\\)，经过active function后获得\\(a^{[1]}\\)，通过\\(w^{[2]}\\)和\\(b^{[2]}\\)获得\\(z^{[2]}\\)，如此下去，一直到最后的输出层，完成forward propagation.</p>\n<p>在整个过程中，一共有四个parameters，分别是\\(w^{[l]}\\)，\\(b^{[l]}\\)，\\( \\beta^{[l]}\\)，\\( \\gamma^{[l]}\\)，我们都知道：</p>\n<script type=\"math/tex; mode=display\">z^{[l]}=w^{[l]}a^{[l-1]}+b^{[l]}</script><p>但是，我们在做batch normal的时候，首先会把\\(z^{[l]}\\)映射到期望为1方差为0的Gaussian distribution上，这就意味着\\(b^{[l]}\\)是可以忽略掉的，因为即使保留，在batch normal的时候也会被减去，因此，我们的parameter只有三个，即：\\(w^{[l]}\\)，\\( \\beta^{[l]}\\)，\\( \\gamma^{[l]}\\)</p>\n<p>在backforward的时候，我们和普通的neural networks一样，只是可以不用再去计算\\(db\\)</p>\n<h3 id=\"Solve-covariate-shift\"><a href=\"#Solve-covariate-shift\" class=\"headerlink\" title=\"Solve covariate shift\"></a>Solve covariate shift</h3><p>什么是covariate shift？简单的理解，就是模型需要随着样本的变化而变化，Ng举的例子就很直观，在猫脸试验中，假设training set里都是黑猫，这样获得的模型，对于花猫识别就是不适用的，这就叫covariate shift. 其实，batch norm可以改善neural networks效果的原因，就可以理解为solve covariate shift的过程。</p>\n<p>OK，我们来详细看看原因，假设我们有一个如图的neural networks：<br><img src=\"http://otmy7guvn.bkt.clouddn.com/blog/8/8-1.png\" alt=\"\"><br>在标示出的位置，有parameter\\(w^{[3]}\\)和\\(b^{[3]}\\)，如果我们盖住前面的部分，那么我们将获得如图的neural networks<br><img src=\"http://otmy7guvn.bkt.clouddn.com/blog/8/8-2.png\" alt=\"\"><br>对于neural networks来说，相当于获得了黑箱输出的\\(a^{[2]}\\)，而\\(a^{[2]}\\)的值其实并不是是固定的，每一次iteration后都有不一样的\\(a^{[2]}\\)，这就产生了covariate shift问题。</p>\n<p>但是，batch norm可以将\\(a^{[2]}\\)的期望和方差限制到\\(\\beta\\)和\\(gamma\\)控制的范围内，以此<strong>极大限度</strong>的缓解了covariate shift现象。</p>\n<p>另外，batch norm还可以有一些regularization的作用，由于每次mini-batch gradient descent中batch norm作用的sample不一样，类似于dropout的效果，会给对应layer加入一些噪声，以此产生一些regularization的效果。但是，我们一般不会把batch norm列入regularization范畴内。</p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ul>\n<li><a href=\"https://www.coursera.org/specializations/deep-learning\" target=\"_blank\" rel=\"external\">Deep learning-Coursera Andrew Ng</a></li>\n<li><a href=\"https://mooc.study.163.com/course/deeplearning_ai-2001281003#/info\" target=\"_blank\" rel=\"external\">Deep learning-网易云课堂 Andrew Ng</a></li>\n</ul>\n","site":{"data":{}},"excerpt":"<p>不知不觉来到第三周的课程了，大家加油！这周的主要内容是hyperparameter selection和batch normal的问题，我们一起来看看这一周的内容！<br>","more":"</p>\n<h2 id=\"Hyperparameter-selection\"><a href=\"#Hyperparameter-selection\" class=\"headerlink\" title=\"Hyperparameter selection\"></a>Hyperparameter selection</h2><p>Hyperparameter selection在machine learning中是一个非常重要的优化过程，例如gradient descent中的learning rate \\(\\alpha\\)就是关乎算法结果的重要hyperparameter，那么我们应该怎么去选择呢？Ng给出了两个建议：</p>\n<ul>\n<li>构建多个hyperparameter交叉，随机选择大小，选择效果较好的范围，继续随机选择hyperparameter大小，观察结果。</li>\n<li>选择参数的时候分段选择，并且使用log分段，例如在0.0001到1之间选择，将数轴分成0.0001，0.001，0.01，0.1和1，这样选择出的结果更好</li>\n</ul>\n<p>对于整体模型的hyperparameter selection，Ng也出了建议，那就是babysitting和parallel方法，一种是对一个模型多次调整，一种是同时启动多个不同hyperparameter的模型，最后取效果最好的。</p>\n<p>两种方法殊途同归，可以根据自己的具体情况做出选择。</p>\n<h2 id=\"Batch-norm\"><a href=\"#Batch-norm\" class=\"headerlink\" title=\"Batch norm\"></a>Batch norm</h2><h3 id=\"Normalization\"><a href=\"#Normalization\" class=\"headerlink\" title=\"Normalization\"></a>Normalization</h3><p>相信大家都听说过大名鼎鼎的normalization吧，这是一种很棒的数据预处理的方法，它可以很好的提升数据处理（例如gradient descent）的速度和效果，在引入batch norm之前，我也稍微提一下normalization，下面上公式：</p>\n<p>对于输入数据来说，我们可以按以下方法来normalize</p>\n<script type=\"math/tex; mode=display\">\\mu = \\frac{1}{m} \\sum_i x^{(i)}</script><script type=\"math/tex; mode=display\">X = X- \\mu</script><script type=\"math/tex; mode=display\">\\sigma^2 = \\frac{1}{m} \\sum_i (x^{(i)})^2</script><script type=\"math/tex; mode=display\">X = X/ \\sigma ^2</script><p>这样，我们就把输入数据转化成了符合期望为0，方差为1的Gaussian distribution的数据。</p>\n<p>当然，这只是normaliztion中的一种方法，也是被称作z-score方法。</p>\n<h3 id=\"Batch-norm-1\"><a href=\"#Batch-norm-1\" class=\"headerlink\" title=\"Batch norm\"></a>Batch norm</h3><p>上面说的normalization方法可以推广到neural networks中，对于nerual networks中的某一个layer来说，可以看做是一个孤立的计算过程，在这个过程中，我们可以引入normalization，对于\\(z^{(i)}\\)来说：</p>\n<script type=\"math/tex; mode=display\">\\mu = \\frac{1}{m} \\sum_i z^{(i)}</script><script type=\"math/tex; mode=display\">\\sigma ^2= \\frac{1}{m} \\sum_i (z^{(i)}- \\mu)^2</script><script type=\"math/tex; mode=display\">z^{(i)}_{norm}= \\frac{z^{(i)}- \\mu}{ \\sqrt{ \\sigma^2 + \\epsilon}}</script><script type=\"math/tex; mode=display\">z^{N(i)}= \\gamma z^{(i)}_{norm} + \\beta</script><p>然后我们用最终的\\(z^{N[l](i)}\\)来替换\\(z^{[l](i)}\\) 就可以，其中\\( \\gamma\\)和\\(\\beta\\)是两个parameter，可以通过gradient descent来更新，这两个parameter存在的意义，就是可以调整normalization映射的Gaussian distribution，而不是统统映射到Normal distribution，值得注意的是，\\(\\epsilon\\)是一个很小的数，用来避免分母分0的情况。</p>\n<p>如果\\(\\gamma = \\sqrt{ \\sigma^2 + \\epsilon}\\)且\\( \\beta = \\mu\\)的话，那么其实\\(z^{N(i)}=z^(i)\\)的，大家可以算算，这种情况下，就是相当于没做normalization.</p>\n<h3 id=\"Batch-norm-on-neural-networks\"><a href=\"#Batch-norm-on-neural-networks\" class=\"headerlink\" title=\"Batch norm on neural networks\"></a>Batch norm on neural networks</h3><p>对于neural networks，输入\\(X\\)通过parameter\\(w^{[1]}\\)和\\(b^{[1]}\\)得到\\(z^{[1]}\\)，通过\\(\\beta\\)和\\(\\gamma\\)获得\\(z^{N[1]}\\)，经过active function后获得\\(a^{[1]}\\)，通过\\(w^{[2]}\\)和\\(b^{[2]}\\)获得\\(z^{[2]}\\)，如此下去，一直到最后的输出层，完成forward propagation.</p>\n<p>在整个过程中，一共有四个parameters，分别是\\(w^{[l]}\\)，\\(b^{[l]}\\)，\\( \\beta^{[l]}\\)，\\( \\gamma^{[l]}\\)，我们都知道：</p>\n<script type=\"math/tex; mode=display\">z^{[l]}=w^{[l]}a^{[l-1]}+b^{[l]}</script><p>但是，我们在做batch normal的时候，首先会把\\(z^{[l]}\\)映射到期望为1方差为0的Gaussian distribution上，这就意味着\\(b^{[l]}\\)是可以忽略掉的，因为即使保留，在batch normal的时候也会被减去，因此，我们的parameter只有三个，即：\\(w^{[l]}\\)，\\( \\beta^{[l]}\\)，\\( \\gamma^{[l]}\\)</p>\n<p>在backforward的时候，我们和普通的neural networks一样，只是可以不用再去计算\\(db\\)</p>\n<h3 id=\"Solve-covariate-shift\"><a href=\"#Solve-covariate-shift\" class=\"headerlink\" title=\"Solve covariate shift\"></a>Solve covariate shift</h3><p>什么是covariate shift？简单的理解，就是模型需要随着样本的变化而变化，Ng举的例子就很直观，在猫脸试验中，假设training set里都是黑猫，这样获得的模型，对于花猫识别就是不适用的，这就叫covariate shift. 其实，batch norm可以改善neural networks效果的原因，就可以理解为solve covariate shift的过程。</p>\n<p>OK，我们来详细看看原因，假设我们有一个如图的neural networks：<br><img src=\"http://otmy7guvn.bkt.clouddn.com/blog/8/8-1.png\" alt=\"\"><br>在标示出的位置，有parameter\\(w^{[3]}\\)和\\(b^{[3]}\\)，如果我们盖住前面的部分，那么我们将获得如图的neural networks<br><img src=\"http://otmy7guvn.bkt.clouddn.com/blog/8/8-2.png\" alt=\"\"><br>对于neural networks来说，相当于获得了黑箱输出的\\(a^{[2]}\\)，而\\(a^{[2]}\\)的值其实并不是是固定的，每一次iteration后都有不一样的\\(a^{[2]}\\)，这就产生了covariate shift问题。</p>\n<p>但是，batch norm可以将\\(a^{[2]}\\)的期望和方差限制到\\(\\beta\\)和\\(gamma\\)控制的范围内，以此<strong>极大限度</strong>的缓解了covariate shift现象。</p>\n<p>另外，batch norm还可以有一些regularization的作用，由于每次mini-batch gradient descent中batch norm作用的sample不一样，类似于dropout的效果，会给对应layer加入一些噪声，以此产生一些regularization的效果。但是，我们一般不会把batch norm列入regularization范畴内。</p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ul>\n<li><a href=\"https://www.coursera.org/specializations/deep-learning\" target=\"_blank\" rel=\"external\">Deep learning-Coursera Andrew Ng</a></li>\n<li><a href=\"https://mooc.study.163.com/course/deeplearning_ai-2001281003#/info\" target=\"_blank\" rel=\"external\">Deep learning-网易云课堂 Andrew Ng</a></li>\n</ul>"},{"title":"Hello World","date":"2017-07-26T09:40:40.000Z","_content":"**我终于把blog搭建起来了!**\n\n这是一个属于**Asir** 自己的博客,在这里我会写一些技术分享,记录自己平时学到的东西,也会整点吐槽或者鸡汤.总之,有了一个真正的属于自己的天地,可以随便整,这种感觉非常棒.\n\n其实自己在博客园也尝试过一次,可是体验并不是很理想,在这里我并没有抨击的意思,因为自己搭建起来的成就感那不是一点两点.希望后面可以趁热打铁,开启blog之旅.\n<!--more-->\n***\n在这里感谢下亲铁[**圈羊**](https://www.unbelievable9.info/)为我提供的完美设备和深夜技术支持,非常棒!\n\n最后,作为一个coding man, 在所有事情的最开始,都不应该缺少这句话\n\n**Hello world!!!**\n","source":"_posts/hello.md","raw":"---\ntitle: Hello World\ndate: 2017-07-26 17:40:40\ntags: 抒情一把\ncategories: 杂\n---\n**我终于把blog搭建起来了!**\n\n这是一个属于**Asir** 自己的博客,在这里我会写一些技术分享,记录自己平时学到的东西,也会整点吐槽或者鸡汤.总之,有了一个真正的属于自己的天地,可以随便整,这种感觉非常棒.\n\n其实自己在博客园也尝试过一次,可是体验并不是很理想,在这里我并没有抨击的意思,因为自己搭建起来的成就感那不是一点两点.希望后面可以趁热打铁,开启blog之旅.\n<!--more-->\n***\n在这里感谢下亲铁[**圈羊**](https://www.unbelievable9.info/)为我提供的完美设备和深夜技术支持,非常棒!\n\n最后,作为一个coding man, 在所有事情的最开始,都不应该缺少这句话\n\n**Hello world!!!**\n","slug":"hello","published":1,"updated":"2017-08-01T01:51:27.379Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cj8lez5390006w2kzfritnam7","content":"<p><strong>我终于把blog搭建起来了!</strong></p>\n<p>这是一个属于<strong>Asir</strong> 自己的博客,在这里我会写一些技术分享,记录自己平时学到的东西,也会整点吐槽或者鸡汤.总之,有了一个真正的属于自己的天地,可以随便整,这种感觉非常棒.</p>\n<p>其实自己在博客园也尝试过一次,可是体验并不是很理想,在这里我并没有抨击的意思,因为自己搭建起来的成就感那不是一点两点.希望后面可以趁热打铁,开启blog之旅.<br><a id=\"more\"></a></p>\n<hr>\n<p>在这里感谢下亲铁<a href=\"https://www.unbelievable9.info/\" target=\"_blank\" rel=\"external\"><strong>圈羊</strong></a>为我提供的完美设备和深夜技术支持,非常棒!</p>\n<p>最后,作为一个coding man, 在所有事情的最开始,都不应该缺少这句话</p>\n<p><strong>Hello world!!!</strong></p>\n","site":{"data":{}},"excerpt":"<p><strong>我终于把blog搭建起来了!</strong></p>\n<p>这是一个属于<strong>Asir</strong> 自己的博客,在这里我会写一些技术分享,记录自己平时学到的东西,也会整点吐槽或者鸡汤.总之,有了一个真正的属于自己的天地,可以随便整,这种感觉非常棒.</p>\n<p>其实自己在博客园也尝试过一次,可是体验并不是很理想,在这里我并没有抨击的意思,因为自己搭建起来的成就感那不是一点两点.希望后面可以趁热打铁,开启blog之旅.<br>","more":"</p>\n<hr>\n<p>在这里感谢下亲铁<a href=\"https://www.unbelievable9.info/\" target=\"_blank\" rel=\"external\"><strong>圈羊</strong></a>为我提供的完美设备和深夜技术支持,非常棒!</p>\n<p>最后,作为一个coding man, 在所有事情的最开始,都不应该缺少这句话</p>\n<p><strong>Hello world!!!</strong></p>"},{"title":"再深入聊聊梯度下降和牛顿法","date":"2017-08-11T09:26:56.000Z","_content":"上次我们一起聊到了gradient descent和newton's method，而且我们已经知道了gradient descent和newton's method都是convex optimization的好方法，这次我们就跳出convex optimization，从更大的unconstrained optimization角度来探讨下这两种方法之间的关联和区别。\n<!--more-->\n\n假设我们现有一个的optimization task，要求objective function \\\\(f(x)\\\\)的最小值，我们一般有两种方案：\n* 考虑到\\\\(f(x)\\\\)的最小值很有可能是全局最小值，那么我们可以通过寻找\\\\( \\nabla f(x)=0\\\\)的点来确定最小值，这就是**newton's method**的思想\n* 既然我们要寻找最小值，那我们可以顺着一条\\\\(f(x)\\\\)逐渐减小的路径，顺着这条路径一直走下去，直到不再变小，这就是**gradient descent**的思想\n\nOK，简单的叙述之后，我们开始正题！\n\n## 泰勒级数(Taylor series)\n首先我们需要回忆一下高等数学中重要的Taylor series，如果\\\\( f(x)\\\\)在点\\\\( x_0\\\\)的领域内具有\\\\(n+1\\\\)阶导数，那么，在该领域内，\\\\( f(x)\\\\)可展开成\\\\(n\\\\)阶Taylor series，忽略无限大次项的形式就是\n$$f(x)=f(x_0)+ \\nabla f(x_0)(x-x_0) + \\frac{ \\nabla ^2 f(x_0)}{2!}(x-x_0)^2 +...+\\frac{ \\nabla ^n f(x_0)}{n!}(x-x_0)^n $$\n其实在高等数学中学到Taylor series的时候，我本人是十分无感的，我并不知道这个东西到底有什么用处，相信很多人和我有相似的经历。\n\n> In mathematics, a Taylor series is a representation of a function as an infinite sum of terms that are calculated from the values of the function's derivatives at a single point.\n\n事实上，Taylor series所表现的是，对于\\\\( f(x)\\\\)在点\\\\( x_0\\\\)附近的一个估计，也可以理解为，根据\\\\( x_0\\\\)点处的各阶derivatives之和构成一个新的function，这个function就是对\\\\(f(x)\\\\)的逼近和拟合，而且这种逼近和拟合，随着Taylor series阶数增加而更接近于真实的\\\\(f(x)\\\\)。如果我们使用0阶Taylor series来逼近的话，那我们就粗暴的认为，\\\\( f(x)\\\\)在点\\\\( x_0\\\\)附近的值就都是\\\\(x_0\\\\)，这当然太粗暴直接了，哈哈。\n\n既然这太粗暴了，那么我们就用1st order Taylor series来做一个逼近和估计，这就是gradient descent的思想；如果我们用2nd order Taylor series来估计呢，那就成了newton's method了\n\nOK，我们继续娓娓道来。\n\n## 1st order Taylor series & gradient descent\n假设\\\\(x_k\\\\)是第k次gradient descent迭代后的\\\\(x\\\\)取值，那我们在此处的1st order Taylor series 就是\n$$f(x)=f(x_k)+ \\nabla f(x_k)(x-x_k)$$\n其中\\\\(x\\\\)是迭代的下一个方向，gradient descent的目标就是让\\\\(f(x)\\\\)达到局部甚至全局最小值，那么每一次迭代，也需要尽可能的减小更多以达到这个目的，那么\n$$f(x_k)-f(x)=- \\nabla f(x_k)(x-x_k)$$\n显然，上式应该尽可能的大，即**\\\\(- \\nabla f(x_k)(x-x_k)\\\\)越大越好**，我们现在把\\\\((x-x_k)\\\\)做一个替换，用单位向量\\\\(\\vec g\\\\)和标量\\\\( \\alpha\\\\)分别代表方向和大小，现在的任务就变成了\n$$ \\min \\nabla f(x_k)(x-x_k) = \\min( \\alpha \\vec{\\nabla f(x_k)}⋅ \\vec g)$$\n我们都知道，**对于两个向量来说，当他们方向相反时，他们的内积是最小的**。\n\n>梯度方向的定义是该点梯度在标量场增长最快的方向\n\n因此当\\\\(\\vec g\\\\)的方向是\\\\( \\vec{\\nabla f(x_k)}\\\\)的反方向时，上式可以取到最小值，于是就有\n$$x-x_k=- \\alpha \\nabla f(x_k)$$\n$$x:=x_k- \\alpha \\nabla f(x_k)$$\n到这一步，是不是看到了熟悉的gradient descent呢，yeah mate！We make it!\n## 2nd order Taylor series & newton's method\n和上面的gradient descent相似，假设\\\\(x_k\\\\)是第\\\\(k\\\\)次newton's method迭代后的\\\\(x\\\\)取值，那我们在此处的2nd order Taylor series 是\n$$f(x)=f(x_k)+ \\nabla f(x_k)(x-x_k) + \\frac{1}{2}(x-x_k)^T \\nabla^2 f(x_k)(x-x_k) $$\n我们对等号两边同时对\\\\(x\\\\)求导，并令其为零\n$$ \\nabla f(x) = \\nabla f(x_k) + (x-x_k) \\nabla^2 f(x_k)=0$$\n由于newton's method的原理就是通过\\\\(\\nabla f(x)=0\\\\)来寻找最小值，**故上式为零的解\\\\(x\\\\)其实就是newton's method在\\\\(k+1\\\\)次迭代后的新的\\\\(x\\\\)值**。其中\\\\(\\nabla f(x_k)\\\\)是\\\\(x_k\\\\)处的一阶导数，\\\\( \\nabla^2 f(x_k)\\\\)是\\\\(x_k\\\\)处的二阶导数Hessian矩阵元素\n\n我们令\\\\(\\nabla f(x_k)=g\\\\)，\\\\(\\nabla^2 f(x_k)=H\\\\)，则上式变成\n$$g+H(x-x_k)=0$$\n进一步的\n$$x=x_k-H^{-1}g$$\n由于\\\\(-g H^{-1} \\\\) 是优化的前进方向，在寻找最小值的过程中，这个方向一定是和梯度方向\\\\(g\\\\)相反才可以更快的下降，那么就有\\\\( g^T H^{-1} g > 0\\\\)，这不就是positive definite的定义吗？也就是说，**Hessian矩阵是positive definite的**。\n\n想象一下，如果Hessian是negative definite的话，参数更新的方向就成了和\\\\(g\\\\)相同的方向，newton's method将会发散，这一点，也是newton's method的缺点。在objective function是non-convex function的情况下，如果第\\\\(k\\\\)次迭代获得的\\\\(x_k\\\\)处的Hessian matrix negative definite，那么newton's method将会发散，从而导致不收敛。当然，为了解决这种问题，后续有改进的BFGS等方法，我们在这里暂时不详细讨论。\n## Sum up\n下面我们再来总结性质的对比一下两种方法，来看一张图\n![](http://otmy7guvn.bkt.clouddn.com/blog/2/2-1.png) \n事实上，这两种方法都采用了一种逼近和拟合的思想。假设现在处于迭代\\\\(k\\\\)次之后的\\\\(x_k\\\\)点，对于objective function，我们用\\\\(x_k\\\\)点的Taylor series \\\\(f(x)\\\\)来逼近和拟合，当然了，上图我们看到，gradient descent是用一次function而newton's method采用的是二次function，这是二者之间最显著的区别。\n\n对于new's method，在拟合之后，我们通过\\\\( \\nabla f(x)=0\\\\)求得的\\\\(x \\_{k+1}\\\\)点作为此次迭代的结果，下次迭代时候，又在\\\\(x \\_{k+1}\\\\)处次进行二次function的拟合，并如此迭代下去。\n\nNewton's method采用二次function来拟合，我们可以感性的理解为，newton's method在寻找下降的方向时候，关注的不仅仅是此处objective function value是不是减小(一阶value)，还关注此处value下降的趋势如何(二阶value)，而gradient descent只关心此处function value是不是减小，因此newton's method可以迭代更少次数获得最优解。对于标准二次型的objective function，newton's method甚至可以一次迭代就找到全局最小值。\n\n但是值得注意的是，上面所说的标准二次型function，实质上是convex function，在一般的unconstrained optimization中，更多的情况则是non-convex optimization，对于一般的non-convex optimization，newton's method是相对不稳定的，因为我们很难保证Hessian matrix的positive definite。鉴于此，我们会加入步长\\\\(\\lambda\\\\)限制，防止其一次迭代过大而带来迭代后Hessian matrix negative definite的情况，即\n$$x:=x- \\lambda H^{-1} g$$\n对于这种思想，我个人认为，是在整体non-convex function中寻找一个局部的convex function，通过步长将newton's method限制在这个局部中，最后收敛到局部最优中。由此可见，newton's mtehod在non-convex中受限制比较大。\n\n相比之下，由于gradient descent采用的一次function做拟合，只需要考虑沿着梯度反方向寻找最小值，因此gradient descent适用于各种场景，甚至是non-convex optimization，虽然不能保证是全局最优，但至少gradient descent是可以值得一试的方法。\n\n下面来总结一下：\n* Gradient descent 和 newton's method都是利用Taylor series对objective function进行拟合来实现迭代的；\n* Gradient descent 采用一次型function拟合而 newton's method采用的是二次型function，因此newton's method迭代更迅速；\n* Newton's method每次迭代都会计算Hessian matrix的逆，在高维feature情况下，这使得每次迭代会比较慢；\n* Newton's method在non-convex optimization中很受限制，而gradient descent则不受影响。\n\n好了，先写这么多，这其中的知识量还是很深奥的，也不知道自己有没有叙述明白，欢迎大家一起来讨论！\n\n**最后感谢优男的宝贵意见！**\n## Reference\n* [UCLA courseware](http://www.math.ucla.edu/~biskup/164.2.14f/PDFs/recursions.pdf)\n* [CCU courseware](https://www.cs.ccu.edu.tw/~wtchu/courses/2014s_OPT/Lectures/Chapter%209%20Newton%27s%20Method.pdf)\n* [Taylor series](https://en.wikipedia.org/wiki/Taylor_series)\n","source":"_posts/gd-and-nm.md","raw":"---\ntitle: 再深入聊聊梯度下降和牛顿法\ndate: 2017-08-11 17:26:56\ntags: \n\t- unconstrained optimization\n\t- gradient descent\n\t- newton's method\ncategories: machine learning\n---\n上次我们一起聊到了gradient descent和newton's method，而且我们已经知道了gradient descent和newton's method都是convex optimization的好方法，这次我们就跳出convex optimization，从更大的unconstrained optimization角度来探讨下这两种方法之间的关联和区别。\n<!--more-->\n\n假设我们现有一个的optimization task，要求objective function \\\\(f(x)\\\\)的最小值，我们一般有两种方案：\n* 考虑到\\\\(f(x)\\\\)的最小值很有可能是全局最小值，那么我们可以通过寻找\\\\( \\nabla f(x)=0\\\\)的点来确定最小值，这就是**newton's method**的思想\n* 既然我们要寻找最小值，那我们可以顺着一条\\\\(f(x)\\\\)逐渐减小的路径，顺着这条路径一直走下去，直到不再变小，这就是**gradient descent**的思想\n\nOK，简单的叙述之后，我们开始正题！\n\n## 泰勒级数(Taylor series)\n首先我们需要回忆一下高等数学中重要的Taylor series，如果\\\\( f(x)\\\\)在点\\\\( x_0\\\\)的领域内具有\\\\(n+1\\\\)阶导数，那么，在该领域内，\\\\( f(x)\\\\)可展开成\\\\(n\\\\)阶Taylor series，忽略无限大次项的形式就是\n$$f(x)=f(x_0)+ \\nabla f(x_0)(x-x_0) + \\frac{ \\nabla ^2 f(x_0)}{2!}(x-x_0)^2 +...+\\frac{ \\nabla ^n f(x_0)}{n!}(x-x_0)^n $$\n其实在高等数学中学到Taylor series的时候，我本人是十分无感的，我并不知道这个东西到底有什么用处，相信很多人和我有相似的经历。\n\n> In mathematics, a Taylor series is a representation of a function as an infinite sum of terms that are calculated from the values of the function's derivatives at a single point.\n\n事实上，Taylor series所表现的是，对于\\\\( f(x)\\\\)在点\\\\( x_0\\\\)附近的一个估计，也可以理解为，根据\\\\( x_0\\\\)点处的各阶derivatives之和构成一个新的function，这个function就是对\\\\(f(x)\\\\)的逼近和拟合，而且这种逼近和拟合，随着Taylor series阶数增加而更接近于真实的\\\\(f(x)\\\\)。如果我们使用0阶Taylor series来逼近的话，那我们就粗暴的认为，\\\\( f(x)\\\\)在点\\\\( x_0\\\\)附近的值就都是\\\\(x_0\\\\)，这当然太粗暴直接了，哈哈。\n\n既然这太粗暴了，那么我们就用1st order Taylor series来做一个逼近和估计，这就是gradient descent的思想；如果我们用2nd order Taylor series来估计呢，那就成了newton's method了\n\nOK，我们继续娓娓道来。\n\n## 1st order Taylor series & gradient descent\n假设\\\\(x_k\\\\)是第k次gradient descent迭代后的\\\\(x\\\\)取值，那我们在此处的1st order Taylor series 就是\n$$f(x)=f(x_k)+ \\nabla f(x_k)(x-x_k)$$\n其中\\\\(x\\\\)是迭代的下一个方向，gradient descent的目标就是让\\\\(f(x)\\\\)达到局部甚至全局最小值，那么每一次迭代，也需要尽可能的减小更多以达到这个目的，那么\n$$f(x_k)-f(x)=- \\nabla f(x_k)(x-x_k)$$\n显然，上式应该尽可能的大，即**\\\\(- \\nabla f(x_k)(x-x_k)\\\\)越大越好**，我们现在把\\\\((x-x_k)\\\\)做一个替换，用单位向量\\\\(\\vec g\\\\)和标量\\\\( \\alpha\\\\)分别代表方向和大小，现在的任务就变成了\n$$ \\min \\nabla f(x_k)(x-x_k) = \\min( \\alpha \\vec{\\nabla f(x_k)}⋅ \\vec g)$$\n我们都知道，**对于两个向量来说，当他们方向相反时，他们的内积是最小的**。\n\n>梯度方向的定义是该点梯度在标量场增长最快的方向\n\n因此当\\\\(\\vec g\\\\)的方向是\\\\( \\vec{\\nabla f(x_k)}\\\\)的反方向时，上式可以取到最小值，于是就有\n$$x-x_k=- \\alpha \\nabla f(x_k)$$\n$$x:=x_k- \\alpha \\nabla f(x_k)$$\n到这一步，是不是看到了熟悉的gradient descent呢，yeah mate！We make it!\n## 2nd order Taylor series & newton's method\n和上面的gradient descent相似，假设\\\\(x_k\\\\)是第\\\\(k\\\\)次newton's method迭代后的\\\\(x\\\\)取值，那我们在此处的2nd order Taylor series 是\n$$f(x)=f(x_k)+ \\nabla f(x_k)(x-x_k) + \\frac{1}{2}(x-x_k)^T \\nabla^2 f(x_k)(x-x_k) $$\n我们对等号两边同时对\\\\(x\\\\)求导，并令其为零\n$$ \\nabla f(x) = \\nabla f(x_k) + (x-x_k) \\nabla^2 f(x_k)=0$$\n由于newton's method的原理就是通过\\\\(\\nabla f(x)=0\\\\)来寻找最小值，**故上式为零的解\\\\(x\\\\)其实就是newton's method在\\\\(k+1\\\\)次迭代后的新的\\\\(x\\\\)值**。其中\\\\(\\nabla f(x_k)\\\\)是\\\\(x_k\\\\)处的一阶导数，\\\\( \\nabla^2 f(x_k)\\\\)是\\\\(x_k\\\\)处的二阶导数Hessian矩阵元素\n\n我们令\\\\(\\nabla f(x_k)=g\\\\)，\\\\(\\nabla^2 f(x_k)=H\\\\)，则上式变成\n$$g+H(x-x_k)=0$$\n进一步的\n$$x=x_k-H^{-1}g$$\n由于\\\\(-g H^{-1} \\\\) 是优化的前进方向，在寻找最小值的过程中，这个方向一定是和梯度方向\\\\(g\\\\)相反才可以更快的下降，那么就有\\\\( g^T H^{-1} g > 0\\\\)，这不就是positive definite的定义吗？也就是说，**Hessian矩阵是positive definite的**。\n\n想象一下，如果Hessian是negative definite的话，参数更新的方向就成了和\\\\(g\\\\)相同的方向，newton's method将会发散，这一点，也是newton's method的缺点。在objective function是non-convex function的情况下，如果第\\\\(k\\\\)次迭代获得的\\\\(x_k\\\\)处的Hessian matrix negative definite，那么newton's method将会发散，从而导致不收敛。当然，为了解决这种问题，后续有改进的BFGS等方法，我们在这里暂时不详细讨论。\n## Sum up\n下面我们再来总结性质的对比一下两种方法，来看一张图\n![](http://otmy7guvn.bkt.clouddn.com/blog/2/2-1.png) \n事实上，这两种方法都采用了一种逼近和拟合的思想。假设现在处于迭代\\\\(k\\\\)次之后的\\\\(x_k\\\\)点，对于objective function，我们用\\\\(x_k\\\\)点的Taylor series \\\\(f(x)\\\\)来逼近和拟合，当然了，上图我们看到，gradient descent是用一次function而newton's method采用的是二次function，这是二者之间最显著的区别。\n\n对于new's method，在拟合之后，我们通过\\\\( \\nabla f(x)=0\\\\)求得的\\\\(x \\_{k+1}\\\\)点作为此次迭代的结果，下次迭代时候，又在\\\\(x \\_{k+1}\\\\)处次进行二次function的拟合，并如此迭代下去。\n\nNewton's method采用二次function来拟合，我们可以感性的理解为，newton's method在寻找下降的方向时候，关注的不仅仅是此处objective function value是不是减小(一阶value)，还关注此处value下降的趋势如何(二阶value)，而gradient descent只关心此处function value是不是减小，因此newton's method可以迭代更少次数获得最优解。对于标准二次型的objective function，newton's method甚至可以一次迭代就找到全局最小值。\n\n但是值得注意的是，上面所说的标准二次型function，实质上是convex function，在一般的unconstrained optimization中，更多的情况则是non-convex optimization，对于一般的non-convex optimization，newton's method是相对不稳定的，因为我们很难保证Hessian matrix的positive definite。鉴于此，我们会加入步长\\\\(\\lambda\\\\)限制，防止其一次迭代过大而带来迭代后Hessian matrix negative definite的情况，即\n$$x:=x- \\lambda H^{-1} g$$\n对于这种思想，我个人认为，是在整体non-convex function中寻找一个局部的convex function，通过步长将newton's method限制在这个局部中，最后收敛到局部最优中。由此可见，newton's mtehod在non-convex中受限制比较大。\n\n相比之下，由于gradient descent采用的一次function做拟合，只需要考虑沿着梯度反方向寻找最小值，因此gradient descent适用于各种场景，甚至是non-convex optimization，虽然不能保证是全局最优，但至少gradient descent是可以值得一试的方法。\n\n下面来总结一下：\n* Gradient descent 和 newton's method都是利用Taylor series对objective function进行拟合来实现迭代的；\n* Gradient descent 采用一次型function拟合而 newton's method采用的是二次型function，因此newton's method迭代更迅速；\n* Newton's method每次迭代都会计算Hessian matrix的逆，在高维feature情况下，这使得每次迭代会比较慢；\n* Newton's method在non-convex optimization中很受限制，而gradient descent则不受影响。\n\n好了，先写这么多，这其中的知识量还是很深奥的，也不知道自己有没有叙述明白，欢迎大家一起来讨论！\n\n**最后感谢优男的宝贵意见！**\n## Reference\n* [UCLA courseware](http://www.math.ucla.edu/~biskup/164.2.14f/PDFs/recursions.pdf)\n* [CCU courseware](https://www.cs.ccu.edu.tw/~wtchu/courses/2014s_OPT/Lectures/Chapter%209%20Newton%27s%20Method.pdf)\n* [Taylor series](https://en.wikipedia.org/wiki/Taylor_series)\n","slug":"gd-and-nm","published":1,"updated":"2017-09-13T07:30:17.667Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cj8lez53c0008w2kz5rydfcuf","content":"<p>上次我们一起聊到了gradient descent和newton’s method，而且我们已经知道了gradient descent和newton’s method都是convex optimization的好方法，这次我们就跳出convex optimization，从更大的unconstrained optimization角度来探讨下这两种方法之间的关联和区别。<br><a id=\"more\"></a></p>\n<p>假设我们现有一个的optimization task，要求objective function \\(f(x)\\)的最小值，我们一般有两种方案：</p>\n<ul>\n<li>考虑到\\(f(x)\\)的最小值很有可能是全局最小值，那么我们可以通过寻找\\( \\nabla f(x)=0\\)的点来确定最小值，这就是<strong>newton’s method</strong>的思想</li>\n<li>既然我们要寻找最小值，那我们可以顺着一条\\(f(x)\\)逐渐减小的路径，顺着这条路径一直走下去，直到不再变小，这就是<strong>gradient descent</strong>的思想</li>\n</ul>\n<p>OK，简单的叙述之后，我们开始正题！</p>\n<h2 id=\"泰勒级数-Taylor-series\"><a href=\"#泰勒级数-Taylor-series\" class=\"headerlink\" title=\"泰勒级数(Taylor series)\"></a>泰勒级数(Taylor series)</h2><p>首先我们需要回忆一下高等数学中重要的Taylor series，如果\\( f(x)\\)在点\\( x_0\\)的领域内具有\\(n+1\\)阶导数，那么，在该领域内，\\( f(x)\\)可展开成\\(n\\)阶Taylor series，忽略无限大次项的形式就是</p>\n<script type=\"math/tex; mode=display\">f(x)=f(x_0)+ \\nabla f(x_0)(x-x_0) + \\frac{ \\nabla ^2 f(x_0)}{2!}(x-x_0)^2 +...+\\frac{ \\nabla ^n f(x_0)}{n!}(x-x_0)^n</script><p>其实在高等数学中学到Taylor series的时候，我本人是十分无感的，我并不知道这个东西到底有什么用处，相信很多人和我有相似的经历。</p>\n<blockquote>\n<p>In mathematics, a Taylor series is a representation of a function as an infinite sum of terms that are calculated from the values of the function’s derivatives at a single point.</p>\n</blockquote>\n<p>事实上，Taylor series所表现的是，对于\\( f(x)\\)在点\\( x_0\\)附近的一个估计，也可以理解为，根据\\( x_0\\)点处的各阶derivatives之和构成一个新的function，这个function就是对\\(f(x)\\)的逼近和拟合，而且这种逼近和拟合，随着Taylor series阶数增加而更接近于真实的\\(f(x)\\)。如果我们使用0阶Taylor series来逼近的话，那我们就粗暴的认为，\\( f(x)\\)在点\\( x_0\\)附近的值就都是\\(x_0\\)，这当然太粗暴直接了，哈哈。</p>\n<p>既然这太粗暴了，那么我们就用1st order Taylor series来做一个逼近和估计，这就是gradient descent的思想；如果我们用2nd order Taylor series来估计呢，那就成了newton’s method了</p>\n<p>OK，我们继续娓娓道来。</p>\n<h2 id=\"1st-order-Taylor-series-amp-gradient-descent\"><a href=\"#1st-order-Taylor-series-amp-gradient-descent\" class=\"headerlink\" title=\"1st order Taylor series &amp; gradient descent\"></a>1st order Taylor series &amp; gradient descent</h2><p>假设\\(x_k\\)是第k次gradient descent迭代后的\\(x\\)取值，那我们在此处的1st order Taylor series 就是</p>\n<script type=\"math/tex; mode=display\">f(x)=f(x_k)+ \\nabla f(x_k)(x-x_k)</script><p>其中\\(x\\)是迭代的下一个方向，gradient descent的目标就是让\\(f(x)\\)达到局部甚至全局最小值，那么每一次迭代，也需要尽可能的减小更多以达到这个目的，那么</p>\n<script type=\"math/tex; mode=display\">f(x_k)-f(x)=- \\nabla f(x_k)(x-x_k)</script><p>显然，上式应该尽可能的大，即<strong>\\(- \\nabla f(x_k)(x-x_k)\\)越大越好</strong>，我们现在把\\((x-x_k)\\)做一个替换，用单位向量\\(\\vec g\\)和标量\\( \\alpha\\)分别代表方向和大小，现在的任务就变成了</p>\n<script type=\"math/tex; mode=display\">\\min \\nabla f(x_k)(x-x_k) = \\min( \\alpha \\vec{\\nabla f(x_k)}⋅ \\vec g)</script><p>我们都知道，<strong>对于两个向量来说，当他们方向相反时，他们的内积是最小的</strong>。</p>\n<blockquote>\n<p>梯度方向的定义是该点梯度在标量场增长最快的方向</p>\n</blockquote>\n<p>因此当\\(\\vec g\\)的方向是\\( \\vec{\\nabla f(x_k)}\\)的反方向时，上式可以取到最小值，于是就有</p>\n<script type=\"math/tex; mode=display\">x-x_k=- \\alpha \\nabla f(x_k)</script><script type=\"math/tex; mode=display\">x:=x_k- \\alpha \\nabla f(x_k)</script><p>到这一步，是不是看到了熟悉的gradient descent呢，yeah mate！We make it!</p>\n<h2 id=\"2nd-order-Taylor-series-amp-newton’s-method\"><a href=\"#2nd-order-Taylor-series-amp-newton’s-method\" class=\"headerlink\" title=\"2nd order Taylor series &amp; newton’s method\"></a>2nd order Taylor series &amp; newton’s method</h2><p>和上面的gradient descent相似，假设\\(x_k\\)是第\\(k\\)次newton’s method迭代后的\\(x\\)取值，那我们在此处的2nd order Taylor series 是</p>\n<script type=\"math/tex; mode=display\">f(x)=f(x_k)+ \\nabla f(x_k)(x-x_k) + \\frac{1}{2}(x-x_k)^T \\nabla^2 f(x_k)(x-x_k)</script><p>我们对等号两边同时对\\(x\\)求导，并令其为零</p>\n<script type=\"math/tex; mode=display\">\\nabla f(x) = \\nabla f(x_k) + (x-x_k) \\nabla^2 f(x_k)=0</script><p>由于newton’s method的原理就是通过\\(\\nabla f(x)=0\\)来寻找最小值，<strong>故上式为零的解\\(x\\)其实就是newton’s method在\\(k+1\\)次迭代后的新的\\(x\\)值</strong>。其中\\(\\nabla f(x_k)\\)是\\(x_k\\)处的一阶导数，\\( \\nabla^2 f(x_k)\\)是\\(x_k\\)处的二阶导数Hessian矩阵元素</p>\n<p>我们令\\(\\nabla f(x_k)=g\\)，\\(\\nabla^2 f(x_k)=H\\)，则上式变成</p>\n<script type=\"math/tex; mode=display\">g+H(x-x_k)=0</script><p>进一步的</p>\n<script type=\"math/tex; mode=display\">x=x_k-H^{-1}g</script><p>由于\\(-g H^{-1} \\) 是优化的前进方向，在寻找最小值的过程中，这个方向一定是和梯度方向\\(g\\)相反才可以更快的下降，那么就有\\( g^T H^{-1} g &gt; 0\\)，这不就是positive definite的定义吗？也就是说，<strong>Hessian矩阵是positive definite的</strong>。</p>\n<p>想象一下，如果Hessian是negative definite的话，参数更新的方向就成了和\\(g\\)相同的方向，newton’s method将会发散，这一点，也是newton’s method的缺点。在objective function是non-convex function的情况下，如果第\\(k\\)次迭代获得的\\(x_k\\)处的Hessian matrix negative definite，那么newton’s method将会发散，从而导致不收敛。当然，为了解决这种问题，后续有改进的BFGS等方法，我们在这里暂时不详细讨论。</p>\n<h2 id=\"Sum-up\"><a href=\"#Sum-up\" class=\"headerlink\" title=\"Sum up\"></a>Sum up</h2><p>下面我们再来总结性质的对比一下两种方法，来看一张图<br><img src=\"http://otmy7guvn.bkt.clouddn.com/blog/2/2-1.png\" alt=\"\"><br>事实上，这两种方法都采用了一种逼近和拟合的思想。假设现在处于迭代\\(k\\)次之后的\\(x_k\\)点，对于objective function，我们用\\(x_k\\)点的Taylor series \\(f(x)\\)来逼近和拟合，当然了，上图我们看到，gradient descent是用一次function而newton’s method采用的是二次function，这是二者之间最显著的区别。</p>\n<p>对于new’s method，在拟合之后，我们通过\\( \\nabla f(x)=0\\)求得的\\(x _{k+1}\\)点作为此次迭代的结果，下次迭代时候，又在\\(x _{k+1}\\)处次进行二次function的拟合，并如此迭代下去。</p>\n<p>Newton’s method采用二次function来拟合，我们可以感性的理解为，newton’s method在寻找下降的方向时候，关注的不仅仅是此处objective function value是不是减小(一阶value)，还关注此处value下降的趋势如何(二阶value)，而gradient descent只关心此处function value是不是减小，因此newton’s method可以迭代更少次数获得最优解。对于标准二次型的objective function，newton’s method甚至可以一次迭代就找到全局最小值。</p>\n<p>但是值得注意的是，上面所说的标准二次型function，实质上是convex function，在一般的unconstrained optimization中，更多的情况则是non-convex optimization，对于一般的non-convex optimization，newton’s method是相对不稳定的，因为我们很难保证Hessian matrix的positive definite。鉴于此，我们会加入步长\\(\\lambda\\)限制，防止其一次迭代过大而带来迭代后Hessian matrix negative definite的情况，即</p>\n<script type=\"math/tex; mode=display\">x:=x- \\lambda H^{-1} g</script><p>对于这种思想，我个人认为，是在整体non-convex function中寻找一个局部的convex function，通过步长将newton’s method限制在这个局部中，最后收敛到局部最优中。由此可见，newton’s mtehod在non-convex中受限制比较大。</p>\n<p>相比之下，由于gradient descent采用的一次function做拟合，只需要考虑沿着梯度反方向寻找最小值，因此gradient descent适用于各种场景，甚至是non-convex optimization，虽然不能保证是全局最优，但至少gradient descent是可以值得一试的方法。</p>\n<p>下面来总结一下：</p>\n<ul>\n<li>Gradient descent 和 newton’s method都是利用Taylor series对objective function进行拟合来实现迭代的；</li>\n<li>Gradient descent 采用一次型function拟合而 newton’s method采用的是二次型function，因此newton’s method迭代更迅速；</li>\n<li>Newton’s method每次迭代都会计算Hessian matrix的逆，在高维feature情况下，这使得每次迭代会比较慢；</li>\n<li>Newton’s method在non-convex optimization中很受限制，而gradient descent则不受影响。</li>\n</ul>\n<p>好了，先写这么多，这其中的知识量还是很深奥的，也不知道自己有没有叙述明白，欢迎大家一起来讨论！</p>\n<p><strong>最后感谢优男的宝贵意见！</strong></p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ul>\n<li><a href=\"http://www.math.ucla.edu/~biskup/164.2.14f/PDFs/recursions.pdf\" target=\"_blank\" rel=\"external\">UCLA courseware</a></li>\n<li><a href=\"https://www.cs.ccu.edu.tw/~wtchu/courses/2014s_OPT/Lectures/Chapter%209%20Newton%27s%20Method.pdf\" target=\"_blank\" rel=\"external\">CCU courseware</a></li>\n<li><a href=\"https://en.wikipedia.org/wiki/Taylor_series\" target=\"_blank\" rel=\"external\">Taylor series</a></li>\n</ul>\n","site":{"data":{}},"excerpt":"<p>上次我们一起聊到了gradient descent和newton’s method，而且我们已经知道了gradient descent和newton’s method都是convex optimization的好方法，这次我们就跳出convex optimization，从更大的unconstrained optimization角度来探讨下这两种方法之间的关联和区别。<br>","more":"</p>\n<p>假设我们现有一个的optimization task，要求objective function \\(f(x)\\)的最小值，我们一般有两种方案：</p>\n<ul>\n<li>考虑到\\(f(x)\\)的最小值很有可能是全局最小值，那么我们可以通过寻找\\( \\nabla f(x)=0\\)的点来确定最小值，这就是<strong>newton’s method</strong>的思想</li>\n<li>既然我们要寻找最小值，那我们可以顺着一条\\(f(x)\\)逐渐减小的路径，顺着这条路径一直走下去，直到不再变小，这就是<strong>gradient descent</strong>的思想</li>\n</ul>\n<p>OK，简单的叙述之后，我们开始正题！</p>\n<h2 id=\"泰勒级数-Taylor-series\"><a href=\"#泰勒级数-Taylor-series\" class=\"headerlink\" title=\"泰勒级数(Taylor series)\"></a>泰勒级数(Taylor series)</h2><p>首先我们需要回忆一下高等数学中重要的Taylor series，如果\\( f(x)\\)在点\\( x_0\\)的领域内具有\\(n+1\\)阶导数，那么，在该领域内，\\( f(x)\\)可展开成\\(n\\)阶Taylor series，忽略无限大次项的形式就是</p>\n<script type=\"math/tex; mode=display\">f(x)=f(x_0)+ \\nabla f(x_0)(x-x_0) + \\frac{ \\nabla ^2 f(x_0)}{2!}(x-x_0)^2 +...+\\frac{ \\nabla ^n f(x_0)}{n!}(x-x_0)^n</script><p>其实在高等数学中学到Taylor series的时候，我本人是十分无感的，我并不知道这个东西到底有什么用处，相信很多人和我有相似的经历。</p>\n<blockquote>\n<p>In mathematics, a Taylor series is a representation of a function as an infinite sum of terms that are calculated from the values of the function’s derivatives at a single point.</p>\n</blockquote>\n<p>事实上，Taylor series所表现的是，对于\\( f(x)\\)在点\\( x_0\\)附近的一个估计，也可以理解为，根据\\( x_0\\)点处的各阶derivatives之和构成一个新的function，这个function就是对\\(f(x)\\)的逼近和拟合，而且这种逼近和拟合，随着Taylor series阶数增加而更接近于真实的\\(f(x)\\)。如果我们使用0阶Taylor series来逼近的话，那我们就粗暴的认为，\\( f(x)\\)在点\\( x_0\\)附近的值就都是\\(x_0\\)，这当然太粗暴直接了，哈哈。</p>\n<p>既然这太粗暴了，那么我们就用1st order Taylor series来做一个逼近和估计，这就是gradient descent的思想；如果我们用2nd order Taylor series来估计呢，那就成了newton’s method了</p>\n<p>OK，我们继续娓娓道来。</p>\n<h2 id=\"1st-order-Taylor-series-amp-gradient-descent\"><a href=\"#1st-order-Taylor-series-amp-gradient-descent\" class=\"headerlink\" title=\"1st order Taylor series &amp; gradient descent\"></a>1st order Taylor series &amp; gradient descent</h2><p>假设\\(x_k\\)是第k次gradient descent迭代后的\\(x\\)取值，那我们在此处的1st order Taylor series 就是</p>\n<script type=\"math/tex; mode=display\">f(x)=f(x_k)+ \\nabla f(x_k)(x-x_k)</script><p>其中\\(x\\)是迭代的下一个方向，gradient descent的目标就是让\\(f(x)\\)达到局部甚至全局最小值，那么每一次迭代，也需要尽可能的减小更多以达到这个目的，那么</p>\n<script type=\"math/tex; mode=display\">f(x_k)-f(x)=- \\nabla f(x_k)(x-x_k)</script><p>显然，上式应该尽可能的大，即<strong>\\(- \\nabla f(x_k)(x-x_k)\\)越大越好</strong>，我们现在把\\((x-x_k)\\)做一个替换，用单位向量\\(\\vec g\\)和标量\\( \\alpha\\)分别代表方向和大小，现在的任务就变成了</p>\n<script type=\"math/tex; mode=display\">\\min \\nabla f(x_k)(x-x_k) = \\min( \\alpha \\vec{\\nabla f(x_k)}⋅ \\vec g)</script><p>我们都知道，<strong>对于两个向量来说，当他们方向相反时，他们的内积是最小的</strong>。</p>\n<blockquote>\n<p>梯度方向的定义是该点梯度在标量场增长最快的方向</p>\n</blockquote>\n<p>因此当\\(\\vec g\\)的方向是\\( \\vec{\\nabla f(x_k)}\\)的反方向时，上式可以取到最小值，于是就有</p>\n<script type=\"math/tex; mode=display\">x-x_k=- \\alpha \\nabla f(x_k)</script><script type=\"math/tex; mode=display\">x:=x_k- \\alpha \\nabla f(x_k)</script><p>到这一步，是不是看到了熟悉的gradient descent呢，yeah mate！We make it!</p>\n<h2 id=\"2nd-order-Taylor-series-amp-newton’s-method\"><a href=\"#2nd-order-Taylor-series-amp-newton’s-method\" class=\"headerlink\" title=\"2nd order Taylor series &amp; newton’s method\"></a>2nd order Taylor series &amp; newton’s method</h2><p>和上面的gradient descent相似，假设\\(x_k\\)是第\\(k\\)次newton’s method迭代后的\\(x\\)取值，那我们在此处的2nd order Taylor series 是</p>\n<script type=\"math/tex; mode=display\">f(x)=f(x_k)+ \\nabla f(x_k)(x-x_k) + \\frac{1}{2}(x-x_k)^T \\nabla^2 f(x_k)(x-x_k)</script><p>我们对等号两边同时对\\(x\\)求导，并令其为零</p>\n<script type=\"math/tex; mode=display\">\\nabla f(x) = \\nabla f(x_k) + (x-x_k) \\nabla^2 f(x_k)=0</script><p>由于newton’s method的原理就是通过\\(\\nabla f(x)=0\\)来寻找最小值，<strong>故上式为零的解\\(x\\)其实就是newton’s method在\\(k+1\\)次迭代后的新的\\(x\\)值</strong>。其中\\(\\nabla f(x_k)\\)是\\(x_k\\)处的一阶导数，\\( \\nabla^2 f(x_k)\\)是\\(x_k\\)处的二阶导数Hessian矩阵元素</p>\n<p>我们令\\(\\nabla f(x_k)=g\\)，\\(\\nabla^2 f(x_k)=H\\)，则上式变成</p>\n<script type=\"math/tex; mode=display\">g+H(x-x_k)=0</script><p>进一步的</p>\n<script type=\"math/tex; mode=display\">x=x_k-H^{-1}g</script><p>由于\\(-g H^{-1} \\) 是优化的前进方向，在寻找最小值的过程中，这个方向一定是和梯度方向\\(g\\)相反才可以更快的下降，那么就有\\( g^T H^{-1} g &gt; 0\\)，这不就是positive definite的定义吗？也就是说，<strong>Hessian矩阵是positive definite的</strong>。</p>\n<p>想象一下，如果Hessian是negative definite的话，参数更新的方向就成了和\\(g\\)相同的方向，newton’s method将会发散，这一点，也是newton’s method的缺点。在objective function是non-convex function的情况下，如果第\\(k\\)次迭代获得的\\(x_k\\)处的Hessian matrix negative definite，那么newton’s method将会发散，从而导致不收敛。当然，为了解决这种问题，后续有改进的BFGS等方法，我们在这里暂时不详细讨论。</p>\n<h2 id=\"Sum-up\"><a href=\"#Sum-up\" class=\"headerlink\" title=\"Sum up\"></a>Sum up</h2><p>下面我们再来总结性质的对比一下两种方法，来看一张图<br><img src=\"http://otmy7guvn.bkt.clouddn.com/blog/2/2-1.png\" alt=\"\"><br>事实上，这两种方法都采用了一种逼近和拟合的思想。假设现在处于迭代\\(k\\)次之后的\\(x_k\\)点，对于objective function，我们用\\(x_k\\)点的Taylor series \\(f(x)\\)来逼近和拟合，当然了，上图我们看到，gradient descent是用一次function而newton’s method采用的是二次function，这是二者之间最显著的区别。</p>\n<p>对于new’s method，在拟合之后，我们通过\\( \\nabla f(x)=0\\)求得的\\(x _{k+1}\\)点作为此次迭代的结果，下次迭代时候，又在\\(x _{k+1}\\)处次进行二次function的拟合，并如此迭代下去。</p>\n<p>Newton’s method采用二次function来拟合，我们可以感性的理解为，newton’s method在寻找下降的方向时候，关注的不仅仅是此处objective function value是不是减小(一阶value)，还关注此处value下降的趋势如何(二阶value)，而gradient descent只关心此处function value是不是减小，因此newton’s method可以迭代更少次数获得最优解。对于标准二次型的objective function，newton’s method甚至可以一次迭代就找到全局最小值。</p>\n<p>但是值得注意的是，上面所说的标准二次型function，实质上是convex function，在一般的unconstrained optimization中，更多的情况则是non-convex optimization，对于一般的non-convex optimization，newton’s method是相对不稳定的，因为我们很难保证Hessian matrix的positive definite。鉴于此，我们会加入步长\\(\\lambda\\)限制，防止其一次迭代过大而带来迭代后Hessian matrix negative definite的情况，即</p>\n<script type=\"math/tex; mode=display\">x:=x- \\lambda H^{-1} g</script><p>对于这种思想，我个人认为，是在整体non-convex function中寻找一个局部的convex function，通过步长将newton’s method限制在这个局部中，最后收敛到局部最优中。由此可见，newton’s mtehod在non-convex中受限制比较大。</p>\n<p>相比之下，由于gradient descent采用的一次function做拟合，只需要考虑沿着梯度反方向寻找最小值，因此gradient descent适用于各种场景，甚至是non-convex optimization，虽然不能保证是全局最优，但至少gradient descent是可以值得一试的方法。</p>\n<p>下面来总结一下：</p>\n<ul>\n<li>Gradient descent 和 newton’s method都是利用Taylor series对objective function进行拟合来实现迭代的；</li>\n<li>Gradient descent 采用一次型function拟合而 newton’s method采用的是二次型function，因此newton’s method迭代更迅速；</li>\n<li>Newton’s method每次迭代都会计算Hessian matrix的逆，在高维feature情况下，这使得每次迭代会比较慢；</li>\n<li>Newton’s method在non-convex optimization中很受限制，而gradient descent则不受影响。</li>\n</ul>\n<p>好了，先写这么多，这其中的知识量还是很深奥的，也不知道自己有没有叙述明白，欢迎大家一起来讨论！</p>\n<p><strong>最后感谢优男的宝贵意见！</strong></p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ul>\n<li><a href=\"http://www.math.ucla.edu/~biskup/164.2.14f/PDFs/recursions.pdf\" target=\"_blank\" rel=\"external\">UCLA courseware</a></li>\n<li><a href=\"https://www.cs.ccu.edu.tw/~wtchu/courses/2014s_OPT/Lectures/Chapter%209%20Newton%27s%20Method.pdf\" target=\"_blank\" rel=\"external\">CCU courseware</a></li>\n<li><a href=\"https://en.wikipedia.org/wiki/Taylor_series\" target=\"_blank\" rel=\"external\">Taylor series</a></li>\n</ul>"},{"title":"Reading Notes-Class Imbalance, Redux","date":"2017-09-10T05:21:56.000Z","_content":"再次感谢优男，向我提出了又一个尖锐的问题，使得我有机会思考和研究，并且最终可以看到这篇paper，并且最后可以分享给大家。\n\n我个人在工作之中遇到过imbalanced data的问题，我只是直观的感受到，imbalanced data的最后效果往往不是很棒，网上也只是给出了oversampling和undersampling的建议，并没有提及这其中的一些缘故，今天我们一起通过这篇paper来学习学习。\n<!--more-->\n## Notes\n我们假设有positive和negative两类sample，其中positive samples符合\\\\(P(x)\\\\)的Guassian分布，negative samples符合\\\\(G(x)\\\\)的Guassian分布，分类平面将空间划分成positive region\\\\(\\cal R^{+} \\_{w}\\\\)和negative region\\\\(\\cal R^{-} \\_{w}\\\\)，如下图所示：\n![](http://otmy7guvn.bkt.clouddn.com/blog/5/5-1.png) \n图中\\\\(w^{ \\*}\\\\)是理想的分割平面，\\\\(w^{ \\*}\\\\) 应该是使loss最小的取值，即\n$$w^{*}= \\arg\\underset{w}{\\min} \\cal L^{*}(w)$$\n对于loss值，其实就是分类中被错分的fn(false negative)和fp(false positive)的期望值，显然，通过minimun该loss得到的会是图中的\\\\(w^{*}\\\\)，因为这个分类平面所带来的error明显是最少的。\n$$\\cal L^{*}(w) = \\cal C_{fn} \\int _{\\cal R^{w} _{-}} \\it P(x)dx + \\cal C_{fp} \\int _{\\cal R^{w} _{+}} \\it G(x)dx$$\n对于整个数据集\\\\(\\cal D \\\\)来说，我们假设数据量较少的一类(paper中设定positive类较少)所占比例为\\\\(\\pi\\\\)(小于0.5)，那么对于带有比例\\\\(\\pi\\\\)的数据集\\\\(\\cal D_{\\pi}\\\\)，全局期望是\n$$\\bf E_{\\cal D_{ \\pi}} [\\cal L(w)]=\\pi \\cal C_{fn} \\int _{\\cal R^{w} _{-}} \\it P(x)dx + (1- \\pi) \\cal C_{fp} \\int _{\\cal R^{w} _{+}} \\it G(x)dx$$\n此处，我个人的理解是，在两类数据均衡的情况下，全局情况下的期望其实是和上面的loss等价的，但是imbalanced data带来了不均衡的因子\\\\(\\pi\\\\)，因此，两个公式不再等价。\n\nOK，既然不等价，那么问题就来了，paper上说，通过最小化全局期望获得的\\\\(\\hat w\\\\)，是向着较少数量类别的样本倾斜，也就是第一幅图中，向较少的postive那边skewed，原因是因为\\\\( \\cal R \\_{+} ^{ \\hat w} < \\cal R \\_{+} ^{w^{\\*}}\\\\), 也就是说，\\\\(\\hat w\\\\)分割的positive region面积小于\\\\(w^{\\*}\\\\)分割出的面积，面积的减小势必导致分割平面向positive类别方向偏移。\n\n遗憾的是，关于面积的证明我实在看不明白，也email了一些人，也没有得到一个满意的答案，如果有朋友看明白了的话，**记得留言或者email我！**\n\n到了这里，paper大概介绍了undersampling的裨益，undersampling的核心其实就是消除前面提到的比例\\\\(\\pi\\\\)，让它趋近于0.5后，分类平面\\\\(\\hat w\\\\)就会趋近于理想分类平面\\\\(w^{*}\\\\)。\n\n这里，作者提出了一个bagging方法，就是多次做undersampling，最后最结果做bagging可以获得更好的效果，如下图\n![](http://otmy7guvn.bkt.clouddn.com/blog/5/5-2.png) \npaper还对比了其他的方法，比如Weighted Empirical Cost Minimization(如weighted SVM)和SMOTE方法效果不如bagging undersampling，我上一幅图说明下SMOTE的缺点，更多细节，大家可以详细看看paper，如图：\n![](http://otmy7guvn.bkt.clouddn.com/blog/5/5-3.png) \nSMOTE方法是随机选择方向生成新的sample，但是如果新的sample产生了图中位置，则效果不会很好。\n\nOK，今天就这么多，记得看明白了中间的推导一起分享啊！\n## Reference\n* [Wallace, Byron C., et al. \"Class imbalance, redux.\" Data Mining (ICDM), 2011 IEEE 11th International Conference on. IEEE, 2011.](https://pdfs.semanticscholar.org/a8ef/5a810099178b70d1490a4e6fc4426b642cde.pdf)\n* [PPT-Class Imbalance, Redux](https://www.google.co.jp/url?sa=t&rct=j&q=&esrc=s&source=web&cd=3&cad=rja&uact=8&ved=0ahUKEwiVmqXlkprWAhVEsJQKHYJiCf4QFgg8MAI&url=https%3A%2F%2Fcourse.ccs.neu.edu%2Fcs6140sp15%2F4_boosting%2Fslides%2Fwallace_imbalance_icdm_11_for_class_2012_final.pptx&usg=AFQjCNG6GpjKeinzCsXrZWWY1edtbBMgog)","source":"_posts/paper-imbalance.md","raw":"---\ntitle: Reading Notes-Class Imbalance, Redux\ndate: 2017-09-10 13:21:56\ntags: \n\t- imbalanced data\n\t- undersampling\n\t- bagging\ncategories: reading notes\n---\n再次感谢优男，向我提出了又一个尖锐的问题，使得我有机会思考和研究，并且最终可以看到这篇paper，并且最后可以分享给大家。\n\n我个人在工作之中遇到过imbalanced data的问题，我只是直观的感受到，imbalanced data的最后效果往往不是很棒，网上也只是给出了oversampling和undersampling的建议，并没有提及这其中的一些缘故，今天我们一起通过这篇paper来学习学习。\n<!--more-->\n## Notes\n我们假设有positive和negative两类sample，其中positive samples符合\\\\(P(x)\\\\)的Guassian分布，negative samples符合\\\\(G(x)\\\\)的Guassian分布，分类平面将空间划分成positive region\\\\(\\cal R^{+} \\_{w}\\\\)和negative region\\\\(\\cal R^{-} \\_{w}\\\\)，如下图所示：\n![](http://otmy7guvn.bkt.clouddn.com/blog/5/5-1.png) \n图中\\\\(w^{ \\*}\\\\)是理想的分割平面，\\\\(w^{ \\*}\\\\) 应该是使loss最小的取值，即\n$$w^{*}= \\arg\\underset{w}{\\min} \\cal L^{*}(w)$$\n对于loss值，其实就是分类中被错分的fn(false negative)和fp(false positive)的期望值，显然，通过minimun该loss得到的会是图中的\\\\(w^{*}\\\\)，因为这个分类平面所带来的error明显是最少的。\n$$\\cal L^{*}(w) = \\cal C_{fn} \\int _{\\cal R^{w} _{-}} \\it P(x)dx + \\cal C_{fp} \\int _{\\cal R^{w} _{+}} \\it G(x)dx$$\n对于整个数据集\\\\(\\cal D \\\\)来说，我们假设数据量较少的一类(paper中设定positive类较少)所占比例为\\\\(\\pi\\\\)(小于0.5)，那么对于带有比例\\\\(\\pi\\\\)的数据集\\\\(\\cal D_{\\pi}\\\\)，全局期望是\n$$\\bf E_{\\cal D_{ \\pi}} [\\cal L(w)]=\\pi \\cal C_{fn} \\int _{\\cal R^{w} _{-}} \\it P(x)dx + (1- \\pi) \\cal C_{fp} \\int _{\\cal R^{w} _{+}} \\it G(x)dx$$\n此处，我个人的理解是，在两类数据均衡的情况下，全局情况下的期望其实是和上面的loss等价的，但是imbalanced data带来了不均衡的因子\\\\(\\pi\\\\)，因此，两个公式不再等价。\n\nOK，既然不等价，那么问题就来了，paper上说，通过最小化全局期望获得的\\\\(\\hat w\\\\)，是向着较少数量类别的样本倾斜，也就是第一幅图中，向较少的postive那边skewed，原因是因为\\\\( \\cal R \\_{+} ^{ \\hat w} < \\cal R \\_{+} ^{w^{\\*}}\\\\), 也就是说，\\\\(\\hat w\\\\)分割的positive region面积小于\\\\(w^{\\*}\\\\)分割出的面积，面积的减小势必导致分割平面向positive类别方向偏移。\n\n遗憾的是，关于面积的证明我实在看不明白，也email了一些人，也没有得到一个满意的答案，如果有朋友看明白了的话，**记得留言或者email我！**\n\n到了这里，paper大概介绍了undersampling的裨益，undersampling的核心其实就是消除前面提到的比例\\\\(\\pi\\\\)，让它趋近于0.5后，分类平面\\\\(\\hat w\\\\)就会趋近于理想分类平面\\\\(w^{*}\\\\)。\n\n这里，作者提出了一个bagging方法，就是多次做undersampling，最后最结果做bagging可以获得更好的效果，如下图\n![](http://otmy7guvn.bkt.clouddn.com/blog/5/5-2.png) \npaper还对比了其他的方法，比如Weighted Empirical Cost Minimization(如weighted SVM)和SMOTE方法效果不如bagging undersampling，我上一幅图说明下SMOTE的缺点，更多细节，大家可以详细看看paper，如图：\n![](http://otmy7guvn.bkt.clouddn.com/blog/5/5-3.png) \nSMOTE方法是随机选择方向生成新的sample，但是如果新的sample产生了图中位置，则效果不会很好。\n\nOK，今天就这么多，记得看明白了中间的推导一起分享啊！\n## Reference\n* [Wallace, Byron C., et al. \"Class imbalance, redux.\" Data Mining (ICDM), 2011 IEEE 11th International Conference on. IEEE, 2011.](https://pdfs.semanticscholar.org/a8ef/5a810099178b70d1490a4e6fc4426b642cde.pdf)\n* [PPT-Class Imbalance, Redux](https://www.google.co.jp/url?sa=t&rct=j&q=&esrc=s&source=web&cd=3&cad=rja&uact=8&ved=0ahUKEwiVmqXlkprWAhVEsJQKHYJiCf4QFgg8MAI&url=https%3A%2F%2Fcourse.ccs.neu.edu%2Fcs6140sp15%2F4_boosting%2Fslides%2Fwallace_imbalance_icdm_11_for_class_2012_final.pptx&usg=AFQjCNG6GpjKeinzCsXrZWWY1edtbBMgog)","slug":"paper-imbalance","published":1,"updated":"2017-09-11T00:55:31.135Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cj8lez53f0009w2kzqw2rexux","content":"<p>再次感谢优男，向我提出了又一个尖锐的问题，使得我有机会思考和研究，并且最终可以看到这篇paper，并且最后可以分享给大家。</p>\n<p>我个人在工作之中遇到过imbalanced data的问题，我只是直观的感受到，imbalanced data的最后效果往往不是很棒，网上也只是给出了oversampling和undersampling的建议，并没有提及这其中的一些缘故，今天我们一起通过这篇paper来学习学习。<br><a id=\"more\"></a></p>\n<h2 id=\"Notes\"><a href=\"#Notes\" class=\"headerlink\" title=\"Notes\"></a>Notes</h2><p>我们假设有positive和negative两类sample，其中positive samples符合\\(P(x)\\)的Guassian分布，negative samples符合\\(G(x)\\)的Guassian分布，分类平面将空间划分成positive region\\(\\cal R^{+} _{w}\\)和negative region\\(\\cal R^{-} _{w}\\)，如下图所示：<br><img src=\"http://otmy7guvn.bkt.clouddn.com/blog/5/5-1.png\" alt=\"\"><br>图中\\(w^{ *}\\)是理想的分割平面，\\(w^{ *}\\) 应该是使loss最小的取值，即</p>\n<script type=\"math/tex; mode=display\">w^{*}= \\arg\\underset{w}{\\min} \\cal L^{*}(w)</script><p>对于loss值，其实就是分类中被错分的fn(false negative)和fp(false positive)的期望值，显然，通过minimun该loss得到的会是图中的\\(w^{*}\\)，因为这个分类平面所带来的error明显是最少的。</p>\n<script type=\"math/tex; mode=display\">\\cal L^{*}(w) = \\cal C_{fn} \\int _{\\cal R^{w} _{-}} \\it P(x)dx + \\cal C_{fp} \\int _{\\cal R^{w} _{+}} \\it G(x)dx</script><p>对于整个数据集\\(\\cal D \\)来说，我们假设数据量较少的一类(paper中设定positive类较少)所占比例为\\(\\pi\\)(小于0.5)，那么对于带有比例\\(\\pi\\)的数据集\\(\\cal D_{\\pi}\\)，全局期望是</p>\n<script type=\"math/tex; mode=display\">\\bf E_{\\cal D_{ \\pi}} [\\cal L(w)]=\\pi \\cal C_{fn} \\int _{\\cal R^{w} _{-}} \\it P(x)dx + (1- \\pi) \\cal C_{fp} \\int _{\\cal R^{w} _{+}} \\it G(x)dx</script><p>此处，我个人的理解是，在两类数据均衡的情况下，全局情况下的期望其实是和上面的loss等价的，但是imbalanced data带来了不均衡的因子\\(\\pi\\)，因此，两个公式不再等价。</p>\n<p>OK，既然不等价，那么问题就来了，paper上说，通过最小化全局期望获得的\\(\\hat w\\)，是向着较少数量类别的样本倾斜，也就是第一幅图中，向较少的postive那边skewed，原因是因为\\( \\cal R _{+} ^{ \\hat w} &lt; \\cal R _{+} ^{w^{*}}\\), 也就是说，\\(\\hat w\\)分割的positive region面积小于\\(w^{*}\\)分割出的面积，面积的减小势必导致分割平面向positive类别方向偏移。</p>\n<p>遗憾的是，关于面积的证明我实在看不明白，也email了一些人，也没有得到一个满意的答案，如果有朋友看明白了的话，<strong>记得留言或者email我！</strong></p>\n<p>到了这里，paper大概介绍了undersampling的裨益，undersampling的核心其实就是消除前面提到的比例\\(\\pi\\)，让它趋近于0.5后，分类平面\\(\\hat w\\)就会趋近于理想分类平面\\(w^{*}\\)。</p>\n<p>这里，作者提出了一个bagging方法，就是多次做undersampling，最后最结果做bagging可以获得更好的效果，如下图<br><img src=\"http://otmy7guvn.bkt.clouddn.com/blog/5/5-2.png\" alt=\"\"><br>paper还对比了其他的方法，比如Weighted Empirical Cost Minimization(如weighted SVM)和SMOTE方法效果不如bagging undersampling，我上一幅图说明下SMOTE的缺点，更多细节，大家可以详细看看paper，如图：<br><img src=\"http://otmy7guvn.bkt.clouddn.com/blog/5/5-3.png\" alt=\"\"><br>SMOTE方法是随机选择方向生成新的sample，但是如果新的sample产生了图中位置，则效果不会很好。</p>\n<p>OK，今天就这么多，记得看明白了中间的推导一起分享啊！</p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ul>\n<li><a href=\"https://pdfs.semanticscholar.org/a8ef/5a810099178b70d1490a4e6fc4426b642cde.pdf\" target=\"_blank\" rel=\"external\">Wallace, Byron C., et al. “Class imbalance, redux.” Data Mining (ICDM), 2011 IEEE 11th International Conference on. IEEE, 2011.</a></li>\n<li><a href=\"https://www.google.co.jp/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=3&amp;cad=rja&amp;uact=8&amp;ved=0ahUKEwiVmqXlkprWAhVEsJQKHYJiCf4QFgg8MAI&amp;url=https%3A%2F%2Fcourse.ccs.neu.edu%2Fcs6140sp15%2F4_boosting%2Fslides%2Fwallace_imbalance_icdm_11_for_class_2012_final.pptx&amp;usg=AFQjCNG6GpjKeinzCsXrZWWY1edtbBMgog\" target=\"_blank\" rel=\"external\">PPT-Class Imbalance, Redux</a></li>\n</ul>\n","site":{"data":{}},"excerpt":"<p>再次感谢优男，向我提出了又一个尖锐的问题，使得我有机会思考和研究，并且最终可以看到这篇paper，并且最后可以分享给大家。</p>\n<p>我个人在工作之中遇到过imbalanced data的问题，我只是直观的感受到，imbalanced data的最后效果往往不是很棒，网上也只是给出了oversampling和undersampling的建议，并没有提及这其中的一些缘故，今天我们一起通过这篇paper来学习学习。<br>","more":"</p>\n<h2 id=\"Notes\"><a href=\"#Notes\" class=\"headerlink\" title=\"Notes\"></a>Notes</h2><p>我们假设有positive和negative两类sample，其中positive samples符合\\(P(x)\\)的Guassian分布，negative samples符合\\(G(x)\\)的Guassian分布，分类平面将空间划分成positive region\\(\\cal R^{+} _{w}\\)和negative region\\(\\cal R^{-} _{w}\\)，如下图所示：<br><img src=\"http://otmy7guvn.bkt.clouddn.com/blog/5/5-1.png\" alt=\"\"><br>图中\\(w^{ *}\\)是理想的分割平面，\\(w^{ *}\\) 应该是使loss最小的取值，即</p>\n<script type=\"math/tex; mode=display\">w^{*}= \\arg\\underset{w}{\\min} \\cal L^{*}(w)</script><p>对于loss值，其实就是分类中被错分的fn(false negative)和fp(false positive)的期望值，显然，通过minimun该loss得到的会是图中的\\(w^{*}\\)，因为这个分类平面所带来的error明显是最少的。</p>\n<script type=\"math/tex; mode=display\">\\cal L^{*}(w) = \\cal C_{fn} \\int _{\\cal R^{w} _{-}} \\it P(x)dx + \\cal C_{fp} \\int _{\\cal R^{w} _{+}} \\it G(x)dx</script><p>对于整个数据集\\(\\cal D \\)来说，我们假设数据量较少的一类(paper中设定positive类较少)所占比例为\\(\\pi\\)(小于0.5)，那么对于带有比例\\(\\pi\\)的数据集\\(\\cal D_{\\pi}\\)，全局期望是</p>\n<script type=\"math/tex; mode=display\">\\bf E_{\\cal D_{ \\pi}} [\\cal L(w)]=\\pi \\cal C_{fn} \\int _{\\cal R^{w} _{-}} \\it P(x)dx + (1- \\pi) \\cal C_{fp} \\int _{\\cal R^{w} _{+}} \\it G(x)dx</script><p>此处，我个人的理解是，在两类数据均衡的情况下，全局情况下的期望其实是和上面的loss等价的，但是imbalanced data带来了不均衡的因子\\(\\pi\\)，因此，两个公式不再等价。</p>\n<p>OK，既然不等价，那么问题就来了，paper上说，通过最小化全局期望获得的\\(\\hat w\\)，是向着较少数量类别的样本倾斜，也就是第一幅图中，向较少的postive那边skewed，原因是因为\\( \\cal R _{+} ^{ \\hat w} &lt; \\cal R _{+} ^{w^{*}}\\), 也就是说，\\(\\hat w\\)分割的positive region面积小于\\(w^{*}\\)分割出的面积，面积的减小势必导致分割平面向positive类别方向偏移。</p>\n<p>遗憾的是，关于面积的证明我实在看不明白，也email了一些人，也没有得到一个满意的答案，如果有朋友看明白了的话，<strong>记得留言或者email我！</strong></p>\n<p>到了这里，paper大概介绍了undersampling的裨益，undersampling的核心其实就是消除前面提到的比例\\(\\pi\\)，让它趋近于0.5后，分类平面\\(\\hat w\\)就会趋近于理想分类平面\\(w^{*}\\)。</p>\n<p>这里，作者提出了一个bagging方法，就是多次做undersampling，最后最结果做bagging可以获得更好的效果，如下图<br><img src=\"http://otmy7guvn.bkt.clouddn.com/blog/5/5-2.png\" alt=\"\"><br>paper还对比了其他的方法，比如Weighted Empirical Cost Minimization(如weighted SVM)和SMOTE方法效果不如bagging undersampling，我上一幅图说明下SMOTE的缺点，更多细节，大家可以详细看看paper，如图：<br><img src=\"http://otmy7guvn.bkt.clouddn.com/blog/5/5-3.png\" alt=\"\"><br>SMOTE方法是随机选择方向生成新的sample，但是如果新的sample产生了图中位置，则效果不会很好。</p>\n<p>OK，今天就这么多，记得看明白了中间的推导一起分享啊！</p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ul>\n<li><a href=\"https://pdfs.semanticscholar.org/a8ef/5a810099178b70d1490a4e6fc4426b642cde.pdf\" target=\"_blank\" rel=\"external\">Wallace, Byron C., et al. “Class imbalance, redux.” Data Mining (ICDM), 2011 IEEE 11th International Conference on. IEEE, 2011.</a></li>\n<li><a href=\"https://www.google.co.jp/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=3&amp;cad=rja&amp;uact=8&amp;ved=0ahUKEwiVmqXlkprWAhVEsJQKHYJiCf4QFgg8MAI&amp;url=https%3A%2F%2Fcourse.ccs.neu.edu%2Fcs6140sp15%2F4_boosting%2Fslides%2Fwallace_imbalance_icdm_11_for_class_2012_final.pptx&amp;usg=AFQjCNG6GpjKeinzCsXrZWWY1edtbBMgog\" target=\"_blank\" rel=\"external\">PPT-Class Imbalance, Redux</a></li>\n</ul>"},{"title":"Reading Notes-Practical lessons from predicting clicks on ads at facebook","date":"2017-08-23T03:30:43.000Z","_content":"OK，今天我们来review一篇经典的paper，这篇paper是3年前facebook的研究成果，关于gbt和lr的结合，这个搭配对于近几年的CTR预测以及推荐系统的发展都产生了深远的影响。虽然已经很难被称为一篇新paper了，但是还是值得我们去看看。\n\n我们一起简单看看这篇paper的核心point.\n<!--more-->\n## Notes\n传统CTR预测中，logistic regression一直有着很好的效果，lr不仅可以线性分类，同时也可以给出样本属于该类别的posterior probability\n\n但是传统的lr也有着本身的缺憾，lr本身就是liner分类器，对于线性不可分的features效果不是很理想。同时在对于连续feature离散化的时候，效果很大程度依赖于离散分桶的人为经验。\n\n该paper提出了一种依靠gbt进行feature transform的方法，不多说废话，我们直接上图\n![](http://otmy7guvn.bkt.clouddn.com/blog/3/3-1.png) \n这就是这篇paper最最最核心的部分了。\n\n> Input features are transformed by means of boosted decision trees.The output of each individual tree is treated as a categorical input feature to a sparse linear classifier. Boosted decision trees prove to be very powerful feature transforms.\n\n从图中我们可以看到，原始feature被gbt进行了transform，样本落入到哪个tree node，则该位置1，其他位置0，随后再进入线性分类器lr中进行最后的分类。\n\n假设有一个sample，在图中所示的模型中，gbt有两棵树，从左到右是tree1和tree2，tree1中sample被分到了第一个tree node，tree2中sample被分到了第二个tree node，那么最终transform得到的new sample就变成了(1,0,0,0,1)\n\n通过gbt的transform后，feature不仅从非线性转换成了线性（类似于SVM的kernel效果），而且feature被完全的离散成了0-1稀疏feature，无论从线性可分还是特征稀疏的角度上，都变得比原始feature更加理想！\n\n因为是一篇相对老一些的paper，所以我叙述的比较简单，大家可以get到gbt+lr这个模型的基本原理就可以了。我自己在私下也用python写了一个简单的demo，感兴趣的朋友[可以看看](https://github.com/JoeAsir/Machine-learning-demo/blob/master/algorithm/gbtWithLogisticRegression/gradient_logistic.py)，欢迎提出意见，欢迎folk！\n\n## Reference\n* [He, Xinran, et al. \"Practical lessons from predicting clicks on ads at facebook.\" Proceedings of the Eighth International Workshop on Data Mining for Online Advertising. ACM, 2014.](http://quinonero.net/Publications/predicting-clicks-facebook.pdf)\n","source":"_posts/paper-facebook.md","raw":"---\ntitle: Reading Notes-Practical lessons from predicting clicks on ads at facebook\ndate: 2017-08-23 11:30:43\ntags: \n\t- gbt\n\t- logistic regression\n\t- gradient descent\ncategories: reading notes\n---\nOK，今天我们来review一篇经典的paper，这篇paper是3年前facebook的研究成果，关于gbt和lr的结合，这个搭配对于近几年的CTR预测以及推荐系统的发展都产生了深远的影响。虽然已经很难被称为一篇新paper了，但是还是值得我们去看看。\n\n我们一起简单看看这篇paper的核心point.\n<!--more-->\n## Notes\n传统CTR预测中，logistic regression一直有着很好的效果，lr不仅可以线性分类，同时也可以给出样本属于该类别的posterior probability\n\n但是传统的lr也有着本身的缺憾，lr本身就是liner分类器，对于线性不可分的features效果不是很理想。同时在对于连续feature离散化的时候，效果很大程度依赖于离散分桶的人为经验。\n\n该paper提出了一种依靠gbt进行feature transform的方法，不多说废话，我们直接上图\n![](http://otmy7guvn.bkt.clouddn.com/blog/3/3-1.png) \n这就是这篇paper最最最核心的部分了。\n\n> Input features are transformed by means of boosted decision trees.The output of each individual tree is treated as a categorical input feature to a sparse linear classifier. Boosted decision trees prove to be very powerful feature transforms.\n\n从图中我们可以看到，原始feature被gbt进行了transform，样本落入到哪个tree node，则该位置1，其他位置0，随后再进入线性分类器lr中进行最后的分类。\n\n假设有一个sample，在图中所示的模型中，gbt有两棵树，从左到右是tree1和tree2，tree1中sample被分到了第一个tree node，tree2中sample被分到了第二个tree node，那么最终transform得到的new sample就变成了(1,0,0,0,1)\n\n通过gbt的transform后，feature不仅从非线性转换成了线性（类似于SVM的kernel效果），而且feature被完全的离散成了0-1稀疏feature，无论从线性可分还是特征稀疏的角度上，都变得比原始feature更加理想！\n\n因为是一篇相对老一些的paper，所以我叙述的比较简单，大家可以get到gbt+lr这个模型的基本原理就可以了。我自己在私下也用python写了一个简单的demo，感兴趣的朋友[可以看看](https://github.com/JoeAsir/Machine-learning-demo/blob/master/algorithm/gbtWithLogisticRegression/gradient_logistic.py)，欢迎提出意见，欢迎folk！\n\n## Reference\n* [He, Xinran, et al. \"Practical lessons from predicting clicks on ads at facebook.\" Proceedings of the Eighth International Workshop on Data Mining for Online Advertising. ACM, 2014.](http://quinonero.net/Publications/predicting-clicks-facebook.pdf)\n","slug":"paper-facebook","published":1,"updated":"2017-09-08T03:52:07.030Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cj8lez53h000cw2kz0n691iok","content":"<p>OK，今天我们来review一篇经典的paper，这篇paper是3年前facebook的研究成果，关于gbt和lr的结合，这个搭配对于近几年的CTR预测以及推荐系统的发展都产生了深远的影响。虽然已经很难被称为一篇新paper了，但是还是值得我们去看看。</p>\n<p>我们一起简单看看这篇paper的核心point.<br><a id=\"more\"></a></p>\n<h2 id=\"Notes\"><a href=\"#Notes\" class=\"headerlink\" title=\"Notes\"></a>Notes</h2><p>传统CTR预测中，logistic regression一直有着很好的效果，lr不仅可以线性分类，同时也可以给出样本属于该类别的posterior probability</p>\n<p>但是传统的lr也有着本身的缺憾，lr本身就是liner分类器，对于线性不可分的features效果不是很理想。同时在对于连续feature离散化的时候，效果很大程度依赖于离散分桶的人为经验。</p>\n<p>该paper提出了一种依靠gbt进行feature transform的方法，不多说废话，我们直接上图<br><img src=\"http://otmy7guvn.bkt.clouddn.com/blog/3/3-1.png\" alt=\"\"><br>这就是这篇paper最最最核心的部分了。</p>\n<blockquote>\n<p>Input features are transformed by means of boosted decision trees.The output of each individual tree is treated as a categorical input feature to a sparse linear classifier. Boosted decision trees prove to be very powerful feature transforms.</p>\n</blockquote>\n<p>从图中我们可以看到，原始feature被gbt进行了transform，样本落入到哪个tree node，则该位置1，其他位置0，随后再进入线性分类器lr中进行最后的分类。</p>\n<p>假设有一个sample，在图中所示的模型中，gbt有两棵树，从左到右是tree1和tree2，tree1中sample被分到了第一个tree node，tree2中sample被分到了第二个tree node，那么最终transform得到的new sample就变成了(1,0,0,0,1)</p>\n<p>通过gbt的transform后，feature不仅从非线性转换成了线性（类似于SVM的kernel效果），而且feature被完全的离散成了0-1稀疏feature，无论从线性可分还是特征稀疏的角度上，都变得比原始feature更加理想！</p>\n<p>因为是一篇相对老一些的paper，所以我叙述的比较简单，大家可以get到gbt+lr这个模型的基本原理就可以了。我自己在私下也用python写了一个简单的demo，感兴趣的朋友<a href=\"https://github.com/JoeAsir/Machine-learning-demo/blob/master/algorithm/gbtWithLogisticRegression/gradient_logistic.py\" target=\"_blank\" rel=\"external\">可以看看</a>，欢迎提出意见，欢迎folk！</p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ul>\n<li><a href=\"http://quinonero.net/Publications/predicting-clicks-facebook.pdf\" target=\"_blank\" rel=\"external\">He, Xinran, et al. “Practical lessons from predicting clicks on ads at facebook.” Proceedings of the Eighth International Workshop on Data Mining for Online Advertising. ACM, 2014.</a></li>\n</ul>\n","site":{"data":{}},"excerpt":"<p>OK，今天我们来review一篇经典的paper，这篇paper是3年前facebook的研究成果，关于gbt和lr的结合，这个搭配对于近几年的CTR预测以及推荐系统的发展都产生了深远的影响。虽然已经很难被称为一篇新paper了，但是还是值得我们去看看。</p>\n<p>我们一起简单看看这篇paper的核心point.<br>","more":"</p>\n<h2 id=\"Notes\"><a href=\"#Notes\" class=\"headerlink\" title=\"Notes\"></a>Notes</h2><p>传统CTR预测中，logistic regression一直有着很好的效果，lr不仅可以线性分类，同时也可以给出样本属于该类别的posterior probability</p>\n<p>但是传统的lr也有着本身的缺憾，lr本身就是liner分类器，对于线性不可分的features效果不是很理想。同时在对于连续feature离散化的时候，效果很大程度依赖于离散分桶的人为经验。</p>\n<p>该paper提出了一种依靠gbt进行feature transform的方法，不多说废话，我们直接上图<br><img src=\"http://otmy7guvn.bkt.clouddn.com/blog/3/3-1.png\" alt=\"\"><br>这就是这篇paper最最最核心的部分了。</p>\n<blockquote>\n<p>Input features are transformed by means of boosted decision trees.The output of each individual tree is treated as a categorical input feature to a sparse linear classifier. Boosted decision trees prove to be very powerful feature transforms.</p>\n</blockquote>\n<p>从图中我们可以看到，原始feature被gbt进行了transform，样本落入到哪个tree node，则该位置1，其他位置0，随后再进入线性分类器lr中进行最后的分类。</p>\n<p>假设有一个sample，在图中所示的模型中，gbt有两棵树，从左到右是tree1和tree2，tree1中sample被分到了第一个tree node，tree2中sample被分到了第二个tree node，那么最终transform得到的new sample就变成了(1,0,0,0,1)</p>\n<p>通过gbt的transform后，feature不仅从非线性转换成了线性（类似于SVM的kernel效果），而且feature被完全的离散成了0-1稀疏feature，无论从线性可分还是特征稀疏的角度上，都变得比原始feature更加理想！</p>\n<p>因为是一篇相对老一些的paper，所以我叙述的比较简单，大家可以get到gbt+lr这个模型的基本原理就可以了。我自己在私下也用python写了一个简单的demo，感兴趣的朋友<a href=\"https://github.com/JoeAsir/Machine-learning-demo/blob/master/algorithm/gbtWithLogisticRegression/gradient_logistic.py\" target=\"_blank\" rel=\"external\">可以看看</a>，欢迎提出意见，欢迎folk！</p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ul>\n<li><a href=\"http://quinonero.net/Publications/predicting-clicks-facebook.pdf\" target=\"_blank\" rel=\"external\">He, Xinran, et al. “Practical lessons from predicting clicks on ads at facebook.” Proceedings of the Eighth International Workshop on Data Mining for Online Advertising. ACM, 2014.</a></li>\n</ul>"},{"title":"Learning notes-Deep Learning, course2, week2","date":"2017-09-27T13:38:35.000Z","_content":"大家好，课程来到了第二周，这周主要是一些优化方法，使得整个neural networks可以更快更好的工作，我们一起来recap一下。\n<!--more-->\n## Mini-batch gradient descent\n这一节我就不打算写了，比较基础，其实mini-batch gradient descent是batch gradient和stochastic gradient descent综合后的结果，一方面解决了batch gradient计算量大，容易converge到local minimum的问题，也解决了stochastic gradient descent epoch太多的弊端，是现在gradient descent使用最广泛的方法。\n\n对于mini-batch gradient descent，Ng建议batch大小取决于数据量多少，在小数据量上完全没有必要做mini-batch，直接使用batch gradient descent就可以，对于大数据量的情况，最好选择2的乘方，如64,128,256,512来作为batch size. 当然，batch size也要满足CPU和GPU的内存大小。\n## Exponentially weighted averages\n### Exponentially weighted averages\nExponentially weighted averages，也被称为moving averages，是一种综合历史数据的加权平均方法，课程中Ng用了伦敦一年的气温变化曲线作为例子，对于固有变量\\\\(\\theta\\\\)来说，我们要求的平均值\\\\(v\\\\)应该是\n$$v_0 = 0$$ \n$$v_1 = \\beta v_0 + (1- \\beta) \\theta_1 $$\n$$v_2 = \\beta v_1 + (1- \\beta) \\theta_2$$\n$$v_3= \\beta v_2 + (1- \\beta) \\theta_3$$\n$$\\cdots$$\n$$v_t= \\beta v_{t-1} + (1- \\beta) \\theta_t$$\n其中\\\\(\\beta\\\\)是一个因子，它决定了moving averages大约向前平均了\\\\(\\frac{1}{1- \\beta}\\\\)个\\\\(\\theta\\\\)值，例如\\\\(\\beta = 0.9\\\\)，那么大约向前平均了10个值，并且是向前按指数衰减加权获得的平均值。\n### Bias correction\n在exponentially weighted averages中，有一个问题很尖锐，那就是在最初的求解过程中，由于\\\\(v_0=0\\\\)，导致前面的数字结果距离正确结果较小，如下图所示，紫色曲线是获得的结果，而在起始位置的值明显是偏小的。\n![](http://otmy7guvn.bkt.clouddn.com/blog/7/7-1.png) \n此时，我们引入bias correction，原理也很简单，就是此处我们不使用\\\\(v_t\\\\)作为最终的结果，而是使用\\\\( \\frac{v_t}{1- \\beta^{t}}\\\\)作为最后的结果，通过bias correction，我们会获得绿色的曲线。\n\n我们可以看到，起始绿色钱和紫色曲线在最后基本没有差别，几乎重合，但是在曲线开始的时候，绿色曲线比紫色曲线更加逼近真实情况，因此，Ng给我们以下建议：\n* 当我们不关注moving averages initial value大小的时候，我们可以不使用bias correction\n* Bias correction对于initial value效果更好\n\n## Gradient descent optimization\n### momentum\n在gradient descent中，我们经常会遇到一种情况，如图所示：\n![](http://otmy7guvn.bkt.clouddn.com/blog/7/7-2.png) \n在水平方向上，我们希望更快的下降，而在垂直方向上我们希望更小的下降速率，以避免过多的iteration，针对这种情况，我们讲moving averages的思想带入进来，这就是momentum方法。\n\n在momentum中，我们的每次迭代中：\n$$v_{dW}= \\beta v_{dW}+(1- \\beta)dW$$\n$$v_{db}= \\beta v_{db}+(1- \\beta)db$$\n$$W:=W- \\alpha v_{dW}$$\n$$b:=b - \\alpha v_{db}$$\n在很多的文献中，上面的\\\\(1- \\beta\\\\)项被省略掉了，这样做只是将等式等量做了缩放，并不影响实际的效果，Ng表示，两种方式的momentum几乎没有差别，大家可以放心使用。\n\n实际上，momentum可以理解为将数次之前迭代过程中的gradient变化也带入到了这次迭代过程中，我个人认为就是带入了一种gradient变化的趋势，这样可以更好的控制gradient descent的方向和大小。例如上图中的纵向方向中，加入momentum后可以轻松的将纵向梯度正负抵消到近似0，这样就可以减少在gradient descent在纵向的反复迭代，因为，那既是无用功，也是我们不愿意看到的情况。\n\n另外，Ng给我们了一个\\\\(\\beta\\\\)的理想取值，既0.9，这个值大约取了前10次迭代结果的moving averages。另外，Ng表示，在momen中很少使用bias correction，因为10次迭代之后，这种问题很快就会消除，而一般的gradient descent，iteration次数远远大于10次。\n### RMSprop\n除了momentum，还有一些类似的方法，例如大名鼎鼎的RMSprop(root means square prop)，在这个方法中，主要体现了square的应用，我们来看看，在每次的迭代中：\n$$S_{dW}= \\beta S_{dW} + (1- \\beta)(dW)^2$$\n$$S_{db}= \\beta S_{db}+(1- \\beta)(db)^2$$\n$$W:=W- \\alpha \\frac{dW}{\\sqrt {S_{dW}}+ \\epsilon}$$\n$$b:=b- \\alpha \\frac{db}{\\sqrt {S_{db}} + \\epsilon}$$\n\\\\(\\epsilon\\\\)是为了防止分母为0的一个item，取值建议为\\\\(10^{-8}\\\\)\n\n假设在gradient descent中，\\\\(W\\\\)下降速率太低，也就是\\\\(dW\\\\)太小，那么在RMSprop的迭代中，\\\\(dW\\\\)将会除以一个很小的值\\\\( \\sqrt{S_{dW}}\\\\)，也就是\\\\(W\\\\)将会减去一个较大的值，反之亦然。\n\n通过这种方式，我们缓解了上图所示的情况，改善了gradient descent的合理性，同时可以使用更大的\\\\(\\alpha\\\\)去实现更快的gradient descent.\n### Adam\n我们看到了momentum和RMSprop优化方法的厉害之处，现在Adam方法横空出世，他融合了momentum和RMSprop，他是如何融合的呢，我们把momentum中的\\\\( \\beta\\\\)命名为\\\\( \\beta\\_1\\\\)，把RMSprop中的\\\\( \\beta\\\\)命名为\\\\( \\beta\\_2\\\\)，在第\\\\(t\\\\)次迭代中：\n$$v_{dW}= \\beta_1 v_{dW}+(1- \\beta_1)dW, \\qquad v_{db}= \\beta_1 v_{db}+(1- \\beta_1)db$$\n$$s_{dW}= \\beta_2 s_{dW}+(1- \\beta_2)(dW)^2, \\qquad s_{db}= \\beta_2 s_{db}+(1- \\beta_2)(db)^2$$\n加上bias correction后\n$$v^{corrected}_{dW}= \\frac{v_{dW}}{1- \\beta^t_1}, \\qquad v^{corrected}_{db}= \\frac{v_{db}}{1- \\beta^t_1}$$\n$$s^{corrected}_{dW}= \\frac{s_{dW}}{1- \\beta^t_2}, \\qquad s^{corrected}_{db}= \\frac{s_{db}}{1- \\beta^t_2}$$\n$$W:=W- \\alpha \\frac{v^{corrected}_{dW}}{ \\sqrt{s^{corrected}_{dW}}+ \\epsilon}$$\n$$W:=W- \\alpha \\frac{v^{corrected}_{db}}{ \\sqrt{s^{corrected}_{db}}+ \\epsilon}$$\n其中，\\\\(\\epsilon\\\\)是为了防止分母为0的一个item，取值建议为\\\\(10^{-8}\\\\)，\\\\(\\beta\\_1\\\\)建议取值0.9，\\\\(\\beta\\_2\\\\)建议取值0.999。\n## Learning rate decay\n在gradient descent中，随着迭代的深度，越来越靠近minimum，我们需要更小的learning rate，以避免越过minimum，常用的learning decay方法有：\n$$\\alpha = \\frac{1}{1+decayRate*epochNum} * \\alpha_0$$\n$$\\alpha = 0.95^{epochNum} * \\alpha_0$$\n$$\\alpha = \\frac{k}{\\sqrt{epochNum}}* \\alpha_0$$\n这些方法都可以让\\\\(\\alpha\\\\)随着迭代次数增加而慢慢变小，可以更好的逼近minimum.\n## Reference\n* [Deep learning-Coursera Andrew Ng](https://www.coursera.org/specializations/deep-learning)\n* [Deep learning-网易云课堂 Andrew Ng](https://mooc.study.163.com/course/deeplearning_ai-2001281003#/info)\n* [An overview of gradient descent optimization algorithms ](http://ruder.io/optimizing-gradient-descent/)","source":"_posts/course-deep-learning-course2-week2.md","raw":"---\ntitle: Learning notes-Deep Learning, course2, week2\ndate: 2017-09-27 21:38:35\ntags:\n\t- gradient descent\n\t- moving averages\ncategories: learning notes\n---\n大家好，课程来到了第二周，这周主要是一些优化方法，使得整个neural networks可以更快更好的工作，我们一起来recap一下。\n<!--more-->\n## Mini-batch gradient descent\n这一节我就不打算写了，比较基础，其实mini-batch gradient descent是batch gradient和stochastic gradient descent综合后的结果，一方面解决了batch gradient计算量大，容易converge到local minimum的问题，也解决了stochastic gradient descent epoch太多的弊端，是现在gradient descent使用最广泛的方法。\n\n对于mini-batch gradient descent，Ng建议batch大小取决于数据量多少，在小数据量上完全没有必要做mini-batch，直接使用batch gradient descent就可以，对于大数据量的情况，最好选择2的乘方，如64,128,256,512来作为batch size. 当然，batch size也要满足CPU和GPU的内存大小。\n## Exponentially weighted averages\n### Exponentially weighted averages\nExponentially weighted averages，也被称为moving averages，是一种综合历史数据的加权平均方法，课程中Ng用了伦敦一年的气温变化曲线作为例子，对于固有变量\\\\(\\theta\\\\)来说，我们要求的平均值\\\\(v\\\\)应该是\n$$v_0 = 0$$ \n$$v_1 = \\beta v_0 + (1- \\beta) \\theta_1 $$\n$$v_2 = \\beta v_1 + (1- \\beta) \\theta_2$$\n$$v_3= \\beta v_2 + (1- \\beta) \\theta_3$$\n$$\\cdots$$\n$$v_t= \\beta v_{t-1} + (1- \\beta) \\theta_t$$\n其中\\\\(\\beta\\\\)是一个因子，它决定了moving averages大约向前平均了\\\\(\\frac{1}{1- \\beta}\\\\)个\\\\(\\theta\\\\)值，例如\\\\(\\beta = 0.9\\\\)，那么大约向前平均了10个值，并且是向前按指数衰减加权获得的平均值。\n### Bias correction\n在exponentially weighted averages中，有一个问题很尖锐，那就是在最初的求解过程中，由于\\\\(v_0=0\\\\)，导致前面的数字结果距离正确结果较小，如下图所示，紫色曲线是获得的结果，而在起始位置的值明显是偏小的。\n![](http://otmy7guvn.bkt.clouddn.com/blog/7/7-1.png) \n此时，我们引入bias correction，原理也很简单，就是此处我们不使用\\\\(v_t\\\\)作为最终的结果，而是使用\\\\( \\frac{v_t}{1- \\beta^{t}}\\\\)作为最后的结果，通过bias correction，我们会获得绿色的曲线。\n\n我们可以看到，起始绿色钱和紫色曲线在最后基本没有差别，几乎重合，但是在曲线开始的时候，绿色曲线比紫色曲线更加逼近真实情况，因此，Ng给我们以下建议：\n* 当我们不关注moving averages initial value大小的时候，我们可以不使用bias correction\n* Bias correction对于initial value效果更好\n\n## Gradient descent optimization\n### momentum\n在gradient descent中，我们经常会遇到一种情况，如图所示：\n![](http://otmy7guvn.bkt.clouddn.com/blog/7/7-2.png) \n在水平方向上，我们希望更快的下降，而在垂直方向上我们希望更小的下降速率，以避免过多的iteration，针对这种情况，我们讲moving averages的思想带入进来，这就是momentum方法。\n\n在momentum中，我们的每次迭代中：\n$$v_{dW}= \\beta v_{dW}+(1- \\beta)dW$$\n$$v_{db}= \\beta v_{db}+(1- \\beta)db$$\n$$W:=W- \\alpha v_{dW}$$\n$$b:=b - \\alpha v_{db}$$\n在很多的文献中，上面的\\\\(1- \\beta\\\\)项被省略掉了，这样做只是将等式等量做了缩放，并不影响实际的效果，Ng表示，两种方式的momentum几乎没有差别，大家可以放心使用。\n\n实际上，momentum可以理解为将数次之前迭代过程中的gradient变化也带入到了这次迭代过程中，我个人认为就是带入了一种gradient变化的趋势，这样可以更好的控制gradient descent的方向和大小。例如上图中的纵向方向中，加入momentum后可以轻松的将纵向梯度正负抵消到近似0，这样就可以减少在gradient descent在纵向的反复迭代，因为，那既是无用功，也是我们不愿意看到的情况。\n\n另外，Ng给我们了一个\\\\(\\beta\\\\)的理想取值，既0.9，这个值大约取了前10次迭代结果的moving averages。另外，Ng表示，在momen中很少使用bias correction，因为10次迭代之后，这种问题很快就会消除，而一般的gradient descent，iteration次数远远大于10次。\n### RMSprop\n除了momentum，还有一些类似的方法，例如大名鼎鼎的RMSprop(root means square prop)，在这个方法中，主要体现了square的应用，我们来看看，在每次的迭代中：\n$$S_{dW}= \\beta S_{dW} + (1- \\beta)(dW)^2$$\n$$S_{db}= \\beta S_{db}+(1- \\beta)(db)^2$$\n$$W:=W- \\alpha \\frac{dW}{\\sqrt {S_{dW}}+ \\epsilon}$$\n$$b:=b- \\alpha \\frac{db}{\\sqrt {S_{db}} + \\epsilon}$$\n\\\\(\\epsilon\\\\)是为了防止分母为0的一个item，取值建议为\\\\(10^{-8}\\\\)\n\n假设在gradient descent中，\\\\(W\\\\)下降速率太低，也就是\\\\(dW\\\\)太小，那么在RMSprop的迭代中，\\\\(dW\\\\)将会除以一个很小的值\\\\( \\sqrt{S_{dW}}\\\\)，也就是\\\\(W\\\\)将会减去一个较大的值，反之亦然。\n\n通过这种方式，我们缓解了上图所示的情况，改善了gradient descent的合理性，同时可以使用更大的\\\\(\\alpha\\\\)去实现更快的gradient descent.\n### Adam\n我们看到了momentum和RMSprop优化方法的厉害之处，现在Adam方法横空出世，他融合了momentum和RMSprop，他是如何融合的呢，我们把momentum中的\\\\( \\beta\\\\)命名为\\\\( \\beta\\_1\\\\)，把RMSprop中的\\\\( \\beta\\\\)命名为\\\\( \\beta\\_2\\\\)，在第\\\\(t\\\\)次迭代中：\n$$v_{dW}= \\beta_1 v_{dW}+(1- \\beta_1)dW, \\qquad v_{db}= \\beta_1 v_{db}+(1- \\beta_1)db$$\n$$s_{dW}= \\beta_2 s_{dW}+(1- \\beta_2)(dW)^2, \\qquad s_{db}= \\beta_2 s_{db}+(1- \\beta_2)(db)^2$$\n加上bias correction后\n$$v^{corrected}_{dW}= \\frac{v_{dW}}{1- \\beta^t_1}, \\qquad v^{corrected}_{db}= \\frac{v_{db}}{1- \\beta^t_1}$$\n$$s^{corrected}_{dW}= \\frac{s_{dW}}{1- \\beta^t_2}, \\qquad s^{corrected}_{db}= \\frac{s_{db}}{1- \\beta^t_2}$$\n$$W:=W- \\alpha \\frac{v^{corrected}_{dW}}{ \\sqrt{s^{corrected}_{dW}}+ \\epsilon}$$\n$$W:=W- \\alpha \\frac{v^{corrected}_{db}}{ \\sqrt{s^{corrected}_{db}}+ \\epsilon}$$\n其中，\\\\(\\epsilon\\\\)是为了防止分母为0的一个item，取值建议为\\\\(10^{-8}\\\\)，\\\\(\\beta\\_1\\\\)建议取值0.9，\\\\(\\beta\\_2\\\\)建议取值0.999。\n## Learning rate decay\n在gradient descent中，随着迭代的深度，越来越靠近minimum，我们需要更小的learning rate，以避免越过minimum，常用的learning decay方法有：\n$$\\alpha = \\frac{1}{1+decayRate*epochNum} * \\alpha_0$$\n$$\\alpha = 0.95^{epochNum} * \\alpha_0$$\n$$\\alpha = \\frac{k}{\\sqrt{epochNum}}* \\alpha_0$$\n这些方法都可以让\\\\(\\alpha\\\\)随着迭代次数增加而慢慢变小，可以更好的逼近minimum.\n## Reference\n* [Deep learning-Coursera Andrew Ng](https://www.coursera.org/specializations/deep-learning)\n* [Deep learning-网易云课堂 Andrew Ng](https://mooc.study.163.com/course/deeplearning_ai-2001281003#/info)\n* [An overview of gradient descent optimization algorithms ](http://ruder.io/optimizing-gradient-descent/)","slug":"course-deep-learning-course2-week2","published":1,"updated":"2017-09-30T07:40:51.685Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cj8lez53j000ew2kzahdzxnk7","content":"<p>大家好，课程来到了第二周，这周主要是一些优化方法，使得整个neural networks可以更快更好的工作，我们一起来recap一下。<br><a id=\"more\"></a></p>\n<h2 id=\"Mini-batch-gradient-descent\"><a href=\"#Mini-batch-gradient-descent\" class=\"headerlink\" title=\"Mini-batch gradient descent\"></a>Mini-batch gradient descent</h2><p>这一节我就不打算写了，比较基础，其实mini-batch gradient descent是batch gradient和stochastic gradient descent综合后的结果，一方面解决了batch gradient计算量大，容易converge到local minimum的问题，也解决了stochastic gradient descent epoch太多的弊端，是现在gradient descent使用最广泛的方法。</p>\n<p>对于mini-batch gradient descent，Ng建议batch大小取决于数据量多少，在小数据量上完全没有必要做mini-batch，直接使用batch gradient descent就可以，对于大数据量的情况，最好选择2的乘方，如64,128,256,512来作为batch size. 当然，batch size也要满足CPU和GPU的内存大小。</p>\n<h2 id=\"Exponentially-weighted-averages\"><a href=\"#Exponentially-weighted-averages\" class=\"headerlink\" title=\"Exponentially weighted averages\"></a>Exponentially weighted averages</h2><h3 id=\"Exponentially-weighted-averages-1\"><a href=\"#Exponentially-weighted-averages-1\" class=\"headerlink\" title=\"Exponentially weighted averages\"></a>Exponentially weighted averages</h3><p>Exponentially weighted averages，也被称为moving averages，是一种综合历史数据的加权平均方法，课程中Ng用了伦敦一年的气温变化曲线作为例子，对于固有变量\\(\\theta\\)来说，我们要求的平均值\\(v\\)应该是</p>\n<script type=\"math/tex; mode=display\">v_0 = 0</script><script type=\"math/tex; mode=display\">v_1 = \\beta v_0 + (1- \\beta) \\theta_1</script><script type=\"math/tex; mode=display\">v_2 = \\beta v_1 + (1- \\beta) \\theta_2</script><script type=\"math/tex; mode=display\">v_3= \\beta v_2 + (1- \\beta) \\theta_3</script><script type=\"math/tex; mode=display\">\\cdots</script><script type=\"math/tex; mode=display\">v_t= \\beta v_{t-1} + (1- \\beta) \\theta_t</script><p>其中\\(\\beta\\)是一个因子，它决定了moving averages大约向前平均了\\(\\frac{1}{1- \\beta}\\)个\\(\\theta\\)值，例如\\(\\beta = 0.9\\)，那么大约向前平均了10个值，并且是向前按指数衰减加权获得的平均值。</p>\n<h3 id=\"Bias-correction\"><a href=\"#Bias-correction\" class=\"headerlink\" title=\"Bias correction\"></a>Bias correction</h3><p>在exponentially weighted averages中，有一个问题很尖锐，那就是在最初的求解过程中，由于\\(v_0=0\\)，导致前面的数字结果距离正确结果较小，如下图所示，紫色曲线是获得的结果，而在起始位置的值明显是偏小的。<br><img src=\"http://otmy7guvn.bkt.clouddn.com/blog/7/7-1.png\" alt=\"\"><br>此时，我们引入bias correction，原理也很简单，就是此处我们不使用\\(v_t\\)作为最终的结果，而是使用\\( \\frac{v_t}{1- \\beta^{t}}\\)作为最后的结果，通过bias correction，我们会获得绿色的曲线。</p>\n<p>我们可以看到，起始绿色钱和紫色曲线在最后基本没有差别，几乎重合，但是在曲线开始的时候，绿色曲线比紫色曲线更加逼近真实情况，因此，Ng给我们以下建议：</p>\n<ul>\n<li>当我们不关注moving averages initial value大小的时候，我们可以不使用bias correction</li>\n<li>Bias correction对于initial value效果更好</li>\n</ul>\n<h2 id=\"Gradient-descent-optimization\"><a href=\"#Gradient-descent-optimization\" class=\"headerlink\" title=\"Gradient descent optimization\"></a>Gradient descent optimization</h2><h3 id=\"momentum\"><a href=\"#momentum\" class=\"headerlink\" title=\"momentum\"></a>momentum</h3><p>在gradient descent中，我们经常会遇到一种情况，如图所示：<br><img src=\"http://otmy7guvn.bkt.clouddn.com/blog/7/7-2.png\" alt=\"\"><br>在水平方向上，我们希望更快的下降，而在垂直方向上我们希望更小的下降速率，以避免过多的iteration，针对这种情况，我们讲moving averages的思想带入进来，这就是momentum方法。</p>\n<p>在momentum中，我们的每次迭代中：</p>\n<script type=\"math/tex; mode=display\">v_{dW}= \\beta v_{dW}+(1- \\beta)dW</script><script type=\"math/tex; mode=display\">v_{db}= \\beta v_{db}+(1- \\beta)db</script><script type=\"math/tex; mode=display\">W:=W- \\alpha v_{dW}</script><script type=\"math/tex; mode=display\">b:=b - \\alpha v_{db}</script><p>在很多的文献中，上面的\\(1- \\beta\\)项被省略掉了，这样做只是将等式等量做了缩放，并不影响实际的效果，Ng表示，两种方式的momentum几乎没有差别，大家可以放心使用。</p>\n<p>实际上，momentum可以理解为将数次之前迭代过程中的gradient变化也带入到了这次迭代过程中，我个人认为就是带入了一种gradient变化的趋势，这样可以更好的控制gradient descent的方向和大小。例如上图中的纵向方向中，加入momentum后可以轻松的将纵向梯度正负抵消到近似0，这样就可以减少在gradient descent在纵向的反复迭代，因为，那既是无用功，也是我们不愿意看到的情况。</p>\n<p>另外，Ng给我们了一个\\(\\beta\\)的理想取值，既0.9，这个值大约取了前10次迭代结果的moving averages。另外，Ng表示，在momen中很少使用bias correction，因为10次迭代之后，这种问题很快就会消除，而一般的gradient descent，iteration次数远远大于10次。</p>\n<h3 id=\"RMSprop\"><a href=\"#RMSprop\" class=\"headerlink\" title=\"RMSprop\"></a>RMSprop</h3><p>除了momentum，还有一些类似的方法，例如大名鼎鼎的RMSprop(root means square prop)，在这个方法中，主要体现了square的应用，我们来看看，在每次的迭代中：</p>\n<script type=\"math/tex; mode=display\">S_{dW}= \\beta S_{dW} + (1- \\beta)(dW)^2</script><script type=\"math/tex; mode=display\">S_{db}= \\beta S_{db}+(1- \\beta)(db)^2</script><script type=\"math/tex; mode=display\">W:=W- \\alpha \\frac{dW}{\\sqrt {S_{dW}}+ \\epsilon}</script><script type=\"math/tex; mode=display\">b:=b- \\alpha \\frac{db}{\\sqrt {S_{db}} + \\epsilon}</script><p>\\(\\epsilon\\)是为了防止分母为0的一个item，取值建议为\\(10^{-8}\\)</p>\n<p>假设在gradient descent中，\\(W\\)下降速率太低，也就是\\(dW\\)太小，那么在RMSprop的迭代中，\\(dW\\)将会除以一个很小的值\\( \\sqrt{S_{dW}}\\)，也就是\\(W\\)将会减去一个较大的值，反之亦然。</p>\n<p>通过这种方式，我们缓解了上图所示的情况，改善了gradient descent的合理性，同时可以使用更大的\\(\\alpha\\)去实现更快的gradient descent.</p>\n<h3 id=\"Adam\"><a href=\"#Adam\" class=\"headerlink\" title=\"Adam\"></a>Adam</h3><p>我们看到了momentum和RMSprop优化方法的厉害之处，现在Adam方法横空出世，他融合了momentum和RMSprop，他是如何融合的呢，我们把momentum中的\\( \\beta\\)命名为\\( \\beta_1\\)，把RMSprop中的\\( \\beta\\)命名为\\( \\beta_2\\)，在第\\(t\\)次迭代中：</p>\n<script type=\"math/tex; mode=display\">v_{dW}= \\beta_1 v_{dW}+(1- \\beta_1)dW, \\qquad v_{db}= \\beta_1 v_{db}+(1- \\beta_1)db</script><script type=\"math/tex; mode=display\">s_{dW}= \\beta_2 s_{dW}+(1- \\beta_2)(dW)^2, \\qquad s_{db}= \\beta_2 s_{db}+(1- \\beta_2)(db)^2</script><p>加上bias correction后</p>\n<script type=\"math/tex; mode=display\">v^{corrected}_{dW}= \\frac{v_{dW}}{1- \\beta^t_1}, \\qquad v^{corrected}_{db}= \\frac{v_{db}}{1- \\beta^t_1}</script><script type=\"math/tex; mode=display\">s^{corrected}_{dW}= \\frac{s_{dW}}{1- \\beta^t_2}, \\qquad s^{corrected}_{db}= \\frac{s_{db}}{1- \\beta^t_2}</script><script type=\"math/tex; mode=display\">W:=W- \\alpha \\frac{v^{corrected}_{dW}}{ \\sqrt{s^{corrected}_{dW}}+ \\epsilon}</script><script type=\"math/tex; mode=display\">W:=W- \\alpha \\frac{v^{corrected}_{db}}{ \\sqrt{s^{corrected}_{db}}+ \\epsilon}</script><p>其中，\\(\\epsilon\\)是为了防止分母为0的一个item，取值建议为\\(10^{-8}\\)，\\(\\beta_1\\)建议取值0.9，\\(\\beta_2\\)建议取值0.999。</p>\n<h2 id=\"Learning-rate-decay\"><a href=\"#Learning-rate-decay\" class=\"headerlink\" title=\"Learning rate decay\"></a>Learning rate decay</h2><p>在gradient descent中，随着迭代的深度，越来越靠近minimum，我们需要更小的learning rate，以避免越过minimum，常用的learning decay方法有：</p>\n<script type=\"math/tex; mode=display\">\\alpha = \\frac{1}{1+decayRate*epochNum} * \\alpha_0</script><script type=\"math/tex; mode=display\">\\alpha = 0.95^{epochNum} * \\alpha_0</script><script type=\"math/tex; mode=display\">\\alpha = \\frac{k}{\\sqrt{epochNum}}* \\alpha_0</script><p>这些方法都可以让\\(\\alpha\\)随着迭代次数增加而慢慢变小，可以更好的逼近minimum.</p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ul>\n<li><a href=\"https://www.coursera.org/specializations/deep-learning\" target=\"_blank\" rel=\"external\">Deep learning-Coursera Andrew Ng</a></li>\n<li><a href=\"https://mooc.study.163.com/course/deeplearning_ai-2001281003#/info\" target=\"_blank\" rel=\"external\">Deep learning-网易云课堂 Andrew Ng</a></li>\n<li><a href=\"http://ruder.io/optimizing-gradient-descent/\" target=\"_blank\" rel=\"external\">An overview of gradient descent optimization algorithms </a></li>\n</ul>\n","site":{"data":{}},"excerpt":"<p>大家好，课程来到了第二周，这周主要是一些优化方法，使得整个neural networks可以更快更好的工作，我们一起来recap一下。<br>","more":"</p>\n<h2 id=\"Mini-batch-gradient-descent\"><a href=\"#Mini-batch-gradient-descent\" class=\"headerlink\" title=\"Mini-batch gradient descent\"></a>Mini-batch gradient descent</h2><p>这一节我就不打算写了，比较基础，其实mini-batch gradient descent是batch gradient和stochastic gradient descent综合后的结果，一方面解决了batch gradient计算量大，容易converge到local minimum的问题，也解决了stochastic gradient descent epoch太多的弊端，是现在gradient descent使用最广泛的方法。</p>\n<p>对于mini-batch gradient descent，Ng建议batch大小取决于数据量多少，在小数据量上完全没有必要做mini-batch，直接使用batch gradient descent就可以，对于大数据量的情况，最好选择2的乘方，如64,128,256,512来作为batch size. 当然，batch size也要满足CPU和GPU的内存大小。</p>\n<h2 id=\"Exponentially-weighted-averages\"><a href=\"#Exponentially-weighted-averages\" class=\"headerlink\" title=\"Exponentially weighted averages\"></a>Exponentially weighted averages</h2><h3 id=\"Exponentially-weighted-averages-1\"><a href=\"#Exponentially-weighted-averages-1\" class=\"headerlink\" title=\"Exponentially weighted averages\"></a>Exponentially weighted averages</h3><p>Exponentially weighted averages，也被称为moving averages，是一种综合历史数据的加权平均方法，课程中Ng用了伦敦一年的气温变化曲线作为例子，对于固有变量\\(\\theta\\)来说，我们要求的平均值\\(v\\)应该是</p>\n<script type=\"math/tex; mode=display\">v_0 = 0</script><script type=\"math/tex; mode=display\">v_1 = \\beta v_0 + (1- \\beta) \\theta_1</script><script type=\"math/tex; mode=display\">v_2 = \\beta v_1 + (1- \\beta) \\theta_2</script><script type=\"math/tex; mode=display\">v_3= \\beta v_2 + (1- \\beta) \\theta_3</script><script type=\"math/tex; mode=display\">\\cdots</script><script type=\"math/tex; mode=display\">v_t= \\beta v_{t-1} + (1- \\beta) \\theta_t</script><p>其中\\(\\beta\\)是一个因子，它决定了moving averages大约向前平均了\\(\\frac{1}{1- \\beta}\\)个\\(\\theta\\)值，例如\\(\\beta = 0.9\\)，那么大约向前平均了10个值，并且是向前按指数衰减加权获得的平均值。</p>\n<h3 id=\"Bias-correction\"><a href=\"#Bias-correction\" class=\"headerlink\" title=\"Bias correction\"></a>Bias correction</h3><p>在exponentially weighted averages中，有一个问题很尖锐，那就是在最初的求解过程中，由于\\(v_0=0\\)，导致前面的数字结果距离正确结果较小，如下图所示，紫色曲线是获得的结果，而在起始位置的值明显是偏小的。<br><img src=\"http://otmy7guvn.bkt.clouddn.com/blog/7/7-1.png\" alt=\"\"><br>此时，我们引入bias correction，原理也很简单，就是此处我们不使用\\(v_t\\)作为最终的结果，而是使用\\( \\frac{v_t}{1- \\beta^{t}}\\)作为最后的结果，通过bias correction，我们会获得绿色的曲线。</p>\n<p>我们可以看到，起始绿色钱和紫色曲线在最后基本没有差别，几乎重合，但是在曲线开始的时候，绿色曲线比紫色曲线更加逼近真实情况，因此，Ng给我们以下建议：</p>\n<ul>\n<li>当我们不关注moving averages initial value大小的时候，我们可以不使用bias correction</li>\n<li>Bias correction对于initial value效果更好</li>\n</ul>\n<h2 id=\"Gradient-descent-optimization\"><a href=\"#Gradient-descent-optimization\" class=\"headerlink\" title=\"Gradient descent optimization\"></a>Gradient descent optimization</h2><h3 id=\"momentum\"><a href=\"#momentum\" class=\"headerlink\" title=\"momentum\"></a>momentum</h3><p>在gradient descent中，我们经常会遇到一种情况，如图所示：<br><img src=\"http://otmy7guvn.bkt.clouddn.com/blog/7/7-2.png\" alt=\"\"><br>在水平方向上，我们希望更快的下降，而在垂直方向上我们希望更小的下降速率，以避免过多的iteration，针对这种情况，我们讲moving averages的思想带入进来，这就是momentum方法。</p>\n<p>在momentum中，我们的每次迭代中：</p>\n<script type=\"math/tex; mode=display\">v_{dW}= \\beta v_{dW}+(1- \\beta)dW</script><script type=\"math/tex; mode=display\">v_{db}= \\beta v_{db}+(1- \\beta)db</script><script type=\"math/tex; mode=display\">W:=W- \\alpha v_{dW}</script><script type=\"math/tex; mode=display\">b:=b - \\alpha v_{db}</script><p>在很多的文献中，上面的\\(1- \\beta\\)项被省略掉了，这样做只是将等式等量做了缩放，并不影响实际的效果，Ng表示，两种方式的momentum几乎没有差别，大家可以放心使用。</p>\n<p>实际上，momentum可以理解为将数次之前迭代过程中的gradient变化也带入到了这次迭代过程中，我个人认为就是带入了一种gradient变化的趋势，这样可以更好的控制gradient descent的方向和大小。例如上图中的纵向方向中，加入momentum后可以轻松的将纵向梯度正负抵消到近似0，这样就可以减少在gradient descent在纵向的反复迭代，因为，那既是无用功，也是我们不愿意看到的情况。</p>\n<p>另外，Ng给我们了一个\\(\\beta\\)的理想取值，既0.9，这个值大约取了前10次迭代结果的moving averages。另外，Ng表示，在momen中很少使用bias correction，因为10次迭代之后，这种问题很快就会消除，而一般的gradient descent，iteration次数远远大于10次。</p>\n<h3 id=\"RMSprop\"><a href=\"#RMSprop\" class=\"headerlink\" title=\"RMSprop\"></a>RMSprop</h3><p>除了momentum，还有一些类似的方法，例如大名鼎鼎的RMSprop(root means square prop)，在这个方法中，主要体现了square的应用，我们来看看，在每次的迭代中：</p>\n<script type=\"math/tex; mode=display\">S_{dW}= \\beta S_{dW} + (1- \\beta)(dW)^2</script><script type=\"math/tex; mode=display\">S_{db}= \\beta S_{db}+(1- \\beta)(db)^2</script><script type=\"math/tex; mode=display\">W:=W- \\alpha \\frac{dW}{\\sqrt {S_{dW}}+ \\epsilon}</script><script type=\"math/tex; mode=display\">b:=b- \\alpha \\frac{db}{\\sqrt {S_{db}} + \\epsilon}</script><p>\\(\\epsilon\\)是为了防止分母为0的一个item，取值建议为\\(10^{-8}\\)</p>\n<p>假设在gradient descent中，\\(W\\)下降速率太低，也就是\\(dW\\)太小，那么在RMSprop的迭代中，\\(dW\\)将会除以一个很小的值\\( \\sqrt{S_{dW}}\\)，也就是\\(W\\)将会减去一个较大的值，反之亦然。</p>\n<p>通过这种方式，我们缓解了上图所示的情况，改善了gradient descent的合理性，同时可以使用更大的\\(\\alpha\\)去实现更快的gradient descent.</p>\n<h3 id=\"Adam\"><a href=\"#Adam\" class=\"headerlink\" title=\"Adam\"></a>Adam</h3><p>我们看到了momentum和RMSprop优化方法的厉害之处，现在Adam方法横空出世，他融合了momentum和RMSprop，他是如何融合的呢，我们把momentum中的\\( \\beta\\)命名为\\( \\beta_1\\)，把RMSprop中的\\( \\beta\\)命名为\\( \\beta_2\\)，在第\\(t\\)次迭代中：</p>\n<script type=\"math/tex; mode=display\">v_{dW}= \\beta_1 v_{dW}+(1- \\beta_1)dW, \\qquad v_{db}= \\beta_1 v_{db}+(1- \\beta_1)db</script><script type=\"math/tex; mode=display\">s_{dW}= \\beta_2 s_{dW}+(1- \\beta_2)(dW)^2, \\qquad s_{db}= \\beta_2 s_{db}+(1- \\beta_2)(db)^2</script><p>加上bias correction后</p>\n<script type=\"math/tex; mode=display\">v^{corrected}_{dW}= \\frac{v_{dW}}{1- \\beta^t_1}, \\qquad v^{corrected}_{db}= \\frac{v_{db}}{1- \\beta^t_1}</script><script type=\"math/tex; mode=display\">s^{corrected}_{dW}= \\frac{s_{dW}}{1- \\beta^t_2}, \\qquad s^{corrected}_{db}= \\frac{s_{db}}{1- \\beta^t_2}</script><script type=\"math/tex; mode=display\">W:=W- \\alpha \\frac{v^{corrected}_{dW}}{ \\sqrt{s^{corrected}_{dW}}+ \\epsilon}</script><script type=\"math/tex; mode=display\">W:=W- \\alpha \\frac{v^{corrected}_{db}}{ \\sqrt{s^{corrected}_{db}}+ \\epsilon}</script><p>其中，\\(\\epsilon\\)是为了防止分母为0的一个item，取值建议为\\(10^{-8}\\)，\\(\\beta_1\\)建议取值0.9，\\(\\beta_2\\)建议取值0.999。</p>\n<h2 id=\"Learning-rate-decay\"><a href=\"#Learning-rate-decay\" class=\"headerlink\" title=\"Learning rate decay\"></a>Learning rate decay</h2><p>在gradient descent中，随着迭代的深度，越来越靠近minimum，我们需要更小的learning rate，以避免越过minimum，常用的learning decay方法有：</p>\n<script type=\"math/tex; mode=display\">\\alpha = \\frac{1}{1+decayRate*epochNum} * \\alpha_0</script><script type=\"math/tex; mode=display\">\\alpha = 0.95^{epochNum} * \\alpha_0</script><script type=\"math/tex; mode=display\">\\alpha = \\frac{k}{\\sqrt{epochNum}}* \\alpha_0</script><p>这些方法都可以让\\(\\alpha\\)随着迭代次数增加而慢慢变小，可以更好的逼近minimum.</p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ul>\n<li><a href=\"https://www.coursera.org/specializations/deep-learning\" target=\"_blank\" rel=\"external\">Deep learning-Coursera Andrew Ng</a></li>\n<li><a href=\"https://mooc.study.163.com/course/deeplearning_ai-2001281003#/info\" target=\"_blank\" rel=\"external\">Deep learning-网易云课堂 Andrew Ng</a></li>\n<li><a href=\"http://ruder.io/optimizing-gradient-descent/\" target=\"_blank\" rel=\"external\">An overview of gradient descent optimization algorithms </a></li>\n</ul>"},{"title":"深入聊聊正则化","date":"2017-08-26T17:55:07.000Z","_content":"最近和优男一起聊到了L1和L2 regularization，期间遇到了很多没有想明白的问题，加上最近工作有些忙，空余时间用来倒腾新到货的小米路由器，只能趁周末自己研究研究，下面和大家分享一下regularization中一些深入的问题。不讨论基础知识，直接上干货。\n<!--more-->\n## MAP and regularization\n我们都知道，当cost function在没有加regularization的时候，我们对参数使用的是**MLE**(Maximum likelihood estimation)，对应频率学派所认为的参数本无分布规律的观点；在Andrew Ng经典的CS229中，这位AI大师曾经提到，regularization其实是对参数的**MAP**(Maximum a posteriori estimation)，是基于贝叶斯学派认为的参数本有**priori distribution**，同时吸纳了MLE的一种中间观点。\n\n这里的priori distribution，就是根据经验，认为参数应该大致符合某个distribution，这样，最终获得的参数估计结果也会和这个被认为的distribution有一些相近\n\n而我们所熟知的L1 regularization，其实就是认为参数的priori distribution是**Laplacian distribution**，而L2 regularization，则认为参数的priorit distribution是**Gaussian distribution**，相信大家对Gaussian distribution是很熟悉的，而对于Laplacian distribution，它的分布是\n$$p(x;a)= \\frac{a}{2} e^{-a|x|}$$\n下图就是两者的一个比较：\n![](http://otmy7guvn.bkt.clouddn.com/blog/4/4-1.png) \n在后文中，我们会看到，两种distribution的特点决定了两种regularization的性质。\n### Lasso regression\n在liner regression中，我们假设参数\\\\( \\theta\\\\)服从Laplacian distribution，cost function就成了\n$$J( \\theta) = \\frac{1}{2} \\sum^{m}_{i=1} (y^{(i)}- \\theta^T x^{(i)})+ \\lambda \\sum^{d}_{j=1}| \\theta^{(i)}|$$\n上式就是Lasso regression\n### Ridge regression\n在liner regression中，我们假设参数\\\\( \\theta\\\\)服从Gaussian distribution，cost function就成了\n$$J( \\theta) = \\frac{1}{2} \\sum^{m}_{i=1} (y^{(i)}- \\theta^T x^{(i)})+ \\lambda \\sum^{d}_{j=1} ( \\theta^{(i)})^2$$\n上式就是Ridge regression或shrinkage\n## geometry of error surfaces\n在不考虑参数priori distribution的时候，cost function的形式是\n$$J( \\theta) = \\frac{1}{2} \\sum^{m}_{i=1} (y^{(i)}- \\theta^T x^{(i)})^2$$\n用二维截面图展示就是\n![](http://otmy7guvn.bkt.clouddn.com/blog/4/4-2.png) \n图中只有objective function，横纵轴是参数\\\\( \\theta\\\\)，截取过来的图，所以上面的参数是\\\\(w\\\\)，\\\\(l\\\\)是loss值，箭头指向的点就是cost function的极小点。在不考虑参数priori distribution的时候，这个点就是我们的optimization target.\n\n下面大家来我一起做一个头脑风暴，所谓参数的priori distribution，其实就是用来限制最后optimization结果的一个限定，那么我们其实就是在做一个受限制的的convex optimization，即：\n$$ \\theta=argmin \\frac{1}{2} \\sum^{m}_{i=1} (y^{(i)}- \\theta^T x^{(i)})^2$$\n$$ s.t. \\sum^{d}_{j=1}| \\theta^{(i)}|^p \\geq \\beta$$\n其中，\\\\( \\beta\\\\)是ridge或者lasso的最小值。\n那么此时的图就变成了：\n![](http://otmy7guvn.bkt.clouddn.com/blog/4/4-3.png) \n我们从图中可以看到，在加入了限制后，最终的optimization不是落在极小值点，而是落在图中所示的位置。从另一个角度来想，regularization item的加入，使得整个cost function在寻找最小值的时候，要均衡的考虑objective function和regularization item的大小.\n\n在这个地方，我和优男讨论的时候有一个地方没有想通，例如使用gradient descent进行optimzation的时候，怎么保证优化可以落到图中的点呢，我是这么考虑的：当加入regularization后，cost function本身就有了变化，随之而来的是gradient也发生了变化，在gradient descent迭代过程中就已经把regularization的影响带了进去，因此在每一次迭代的时候，实际上应该都是按照上式的限制进行优化的。\n\n当然，上图也可以用来就是为什么lasso可以获得稀疏特征，那就是因为lasso更可能在坐标轴上和objective function产生交点，进而使得一些特征变成0.\n\n## Reference\n* [CS 195-5: Machine Learning](https://pdfs.semanticscholar.org/91a9/5626d24c8393e3b784e44f62de201d20dede.pdf)\n* [STAT 897D](https://onlinecourses.science.psu.edu/stat857/node/155)","source":"_posts/ridge-lasso.md","raw":"---\ntitle: 深入聊聊正则化\ndate: 2017-08-27 01:55:07\ntags: \n\t- regularization\n\t- MAP\n\t- ridge regression\n\t- lasso regression\ncategories: machine learning\n---\n最近和优男一起聊到了L1和L2 regularization，期间遇到了很多没有想明白的问题，加上最近工作有些忙，空余时间用来倒腾新到货的小米路由器，只能趁周末自己研究研究，下面和大家分享一下regularization中一些深入的问题。不讨论基础知识，直接上干货。\n<!--more-->\n## MAP and regularization\n我们都知道，当cost function在没有加regularization的时候，我们对参数使用的是**MLE**(Maximum likelihood estimation)，对应频率学派所认为的参数本无分布规律的观点；在Andrew Ng经典的CS229中，这位AI大师曾经提到，regularization其实是对参数的**MAP**(Maximum a posteriori estimation)，是基于贝叶斯学派认为的参数本有**priori distribution**，同时吸纳了MLE的一种中间观点。\n\n这里的priori distribution，就是根据经验，认为参数应该大致符合某个distribution，这样，最终获得的参数估计结果也会和这个被认为的distribution有一些相近\n\n而我们所熟知的L1 regularization，其实就是认为参数的priori distribution是**Laplacian distribution**，而L2 regularization，则认为参数的priorit distribution是**Gaussian distribution**，相信大家对Gaussian distribution是很熟悉的，而对于Laplacian distribution，它的分布是\n$$p(x;a)= \\frac{a}{2} e^{-a|x|}$$\n下图就是两者的一个比较：\n![](http://otmy7guvn.bkt.clouddn.com/blog/4/4-1.png) \n在后文中，我们会看到，两种distribution的特点决定了两种regularization的性质。\n### Lasso regression\n在liner regression中，我们假设参数\\\\( \\theta\\\\)服从Laplacian distribution，cost function就成了\n$$J( \\theta) = \\frac{1}{2} \\sum^{m}_{i=1} (y^{(i)}- \\theta^T x^{(i)})+ \\lambda \\sum^{d}_{j=1}| \\theta^{(i)}|$$\n上式就是Lasso regression\n### Ridge regression\n在liner regression中，我们假设参数\\\\( \\theta\\\\)服从Gaussian distribution，cost function就成了\n$$J( \\theta) = \\frac{1}{2} \\sum^{m}_{i=1} (y^{(i)}- \\theta^T x^{(i)})+ \\lambda \\sum^{d}_{j=1} ( \\theta^{(i)})^2$$\n上式就是Ridge regression或shrinkage\n## geometry of error surfaces\n在不考虑参数priori distribution的时候，cost function的形式是\n$$J( \\theta) = \\frac{1}{2} \\sum^{m}_{i=1} (y^{(i)}- \\theta^T x^{(i)})^2$$\n用二维截面图展示就是\n![](http://otmy7guvn.bkt.clouddn.com/blog/4/4-2.png) \n图中只有objective function，横纵轴是参数\\\\( \\theta\\\\)，截取过来的图，所以上面的参数是\\\\(w\\\\)，\\\\(l\\\\)是loss值，箭头指向的点就是cost function的极小点。在不考虑参数priori distribution的时候，这个点就是我们的optimization target.\n\n下面大家来我一起做一个头脑风暴，所谓参数的priori distribution，其实就是用来限制最后optimization结果的一个限定，那么我们其实就是在做一个受限制的的convex optimization，即：\n$$ \\theta=argmin \\frac{1}{2} \\sum^{m}_{i=1} (y^{(i)}- \\theta^T x^{(i)})^2$$\n$$ s.t. \\sum^{d}_{j=1}| \\theta^{(i)}|^p \\geq \\beta$$\n其中，\\\\( \\beta\\\\)是ridge或者lasso的最小值。\n那么此时的图就变成了：\n![](http://otmy7guvn.bkt.clouddn.com/blog/4/4-3.png) \n我们从图中可以看到，在加入了限制后，最终的optimization不是落在极小值点，而是落在图中所示的位置。从另一个角度来想，regularization item的加入，使得整个cost function在寻找最小值的时候，要均衡的考虑objective function和regularization item的大小.\n\n在这个地方，我和优男讨论的时候有一个地方没有想通，例如使用gradient descent进行optimzation的时候，怎么保证优化可以落到图中的点呢，我是这么考虑的：当加入regularization后，cost function本身就有了变化，随之而来的是gradient也发生了变化，在gradient descent迭代过程中就已经把regularization的影响带了进去，因此在每一次迭代的时候，实际上应该都是按照上式的限制进行优化的。\n\n当然，上图也可以用来就是为什么lasso可以获得稀疏特征，那就是因为lasso更可能在坐标轴上和objective function产生交点，进而使得一些特征变成0.\n\n## Reference\n* [CS 195-5: Machine Learning](https://pdfs.semanticscholar.org/91a9/5626d24c8393e3b784e44f62de201d20dede.pdf)\n* [STAT 897D](https://onlinecourses.science.psu.edu/stat857/node/155)","slug":"ridge-lasso","published":1,"updated":"2017-08-28T09:44:13.916Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cj8lez593001jw2kzf6mf2xtd","content":"<p>最近和优男一起聊到了L1和L2 regularization，期间遇到了很多没有想明白的问题，加上最近工作有些忙，空余时间用来倒腾新到货的小米路由器，只能趁周末自己研究研究，下面和大家分享一下regularization中一些深入的问题。不讨论基础知识，直接上干货。<br><a id=\"more\"></a></p>\n<h2 id=\"MAP-and-regularization\"><a href=\"#MAP-and-regularization\" class=\"headerlink\" title=\"MAP and regularization\"></a>MAP and regularization</h2><p>我们都知道，当cost function在没有加regularization的时候，我们对参数使用的是<strong>MLE</strong>(Maximum likelihood estimation)，对应频率学派所认为的参数本无分布规律的观点；在Andrew Ng经典的CS229中，这位AI大师曾经提到，regularization其实是对参数的<strong>MAP</strong>(Maximum a posteriori estimation)，是基于贝叶斯学派认为的参数本有<strong>priori distribution</strong>，同时吸纳了MLE的一种中间观点。</p>\n<p>这里的priori distribution，就是根据经验，认为参数应该大致符合某个distribution，这样，最终获得的参数估计结果也会和这个被认为的distribution有一些相近</p>\n<p>而我们所熟知的L1 regularization，其实就是认为参数的priori distribution是<strong>Laplacian distribution</strong>，而L2 regularization，则认为参数的priorit distribution是<strong>Gaussian distribution</strong>，相信大家对Gaussian distribution是很熟悉的，而对于Laplacian distribution，它的分布是</p>\n<script type=\"math/tex; mode=display\">p(x;a)= \\frac{a}{2} e^{-a|x|}</script><p>下图就是两者的一个比较：<br><img src=\"http://otmy7guvn.bkt.clouddn.com/blog/4/4-1.png\" alt=\"\"><br>在后文中，我们会看到，两种distribution的特点决定了两种regularization的性质。</p>\n<h3 id=\"Lasso-regression\"><a href=\"#Lasso-regression\" class=\"headerlink\" title=\"Lasso regression\"></a>Lasso regression</h3><p>在liner regression中，我们假设参数\\( \\theta\\)服从Laplacian distribution，cost function就成了</p>\n<script type=\"math/tex; mode=display\">J( \\theta) = \\frac{1}{2} \\sum^{m}_{i=1} (y^{(i)}- \\theta^T x^{(i)})+ \\lambda \\sum^{d}_{j=1}| \\theta^{(i)}|</script><p>上式就是Lasso regression</p>\n<h3 id=\"Ridge-regression\"><a href=\"#Ridge-regression\" class=\"headerlink\" title=\"Ridge regression\"></a>Ridge regression</h3><p>在liner regression中，我们假设参数\\( \\theta\\)服从Gaussian distribution，cost function就成了</p>\n<script type=\"math/tex; mode=display\">J( \\theta) = \\frac{1}{2} \\sum^{m}_{i=1} (y^{(i)}- \\theta^T x^{(i)})+ \\lambda \\sum^{d}_{j=1} ( \\theta^{(i)})^2</script><p>上式就是Ridge regression或shrinkage</p>\n<h2 id=\"geometry-of-error-surfaces\"><a href=\"#geometry-of-error-surfaces\" class=\"headerlink\" title=\"geometry of error surfaces\"></a>geometry of error surfaces</h2><p>在不考虑参数priori distribution的时候，cost function的形式是</p>\n<script type=\"math/tex; mode=display\">J( \\theta) = \\frac{1}{2} \\sum^{m}_{i=1} (y^{(i)}- \\theta^T x^{(i)})^2</script><p>用二维截面图展示就是<br><img src=\"http://otmy7guvn.bkt.clouddn.com/blog/4/4-2.png\" alt=\"\"><br>图中只有objective function，横纵轴是参数\\( \\theta\\)，截取过来的图，所以上面的参数是\\(w\\)，\\(l\\)是loss值，箭头指向的点就是cost function的极小点。在不考虑参数priori distribution的时候，这个点就是我们的optimization target.</p>\n<p>下面大家来我一起做一个头脑风暴，所谓参数的priori distribution，其实就是用来限制最后optimization结果的一个限定，那么我们其实就是在做一个受限制的的convex optimization，即：</p>\n<script type=\"math/tex; mode=display\">\\theta=argmin \\frac{1}{2} \\sum^{m}_{i=1} (y^{(i)}- \\theta^T x^{(i)})^2</script><script type=\"math/tex; mode=display\">s.t. \\sum^{d}_{j=1}| \\theta^{(i)}|^p \\geq \\beta</script><p>其中，\\( \\beta\\)是ridge或者lasso的最小值。<br>那么此时的图就变成了：<br><img src=\"http://otmy7guvn.bkt.clouddn.com/blog/4/4-3.png\" alt=\"\"><br>我们从图中可以看到，在加入了限制后，最终的optimization不是落在极小值点，而是落在图中所示的位置。从另一个角度来想，regularization item的加入，使得整个cost function在寻找最小值的时候，要均衡的考虑objective function和regularization item的大小.</p>\n<p>在这个地方，我和优男讨论的时候有一个地方没有想通，例如使用gradient descent进行optimzation的时候，怎么保证优化可以落到图中的点呢，我是这么考虑的：当加入regularization后，cost function本身就有了变化，随之而来的是gradient也发生了变化，在gradient descent迭代过程中就已经把regularization的影响带了进去，因此在每一次迭代的时候，实际上应该都是按照上式的限制进行优化的。</p>\n<p>当然，上图也可以用来就是为什么lasso可以获得稀疏特征，那就是因为lasso更可能在坐标轴上和objective function产生交点，进而使得一些特征变成0.</p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ul>\n<li><a href=\"https://pdfs.semanticscholar.org/91a9/5626d24c8393e3b784e44f62de201d20dede.pdf\" target=\"_blank\" rel=\"external\">CS 195-5: Machine Learning</a></li>\n<li><a href=\"https://onlinecourses.science.psu.edu/stat857/node/155\" target=\"_blank\" rel=\"external\">STAT 897D</a></li>\n</ul>\n","site":{"data":{}},"excerpt":"<p>最近和优男一起聊到了L1和L2 regularization，期间遇到了很多没有想明白的问题，加上最近工作有些忙，空余时间用来倒腾新到货的小米路由器，只能趁周末自己研究研究，下面和大家分享一下regularization中一些深入的问题。不讨论基础知识，直接上干货。<br>","more":"</p>\n<h2 id=\"MAP-and-regularization\"><a href=\"#MAP-and-regularization\" class=\"headerlink\" title=\"MAP and regularization\"></a>MAP and regularization</h2><p>我们都知道，当cost function在没有加regularization的时候，我们对参数使用的是<strong>MLE</strong>(Maximum likelihood estimation)，对应频率学派所认为的参数本无分布规律的观点；在Andrew Ng经典的CS229中，这位AI大师曾经提到，regularization其实是对参数的<strong>MAP</strong>(Maximum a posteriori estimation)，是基于贝叶斯学派认为的参数本有<strong>priori distribution</strong>，同时吸纳了MLE的一种中间观点。</p>\n<p>这里的priori distribution，就是根据经验，认为参数应该大致符合某个distribution，这样，最终获得的参数估计结果也会和这个被认为的distribution有一些相近</p>\n<p>而我们所熟知的L1 regularization，其实就是认为参数的priori distribution是<strong>Laplacian distribution</strong>，而L2 regularization，则认为参数的priorit distribution是<strong>Gaussian distribution</strong>，相信大家对Gaussian distribution是很熟悉的，而对于Laplacian distribution，它的分布是</p>\n<script type=\"math/tex; mode=display\">p(x;a)= \\frac{a}{2} e^{-a|x|}</script><p>下图就是两者的一个比较：<br><img src=\"http://otmy7guvn.bkt.clouddn.com/blog/4/4-1.png\" alt=\"\"><br>在后文中，我们会看到，两种distribution的特点决定了两种regularization的性质。</p>\n<h3 id=\"Lasso-regression\"><a href=\"#Lasso-regression\" class=\"headerlink\" title=\"Lasso regression\"></a>Lasso regression</h3><p>在liner regression中，我们假设参数\\( \\theta\\)服从Laplacian distribution，cost function就成了</p>\n<script type=\"math/tex; mode=display\">J( \\theta) = \\frac{1}{2} \\sum^{m}_{i=1} (y^{(i)}- \\theta^T x^{(i)})+ \\lambda \\sum^{d}_{j=1}| \\theta^{(i)}|</script><p>上式就是Lasso regression</p>\n<h3 id=\"Ridge-regression\"><a href=\"#Ridge-regression\" class=\"headerlink\" title=\"Ridge regression\"></a>Ridge regression</h3><p>在liner regression中，我们假设参数\\( \\theta\\)服从Gaussian distribution，cost function就成了</p>\n<script type=\"math/tex; mode=display\">J( \\theta) = \\frac{1}{2} \\sum^{m}_{i=1} (y^{(i)}- \\theta^T x^{(i)})+ \\lambda \\sum^{d}_{j=1} ( \\theta^{(i)})^2</script><p>上式就是Ridge regression或shrinkage</p>\n<h2 id=\"geometry-of-error-surfaces\"><a href=\"#geometry-of-error-surfaces\" class=\"headerlink\" title=\"geometry of error surfaces\"></a>geometry of error surfaces</h2><p>在不考虑参数priori distribution的时候，cost function的形式是</p>\n<script type=\"math/tex; mode=display\">J( \\theta) = \\frac{1}{2} \\sum^{m}_{i=1} (y^{(i)}- \\theta^T x^{(i)})^2</script><p>用二维截面图展示就是<br><img src=\"http://otmy7guvn.bkt.clouddn.com/blog/4/4-2.png\" alt=\"\"><br>图中只有objective function，横纵轴是参数\\( \\theta\\)，截取过来的图，所以上面的参数是\\(w\\)，\\(l\\)是loss值，箭头指向的点就是cost function的极小点。在不考虑参数priori distribution的时候，这个点就是我们的optimization target.</p>\n<p>下面大家来我一起做一个头脑风暴，所谓参数的priori distribution，其实就是用来限制最后optimization结果的一个限定，那么我们其实就是在做一个受限制的的convex optimization，即：</p>\n<script type=\"math/tex; mode=display\">\\theta=argmin \\frac{1}{2} \\sum^{m}_{i=1} (y^{(i)}- \\theta^T x^{(i)})^2</script><script type=\"math/tex; mode=display\">s.t. \\sum^{d}_{j=1}| \\theta^{(i)}|^p \\geq \\beta</script><p>其中，\\( \\beta\\)是ridge或者lasso的最小值。<br>那么此时的图就变成了：<br><img src=\"http://otmy7guvn.bkt.clouddn.com/blog/4/4-3.png\" alt=\"\"><br>我们从图中可以看到，在加入了限制后，最终的optimization不是落在极小值点，而是落在图中所示的位置。从另一个角度来想，regularization item的加入，使得整个cost function在寻找最小值的时候，要均衡的考虑objective function和regularization item的大小.</p>\n<p>在这个地方，我和优男讨论的时候有一个地方没有想通，例如使用gradient descent进行optimzation的时候，怎么保证优化可以落到图中的点呢，我是这么考虑的：当加入regularization后，cost function本身就有了变化，随之而来的是gradient也发生了变化，在gradient descent迭代过程中就已经把regularization的影响带了进去，因此在每一次迭代的时候，实际上应该都是按照上式的限制进行优化的。</p>\n<p>当然，上图也可以用来就是为什么lasso可以获得稀疏特征，那就是因为lasso更可能在坐标轴上和objective function产生交点，进而使得一些特征变成0.</p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ul>\n<li><a href=\"https://pdfs.semanticscholar.org/91a9/5626d24c8393e3b784e44f62de201d20dede.pdf\" target=\"_blank\" rel=\"external\">CS 195-5: Machine Learning</a></li>\n<li><a href=\"https://onlinecourses.science.psu.edu/stat857/node/155\" target=\"_blank\" rel=\"external\">STAT 897D</a></li>\n</ul>"},{"title":"从凸函数到梯度下降和牛顿法","date":"2017-08-02T07:53:55.000Z","_content":"记得我在和优男一起研究logistic regression的时候，他问了我几个非常尖锐的问题，让我顿时哑口无言\n* 怎么保证logistic regression通过gradient descent找到的是最优解；\n* 为什么logistic regression可以用newton's method呢？\n* Newton's method中Hessian matrix必须positive definite有什么意义呢，log cost function能保证吗？\n\n这些细节问题，说实话我也没有认真的想过。在夸奖他之余，我们也一起开始了研究，希望从中学习到一些更深层的东西，趁着现在有个blog分享给大家\n<!--more-->\n\n## 凸函数(Convex function)\n在开始之前，我有一个关于术语的倡议。中文里的“凸函数”，看上去是凹下去的，对应的，中文里的“凹函数”看上去凸起来的，amazing吧？这是有一定历史原因的，感兴趣的朋友可以去查阅下资料，这里我们不再复述。所以为了避免让大家产生误解，我鼓励大家使用英文，**convex function**和**concave function**.这样会避免很多不必要的麻烦。\n\nOK，我们来一起看看，convex function\n\n对于一维函数 \\\\(f(x)\\\\)来说，在定义域内的任意值 \\\\(a\\\\)和\\\\(b\\\\)，对于任意的 \\\\( 0 \\leq \\theta \\leq 1\\\\)，如果满足以下条件，则称为convex function\n$$f(\\theta a+(1-\\theta b)) \\leq \\theta f(a) + (1- \\theta)f(b)$$\n我们再用图片直观的感受一下\n\n![](http://otmy7guvn.bkt.clouddn.com/blog/1/1-1.png) \n\n显而易见的是，当公式中等号去掉的时候，函数就是**strictly convex function**.\n\nConvex function具有一定的性质，我们简单的描述一下。\n\n### First order condition\n对于 function \\\\(f\\\\)，在定义域内**一阶可导**，且导数为\n$$ \t\\nabla f=( \\frac{\\partial f(x)}{x_1}, \\frac{\\partial f(x)}{x_2},...,  \\frac{\\partial f(x)}{x_n})$$\n那么 \\\\(f\\\\)是convex function的**充要条件**是：对于定义域内任意 \\\\(x\\\\) 和 \\\\(y\\\\)\n$$ f(y) \\geq f(x) + \\nabla f(x)^T (y - x)$$\nOK，再来张图片直观感受一下：\n\n![](http://otmy7guvn.bkt.clouddn.com/blog/1/1-2.png) \n\n其实简单的来讲，就是对于convex function \\\\(f\\\\)，它的函数值永远大于等于切线上的值！\n\n### Second order condition\n对于 function \\\\(f\\\\)，在定义域内**二阶可导**，且 \\\\(n\\\\) 维方阵Hessian matrix的元素为\n$$ \\nabla ^2 f(x) = \\frac{ \\partial ^2 f(x)}{ \\partial x_i \\partial x_j}, \\quad i,j = 1,2,...,n$$\n当且仅当Hessian matrix positive semi-defnite的时候，\\\\(f\\\\) 是convex function。以上互为**充要条件**。这里的证明我不想展开讲，在后面我会给出reference链接。\n\n---\n下面给出一些 \\\\(\\Bbb R\\\\) 空间下常见的convex function：\n* 线性函数：\\\\(f(x) = ax+b\\\\)\n* 指数函数：\\\\(f(x)=e^ {ax}\\\\)\n* 负熵函数： \\\\(f(x)=xlogx \\quad on \\quad \\Bbb R_{++}\\\\)\n\n对应的，一些常见的concave function：\n* 线性函数：\\\\(f(x) = ax+b\\\\)\n* 对数函数： \\\\(f(x)=logx  \\quad on \\quad \\Bbb R_{++}\\\\)\n\n其中大家可以看到，线性函数既是convex也是concave函数，比较特殊，这和它本身的first order condition为常数有关。\n\n以上就是convex function的一个简单介绍，你也许会问，为什么花这么多力气来介绍convex function. 其实，在machine learning中，convex function的优化是非常重要的，很多算法说到底，都是要optimize一个convex function，我们会用liner regression和logistic regression为例子，进一步从convex function的简介过渡到gradient descent和newton's method.\n\n## 梯度下降法(Gradient descent)\n关于gradient descent，我们使用liner regression作为例子来讨论。Liner regression算法的实质是least square method，他的cost function是\n$$J( \\theta)= \\frac{1}{2} \\sum_{i=1} ^m (h_ \\theta (x ^{(i)})-y^{(i)})^2, \\quad h_ \\theta (x)= \\theta ^Tx $$\n对于liner regression来说，算法的实质就是去求出\\\\( J( \\theta) \\\\) **以\\\\( \\theta\\\\)为参数**的minimum，gradient descent算法的作用就是去实现了这个过程，gradient descent的基础知识详见reference. \n\n那么针对cost function，gradient descent是如何保证收敛的呢，我们一起来看看\n\n对于\\\\(J( \\theta)\\\\)，我们将其带入convex function的定义公式中，注意这里我们的自变量是\\\\( \\theta\\\\)，我们可以通过推导证明该式成立，也就是说，least square cost function是convex function.\n\n既然有这个结论了，那么我们可以想象一下，least square cost function作为convex function，是存在全局最小值的，也就是说，gradient descent不会出现陷入局部最优无法自拔的现象，只要gradient descent保证参数足够好的情况下，理论上，是完全可以很好的逼近全局最优的解的。\n\n> Gradien descent算法本身并不能保证获得全局最小值，只有在objective function是convex function的时候才可以保证\n\n下图可以看出，右边的object function是non-convex function，因而很容易陷入到局部最小值无法自拔，而左边的objective function是一个标准的convex function，在gradient descent参数合理的前提下，可以逼近全局最优。\n\n![](http://otmy7guvn.bkt.clouddn.com/blog/1/1-3.png) \n\n当然，gradien descent的一些改进方法，例如stochastic gradient descent在解决non-convex optimization上有一些帮助，但是我们在这里不做讨论，后面有时间我会专门再写。\n\n由此，我们可以得出，gradient descent不仅仅是minimize liner regression的一个很好的方法，也是convex optimization的一种理想方法\n\n## 牛顿法(Newton's method)\nNewton's method 这块内容，我们将会用logistic regression作为例子。同样，我们先来关注下log cost function，这里，**我们取label为-1和+1**，因为这样得到的cost function比0,1下的计算更加简单\n$$J( \\omega)= - \\frac{1}{m} \\sum_{i=1} ^{m} log(1+e^{-y^{(i)} \\omega^T x^{(i)}})$$\n这里我们采用了-1和+1作为标签值，和大多数教材中不一样，大家可以下来自己推导一下\\\\(J( \\omega)\\\\)，这种写法广泛的应用在了比较logistic regression和SVM两大分类器的文献中，希望大家熟知。\n\n此处我们对原始的likehood function加上了 \\\\(- \\frac{1}{m}\\\\)的系数，同样，当我们把 \\\\(J( \\omega)\\\\)带入到convex function的定义中，可以验证上式为convex function，值得注意的是，\\\\(J( \\omega)\\\\)是\\\\( \\omega\\\\)的函数。\n\n其实，我们也可以将log cost function展开后，利用最基本的函数convex和concave性质来获得上式是convex function的结论，碍于公式实在太难打，就留给大家去证明吧。\n\nOK，既然log cost function是convex function，我们一定是可以用gradient descent去求解的。问题是，如果我们用newton's method呢？\n\nNewton's method的基本原理详见reference，这里我们可以发现，既然log cost function是convex function，那么根据second order condition可以知道，它的Hessian matrix一定是positive semi-definite的。如果我们加上了L2 regularizer，**由于L2 regularizer本身就是一个strict convex function**，那么log cost function就一定是strict convex function了，也就是：\n$$J( \\omega)= - \\frac{1}{m} \\sum_{i=1} ^{m} log(1+e^{-y^{(i)} \\omega^T x^{(i)}})+ \\frac{1}{2}|| \\omega||^2$$\n因此，在log cost function中，**Hessian matrix是positive definite的**，完全满足newton's method 的要求。同样，类似于上一部分，newton's method也可以找到log cost function的全局最优。\n## Sum up\nOK，我们说到这里也确实讲了不少，这篇blog有些冗长，希望朋友们不要焦虑。总体来说，我想表达的是以下几个观点：\n* Machine learning中我们寻求的其实就是objective function一个全局最优值，这些问题是通过gradient descent等方法解决的；\n* Gradient descent和newton's method都是convex optimization的好方法，他们都可以对于convex function获得全局最优；\n* 对于non-convex optimization问题，stochastic gradient descent也很有效果，我们后续再慢慢学习。\n\n好了，核心思想就这三点，今天先说这么多！\n\n## Reference\n* [EE364, Convex Optimization Stanford University](https://see.stanford.edu/materials/lsocoee364a/03ConvexFunctions.pdf)\n* [Regularized Logistic Regression is Strictly Convex](http://qwone.com/~jason/writing/convexLR.pdf)\n* [XinyiLI大神的blog](https://www.yangzhou301.com/2016/03/14/826442654/)\n* [Liner regression](https://en.wikipedia.org/wiki/Linear_regression)\n* [Logsitc regression](https://en.wikipedia.org/wiki/Logistic_regression)\n* [Gradient descent](https://en.wikipedia.org/wiki/Gradient_descent)\n* [Newton's method](https://en.wikipedia.org/wiki/Newton%27s_method)\n\n","source":"_posts/convex-opt.md","raw":"---\ntitle: 从凸函数到梯度下降和牛顿法\ndate: 2017-08-02 15:53:55\ntags: \n    - convex optimization\n    - gradient descent\n    - newton's method\ncategories: machine learning\n---\n记得我在和优男一起研究logistic regression的时候，他问了我几个非常尖锐的问题，让我顿时哑口无言\n* 怎么保证logistic regression通过gradient descent找到的是最优解；\n* 为什么logistic regression可以用newton's method呢？\n* Newton's method中Hessian matrix必须positive definite有什么意义呢，log cost function能保证吗？\n\n这些细节问题，说实话我也没有认真的想过。在夸奖他之余，我们也一起开始了研究，希望从中学习到一些更深层的东西，趁着现在有个blog分享给大家\n<!--more-->\n\n## 凸函数(Convex function)\n在开始之前，我有一个关于术语的倡议。中文里的“凸函数”，看上去是凹下去的，对应的，中文里的“凹函数”看上去凸起来的，amazing吧？这是有一定历史原因的，感兴趣的朋友可以去查阅下资料，这里我们不再复述。所以为了避免让大家产生误解，我鼓励大家使用英文，**convex function**和**concave function**.这样会避免很多不必要的麻烦。\n\nOK，我们来一起看看，convex function\n\n对于一维函数 \\\\(f(x)\\\\)来说，在定义域内的任意值 \\\\(a\\\\)和\\\\(b\\\\)，对于任意的 \\\\( 0 \\leq \\theta \\leq 1\\\\)，如果满足以下条件，则称为convex function\n$$f(\\theta a+(1-\\theta b)) \\leq \\theta f(a) + (1- \\theta)f(b)$$\n我们再用图片直观的感受一下\n\n![](http://otmy7guvn.bkt.clouddn.com/blog/1/1-1.png) \n\n显而易见的是，当公式中等号去掉的时候，函数就是**strictly convex function**.\n\nConvex function具有一定的性质，我们简单的描述一下。\n\n### First order condition\n对于 function \\\\(f\\\\)，在定义域内**一阶可导**，且导数为\n$$ \t\\nabla f=( \\frac{\\partial f(x)}{x_1}, \\frac{\\partial f(x)}{x_2},...,  \\frac{\\partial f(x)}{x_n})$$\n那么 \\\\(f\\\\)是convex function的**充要条件**是：对于定义域内任意 \\\\(x\\\\) 和 \\\\(y\\\\)\n$$ f(y) \\geq f(x) + \\nabla f(x)^T (y - x)$$\nOK，再来张图片直观感受一下：\n\n![](http://otmy7guvn.bkt.clouddn.com/blog/1/1-2.png) \n\n其实简单的来讲，就是对于convex function \\\\(f\\\\)，它的函数值永远大于等于切线上的值！\n\n### Second order condition\n对于 function \\\\(f\\\\)，在定义域内**二阶可导**，且 \\\\(n\\\\) 维方阵Hessian matrix的元素为\n$$ \\nabla ^2 f(x) = \\frac{ \\partial ^2 f(x)}{ \\partial x_i \\partial x_j}, \\quad i,j = 1,2,...,n$$\n当且仅当Hessian matrix positive semi-defnite的时候，\\\\(f\\\\) 是convex function。以上互为**充要条件**。这里的证明我不想展开讲，在后面我会给出reference链接。\n\n---\n下面给出一些 \\\\(\\Bbb R\\\\) 空间下常见的convex function：\n* 线性函数：\\\\(f(x) = ax+b\\\\)\n* 指数函数：\\\\(f(x)=e^ {ax}\\\\)\n* 负熵函数： \\\\(f(x)=xlogx \\quad on \\quad \\Bbb R_{++}\\\\)\n\n对应的，一些常见的concave function：\n* 线性函数：\\\\(f(x) = ax+b\\\\)\n* 对数函数： \\\\(f(x)=logx  \\quad on \\quad \\Bbb R_{++}\\\\)\n\n其中大家可以看到，线性函数既是convex也是concave函数，比较特殊，这和它本身的first order condition为常数有关。\n\n以上就是convex function的一个简单介绍，你也许会问，为什么花这么多力气来介绍convex function. 其实，在machine learning中，convex function的优化是非常重要的，很多算法说到底，都是要optimize一个convex function，我们会用liner regression和logistic regression为例子，进一步从convex function的简介过渡到gradient descent和newton's method.\n\n## 梯度下降法(Gradient descent)\n关于gradient descent，我们使用liner regression作为例子来讨论。Liner regression算法的实质是least square method，他的cost function是\n$$J( \\theta)= \\frac{1}{2} \\sum_{i=1} ^m (h_ \\theta (x ^{(i)})-y^{(i)})^2, \\quad h_ \\theta (x)= \\theta ^Tx $$\n对于liner regression来说，算法的实质就是去求出\\\\( J( \\theta) \\\\) **以\\\\( \\theta\\\\)为参数**的minimum，gradient descent算法的作用就是去实现了这个过程，gradient descent的基础知识详见reference. \n\n那么针对cost function，gradient descent是如何保证收敛的呢，我们一起来看看\n\n对于\\\\(J( \\theta)\\\\)，我们将其带入convex function的定义公式中，注意这里我们的自变量是\\\\( \\theta\\\\)，我们可以通过推导证明该式成立，也就是说，least square cost function是convex function.\n\n既然有这个结论了，那么我们可以想象一下，least square cost function作为convex function，是存在全局最小值的，也就是说，gradient descent不会出现陷入局部最优无法自拔的现象，只要gradient descent保证参数足够好的情况下，理论上，是完全可以很好的逼近全局最优的解的。\n\n> Gradien descent算法本身并不能保证获得全局最小值，只有在objective function是convex function的时候才可以保证\n\n下图可以看出，右边的object function是non-convex function，因而很容易陷入到局部最小值无法自拔，而左边的objective function是一个标准的convex function，在gradient descent参数合理的前提下，可以逼近全局最优。\n\n![](http://otmy7guvn.bkt.clouddn.com/blog/1/1-3.png) \n\n当然，gradien descent的一些改进方法，例如stochastic gradient descent在解决non-convex optimization上有一些帮助，但是我们在这里不做讨论，后面有时间我会专门再写。\n\n由此，我们可以得出，gradient descent不仅仅是minimize liner regression的一个很好的方法，也是convex optimization的一种理想方法\n\n## 牛顿法(Newton's method)\nNewton's method 这块内容，我们将会用logistic regression作为例子。同样，我们先来关注下log cost function，这里，**我们取label为-1和+1**，因为这样得到的cost function比0,1下的计算更加简单\n$$J( \\omega)= - \\frac{1}{m} \\sum_{i=1} ^{m} log(1+e^{-y^{(i)} \\omega^T x^{(i)}})$$\n这里我们采用了-1和+1作为标签值，和大多数教材中不一样，大家可以下来自己推导一下\\\\(J( \\omega)\\\\)，这种写法广泛的应用在了比较logistic regression和SVM两大分类器的文献中，希望大家熟知。\n\n此处我们对原始的likehood function加上了 \\\\(- \\frac{1}{m}\\\\)的系数，同样，当我们把 \\\\(J( \\omega)\\\\)带入到convex function的定义中，可以验证上式为convex function，值得注意的是，\\\\(J( \\omega)\\\\)是\\\\( \\omega\\\\)的函数。\n\n其实，我们也可以将log cost function展开后，利用最基本的函数convex和concave性质来获得上式是convex function的结论，碍于公式实在太难打，就留给大家去证明吧。\n\nOK，既然log cost function是convex function，我们一定是可以用gradient descent去求解的。问题是，如果我们用newton's method呢？\n\nNewton's method的基本原理详见reference，这里我们可以发现，既然log cost function是convex function，那么根据second order condition可以知道，它的Hessian matrix一定是positive semi-definite的。如果我们加上了L2 regularizer，**由于L2 regularizer本身就是一个strict convex function**，那么log cost function就一定是strict convex function了，也就是：\n$$J( \\omega)= - \\frac{1}{m} \\sum_{i=1} ^{m} log(1+e^{-y^{(i)} \\omega^T x^{(i)}})+ \\frac{1}{2}|| \\omega||^2$$\n因此，在log cost function中，**Hessian matrix是positive definite的**，完全满足newton's method 的要求。同样，类似于上一部分，newton's method也可以找到log cost function的全局最优。\n## Sum up\nOK，我们说到这里也确实讲了不少，这篇blog有些冗长，希望朋友们不要焦虑。总体来说，我想表达的是以下几个观点：\n* Machine learning中我们寻求的其实就是objective function一个全局最优值，这些问题是通过gradient descent等方法解决的；\n* Gradient descent和newton's method都是convex optimization的好方法，他们都可以对于convex function获得全局最优；\n* 对于non-convex optimization问题，stochastic gradient descent也很有效果，我们后续再慢慢学习。\n\n好了，核心思想就这三点，今天先说这么多！\n\n## Reference\n* [EE364, Convex Optimization Stanford University](https://see.stanford.edu/materials/lsocoee364a/03ConvexFunctions.pdf)\n* [Regularized Logistic Regression is Strictly Convex](http://qwone.com/~jason/writing/convexLR.pdf)\n* [XinyiLI大神的blog](https://www.yangzhou301.com/2016/03/14/826442654/)\n* [Liner regression](https://en.wikipedia.org/wiki/Linear_regression)\n* [Logsitc regression](https://en.wikipedia.org/wiki/Logistic_regression)\n* [Gradient descent](https://en.wikipedia.org/wiki/Gradient_descent)\n* [Newton's method](https://en.wikipedia.org/wiki/Newton%27s_method)\n\n","slug":"convex-opt","published":1,"updated":"2017-08-13T03:56:47.696Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cj8lez595001kw2kzghc4db7n","content":"<p>记得我在和优男一起研究logistic regression的时候，他问了我几个非常尖锐的问题，让我顿时哑口无言</p>\n<ul>\n<li>怎么保证logistic regression通过gradient descent找到的是最优解；</li>\n<li>为什么logistic regression可以用newton’s method呢？</li>\n<li>Newton’s method中Hessian matrix必须positive definite有什么意义呢，log cost function能保证吗？</li>\n</ul>\n<p>这些细节问题，说实话我也没有认真的想过。在夸奖他之余，我们也一起开始了研究，希望从中学习到一些更深层的东西，趁着现在有个blog分享给大家<br><a id=\"more\"></a></p>\n<h2 id=\"凸函数-Convex-function\"><a href=\"#凸函数-Convex-function\" class=\"headerlink\" title=\"凸函数(Convex function)\"></a>凸函数(Convex function)</h2><p>在开始之前，我有一个关于术语的倡议。中文里的“凸函数”，看上去是凹下去的，对应的，中文里的“凹函数”看上去凸起来的，amazing吧？这是有一定历史原因的，感兴趣的朋友可以去查阅下资料，这里我们不再复述。所以为了避免让大家产生误解，我鼓励大家使用英文，<strong>convex function</strong>和<strong>concave function</strong>.这样会避免很多不必要的麻烦。</p>\n<p>OK，我们来一起看看，convex function</p>\n<p>对于一维函数 \\(f(x)\\)来说，在定义域内的任意值 \\(a\\)和\\(b\\)，对于任意的 \\( 0 \\leq \\theta \\leq 1\\)，如果满足以下条件，则称为convex function</p>\n<script type=\"math/tex; mode=display\">f(\\theta a+(1-\\theta b)) \\leq \\theta f(a) + (1- \\theta)f(b)</script><p>我们再用图片直观的感受一下</p>\n<p><img src=\"http://otmy7guvn.bkt.clouddn.com/blog/1/1-1.png\" alt=\"\"> </p>\n<p>显而易见的是，当公式中等号去掉的时候，函数就是<strong>strictly convex function</strong>.</p>\n<p>Convex function具有一定的性质，我们简单的描述一下。</p>\n<h3 id=\"First-order-condition\"><a href=\"#First-order-condition\" class=\"headerlink\" title=\"First order condition\"></a>First order condition</h3><p>对于 function \\(f\\)，在定义域内<strong>一阶可导</strong>，且导数为</p>\n<script type=\"math/tex; mode=display\">\\nabla f=( \\frac{\\partial f(x)}{x_1}, \\frac{\\partial f(x)}{x_2},...,  \\frac{\\partial f(x)}{x_n})</script><p>那么 \\(f\\)是convex function的<strong>充要条件</strong>是：对于定义域内任意 \\(x\\) 和 \\(y\\)</p>\n<script type=\"math/tex; mode=display\">f(y) \\geq f(x) + \\nabla f(x)^T (y - x)</script><p>OK，再来张图片直观感受一下：</p>\n<p><img src=\"http://otmy7guvn.bkt.clouddn.com/blog/1/1-2.png\" alt=\"\"> </p>\n<p>其实简单的来讲，就是对于convex function \\(f\\)，它的函数值永远大于等于切线上的值！</p>\n<h3 id=\"Second-order-condition\"><a href=\"#Second-order-condition\" class=\"headerlink\" title=\"Second order condition\"></a>Second order condition</h3><p>对于 function \\(f\\)，在定义域内<strong>二阶可导</strong>，且 \\(n\\) 维方阵Hessian matrix的元素为</p>\n<script type=\"math/tex; mode=display\">\\nabla ^2 f(x) = \\frac{ \\partial ^2 f(x)}{ \\partial x_i \\partial x_j}, \\quad i,j = 1,2,...,n</script><p>当且仅当Hessian matrix positive semi-defnite的时候，\\(f\\) 是convex function。以上互为<strong>充要条件</strong>。这里的证明我不想展开讲，在后面我会给出reference链接。</p>\n<hr>\n<p>下面给出一些 \\(\\Bbb R\\) 空间下常见的convex function：</p>\n<ul>\n<li>线性函数：\\(f(x) = ax+b\\)</li>\n<li>指数函数：\\(f(x)=e^ {ax}\\)</li>\n<li>负熵函数： \\(f(x)=xlogx \\quad on \\quad \\Bbb R_{++}\\)</li>\n</ul>\n<p>对应的，一些常见的concave function：</p>\n<ul>\n<li>线性函数：\\(f(x) = ax+b\\)</li>\n<li>对数函数： \\(f(x)=logx  \\quad on \\quad \\Bbb R_{++}\\)</li>\n</ul>\n<p>其中大家可以看到，线性函数既是convex也是concave函数，比较特殊，这和它本身的first order condition为常数有关。</p>\n<p>以上就是convex function的一个简单介绍，你也许会问，为什么花这么多力气来介绍convex function. 其实，在machine learning中，convex function的优化是非常重要的，很多算法说到底，都是要optimize一个convex function，我们会用liner regression和logistic regression为例子，进一步从convex function的简介过渡到gradient descent和newton’s method.</p>\n<h2 id=\"梯度下降法-Gradient-descent\"><a href=\"#梯度下降法-Gradient-descent\" class=\"headerlink\" title=\"梯度下降法(Gradient descent)\"></a>梯度下降法(Gradient descent)</h2><p>关于gradient descent，我们使用liner regression作为例子来讨论。Liner regression算法的实质是least square method，他的cost function是</p>\n<script type=\"math/tex; mode=display\">J( \\theta)= \\frac{1}{2} \\sum_{i=1} ^m (h_ \\theta (x ^{(i)})-y^{(i)})^2, \\quad h_ \\theta (x)= \\theta ^Tx</script><p>对于liner regression来说，算法的实质就是去求出\\( J( \\theta) \\) <strong>以\\( \\theta\\)为参数</strong>的minimum，gradient descent算法的作用就是去实现了这个过程，gradient descent的基础知识详见reference. </p>\n<p>那么针对cost function，gradient descent是如何保证收敛的呢，我们一起来看看</p>\n<p>对于\\(J( \\theta)\\)，我们将其带入convex function的定义公式中，注意这里我们的自变量是\\( \\theta\\)，我们可以通过推导证明该式成立，也就是说，least square cost function是convex function.</p>\n<p>既然有这个结论了，那么我们可以想象一下，least square cost function作为convex function，是存在全局最小值的，也就是说，gradient descent不会出现陷入局部最优无法自拔的现象，只要gradient descent保证参数足够好的情况下，理论上，是完全可以很好的逼近全局最优的解的。</p>\n<blockquote>\n<p>Gradien descent算法本身并不能保证获得全局最小值，只有在objective function是convex function的时候才可以保证</p>\n</blockquote>\n<p>下图可以看出，右边的object function是non-convex function，因而很容易陷入到局部最小值无法自拔，而左边的objective function是一个标准的convex function，在gradient descent参数合理的前提下，可以逼近全局最优。</p>\n<p><img src=\"http://otmy7guvn.bkt.clouddn.com/blog/1/1-3.png\" alt=\"\"> </p>\n<p>当然，gradien descent的一些改进方法，例如stochastic gradient descent在解决non-convex optimization上有一些帮助，但是我们在这里不做讨论，后面有时间我会专门再写。</p>\n<p>由此，我们可以得出，gradient descent不仅仅是minimize liner regression的一个很好的方法，也是convex optimization的一种理想方法</p>\n<h2 id=\"牛顿法-Newton’s-method\"><a href=\"#牛顿法-Newton’s-method\" class=\"headerlink\" title=\"牛顿法(Newton’s method)\"></a>牛顿法(Newton’s method)</h2><p>Newton’s method 这块内容，我们将会用logistic regression作为例子。同样，我们先来关注下log cost function，这里，<strong>我们取label为-1和+1</strong>，因为这样得到的cost function比0,1下的计算更加简单</p>\n<script type=\"math/tex; mode=display\">J( \\omega)= - \\frac{1}{m} \\sum_{i=1} ^{m} log(1+e^{-y^{(i)} \\omega^T x^{(i)}})</script><p>这里我们采用了-1和+1作为标签值，和大多数教材中不一样，大家可以下来自己推导一下\\(J( \\omega)\\)，这种写法广泛的应用在了比较logistic regression和SVM两大分类器的文献中，希望大家熟知。</p>\n<p>此处我们对原始的likehood function加上了 \\(- \\frac{1}{m}\\)的系数，同样，当我们把 \\(J( \\omega)\\)带入到convex function的定义中，可以验证上式为convex function，值得注意的是，\\(J( \\omega)\\)是\\( \\omega\\)的函数。</p>\n<p>其实，我们也可以将log cost function展开后，利用最基本的函数convex和concave性质来获得上式是convex function的结论，碍于公式实在太难打，就留给大家去证明吧。</p>\n<p>OK，既然log cost function是convex function，我们一定是可以用gradient descent去求解的。问题是，如果我们用newton’s method呢？</p>\n<p>Newton’s method的基本原理详见reference，这里我们可以发现，既然log cost function是convex function，那么根据second order condition可以知道，它的Hessian matrix一定是positive semi-definite的。如果我们加上了L2 regularizer，<strong>由于L2 regularizer本身就是一个strict convex function</strong>，那么log cost function就一定是strict convex function了，也就是：</p>\n<script type=\"math/tex; mode=display\">J( \\omega)= - \\frac{1}{m} \\sum_{i=1} ^{m} log(1+e^{-y^{(i)} \\omega^T x^{(i)}})+ \\frac{1}{2}|| \\omega||^2</script><p>因此，在log cost function中，<strong>Hessian matrix是positive definite的</strong>，完全满足newton’s method 的要求。同样，类似于上一部分，newton’s method也可以找到log cost function的全局最优。</p>\n<h2 id=\"Sum-up\"><a href=\"#Sum-up\" class=\"headerlink\" title=\"Sum up\"></a>Sum up</h2><p>OK，我们说到这里也确实讲了不少，这篇blog有些冗长，希望朋友们不要焦虑。总体来说，我想表达的是以下几个观点：</p>\n<ul>\n<li>Machine learning中我们寻求的其实就是objective function一个全局最优值，这些问题是通过gradient descent等方法解决的；</li>\n<li>Gradient descent和newton’s method都是convex optimization的好方法，他们都可以对于convex function获得全局最优；</li>\n<li>对于non-convex optimization问题，stochastic gradient descent也很有效果，我们后续再慢慢学习。</li>\n</ul>\n<p>好了，核心思想就这三点，今天先说这么多！</p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ul>\n<li><a href=\"https://see.stanford.edu/materials/lsocoee364a/03ConvexFunctions.pdf\" target=\"_blank\" rel=\"external\">EE364, Convex Optimization Stanford University</a></li>\n<li><a href=\"http://qwone.com/~jason/writing/convexLR.pdf\" target=\"_blank\" rel=\"external\">Regularized Logistic Regression is Strictly Convex</a></li>\n<li><a href=\"https://www.yangzhou301.com/2016/03/14/826442654/\" target=\"_blank\" rel=\"external\">XinyiLI大神的blog</a></li>\n<li><a href=\"https://en.wikipedia.org/wiki/Linear_regression\" target=\"_blank\" rel=\"external\">Liner regression</a></li>\n<li><a href=\"https://en.wikipedia.org/wiki/Logistic_regression\" target=\"_blank\" rel=\"external\">Logsitc regression</a></li>\n<li><a href=\"https://en.wikipedia.org/wiki/Gradient_descent\" target=\"_blank\" rel=\"external\">Gradient descent</a></li>\n<li><a href=\"https://en.wikipedia.org/wiki/Newton%27s_method\" target=\"_blank\" rel=\"external\">Newton’s method</a></li>\n</ul>\n","site":{"data":{}},"excerpt":"<p>记得我在和优男一起研究logistic regression的时候，他问了我几个非常尖锐的问题，让我顿时哑口无言</p>\n<ul>\n<li>怎么保证logistic regression通过gradient descent找到的是最优解；</li>\n<li>为什么logistic regression可以用newton’s method呢？</li>\n<li>Newton’s method中Hessian matrix必须positive definite有什么意义呢，log cost function能保证吗？</li>\n</ul>\n<p>这些细节问题，说实话我也没有认真的想过。在夸奖他之余，我们也一起开始了研究，希望从中学习到一些更深层的东西，趁着现在有个blog分享给大家<br>","more":"</p>\n<h2 id=\"凸函数-Convex-function\"><a href=\"#凸函数-Convex-function\" class=\"headerlink\" title=\"凸函数(Convex function)\"></a>凸函数(Convex function)</h2><p>在开始之前，我有一个关于术语的倡议。中文里的“凸函数”，看上去是凹下去的，对应的，中文里的“凹函数”看上去凸起来的，amazing吧？这是有一定历史原因的，感兴趣的朋友可以去查阅下资料，这里我们不再复述。所以为了避免让大家产生误解，我鼓励大家使用英文，<strong>convex function</strong>和<strong>concave function</strong>.这样会避免很多不必要的麻烦。</p>\n<p>OK，我们来一起看看，convex function</p>\n<p>对于一维函数 \\(f(x)\\)来说，在定义域内的任意值 \\(a\\)和\\(b\\)，对于任意的 \\( 0 \\leq \\theta \\leq 1\\)，如果满足以下条件，则称为convex function</p>\n<script type=\"math/tex; mode=display\">f(\\theta a+(1-\\theta b)) \\leq \\theta f(a) + (1- \\theta)f(b)</script><p>我们再用图片直观的感受一下</p>\n<p><img src=\"http://otmy7guvn.bkt.clouddn.com/blog/1/1-1.png\" alt=\"\"> </p>\n<p>显而易见的是，当公式中等号去掉的时候，函数就是<strong>strictly convex function</strong>.</p>\n<p>Convex function具有一定的性质，我们简单的描述一下。</p>\n<h3 id=\"First-order-condition\"><a href=\"#First-order-condition\" class=\"headerlink\" title=\"First order condition\"></a>First order condition</h3><p>对于 function \\(f\\)，在定义域内<strong>一阶可导</strong>，且导数为</p>\n<script type=\"math/tex; mode=display\">\\nabla f=( \\frac{\\partial f(x)}{x_1}, \\frac{\\partial f(x)}{x_2},...,  \\frac{\\partial f(x)}{x_n})</script><p>那么 \\(f\\)是convex function的<strong>充要条件</strong>是：对于定义域内任意 \\(x\\) 和 \\(y\\)</p>\n<script type=\"math/tex; mode=display\">f(y) \\geq f(x) + \\nabla f(x)^T (y - x)</script><p>OK，再来张图片直观感受一下：</p>\n<p><img src=\"http://otmy7guvn.bkt.clouddn.com/blog/1/1-2.png\" alt=\"\"> </p>\n<p>其实简单的来讲，就是对于convex function \\(f\\)，它的函数值永远大于等于切线上的值！</p>\n<h3 id=\"Second-order-condition\"><a href=\"#Second-order-condition\" class=\"headerlink\" title=\"Second order condition\"></a>Second order condition</h3><p>对于 function \\(f\\)，在定义域内<strong>二阶可导</strong>，且 \\(n\\) 维方阵Hessian matrix的元素为</p>\n<script type=\"math/tex; mode=display\">\\nabla ^2 f(x) = \\frac{ \\partial ^2 f(x)}{ \\partial x_i \\partial x_j}, \\quad i,j = 1,2,...,n</script><p>当且仅当Hessian matrix positive semi-defnite的时候，\\(f\\) 是convex function。以上互为<strong>充要条件</strong>。这里的证明我不想展开讲，在后面我会给出reference链接。</p>\n<hr>\n<p>下面给出一些 \\(\\Bbb R\\) 空间下常见的convex function：</p>\n<ul>\n<li>线性函数：\\(f(x) = ax+b\\)</li>\n<li>指数函数：\\(f(x)=e^ {ax}\\)</li>\n<li>负熵函数： \\(f(x)=xlogx \\quad on \\quad \\Bbb R_{++}\\)</li>\n</ul>\n<p>对应的，一些常见的concave function：</p>\n<ul>\n<li>线性函数：\\(f(x) = ax+b\\)</li>\n<li>对数函数： \\(f(x)=logx  \\quad on \\quad \\Bbb R_{++}\\)</li>\n</ul>\n<p>其中大家可以看到，线性函数既是convex也是concave函数，比较特殊，这和它本身的first order condition为常数有关。</p>\n<p>以上就是convex function的一个简单介绍，你也许会问，为什么花这么多力气来介绍convex function. 其实，在machine learning中，convex function的优化是非常重要的，很多算法说到底，都是要optimize一个convex function，我们会用liner regression和logistic regression为例子，进一步从convex function的简介过渡到gradient descent和newton’s method.</p>\n<h2 id=\"梯度下降法-Gradient-descent\"><a href=\"#梯度下降法-Gradient-descent\" class=\"headerlink\" title=\"梯度下降法(Gradient descent)\"></a>梯度下降法(Gradient descent)</h2><p>关于gradient descent，我们使用liner regression作为例子来讨论。Liner regression算法的实质是least square method，他的cost function是</p>\n<script type=\"math/tex; mode=display\">J( \\theta)= \\frac{1}{2} \\sum_{i=1} ^m (h_ \\theta (x ^{(i)})-y^{(i)})^2, \\quad h_ \\theta (x)= \\theta ^Tx</script><p>对于liner regression来说，算法的实质就是去求出\\( J( \\theta) \\) <strong>以\\( \\theta\\)为参数</strong>的minimum，gradient descent算法的作用就是去实现了这个过程，gradient descent的基础知识详见reference. </p>\n<p>那么针对cost function，gradient descent是如何保证收敛的呢，我们一起来看看</p>\n<p>对于\\(J( \\theta)\\)，我们将其带入convex function的定义公式中，注意这里我们的自变量是\\( \\theta\\)，我们可以通过推导证明该式成立，也就是说，least square cost function是convex function.</p>\n<p>既然有这个结论了，那么我们可以想象一下，least square cost function作为convex function，是存在全局最小值的，也就是说，gradient descent不会出现陷入局部最优无法自拔的现象，只要gradient descent保证参数足够好的情况下，理论上，是完全可以很好的逼近全局最优的解的。</p>\n<blockquote>\n<p>Gradien descent算法本身并不能保证获得全局最小值，只有在objective function是convex function的时候才可以保证</p>\n</blockquote>\n<p>下图可以看出，右边的object function是non-convex function，因而很容易陷入到局部最小值无法自拔，而左边的objective function是一个标准的convex function，在gradient descent参数合理的前提下，可以逼近全局最优。</p>\n<p><img src=\"http://otmy7guvn.bkt.clouddn.com/blog/1/1-3.png\" alt=\"\"> </p>\n<p>当然，gradien descent的一些改进方法，例如stochastic gradient descent在解决non-convex optimization上有一些帮助，但是我们在这里不做讨论，后面有时间我会专门再写。</p>\n<p>由此，我们可以得出，gradient descent不仅仅是minimize liner regression的一个很好的方法，也是convex optimization的一种理想方法</p>\n<h2 id=\"牛顿法-Newton’s-method\"><a href=\"#牛顿法-Newton’s-method\" class=\"headerlink\" title=\"牛顿法(Newton’s method)\"></a>牛顿法(Newton’s method)</h2><p>Newton’s method 这块内容，我们将会用logistic regression作为例子。同样，我们先来关注下log cost function，这里，<strong>我们取label为-1和+1</strong>，因为这样得到的cost function比0,1下的计算更加简单</p>\n<script type=\"math/tex; mode=display\">J( \\omega)= - \\frac{1}{m} \\sum_{i=1} ^{m} log(1+e^{-y^{(i)} \\omega^T x^{(i)}})</script><p>这里我们采用了-1和+1作为标签值，和大多数教材中不一样，大家可以下来自己推导一下\\(J( \\omega)\\)，这种写法广泛的应用在了比较logistic regression和SVM两大分类器的文献中，希望大家熟知。</p>\n<p>此处我们对原始的likehood function加上了 \\(- \\frac{1}{m}\\)的系数，同样，当我们把 \\(J( \\omega)\\)带入到convex function的定义中，可以验证上式为convex function，值得注意的是，\\(J( \\omega)\\)是\\( \\omega\\)的函数。</p>\n<p>其实，我们也可以将log cost function展开后，利用最基本的函数convex和concave性质来获得上式是convex function的结论，碍于公式实在太难打，就留给大家去证明吧。</p>\n<p>OK，既然log cost function是convex function，我们一定是可以用gradient descent去求解的。问题是，如果我们用newton’s method呢？</p>\n<p>Newton’s method的基本原理详见reference，这里我们可以发现，既然log cost function是convex function，那么根据second order condition可以知道，它的Hessian matrix一定是positive semi-definite的。如果我们加上了L2 regularizer，<strong>由于L2 regularizer本身就是一个strict convex function</strong>，那么log cost function就一定是strict convex function了，也就是：</p>\n<script type=\"math/tex; mode=display\">J( \\omega)= - \\frac{1}{m} \\sum_{i=1} ^{m} log(1+e^{-y^{(i)} \\omega^T x^{(i)}})+ \\frac{1}{2}|| \\omega||^2</script><p>因此，在log cost function中，<strong>Hessian matrix是positive definite的</strong>，完全满足newton’s method 的要求。同样，类似于上一部分，newton’s method也可以找到log cost function的全局最优。</p>\n<h2 id=\"Sum-up\"><a href=\"#Sum-up\" class=\"headerlink\" title=\"Sum up\"></a>Sum up</h2><p>OK，我们说到这里也确实讲了不少，这篇blog有些冗长，希望朋友们不要焦虑。总体来说，我想表达的是以下几个观点：</p>\n<ul>\n<li>Machine learning中我们寻求的其实就是objective function一个全局最优值，这些问题是通过gradient descent等方法解决的；</li>\n<li>Gradient descent和newton’s method都是convex optimization的好方法，他们都可以对于convex function获得全局最优；</li>\n<li>对于non-convex optimization问题，stochastic gradient descent也很有效果，我们后续再慢慢学习。</li>\n</ul>\n<p>好了，核心思想就这三点，今天先说这么多！</p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ul>\n<li><a href=\"https://see.stanford.edu/materials/lsocoee364a/03ConvexFunctions.pdf\" target=\"_blank\" rel=\"external\">EE364, Convex Optimization Stanford University</a></li>\n<li><a href=\"http://qwone.com/~jason/writing/convexLR.pdf\" target=\"_blank\" rel=\"external\">Regularized Logistic Regression is Strictly Convex</a></li>\n<li><a href=\"https://www.yangzhou301.com/2016/03/14/826442654/\" target=\"_blank\" rel=\"external\">XinyiLI大神的blog</a></li>\n<li><a href=\"https://en.wikipedia.org/wiki/Linear_regression\" target=\"_blank\" rel=\"external\">Liner regression</a></li>\n<li><a href=\"https://en.wikipedia.org/wiki/Logistic_regression\" target=\"_blank\" rel=\"external\">Logsitc regression</a></li>\n<li><a href=\"https://en.wikipedia.org/wiki/Gradient_descent\" target=\"_blank\" rel=\"external\">Gradient descent</a></li>\n<li><a href=\"https://en.wikipedia.org/wiki/Newton%27s_method\" target=\"_blank\" rel=\"external\">Newton’s method</a></li>\n</ul>"}],"PostAsset":[],"PostCategory":[{"post_id":"cj8lez52t0000w2kzcmr3yrfe","category_id":"cj8lez5350004w2kzpxd9gm3h","_id":"cj8lez53i000dw2kz94ild68n"},{"post_id":"cj8lez5310002w2kzkfv6p2ny","category_id":"cj8lez5350004w2kzpxd9gm3h","_id":"cj8lez53l000iw2kzrn2t4z5n"},{"post_id":"cj8lez53j000ew2kzahdzxnk7","category_id":"cj8lez5350004w2kzpxd9gm3h","_id":"cj8lez53m000mw2kzyua53v0l"},{"post_id":"cj8lez5390006w2kzfritnam7","category_id":"cj8lez53j000fw2kzm63nnwuo","_id":"cj8lez53n000nw2kz5mbccaop"},{"post_id":"cj8lez53c0008w2kz5rydfcuf","category_id":"cj8lez53m000kw2kzuzvi66kk","_id":"cj8lez53p000qw2kzq4oe1x42"},{"post_id":"cj8lez53f0009w2kzqw2rexux","category_id":"cj8lez53n000ow2kziuub29yp","_id":"cj8lez53r000uw2kzyamu0v5r"},{"post_id":"cj8lez53h000cw2kz0n691iok","category_id":"cj8lez53n000ow2kziuub29yp","_id":"cj8lez53s000yw2kzb8sx69d1"},{"post_id":"cj8lez593001jw2kzf6mf2xtd","category_id":"cj8lez53m000kw2kzuzvi66kk","_id":"cj8lez597001mw2kzziy354tr"},{"post_id":"cj8lez595001kw2kzghc4db7n","category_id":"cj8lez53m000kw2kzuzvi66kk","_id":"cj8lez597001nw2kziybtqnyh"}],"PostTag":[{"post_id":"cj8lez52t0000w2kzcmr3yrfe","tag_id":"cj8lez5390005w2kztt85kszi","_id":"cj8lez53l000hw2kzs2lth1ve"},{"post_id":"cj8lez52t0000w2kzcmr3yrfe","tag_id":"cj8lez53g000bw2kzso3xze2v","_id":"cj8lez53m000jw2kzafmpyz3w"},{"post_id":"cj8lez5310002w2kzkfv6p2ny","tag_id":"cj8lez53k000gw2kzq6y081cf","_id":"cj8lez53q000tw2kzggqbjp1l"},{"post_id":"cj8lez5310002w2kzkfv6p2ny","tag_id":"cj8lez53m000lw2kzmh8ks6wp","_id":"cj8lez53r000vw2kz2phj6mgb"},{"post_id":"cj8lez5310002w2kzkfv6p2ny","tag_id":"cj8lez53o000pw2kz37rvivgm","_id":"cj8lez53r000xw2kzel2l0ltk"},{"post_id":"cj8lez5390006w2kzfritnam7","tag_id":"cj8lez53q000sw2kzgd6uxzv1","_id":"cj8lez53s000zw2kz05pctwys"},{"post_id":"cj8lez53c0008w2kz5rydfcuf","tag_id":"cj8lez53r000ww2kza7xgtwcz","_id":"cj8lez53u0013w2kzecfeetvf"},{"post_id":"cj8lez53c0008w2kz5rydfcuf","tag_id":"cj8lez53g000bw2kzso3xze2v","_id":"cj8lez53u0014w2kze7qtab79"},{"post_id":"cj8lez53c0008w2kz5rydfcuf","tag_id":"cj8lez53s0011w2kzbeouv7i7","_id":"cj8lez53w0016w2kzsajdlglv"},{"post_id":"cj8lez53f0009w2kzqw2rexux","tag_id":"cj8lez53t0012w2kz0kz254an","_id":"cj8lez5400019w2kzoaetwzwn"},{"post_id":"cj8lez53f0009w2kzqw2rexux","tag_id":"cj8lez53u0015w2kzeryf8ibe","_id":"cj8lez540001aw2kztcvk2v6r"},{"post_id":"cj8lez53f0009w2kzqw2rexux","tag_id":"cj8lez53x0017w2kz84nqx5re","_id":"cj8lez541001cw2kzaiuncf64"},{"post_id":"cj8lez53h000cw2kz0n691iok","tag_id":"cj8lez5400018w2kzmb3bcbub","_id":"cj8lez542001ew2kz3opl7bu9"},{"post_id":"cj8lez53h000cw2kz0n691iok","tag_id":"cj8lez541001bw2kzvextaixn","_id":"cj8lez542001fw2kzyt4q61l8"},{"post_id":"cj8lez53h000cw2kz0n691iok","tag_id":"cj8lez53g000bw2kzso3xze2v","_id":"cj8lez542001gw2kzflyjeoyw"},{"post_id":"cj8lez53j000ew2kzahdzxnk7","tag_id":"cj8lez53g000bw2kzso3xze2v","_id":"cj8lez542001hw2kzo4yyf0rb"},{"post_id":"cj8lez53j000ew2kzahdzxnk7","tag_id":"cj8lez541001dw2kzzkvugava","_id":"cj8lez543001iw2kz9u247wfc"},{"post_id":"cj8lez593001jw2kzf6mf2xtd","tag_id":"cj8lez5390005w2kztt85kszi","_id":"cj8lez598001rw2kzz0errrn5"},{"post_id":"cj8lez593001jw2kzf6mf2xtd","tag_id":"cj8lez597001lw2kzuc2v6drh","_id":"cj8lez598001sw2kzjsfnhxfm"},{"post_id":"cj8lez593001jw2kzf6mf2xtd","tag_id":"cj8lez598001ow2kzbqkp1q7a","_id":"cj8lez599001tw2kzd5exs8vh"},{"post_id":"cj8lez593001jw2kzf6mf2xtd","tag_id":"cj8lez598001pw2kzdh54j6hf","_id":"cj8lez599001uw2kzlbau5v5d"},{"post_id":"cj8lez595001kw2kzghc4db7n","tag_id":"cj8lez598001qw2kzwv9i2mmk","_id":"cj8lez599001vw2kz6wb3f1ww"},{"post_id":"cj8lez595001kw2kzghc4db7n","tag_id":"cj8lez53g000bw2kzso3xze2v","_id":"cj8lez599001ww2kzr0u1vayk"},{"post_id":"cj8lez595001kw2kzghc4db7n","tag_id":"cj8lez53s0011w2kzbeouv7i7","_id":"cj8lez599001xw2kzt9kyjni9"}],"Tag":[{"name":"regularization","_id":"cj8lez5390005w2kztt85kszi"},{"name":"gradient descent","_id":"cj8lez53g000bw2kzso3xze2v"},{"name":"hyperparameter","_id":"cj8lez53k000gw2kzq6y081cf"},{"name":"batch norm","_id":"cj8lez53m000lw2kzmh8ks6wp"},{"name":"covariate shift","_id":"cj8lez53o000pw2kz37rvivgm"},{"name":"抒情一把","_id":"cj8lez53q000sw2kzgd6uxzv1"},{"name":"unconstrained optimization","_id":"cj8lez53r000ww2kza7xgtwcz"},{"name":"newton's method","_id":"cj8lez53s0011w2kzbeouv7i7"},{"name":"imbalanced data","_id":"cj8lez53t0012w2kz0kz254an"},{"name":"undersampling","_id":"cj8lez53u0015w2kzeryf8ibe"},{"name":"bagging","_id":"cj8lez53x0017w2kz84nqx5re"},{"name":"gbt","_id":"cj8lez5400018w2kzmb3bcbub"},{"name":"logistic regression","_id":"cj8lez541001bw2kzvextaixn"},{"name":"moving averages","_id":"cj8lez541001dw2kzzkvugava"},{"name":"MAP","_id":"cj8lez597001lw2kzuc2v6drh"},{"name":"ridge regression","_id":"cj8lez598001ow2kzbqkp1q7a"},{"name":"lasso regression","_id":"cj8lez598001pw2kzdh54j6hf"},{"name":"convex optimization","_id":"cj8lez598001qw2kzwv9i2mmk"}]}}