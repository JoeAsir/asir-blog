{"meta":{"version":1,"warehouse":"2.2.0"},"models":{"Asset":[{"_id":"source/favicon.ico","path":"favicon.ico","modified":1,"renderable":0},{"_id":"themes/minos/source/css/insight.scss","path":"css/insight.scss","modified":1,"renderable":1},{"_id":"themes/minos/source/images/check.svg","path":"images/check.svg","modified":1,"renderable":1},{"_id":"themes/minos/source/css/style.scss","path":"css/style.scss","modified":1,"renderable":1},{"_id":"themes/minos/source/images/exclamation.svg","path":"images/exclamation.svg","modified":1,"renderable":1},{"_id":"themes/minos/source/images/info.svg","path":"images/info.svg","modified":1,"renderable":1},{"_id":"themes/minos/source/images/question.svg","path":"images/question.svg","modified":1,"renderable":1},{"_id":"themes/minos/source/images/logo.png","path":"images/logo.png","modified":1,"renderable":1},{"_id":"themes/minos/source/images/quote-left.svg","path":"images/quote-left.svg","modified":1,"renderable":1},{"_id":"themes/minos/source/js/script.js","path":"js/script.js","modified":1,"renderable":1},{"_id":"themes/minos/source/js/insight.js","path":"js/insight.js","modified":1,"renderable":1}],"Cache":[{"_id":"source/favicon.ico","hash":"7be62720671a143fb8b79a1a196730df57fee81d","modified":1589093412533},{"_id":"themes/minos/.gitignore","hash":"8b02e7219e2dd9b50d198819fd7d8f74ebc9db2a","modified":1589105376917},{"_id":"themes/minos/_config.yml","hash":"aab3d9c99141349fac8876868fbd618dcfd75409","modified":1589105938119},{"_id":"themes/minos/LICENSE","hash":"ca01a2d52b59346e82f079c593df6cb26dd9a7a5","modified":1589105376918},{"_id":"themes/minos/README.md","hash":"ba6b4e134d718704cfd030e106bf24d6ef8b496d","modified":1589105376918},{"_id":"themes/minos/package-lock.json","hash":"e1fbecec56fb65379bf651f21fb485376e692b38","modified":1589105376926},{"_id":"themes/minos/_config.yml.example","hash":"28c6c5604380fb6cc5a99639fa445c40205764ba","modified":1589105376918},{"_id":"themes/minos/package.json","hash":"f9d450db80149dea6c372990cdf51dfde901e5cc","modified":1589105376926},{"_id":"source/_posts/course-deep-learning-course2-week1.md","hash":"d6800156c26f9bba8da2f91ae2636be09931f768","modified":1589093412527},{"_id":"source/_posts/course-deep-learning-course2-week3.md","hash":"b72e8ee53bccc08d6fbe1fce659e7b700fe4805e","modified":1589093412528},{"_id":"source/_posts/course-deep-learning-course2-week2.md","hash":"ae283834c58e2122c02c378c20a2794dbd5a385f","modified":1589093412528},{"_id":"source/_posts/course-deep-learning-course3-week2.md","hash":"c2627a2a91fc54795eccd2bed06aa6bda780f485","modified":1589093412528},{"_id":"source/_posts/course-deep-learning-course3-week1.md","hash":"a2cff4f897c97f0207001c986510d93c17ffb687","modified":1589093412528},{"_id":"source/_posts/course-deep-learning-course4-week1.md","hash":"a27fb439d13ad9827e7ff2b43a4b0068ce874c8f","modified":1589093412528},{"_id":"source/_posts/course-deep-learning-course4-week2.md","hash":"be776844490a752e434e0c7884bb455317f680aa","modified":1589093412529},{"_id":"source/_posts/hdfs-arvo-parquet-orc.md","hash":"ac3cac511beafc7c3a386fbe8936e9b63b4c82e6","modified":1589093412529},{"_id":"source/_posts/hdfs-architecture.md","hash":"6a84a0ce6614d98ab5d72e946baaa12f1c05c3d2","modified":1589093412529},{"_id":"source/_posts/hdfs-orc-tips.md","hash":"61bc206298217a742ad5df929c3cbf114f728569","modified":1589093412529},{"_id":"source/_posts/hdfs-yarn-architecture.md","hash":"c95cac3ff9024d9e9f9c136a4b4811a083cd9660","modified":1589093412529},{"_id":"source/_posts/jvm-java-garbage-collection-overview.md","hash":"681a5a767637592b2255363adc0e0459b6da4af5","modified":1589093412529},{"_id":"source/_posts/ml-convex-opt.md","hash":"2851eae3feaacd89c5dd0242c4d70b4c1bce150e","modified":1589093412530},{"_id":"source/_posts/ml-gd-and-nm.md","hash":"f966967debd4744d6435418cb054b2bca0918d9c","modified":1589093412530},{"_id":"source/_posts/ml-ridge-lasso.md","hash":"9e9d106ddf90f0f578a6f368ce2e056c8680f5d7","modified":1589093412530},{"_id":"source/_posts/ml-imbalanced-data-solution.md","hash":"6a3e85f66233c0534de708e81924a0c8fdd05def","modified":1589093412530},{"_id":"source/_posts/other-hello.md","hash":"e93c503e52dd939d6dacfaad2b4352bd3cf5979e","modified":1589093412530},{"_id":"source/_posts/paper-facebook.md","hash":"02fbeb86579cbfcf0a85f30aaec4e46bd0e3a452","modified":1589093412530},{"_id":"source/_posts/paper-swish.md","hash":"e999660a6a950168cddde524b6fadbbe4ec2d64a","modified":1589093412531},{"_id":"source/_posts/paper-imbalance.md","hash":"5d12080e1692a0c898cc79753d53f9cf17a696f2","modified":1589093412531},{"_id":"source/_posts/spark-catalyst-optimization.md","hash":"735387b6a862beb4cf7c8cbbba558eb3904b1026","modified":1589093412531},{"_id":"source/_posts/spark-from-rdd-to-dataframe-dataset.md","hash":"ae3798c52aaf9ec55065d1603c47162f1104323f","modified":1589093412531},{"_id":"source/_posts/spark-spark-tune.md","hash":"642f48855cc724fc19f0c0eae8457eccb5044fdf","modified":1589093412531},{"_id":"source/_posts/spark-second-generation-tungsten-in-spark.md","hash":"e044d136c50d2b392cc6649f51955b7d5ffc28f5","modified":1589093412531},{"_id":"source/_posts/spark-sumup-part-1.md","hash":"0c1d556517d3b08b50bd0c66b5a2c545845185ce","modified":1589093412532},{"_id":"source/_posts/spark-spark-workflow.md","hash":"34b05704261de4ab0b883b42857bacd0c1f3f953","modified":1589093412532},{"_id":"source/_posts/spark-sumup-part-2.md","hash":"7d198f74ba09c7a5115224669948ac82398dc880","modified":1589093412532},{"_id":"source/about/index.md","hash":"ffbc4488cbe9e131bcea2156946d8aefd5eda48e","modified":1589093412532},{"_id":"themes/minos/.git/HEAD","hash":"acbaef275e46a7f14c1ef456fff2c8bbe8c84724","modified":1589105376913},{"_id":"source/categories/index.md","hash":"4b365c6059e6bf92e31df59be1a2597c25bb4ba3","modified":1589093412533},{"_id":"source/_posts/spark-sumup-part-3.md","hash":"f744eaf569a8df1f40fae638b360798be069aa7b","modified":1589093412532},{"_id":"themes/minos/.git/config","hash":"9e7e2c8781d5128bd00bdf7f3188a56f8731efb7","modified":1589105376915},{"_id":"source/tags/index.md","hash":"6069f89020880bad30d7801a8e1a4c2a77d7bc5b","modified":1589093412533},{"_id":"themes/minos/.git/description","hash":"9635f1b7e12c045212819dd934d809ef07efa2f4","modified":1589105106164},{"_id":"themes/minos/.git/index","hash":"e0bc65276c8503b5185c2bb36c2ebe82362b2e9e","modified":1589105629791},{"_id":"themes/minos/.git/packed-refs","hash":"7da77cf797cfac7a8b23d9b120867da2c4077743","modified":1589105376911},{"_id":"themes/minos/languages/es.yml","hash":"5c35950221411e34e7a9821d0b0671da9a458d8c","modified":1589105376918},{"_id":"themes/minos/languages/ko.yml","hash":"1acf3f959f1d2b4f7a77e7e82851821aa8635362","modified":1589105376919},{"_id":"themes/minos/languages/en.yml","hash":"ef98c8674fed78f2350598ee8b15fcd53fbd2ae5","modified":1589105376918},{"_id":"themes/minos/languages/ru.yml","hash":"8e5a58176bf943432ba6e4f1981d9b98fdea36a4","modified":1589105376919},{"_id":"themes/minos/languages/zh-cn.yml","hash":"9c5a489b11a056d1ea7b9d4a0e127aef9e192ee4","modified":1589105376919},{"_id":"themes/minos/layout/archive.ejs","hash":"e3eefe819d61b4d0ee069bb705a9f5707a8bf3da","modified":1589105376919},{"_id":"themes/minos/layout/category.ejs","hash":"403c646878834964883ac41e63952f7b1595c0ba","modified":1589105376920},{"_id":"themes/minos/layout/categories.ejs","hash":"fff6f911d0f548ee749292bc1942f8fbbb1fbfe7","modified":1589105376919},{"_id":"themes/minos/layout/index.ejs","hash":"dff9e199d394f82c5416b814f9e644edbe4090f0","modified":1589105376923},{"_id":"themes/minos/layout/layout.ejs","hash":"45588aa46857cf9403fa79d738ab37a46ddcf773","modified":1589105376923},{"_id":"themes/minos/layout/post.ejs","hash":"68b84a717efc5ca59ee9eb6202ccf05c5a8abda5","modified":1589105376924},{"_id":"themes/minos/layout/tag.ejs","hash":"5593c7cf9618ef5650c779ed9d75424f057aa210","modified":1589105376925},{"_id":"themes/minos/layout/tags.ejs","hash":"e4a9909119294f131a45f10b2cb1058af5fb9be1","modified":1589105376926},{"_id":"themes/minos/scripts/01_check.js","hash":"b26b19011a6eb61e61419331a7f9c5fdf553d830","modified":1589105376926},{"_id":"themes/minos/scripts/10_i18n.js","hash":"346a09259e15913871e12c7418a639b8d65df570","modified":1589105376927},{"_id":"themes/minos/scripts/99_config.js","hash":"d41a5df0a442728fbc66514476fe043e416d7438","modified":1589105376927},{"_id":"themes/minos/scripts/99_content.js","hash":"5d19de210e9172a9acb61667a26810899fce917d","modified":1589105376927},{"_id":"themes/minos/scripts/99_tags.js","hash":"91369a4d8376bc981a9bad9c5dde4b3e775e5cb5","modified":1589105376928},{"_id":"themes/minos/scripts/rfc5646.js","hash":"8ecf38d0ec7145720ea8e888da314131712770e8","modified":1589105376928},{"_id":"themes/minos/.git/hooks/commit-msg.sample","hash":"ee1ed5aad98a435f2020b6de35c173b75d9affac","modified":1589105106164},{"_id":"themes/minos/.git/hooks/applypatch-msg.sample","hash":"4de88eb95a5e93fd27e78b5fb3b5231a8d8917dd","modified":1589105106165},{"_id":"themes/minos/.git/hooks/fsmonitor-watchman.sample","hash":"f7c0aa40cb0d620ff0bca3efe3521ec79e5d7156","modified":1589105106166},{"_id":"themes/minos/.git/hooks/post-update.sample","hash":"b614c2f63da7dca9f1db2e7ade61ef30448fc96c","modified":1589105106167},{"_id":"themes/minos/.git/hooks/pre-applypatch.sample","hash":"f208287c1a92525de9f5462e905a9d31de1e2d75","modified":1589105106167},{"_id":"themes/minos/.git/hooks/pre-push.sample","hash":"5c8518bfd1d1d3d2c1a7194994c0a16d8a313a41","modified":1589105106168},{"_id":"themes/minos/.git/hooks/pre-receive.sample","hash":"705a17d259e7896f0082fe2e9f2c0c3b127be5ac","modified":1589105106166},{"_id":"themes/minos/.git/hooks/pre-commit.sample","hash":"33729ad4ce51acda35094e581e4088f3167a0af8","modified":1589105106165},{"_id":"themes/minos/.git/hooks/pre-rebase.sample","hash":"288efdc0027db4cfd8b7c47c4aeddba09b6ded12","modified":1589105106165},{"_id":"themes/minos/.git/info/exclude","hash":"c879df015d97615050afa7b9641e3352a1e701ac","modified":1589105106163},{"_id":"themes/minos/.git/hooks/prepare-commit-msg.sample","hash":"2584806ba147152ae005cb675aa4f01d5d068456","modified":1589105106166},{"_id":"themes/minos/.git/hooks/update.sample","hash":"e729cd61b27c128951d139de8e7c63d1a3758dde","modified":1589105106168},{"_id":"themes/minos/.git/logs/HEAD","hash":"2f95d56499c2838ea5060c106fd4a0bdc69ca2b8","modified":1589105376914},{"_id":"themes/minos/layout/comment/facebook.ejs","hash":"e73b6f93d98b27ba9068c1685874ecccfbac737b","modified":1589105376920},{"_id":"themes/minos/layout/comment/disqus.ejs","hash":"a2becdc02214a673c804af93488489807fa2c99c","modified":1589105376920},{"_id":"themes/minos/layout/comment/changyan.ejs","hash":"9ccc7ec354b968e60bdcfcd1dba451d38de61f12","modified":1589105376920},{"_id":"themes/minos/layout/comment/isso.ejs","hash":"cc6a43bd24be764086f88ad7c5c97ff04df87e0b","modified":1589105376920},{"_id":"themes/minos/layout/comment/gitment.ejs","hash":"430416210933b7edcbfcc67ede4aa55539da2750","modified":1589105376920},{"_id":"themes/minos/layout/comment/livere.ejs","hash":"12ff9a345f6bba2f732f592e39508c2afde89b00","modified":1589105376921},{"_id":"themes/minos/layout/comment/valine.ejs","hash":"350f28986dd610ebdfdeb16dc618d1d034312af1","modified":1589105376921},{"_id":"themes/minos/layout/common/article.ejs","hash":"45d276fb6bfcee6690cfffa7cbdec18709cd8766","modified":1589105376921},{"_id":"themes/minos/layout/comment/youyan.ejs","hash":"3d6cf9c523a7a5510ec2864bb29f861f9bb78af3","modified":1589105376921},{"_id":"themes/minos/layout/common/footer.ejs","hash":"367c5f2e69c66d4d6fbd8beeade0b60024ce9e6e","modified":1589105376922},{"_id":"themes/minos/layout/common/languages.ejs","hash":"89665c656a1ffebc9c97f03e7f9c12dd1d90702a","modified":1589105376922},{"_id":"themes/minos/layout/common/head.ejs","hash":"7565aeb729327b872f3c6ee6aec7951d2d9a6270","modified":1589105376922},{"_id":"themes/minos/layout/common/scripts.ejs","hash":"7a5a5271930423b95046836597e30e31fa708f66","modified":1589105376923},{"_id":"themes/minos/layout/common/paginator.ejs","hash":"8f5060e4c8a86a3f4e58455c41c98e831e23e4a4","modified":1589105376922},{"_id":"themes/minos/layout/common/navbar.ejs","hash":"965e931095896445a30aa404c7ff5295c127408a","modified":1589105376922},{"_id":"themes/minos/layout/plugins/gallery.ejs","hash":"7c2becafdf6b60e677cdd5756b9d55eba2af4944","modified":1589105376923},{"_id":"themes/minos/layout/plugins/mathjax.ejs","hash":"b460310078d3506dce8dccc67310e3b9b3c124a9","modified":1589105376924},{"_id":"themes/minos/layout/plugins/google-analytics.ejs","hash":"2a9d944a60aff7df27def5215bdc071e605c3c42","modified":1589105376923},{"_id":"themes/minos/layout/search/google-cse.ejs","hash":"a6bf5c30339735126efa7efa684f9eb14dd6136a","modified":1589105376924},{"_id":"themes/minos/layout/search/insight.ejs","hash":"6fb7d27ef40145d8587b46b44a43516135b5a81a","modified":1589105376925},{"_id":"themes/minos/layout/share/addthis.ejs","hash":"f1c5f337333009d5f00dfbac4864a16ef8f9cb8d","modified":1589105376925},{"_id":"themes/minos/layout/share/sharethis.ejs","hash":"4f2c40f790f3be0a4e79db04f02ea41ba2f4d4c0","modified":1589105376925},{"_id":"themes/minos/source/css/insight.scss","hash":"f785fc6574d2853c660be39b2e3149d4846b577f","modified":1589105376928},{"_id":"themes/minos/source/images/check.svg","hash":"029b8b3523b7daa4005983b4463cd93408308aab","modified":1589105376929},{"_id":"themes/minos/source/css/style.scss","hash":"6b6c06a88e57c366a2bb665462e6366461b392a1","modified":1589105376929},{"_id":"themes/minos/source/images/exclamation.svg","hash":"b2db56f2cc13fce73dbea46c7b446d9bcb3bf0fd","modified":1589105376930},{"_id":"themes/minos/source/images/info.svg","hash":"c8aa387e935ba9a7fa72c5dd000b7d46f2e030c4","modified":1589105376930},{"_id":"themes/minos/source/images/question.svg","hash":"7153fa2a0c21e32da6a1f96a333d8b66a178569d","modified":1589105376930},{"_id":"themes/minos/source/images/logo.png","hash":"4e012d9ba58cb8f87ee775262ef871c158ac5948","modified":1589105376930},{"_id":"themes/minos/source/images/quote-left.svg","hash":"d2561fa8d13e63ff196b71232a5968415ec6e372","modified":1589105376931},{"_id":"themes/minos/source/js/script.js","hash":"6b670ec4f90fb43b21a0bbd750a217af5d8aab6b","modified":1589105376931},{"_id":"themes/minos/source/js/insight.js","hash":"eb23c31141784eef7300f1d1c548950e77883f56","modified":1589105376931},{"_id":"themes/minos/.git/refs/heads/master","hash":"56e9924b6daf71a7611ad2a155f0442e055b0faf","modified":1589105376914},{"_id":"themes/minos/.git/objects/pack/pack-d3c076102c0f9ac1646a86bb0bf276a40f569259.idx","hash":"0c1fb75b892444a0cc4b544771d9f9a022133fa4","modified":1589105376887},{"_id":"themes/minos/.git/logs/refs/heads/master","hash":"2f95d56499c2838ea5060c106fd4a0bdc69ca2b8","modified":1589105376914},{"_id":"themes/minos/.git/refs/remotes/origin/HEAD","hash":"d9427cda09aba1cdde5c69c2b13c905bddb0bc51","modified":1589105376913},{"_id":"themes/minos/.git/logs/refs/remotes/origin/HEAD","hash":"2f95d56499c2838ea5060c106fd4a0bdc69ca2b8","modified":1589105376913},{"_id":"themes/minos/.git/objects/pack/pack-d3c076102c0f9ac1646a86bb0bf276a40f569259.pack","hash":"62b4c02a1813c71883eb1b675512e38ca0a691cd","modified":1589105376886},{"_id":"public/atom.xml","hash":"a1ef7df8d8f29f5479b2cb3669f240d72117a2f8","modified":1589105953813},{"_id":"public/sitemap.xml","hash":"cb5490731d85488deacde6e221a2c1e3132ee4db","modified":1589105953814},{"_id":"public/content.json","hash":"58f3b9d7ca0d099c6c194111a2233c5f4f8dd743","modified":1589105954418},{"_id":"public/about/index.html","hash":"16e2c55752cb6d58db553ea018065cbb4bc24060","modified":1589105954440},{"_id":"public/categories/index.html","hash":"78ea416001f1ffdb48cacce3b94ca345acf2c7a4","modified":1589105954441},{"_id":"public/tags/index.html","hash":"beca64a9043e4b47dd16aa8551dd0bf026726ccb","modified":1589105954441},{"_id":"public/2017/10/22/paper-swish/index.html","hash":"e404130718ab604151a304a4a9aa337e8b23b3ab","modified":1589105954441},{"_id":"public/2017/09/10/paper-imbalance/index.html","hash":"0eef885d5af7292f97f0005cb3075f90295dc3b5","modified":1589105954441},{"_id":"public/2017/08/23/paper-facebook/index.html","hash":"9cc1521df3b7a30039c377ef53ddc5342d747d43","modified":1589105954441},{"_id":"public/2017/07/26/other-hello/index.html","hash":"f8999be825693585c2b7b91793830e1cd0b71932","modified":1589105954441},{"_id":"public/archives/index.html","hash":"1b1422f2a4b26a085f4dc0f4d75b77585313dcb3","modified":1589105954441},{"_id":"public/archives/page/2/index.html","hash":"3b825308544e9fadb8733384bcf92ede983c8bca","modified":1589105954441},{"_id":"public/archives/page/3/index.html","hash":"21eb7750bd9d6a3f5c53b6f30cc94c9e7ff1329a","modified":1589105954441},{"_id":"public/archives/2017/index.html","hash":"3139923259f31272a6ffc4ebe12155296ed4aace","modified":1589105954441},{"_id":"public/archives/2017/page/2/index.html","hash":"d025f7d9029cc5adbd06e54dabbf999b50bf1cd4","modified":1589105954441},{"_id":"public/archives/2017/07/index.html","hash":"aa09bea51a2a75428957faac0cfb14118f9832b6","modified":1589105954441},{"_id":"public/archives/2017/08/index.html","hash":"f84e7b2be488a280a021be0bb4f8d43769a676d3","modified":1589105954441},{"_id":"public/archives/2017/09/index.html","hash":"a5b43e635b6467dac6c0f540ffa776de08cd3378","modified":1589105954441},{"_id":"public/archives/2017/10/index.html","hash":"da769a06e1684d94117678bbee66a2a8017d9fcb","modified":1589105954441},{"_id":"public/archives/2017/11/index.html","hash":"f43e99d78573aa2a9921ddd23fc896d396dd4e6b","modified":1589105954442},{"_id":"public/archives/2018/index.html","hash":"aeca9224e009f14d0a36d86b6c92d7a0920e0c46","modified":1589105954442},{"_id":"public/archives/2018/01/index.html","hash":"97e3c9609adbc8ff9e76ef7d804e6eae15e19e8f","modified":1589105954442},{"_id":"public/archives/2018/02/index.html","hash":"1c2f3359ce7d3acd3f79b56d96ed2258b6add25e","modified":1589105954442},{"_id":"public/archives/2018/09/index.html","hash":"12b91d7ded449b7e405d49a10a2380cefa231552","modified":1589105954442},{"_id":"public/archives/2018/10/index.html","hash":"4870ac062666ef8831d9c94cef347e59763643d1","modified":1589105954442},{"_id":"public/archives/2018/11/index.html","hash":"d68416d63f269c81a7bb5453a00bc2624a5d6a4e","modified":1589105954442},{"_id":"public/archives/2019/index.html","hash":"e3bd53e3ddec2f65383895a965e8e51d4ae38f63","modified":1589105954442},{"_id":"public/archives/2019/02/index.html","hash":"bf5c430206558266c996e2cf22b404d0deec5e4a","modified":1589105954442},{"_id":"public/archives/2019/03/index.html","hash":"f2b290cb5b850f7841e98167d534e741bd924b3b","modified":1589105954442},{"_id":"public/archives/2019/04/index.html","hash":"0f0af8700e646fdb6f5b851d464912a40a62a8e6","modified":1589105954442},{"_id":"public/archives/2019/05/index.html","hash":"8768d316f18adb033e3df4c0ca3fab1efe594e77","modified":1589105954442},{"_id":"public/archives/2019/06/index.html","hash":"4db317879bb22cc0e9e9a0fc48145c489439d7ac","modified":1589105954442},{"_id":"public/categories/jvm/index.html","hash":"f1bcaea4091d4ee1231a5325cbed207daf15c0a2","modified":1589105954442},{"_id":"public/categories/machine-learning/index.html","hash":"d32a5cbf6ef37bda49dddd8846a5e00869e894f3","modified":1589105954442},{"_id":"public/categories/others/index.html","hash":"7b004dcd576955b0fe80cfbbde23df80c78073a1","modified":1589105954442},{"_id":"public/categories/reading-notes/index.html","hash":"80d7dc63fd5b71c4a14048e7c45185b3f3034bf4","modified":1589105954442},{"_id":"public/page/6/index.html","hash":"3192e11037a66fbb126e16dc692d73bf2035b1bb","modified":1589105954442},{"_id":"public/tags/regularization/index.html","hash":"a7db959fd3dafdfaedfcb81c8e98dbceba65067e","modified":1589105954442},{"_id":"public/tags/hyperparameter/index.html","hash":"bca2c281c43bd1f9a5fa1c815159656b5a4055c8","modified":1589105954443},{"_id":"public/tags/batch-norm/index.html","hash":"0e6042d9c1a80d22d7ec910701bc66519a53d883","modified":1589105954443},{"_id":"public/tags/covariate-shift/index.html","hash":"319fb7e35446181368f605f70ecee3275585d518","modified":1589105954443},{"_id":"public/tags/moving-averages/index.html","hash":"5102604c1b802cc18cb014285281880c1ee94d98","modified":1589105954443},{"_id":"public/tags/learning-strategy/index.html","hash":"e5be6c5215e741aaa39727a26599325753b969bc","modified":1589105954443},{"_id":"public/tags/transfer-learning/index.html","hash":"b294843a16467828d715e11d5f24265e8218eca1","modified":1589105954443},{"_id":"public/tags/multi-task-learning/index.html","hash":"4c1e374b70d7a796c2465e8455f1ff106b4de606","modified":1589105954443},{"_id":"public/tags/orthogonalization/index.html","hash":"eac48f0f54d5df8a436b83b043671f9938f779cb","modified":1589105954443},{"_id":"public/tags/CNN/index.html","hash":"514b4337c73836cb54bc3539c89a52437d067ce5","modified":1589105954443},{"_id":"public/tags/hive/index.html","hash":"f1a68c6383436c8eafaffb3927aec6245b097a89","modified":1589105954443},{"_id":"public/tags/yarn/index.html","hash":"d0f7448542a6da37ac8252cbbcf27361aef4e3ca","modified":1589105954443},{"_id":"public/tags/hadoop/index.html","hash":"fa2295ea529ec877697293d8a7dd672953f0bbac","modified":1589105954443},{"_id":"public/tags/jvm/index.html","hash":"42e4248d0c9f3395f8c0ba8d18956ebba7c1ce1b","modified":1589105954443},{"_id":"public/tags/unconstrained-optimization/index.html","hash":"6ac52637b7ad8302d0b31a11c44b6c64ffb483fc","modified":1589105954443},{"_id":"public/tags/newton-s-method/index.html","hash":"ad392d13bb5afdaa5cc158ce0de15af9261b378e","modified":1589105954443},{"_id":"public/tags/convex-optimization/index.html","hash":"c2772c93dcb48ea3eefa3d8bf7f010613478b67b","modified":1589105954443},{"_id":"public/tags/MAP/index.html","hash":"fa8bfb0ca99e70a17cdb80cc26981ef8926de7a7","modified":1589105954443},{"_id":"public/tags/ridge-regression/index.html","hash":"601f9b4c97144105d04af784addec24316f6d5b7","modified":1589105954443},{"_id":"public/tags/lasso-regression/index.html","hash":"cce6953aee45d55efebf62c03feb7789f48430ea","modified":1589105954443},{"_id":"public/tags/imbalanced-data/index.html","hash":"1ce36b7cb633a2bb108b6b38bba556752ef6577b","modified":1589105954443},{"_id":"public/tags/life/index.html","hash":"32e8cd8220fbc9f0fa452e003d0c797d4e47aff8","modified":1589105954444},{"_id":"public/tags/gbt/index.html","hash":"493f7db0c99fef1a5e156615a09a400392f17231","modified":1589105954444},{"_id":"public/tags/logistic-regression/index.html","hash":"f8e403d98142f1aa658167747190395105ac0ddc","modified":1589105954444},{"_id":"public/tags/activtion-function/index.html","hash":"165469f7f744ce36cffe39b8c8854788ab40f141","modified":1589105954444},{"_id":"public/tags/undersampling/index.html","hash":"46cb56ee1e51b3ac93412f6151fea128ca26e092","modified":1589105954444},{"_id":"public/tags/bagging/index.html","hash":"223e57b0e71e7cd96edb4e33000219a26080d931","modified":1589105954444},{"_id":"public/2019/06/22/hdfs-orc-tips/index.html","hash":"4229b6dbeae087dbebad9b793be6cd834421ca29","modified":1589105954444},{"_id":"public/2019/06/11/hdfs-yarn-architecture/index.html","hash":"249e7c14880d303afc0e33bebf2c6c048a282b93","modified":1589105954444},{"_id":"public/2019/05/05/hdfs-arvo-parquet-orc/index.html","hash":"3a371cb0d66fcb0ff23bd96bee7976ff886ba79d","modified":1589105954444},{"_id":"public/2019/04/28/hdfs-architecture/index.html","hash":"d1d9c4e9223452aa450aa5b99b4d8ae8ff651ac8","modified":1589105954444},{"_id":"public/2019/03/06/spark-sumup-part-3/index.html","hash":"516451a998f3636b3708317e18685565e3fa5dca","modified":1589105954444},{"_id":"public/2019/02/26/jvm-java-garbage-collection-overview/index.html","hash":"877fb4c81b4eeae14d727a12bfd32162ff3a1f06","modified":1589105954444},{"_id":"public/2018/11/14/spark-second-generation-tungsten-in-spark/index.html","hash":"77022c61ba22671b6e6854f897697ad5cdc9ae04","modified":1589105954444},{"_id":"public/2018/10/13/spark-sumup-part-2/index.html","hash":"42a639d4606b09d52e7b97529a5a024ca077202a","modified":1589105954444},{"_id":"public/2018/09/25/spark-catalyst-optimization/index.html","hash":"ee2b8fb9abb08b8d33e1ed85a2c7110e65f61b02","modified":1589105954444},{"_id":"public/2018/09/22/spark-from-rdd-to-dataframe-dataset/index.html","hash":"7d332fb34e4f4103b561ce0aad8ef13f04bc27ce","modified":1589105954444},{"_id":"public/2018/09/15/spark-sumup-part-1/index.html","hash":"29e6f391fc81ec0821ff193719f963ad4ef416a9","modified":1589105954444},{"_id":"public/2018/02/23/spark-spark-tune/index.html","hash":"58a51aaeb59d3659dce6d9e0ae69818b0eb4e595","modified":1589105954445},{"_id":"public/2018/01/07/spark-spark-workflow/index.html","hash":"b3023553fe469b5932526bf89a21bf5e5ad1cc4a","modified":1589105954445},{"_id":"public/2017/11/29/course-deep-learning-course4-week2/index.html","hash":"c154234409d344edd0767d1171375ff60ddd285f","modified":1589105954445},{"_id":"public/2017/11/26/course-deep-learning-course4-week1/index.html","hash":"a7f72cd92dd9b80a2f1f83e23d7a642674ae58f2","modified":1589105954445},{"_id":"public/2017/11/11/ml-imbalanced-data-solution/index.html","hash":"6b519be1320464a0294b1e79f7db41b832c5950c","modified":1589105954445},{"_id":"public/2017/10/18/course-deep-learning-course3-week2/index.html","hash":"85e30f64b3b920b37e5e7a2a634b17cadc9f16a5","modified":1589105954445},{"_id":"public/2017/10/12/course-deep-learning-course3-week1/index.html","hash":"5e6a04efa8ed76ae15fc89cc9425d4eace7222c4","modified":1589105954445},{"_id":"public/2017/09/30/course-deep-learning-course2-week3/index.html","hash":"f4926105e836bacd69667c1228249663f5488c19","modified":1589105954445},{"_id":"public/2017/09/27/course-deep-learning-course2-week2/index.html","hash":"1104af2ba74e6a5150cf8c93c9864553e31d5728","modified":1589105954445},{"_id":"public/2017/09/24/course-deep-learning-course2-week1/index.html","hash":"9b4c911352d80779a41f2b28d6ad81ce43fcd4fa","modified":1589105954445},{"_id":"public/2017/08/27/ml-ridge-lasso/index.html","hash":"05c115930987d67244ae98b3212e495c7329e8ab","modified":1589105954445},{"_id":"public/2017/08/11/ml-gd-and-nm/index.html","hash":"24cedfa1ac71c6ba85e9f4a85aa663dad875ea02","modified":1589105954445},{"_id":"public/2017/08/02/ml-convex-opt/index.html","hash":"4a68b9b5024b47ff96ebf6dca95099adf30224ee","modified":1589105954445},{"_id":"public/categories/learning-notes/index.html","hash":"1596f56e8185c6e0565e50b2e40577055859cec6","modified":1589105954445},{"_id":"public/categories/hadoop/index.html","hash":"c083c60ebcf9b7613f4791a635fe01fbadf74400","modified":1589105954445},{"_id":"public/categories/spark/index.html","hash":"7dce37e7db852fa24cceb1e87f399ea694de7d41","modified":1589105954445},{"_id":"public/index.html","hash":"e1dfeedef7b792c3833ba398e996c89ceacdf6ba","modified":1589105954445},{"_id":"public/page/2/index.html","hash":"fdca7489ee041f35d5269adc2b562e0181b873e4","modified":1589105954445},{"_id":"public/page/3/index.html","hash":"7012ebb41c0b0e7606faa755f8e10b92affdb9e5","modified":1589105954445},{"_id":"public/page/4/index.html","hash":"16e2f8eaff991aaff27006b01828e657bb76931a","modified":1589105954445},{"_id":"public/page/5/index.html","hash":"88b52565c6de5bac572666cb4e07e3a338434a23","modified":1589105954446},{"_id":"public/tags/gradient-descent/index.html","hash":"858cee4f990f3c65404811b0ab9d9d5b35901dd8","modified":1589105954446},{"_id":"public/tags/hdfs/index.html","hash":"02aa96e95a49b1001204f077e8b0beceb75a3c2e","modified":1589105954446},{"_id":"public/tags/spark/index.html","hash":"6826197b97e8ebecd12e32baad4a83deedf330dd","modified":1589105954446},{"_id":"public/favicon.ico","hash":"7be62720671a143fb8b79a1a196730df57fee81d","modified":1589105954449},{"_id":"public/images/check.svg","hash":"029b8b3523b7daa4005983b4463cd93408308aab","modified":1589105954449},{"_id":"public/images/exclamation.svg","hash":"b2db56f2cc13fce73dbea46c7b446d9bcb3bf0fd","modified":1589105954449},{"_id":"public/images/info.svg","hash":"c8aa387e935ba9a7fa72c5dd000b7d46f2e030c4","modified":1589105954449},{"_id":"public/images/question.svg","hash":"7153fa2a0c21e32da6a1f96a333d8b66a178569d","modified":1589105954449},{"_id":"public/images/logo.png","hash":"4e012d9ba58cb8f87ee775262ef871c158ac5948","modified":1589105954449},{"_id":"public/images/quote-left.svg","hash":"d2561fa8d13e63ff196b71232a5968415ec6e372","modified":1589105954449},{"_id":"public/css/insight.css","hash":"f376dcda6bb50b708f3206c15a49f7530b3c534d","modified":1589105954470},{"_id":"public/css/style.css","hash":"c784fe865fc3abf5455e419bfac380dbbdb36a7d","modified":1589105954470},{"_id":"public/js/script.js","hash":"6b670ec4f90fb43b21a0bbd750a217af5d8aab6b","modified":1589105954470},{"_id":"public/js/insight.js","hash":"eb23c31141784eef7300f1d1c548950e77883f56","modified":1589105954470}],"Category":[{"name":"learning notes","_id":"cka0wnkvy0002qxot8palizly"},{"name":"hadoop","_id":"cka0wnkwc000lqxotc4a14ese"},{"name":"jvm","_id":"cka0wnkwk0015qxotxmp0hl9w"},{"name":"machine learning","_id":"cka0wnkwm001cqxotb6ygxkwn"},{"name":"others","_id":"cka0wnkwy001yqxot9idcrf97"},{"name":"reading notes","_id":"cka0wnkwz0024qxotkanmimmx"},{"name":"spark","_id":"cka0wnkx1002gqxotzza6pdj4"}],"Data":[],"Page":[{"title":"About","date":"2017-07-25T23:25:29.000Z","type":"about","comments":0,"_content":"Welome to **superAsir's Notes**. \nAll the cover images are downloaded from [PEXELS](https://www.pexels.com/).\n\n* 📖 M.E. from [BUAA](http://ev.buaa.edu.cn/)\n* 💻 Software Developer/Data Scientist\n* 🇨🇳 Beijing, China\n* 💡 Machine learning, deep learning and distributed computing \n* 🔴 COME ON YOU GOONERS!\n* 🎸 Punk's not dead \n* 🥃 Prefer rum to vodka\n* 🍺 Prefer Tsingtao to Corona\n* ☕️ Could't love Caffè Americano more\n","source":"about/index.md","raw":"---\ntitle: About\ndate: 2017-07-26 07:25:29\ntype: \"about\"\ncomments: false\n---\nWelome to **superAsir's Notes**. \nAll the cover images are downloaded from [PEXELS](https://www.pexels.com/).\n\n* 📖 M.E. from [BUAA](http://ev.buaa.edu.cn/)\n* 💻 Software Developer/Data Scientist\n* 🇨🇳 Beijing, China\n* 💡 Machine learning, deep learning and distributed computing \n* 🔴 COME ON YOU GOONERS!\n* 🎸 Punk's not dead \n* 🥃 Prefer rum to vodka\n* 🍺 Prefer Tsingtao to Corona\n* ☕️ Could't love Caffè Americano more\n","updated":"2020-05-10T06:50:12.532Z","path":"about/index.html","layout":"page","_id":"cka0wnkxp004mqxoti79ntep7","content":"<p>Welome to <strong>superAsir’s Notes</strong>.<br>All the cover images are downloaded from <a href=\"https://www.pexels.com/\" target=\"_blank\" rel=\"noopener\">PEXELS</a>.</p>\n<ul>\n<li>📖 M.E. from <a href=\"http://ev.buaa.edu.cn/\" target=\"_blank\" rel=\"noopener\">BUAA</a></li>\n<li>💻 Software Developer/Data Scientist</li>\n<li>🇨🇳 Beijing, China</li>\n<li>💡 Machine learning, deep learning and distributed computing </li>\n<li>🔴 COME ON YOU GOONERS!</li>\n<li>🎸 Punk’s not dead </li>\n<li>🥃 Prefer rum to vodka</li>\n<li>🍺 Prefer Tsingtao to Corona</li>\n<li>☕️ Could’t love Caffè Americano more</li>\n</ul>\n","site":{"data":{}},"_categories":[],"_tags":[],"excerpt":"","more":"<p>Welome to <strong>superAsir’s Notes</strong>.<br>All the cover images are downloaded from <a href=\"https://www.pexels.com/\" target=\"_blank\" rel=\"noopener\">PEXELS</a>.</p>\n<ul>\n<li>📖 M.E. from <a href=\"http://ev.buaa.edu.cn/\" target=\"_blank\" rel=\"noopener\">BUAA</a></li>\n<li>💻 Software Developer/Data Scientist</li>\n<li>🇨🇳 Beijing, China</li>\n<li>💡 Machine learning, deep learning and distributed computing </li>\n<li>🔴 COME ON YOU GOONERS!</li>\n<li>🎸 Punk’s not dead </li>\n<li>🥃 Prefer rum to vodka</li>\n<li>🍺 Prefer Tsingtao to Corona</li>\n<li>☕️ Could’t love Caffè Americano more</li>\n</ul>\n"},{"title":"Categories","date":"2017-07-25T23:25:10.000Z","type":"categories","comments":0,"_content":"","source":"categories/index.md","raw":"---\ntitle: Categories\ndate: 2017-07-26 07:25:10\ntype: categories\ncomments: false\n---\n","updated":"2020-05-10T06:50:12.533Z","path":"categories/index.html","layout":"page","_id":"cka0wnkxq004oqxot32wfdfk3","content":"","site":{"data":{}},"_categories":[],"_tags":[],"excerpt":"","more":""},{"title":"Tags","date":"2017-07-25T23:25:21.000Z","type":"tags","comments":0,"_content":"","source":"tags/index.md","raw":"---\ntitle: Tags \ndate: 2017-07-26 07:25:21\ntype: tags\ncomments: false\n---\n","updated":"2020-05-10T06:50:12.533Z","path":"tags/index.html","layout":"page","_id":"cka0wnkxs004qqxotvufoski9","content":"","site":{"data":{}},"_categories":[],"_tags":[],"excerpt":"","more":""}],"Post":[{"title":"Learning Notes-Deep Learning, course2, week1","date":"2017-09-24T06:06:08.000Z","_content":"大家好，最近在学习Andrew Ng的Deep learning课程，于是决定写一些learning notes来recap和mark一下学到的知识，避免遗忘。由于该课程的course1比较基础，我个人认为没有mark的必要，所以从course2开始，按照week来mark.\n<!--more-->\n## Data set\n在machine learning中，data set可以说是最重要的部分，区别于传统machine learning，deep learning中的data set分布更侧重于training，Ng建议我们讲data set分为三部分：\n* training set——训练数据集\n* dev/validation set——模型选择和参数调整，泛化能力测试\n* testing set——模型效果测试\n一定有很多人对于dev和testing set有一些疑问，最开始我也是懵逼的，来看看下面这段话\n> Dev/Validation Set: this data set is used to minimize overfitting. You're not adjusting the weights of the network with this data set, you're just verifying that any increase in accuracy over the training data set actually yields an increase in accuracy over a data set that has not been shown to the network before, or at least the network hasn't trained on it (i.e. validation data set). If the accuracy over the training data set increases, but the accuracy over then validation data set stays the same or decreases, then you're overfitting your neural network and you should stop training.\nTesting Set: this data set is used only for testing the final solution in order to confirm the actual predictive power of the network.\n\n这三者的比例则是/99.5%/2.5%/2.5%/，这样的原因是因为deep learning中，数据量足够大而且deep learning的学习能力很强，大家一定注意这一点。当然，如果实在没有test set，但是有dev set也是可以接受的。\n## Bias and variance\n### 什么是bias和variance\nbias & variance是machine learning 领域一个经典的辩证问题，在Ng经典的CS229中就重点的讲述过，具体的定义我不太想给出了，后续有时间可以专门写一篇，后面会给出一些资料链接。我们简单的看一幅图\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/6/6-5.png)\n左图就是一个典型的high bias situation，模型没有办法很好的拟合数据，这也就是我们常说的under fitting，右图则是典型的high variance situation，模型过分的拟合了training set，这就是我们最需要防范的over fitting.当然，中间的则是比较理想的状况。\n### Solution\n在实际的工作中，我们应该怎么分析自己模型的bias和variance情况呢，Ng给了我们一个流程图，如下：\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/6/6-2.png)\n首先检验是否存在high bias 情况，具体方法是在training set 和 dev set上计算error，对比training error和dev error，如果两者都很高，那么就是high bias，如果training error很小而dev error很高，那么一定是high variance，如果两者都很大，那么就是最差的情况了既high bias又high variance\n\n对于high bias，我们可以通过更复杂的神经网络、更长的训练时间，更强的网络结构来解决这个问题；\n对于high variance，我们可以通过更多的数据，regularization的方法来解决。\n## Regularization\n### L1&L2 regularization\n这部分内容我就不多说了，我之前专门详细深入的讲述过L1和L2 regularization，大家可以去看一看。\n\n唯一需要明确的一点是，在加入L1或者L2 regularization之后，在观测cost function convergence 的时候，**一定要带上regularization item**，否则结果是很难看到convergence的，这和regularization性质有很大的关系。 \n### Dropout\nDropout是neural network中一种经典的regularization方法，经典到什么程度呢，我当年毕设课题中都用到了这个方法，而且效果超赞\n\nDropout方法的实质是**按比例随机隐藏**掉neural network中layer里的某些units，也就是说，再一次epoch中，只有一部分的units对应的weights和bias会得到更新，而下一次epoch中，则是另一部分units对应的weights和bias得到更新，如下图\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/6/6-3.png)\n那么为什么Dropout可以实现regularization效果呢，Ng告诉我们：\n> Intuition:Can't rely on any one feature, so have to spread out weights\n\n如何理解呢？加入dropout后，每个unit对应的weights和bias不能完全依赖上层units，因为他并不是每一次epoch都可以work on，因此在学习的过程中，见笑了over fitting的风险。实际上，dropout可以产生shrink weights的效果，和L2 regularization相似，因此也是一种regularization方法。\n\n但是，dropout和L2 regularization唯一的区别在于，他很难给出一个regularization item，所以你没有办法画出cost function convergence的轨迹。\n### Other methods\n除了经典的L1、L2 regularization和dropout方法，还有一些防止over fitting的方法，例如图像处理中，我们可以用data augmentation，旋转，翻转，加噪声等方法。\n\n还有一个early stopping方法，我们都知道，随着training 的epoch增多，模型对training set拟合会越来越好，随之带来的问题就是可能over fitting，我们可以通过early stopping，让模型在没有产生over fitting的时候停下来，效果可能会更好。\n## Exploding/vanishing gradient\n### 什么是exploding/vanishing gradient\n对于deep learning，曾经最为棘手的问题就是exploding/vanishing gradient，甚至是限制deep learning发展的瓶颈，我们来一起看看。\n 假设我们有一个**比较深**的neural network，假设一共有\\\\(l\\\\)层，对应的weights是\\\\(W^{[1]}\\\\)到\\\\(W^{[l]}\\\\)，bias是\\\\(b^{[1]}\\\\)到\\\\(b^{[l]}\\\\)，我们为了计算方便，假设bias均为0，active function为\\\\(g(z)=z\\\\)，那么，\\\\(y\\\\)就等于\n$$\\hat{y}=W^{[l]}W^{[l-1]}W^{[l-2]} \\cdots W^{[3]}W^{[2]}W^{[1]}X$$\n大家感兴趣的话可以验证一下，很简单的。\n\n那么现在问题来了，当\\\\(l\\\\)很大的情况下，如果\\\\(W\\\\)元素都大于1，那么最后的结果就会非常非常大，甚至到无限大，这种情况叫exploding gradient；相应的，如果\\\\(W\\\\)元素都小于1，那么最后的结果就会特别小，甚至为零，这就是vanishing gradient.\n### Solution\n对于上面的问题，我们一般在weights初始化的时候做一些工作来解决可能出现的exploding or vanishing gradient。我们可以直观的理解一下，对于active function\\\\(g(z)\\\\)，假设bias为零，\n$$z=w_{1}x_1 +w_{2}x_2+ \\cdots +w_{n}x_n$$\n我们要我们可以看到，\\\\(n\\\\)的增大，\\\\(w\\\\)会变小，我们让\\\\(w\\\\)始终保持以0为mean，1为varance的Gaussian distribution下，就可以很好的控制\\\\(w\\\\)的大小，那么我们可以看到，\\\\(var(w)= \\frac{1}{n^{l-1}}\\\\)\n\n在Ng的建议中，如果active function是sigmoid，我们一般取\\\\(var(w)= \\frac{1}{n^{l-1}}\\\\)，如果是reLu，我们取\\\\(var(w)= \\frac{2}{n^{l-1}}\\\\)，对于tanh，\\\\(var(w)= \\frac{1}{n^{l-1}}\\\\)或者\\\\(var(w)= \\frac{2}{n^{l-1}+n^{l}}\\\\)，这样可以很好的避免exploding or vanishing gradient.\n## Gradient checking\n### Gradient approximation\n在调试neural network的时候，我们会经常做gradient check的工作，以确定整个network正常的运行，Ng在这里建议我们使用双边逼近的方法去做gradient check，这里我不做太多描述，主要上一张图：\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/6/6-6.png)\n通常来说，双边逼近的方法获得结果更加准确。\n### Gradient checking notes\n> 1. Don't use in training-only to debug(too slow)\n2. If algorithm fails grad check, look at components to try to identify bug\n3. Remeber regularization\n4. Dosen't wrok with dropout\n5. Run at random initialzation; perhaps again after some training.\n\n## Reference\n* [Deep learning-Coursera Andrew Ng](https://www.coursera.org/specializations/deep-learning)\n* [Deep learning-网易云课堂 Andrew Ng](https://mooc.study.163.com/course/deeplearning_ai-2001281003#/info)\n* [Bias and variance](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff)\n","source":"_posts/course-deep-learning-course2-week1.md","raw":"---\ntitle: Learning Notes-Deep Learning, course2, week1\ndate: 2017-09-24 14:06:08\ntags: \n\t- regularization\n\t- gradient descent\ncategories: learning notes\n---\n大家好，最近在学习Andrew Ng的Deep learning课程，于是决定写一些learning notes来recap和mark一下学到的知识，避免遗忘。由于该课程的course1比较基础，我个人认为没有mark的必要，所以从course2开始，按照week来mark.\n<!--more-->\n## Data set\n在machine learning中，data set可以说是最重要的部分，区别于传统machine learning，deep learning中的data set分布更侧重于training，Ng建议我们讲data set分为三部分：\n* training set——训练数据集\n* dev/validation set——模型选择和参数调整，泛化能力测试\n* testing set——模型效果测试\n一定有很多人对于dev和testing set有一些疑问，最开始我也是懵逼的，来看看下面这段话\n> Dev/Validation Set: this data set is used to minimize overfitting. You're not adjusting the weights of the network with this data set, you're just verifying that any increase in accuracy over the training data set actually yields an increase in accuracy over a data set that has not been shown to the network before, or at least the network hasn't trained on it (i.e. validation data set). If the accuracy over the training data set increases, but the accuracy over then validation data set stays the same or decreases, then you're overfitting your neural network and you should stop training.\nTesting Set: this data set is used only for testing the final solution in order to confirm the actual predictive power of the network.\n\n这三者的比例则是/99.5%/2.5%/2.5%/，这样的原因是因为deep learning中，数据量足够大而且deep learning的学习能力很强，大家一定注意这一点。当然，如果实在没有test set，但是有dev set也是可以接受的。\n## Bias and variance\n### 什么是bias和variance\nbias & variance是machine learning 领域一个经典的辩证问题，在Ng经典的CS229中就重点的讲述过，具体的定义我不太想给出了，后续有时间可以专门写一篇，后面会给出一些资料链接。我们简单的看一幅图\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/6/6-5.png)\n左图就是一个典型的high bias situation，模型没有办法很好的拟合数据，这也就是我们常说的under fitting，右图则是典型的high variance situation，模型过分的拟合了training set，这就是我们最需要防范的over fitting.当然，中间的则是比较理想的状况。\n### Solution\n在实际的工作中，我们应该怎么分析自己模型的bias和variance情况呢，Ng给了我们一个流程图，如下：\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/6/6-2.png)\n首先检验是否存在high bias 情况，具体方法是在training set 和 dev set上计算error，对比training error和dev error，如果两者都很高，那么就是high bias，如果training error很小而dev error很高，那么一定是high variance，如果两者都很大，那么就是最差的情况了既high bias又high variance\n\n对于high bias，我们可以通过更复杂的神经网络、更长的训练时间，更强的网络结构来解决这个问题；\n对于high variance，我们可以通过更多的数据，regularization的方法来解决。\n## Regularization\n### L1&L2 regularization\n这部分内容我就不多说了，我之前专门详细深入的讲述过L1和L2 regularization，大家可以去看一看。\n\n唯一需要明确的一点是，在加入L1或者L2 regularization之后，在观测cost function convergence 的时候，**一定要带上regularization item**，否则结果是很难看到convergence的，这和regularization性质有很大的关系。 \n### Dropout\nDropout是neural network中一种经典的regularization方法，经典到什么程度呢，我当年毕设课题中都用到了这个方法，而且效果超赞\n\nDropout方法的实质是**按比例随机隐藏**掉neural network中layer里的某些units，也就是说，再一次epoch中，只有一部分的units对应的weights和bias会得到更新，而下一次epoch中，则是另一部分units对应的weights和bias得到更新，如下图\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/6/6-3.png)\n那么为什么Dropout可以实现regularization效果呢，Ng告诉我们：\n> Intuition:Can't rely on any one feature, so have to spread out weights\n\n如何理解呢？加入dropout后，每个unit对应的weights和bias不能完全依赖上层units，因为他并不是每一次epoch都可以work on，因此在学习的过程中，见笑了over fitting的风险。实际上，dropout可以产生shrink weights的效果，和L2 regularization相似，因此也是一种regularization方法。\n\n但是，dropout和L2 regularization唯一的区别在于，他很难给出一个regularization item，所以你没有办法画出cost function convergence的轨迹。\n### Other methods\n除了经典的L1、L2 regularization和dropout方法，还有一些防止over fitting的方法，例如图像处理中，我们可以用data augmentation，旋转，翻转，加噪声等方法。\n\n还有一个early stopping方法，我们都知道，随着training 的epoch增多，模型对training set拟合会越来越好，随之带来的问题就是可能over fitting，我们可以通过early stopping，让模型在没有产生over fitting的时候停下来，效果可能会更好。\n## Exploding/vanishing gradient\n### 什么是exploding/vanishing gradient\n对于deep learning，曾经最为棘手的问题就是exploding/vanishing gradient，甚至是限制deep learning发展的瓶颈，我们来一起看看。\n 假设我们有一个**比较深**的neural network，假设一共有\\\\(l\\\\)层，对应的weights是\\\\(W^{[1]}\\\\)到\\\\(W^{[l]}\\\\)，bias是\\\\(b^{[1]}\\\\)到\\\\(b^{[l]}\\\\)，我们为了计算方便，假设bias均为0，active function为\\\\(g(z)=z\\\\)，那么，\\\\(y\\\\)就等于\n$$\\hat{y}=W^{[l]}W^{[l-1]}W^{[l-2]} \\cdots W^{[3]}W^{[2]}W^{[1]}X$$\n大家感兴趣的话可以验证一下，很简单的。\n\n那么现在问题来了，当\\\\(l\\\\)很大的情况下，如果\\\\(W\\\\)元素都大于1，那么最后的结果就会非常非常大，甚至到无限大，这种情况叫exploding gradient；相应的，如果\\\\(W\\\\)元素都小于1，那么最后的结果就会特别小，甚至为零，这就是vanishing gradient.\n### Solution\n对于上面的问题，我们一般在weights初始化的时候做一些工作来解决可能出现的exploding or vanishing gradient。我们可以直观的理解一下，对于active function\\\\(g(z)\\\\)，假设bias为零，\n$$z=w_{1}x_1 +w_{2}x_2+ \\cdots +w_{n}x_n$$\n我们要我们可以看到，\\\\(n\\\\)的增大，\\\\(w\\\\)会变小，我们让\\\\(w\\\\)始终保持以0为mean，1为varance的Gaussian distribution下，就可以很好的控制\\\\(w\\\\)的大小，那么我们可以看到，\\\\(var(w)= \\frac{1}{n^{l-1}}\\\\)\n\n在Ng的建议中，如果active function是sigmoid，我们一般取\\\\(var(w)= \\frac{1}{n^{l-1}}\\\\)，如果是reLu，我们取\\\\(var(w)= \\frac{2}{n^{l-1}}\\\\)，对于tanh，\\\\(var(w)= \\frac{1}{n^{l-1}}\\\\)或者\\\\(var(w)= \\frac{2}{n^{l-1}+n^{l}}\\\\)，这样可以很好的避免exploding or vanishing gradient.\n## Gradient checking\n### Gradient approximation\n在调试neural network的时候，我们会经常做gradient check的工作，以确定整个network正常的运行，Ng在这里建议我们使用双边逼近的方法去做gradient check，这里我不做太多描述，主要上一张图：\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/6/6-6.png)\n通常来说，双边逼近的方法获得结果更加准确。\n### Gradient checking notes\n> 1. Don't use in training-only to debug(too slow)\n2. If algorithm fails grad check, look at components to try to identify bug\n3. Remeber regularization\n4. Dosen't wrok with dropout\n5. Run at random initialzation; perhaps again after some training.\n\n## Reference\n* [Deep learning-Coursera Andrew Ng](https://www.coursera.org/specializations/deep-learning)\n* [Deep learning-网易云课堂 Andrew Ng](https://mooc.study.163.com/course/deeplearning_ai-2001281003#/info)\n* [Bias and variance](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff)\n","slug":"course-deep-learning-course2-week1","published":1,"updated":"2020-05-10T06:50:12.527Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cka0wnkvt0000qxotbi9706ol","content":"<p>大家好，最近在学习Andrew Ng的Deep learning课程，于是决定写一些learning notes来recap和mark一下学到的知识，避免遗忘。由于该课程的course1比较基础，我个人认为没有mark的必要，所以从course2开始，按照week来mark.<br><a id=\"more\"></a></p>\n<h2 id=\"Data-set\"><a href=\"#Data-set\" class=\"headerlink\" title=\"Data set\"></a>Data set</h2><p>在machine learning中，data set可以说是最重要的部分，区别于传统machine learning，deep learning中的data set分布更侧重于training，Ng建议我们讲data set分为三部分：</p>\n<ul>\n<li>training set——训练数据集</li>\n<li>dev/validation set——模型选择和参数调整，泛化能力测试</li>\n<li>testing set——模型效果测试<br>一定有很多人对于dev和testing set有一些疑问，最开始我也是懵逼的，来看看下面这段话<blockquote>\n<p>Dev/Validation Set: this data set is used to minimize overfitting. You’re not adjusting the weights of the network with this data set, you’re just verifying that any increase in accuracy over the training data set actually yields an increase in accuracy over a data set that has not been shown to the network before, or at least the network hasn’t trained on it (i.e. validation data set). If the accuracy over the training data set increases, but the accuracy over then validation data set stays the same or decreases, then you’re overfitting your neural network and you should stop training.<br>Testing Set: this data set is used only for testing the final solution in order to confirm the actual predictive power of the network.</p>\n</blockquote>\n</li>\n</ul>\n<p>这三者的比例则是/99.5%/2.5%/2.5%/，这样的原因是因为deep learning中，数据量足够大而且deep learning的学习能力很强，大家一定注意这一点。当然，如果实在没有test set，但是有dev set也是可以接受的。</p>\n<h2 id=\"Bias-and-variance\"><a href=\"#Bias-and-variance\" class=\"headerlink\" title=\"Bias and variance\"></a>Bias and variance</h2><h3 id=\"什么是bias和variance\"><a href=\"#什么是bias和variance\" class=\"headerlink\" title=\"什么是bias和variance\"></a>什么是bias和variance</h3><p>bias &amp; variance是machine learning 领域一个经典的辩证问题，在Ng经典的CS229中就重点的讲述过，具体的定义我不太想给出了，后续有时间可以专门写一篇，后面会给出一些资料链接。我们简单的看一幅图<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/6/6-5.png\" alt=\"\"><br>左图就是一个典型的high bias situation，模型没有办法很好的拟合数据，这也就是我们常说的under fitting，右图则是典型的high variance situation，模型过分的拟合了training set，这就是我们最需要防范的over fitting.当然，中间的则是比较理想的状况。</p>\n<h3 id=\"Solution\"><a href=\"#Solution\" class=\"headerlink\" title=\"Solution\"></a>Solution</h3><p>在实际的工作中，我们应该怎么分析自己模型的bias和variance情况呢，Ng给了我们一个流程图，如下：<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/6/6-2.png\" alt=\"\"><br>首先检验是否存在high bias 情况，具体方法是在training set 和 dev set上计算error，对比training error和dev error，如果两者都很高，那么就是high bias，如果training error很小而dev error很高，那么一定是high variance，如果两者都很大，那么就是最差的情况了既high bias又high variance</p>\n<p>对于high bias，我们可以通过更复杂的神经网络、更长的训练时间，更强的网络结构来解决这个问题；<br>对于high variance，我们可以通过更多的数据，regularization的方法来解决。</p>\n<h2 id=\"Regularization\"><a href=\"#Regularization\" class=\"headerlink\" title=\"Regularization\"></a>Regularization</h2><h3 id=\"L1-amp-L2-regularization\"><a href=\"#L1-amp-L2-regularization\" class=\"headerlink\" title=\"L1&amp;L2 regularization\"></a>L1&amp;L2 regularization</h3><p>这部分内容我就不多说了，我之前专门详细深入的讲述过L1和L2 regularization，大家可以去看一看。</p>\n<p>唯一需要明确的一点是，在加入L1或者L2 regularization之后，在观测cost function convergence 的时候，<strong>一定要带上regularization item</strong>，否则结果是很难看到convergence的，这和regularization性质有很大的关系。 </p>\n<h3 id=\"Dropout\"><a href=\"#Dropout\" class=\"headerlink\" title=\"Dropout\"></a>Dropout</h3><p>Dropout是neural network中一种经典的regularization方法，经典到什么程度呢，我当年毕设课题中都用到了这个方法，而且效果超赞</p>\n<p>Dropout方法的实质是<strong>按比例随机隐藏</strong>掉neural network中layer里的某些units，也就是说，再一次epoch中，只有一部分的units对应的weights和bias会得到更新，而下一次epoch中，则是另一部分units对应的weights和bias得到更新，如下图<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/6/6-3.png\" alt=\"\"><br>那么为什么Dropout可以实现regularization效果呢，Ng告诉我们：</p>\n<blockquote>\n<p>Intuition:Can’t rely on any one feature, so have to spread out weights</p>\n</blockquote>\n<p>如何理解呢？加入dropout后，每个unit对应的weights和bias不能完全依赖上层units，因为他并不是每一次epoch都可以work on，因此在学习的过程中，见笑了over fitting的风险。实际上，dropout可以产生shrink weights的效果，和L2 regularization相似，因此也是一种regularization方法。</p>\n<p>但是，dropout和L2 regularization唯一的区别在于，他很难给出一个regularization item，所以你没有办法画出cost function convergence的轨迹。</p>\n<h3 id=\"Other-methods\"><a href=\"#Other-methods\" class=\"headerlink\" title=\"Other methods\"></a>Other methods</h3><p>除了经典的L1、L2 regularization和dropout方法，还有一些防止over fitting的方法，例如图像处理中，我们可以用data augmentation，旋转，翻转，加噪声等方法。</p>\n<p>还有一个early stopping方法，我们都知道，随着training 的epoch增多，模型对training set拟合会越来越好，随之带来的问题就是可能over fitting，我们可以通过early stopping，让模型在没有产生over fitting的时候停下来，效果可能会更好。</p>\n<h2 id=\"Exploding-vanishing-gradient\"><a href=\"#Exploding-vanishing-gradient\" class=\"headerlink\" title=\"Exploding/vanishing gradient\"></a>Exploding/vanishing gradient</h2><h3 id=\"什么是exploding-vanishing-gradient\"><a href=\"#什么是exploding-vanishing-gradient\" class=\"headerlink\" title=\"什么是exploding/vanishing gradient\"></a>什么是exploding/vanishing gradient</h3><p>对于deep learning，曾经最为棘手的问题就是exploding/vanishing gradient，甚至是限制deep learning发展的瓶颈，我们来一起看看。<br> 假设我们有一个<strong>比较深</strong>的neural network，假设一共有\\(l\\)层，对应的weights是\\(W^{[1]}\\)到\\(W^{[l]}\\)，bias是\\(b^{[1]}\\)到\\(b^{[l]}\\)，我们为了计算方便，假设bias均为0，active function为\\(g(z)=z\\)，那么，\\(y\\)就等于<br>$$\\hat{y}=W^{[l]}W^{[l-1]}W^{[l-2]} \\cdots W^{[3]}W^{[2]}W^{[1]}X$$<br>大家感兴趣的话可以验证一下，很简单的。</p>\n<p>那么现在问题来了，当\\(l\\)很大的情况下，如果\\(W\\)元素都大于1，那么最后的结果就会非常非常大，甚至到无限大，这种情况叫exploding gradient；相应的，如果\\(W\\)元素都小于1，那么最后的结果就会特别小，甚至为零，这就是vanishing gradient.</p>\n<h3 id=\"Solution-1\"><a href=\"#Solution-1\" class=\"headerlink\" title=\"Solution\"></a>Solution</h3><p>对于上面的问题，我们一般在weights初始化的时候做一些工作来解决可能出现的exploding or vanishing gradient。我们可以直观的理解一下，对于active function\\(g(z)\\)，假设bias为零，<br>$$z=w_{1}x_1 +w_{2}x_2+ \\cdots +w_{n}x_n$$<br>我们要我们可以看到，\\(n\\)的增大，\\(w\\)会变小，我们让\\(w\\)始终保持以0为mean，1为varance的Gaussian distribution下，就可以很好的控制\\(w\\)的大小，那么我们可以看到，\\(var(w)= \\frac{1}{n^{l-1}}\\)</p>\n<p>在Ng的建议中，如果active function是sigmoid，我们一般取\\(var(w)= \\frac{1}{n^{l-1}}\\)，如果是reLu，我们取\\(var(w)= \\frac{2}{n^{l-1}}\\)，对于tanh，\\(var(w)= \\frac{1}{n^{l-1}}\\)或者\\(var(w)= \\frac{2}{n^{l-1}+n^{l}}\\)，这样可以很好的避免exploding or vanishing gradient.</p>\n<h2 id=\"Gradient-checking\"><a href=\"#Gradient-checking\" class=\"headerlink\" title=\"Gradient checking\"></a>Gradient checking</h2><h3 id=\"Gradient-approximation\"><a href=\"#Gradient-approximation\" class=\"headerlink\" title=\"Gradient approximation\"></a>Gradient approximation</h3><p>在调试neural network的时候，我们会经常做gradient check的工作，以确定整个network正常的运行，Ng在这里建议我们使用双边逼近的方法去做gradient check，这里我不做太多描述，主要上一张图：<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/6/6-6.png\" alt=\"\"><br>通常来说，双边逼近的方法获得结果更加准确。</p>\n<h3 id=\"Gradient-checking-notes\"><a href=\"#Gradient-checking-notes\" class=\"headerlink\" title=\"Gradient checking notes\"></a>Gradient checking notes</h3><blockquote>\n<ol>\n<li>Don’t use in training-only to debug(too slow)</li>\n<li>If algorithm fails grad check, look at components to try to identify bug</li>\n<li>Remeber regularization</li>\n<li>Dosen’t wrok with dropout</li>\n<li>Run at random initialzation; perhaps again after some training.</li>\n</ol>\n</blockquote>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ul>\n<li><a href=\"https://www.coursera.org/specializations/deep-learning\" target=\"_blank\" rel=\"noopener\">Deep learning-Coursera Andrew Ng</a></li>\n<li><a href=\"https://mooc.study.163.com/course/deeplearning_ai-2001281003#/info\" target=\"_blank\" rel=\"noopener\">Deep learning-网易云课堂 Andrew Ng</a></li>\n<li><a href=\"https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff\" target=\"_blank\" rel=\"noopener\">Bias and variance</a></li>\n</ul>\n","site":{"data":{}},"_categories":[{"name":"learning notes","path":"categories/learning-notes/"}],"_tags":[{"name":"regularization","path":"tags/regularization/"},{"name":"gradient descent","path":"tags/gradient-descent/"}],"excerpt":"<p>大家好，最近在学习Andrew Ng的Deep learning课程，于是决定写一些learning notes来recap和mark一下学到的知识，避免遗忘。由于该课程的course1比较基础，我个人认为没有mark的必要，所以从course2开始，按照week来mark.<br></p>","more":"</p>\n<h2 id=\"Data-set\"><a href=\"#Data-set\" class=\"headerlink\" title=\"Data set\"></a>Data set</h2><p>在machine learning中，data set可以说是最重要的部分，区别于传统machine learning，deep learning中的data set分布更侧重于training，Ng建议我们讲data set分为三部分：</p>\n<ul>\n<li>training set——训练数据集</li>\n<li>dev/validation set——模型选择和参数调整，泛化能力测试</li>\n<li>testing set——模型效果测试<br>一定有很多人对于dev和testing set有一些疑问，最开始我也是懵逼的，来看看下面这段话<blockquote>\n<p>Dev/Validation Set: this data set is used to minimize overfitting. You’re not adjusting the weights of the network with this data set, you’re just verifying that any increase in accuracy over the training data set actually yields an increase in accuracy over a data set that has not been shown to the network before, or at least the network hasn’t trained on it (i.e. validation data set). If the accuracy over the training data set increases, but the accuracy over then validation data set stays the same or decreases, then you’re overfitting your neural network and you should stop training.<br>Testing Set: this data set is used only for testing the final solution in order to confirm the actual predictive power of the network.</p>\n</blockquote>\n</li>\n</ul>\n<p>这三者的比例则是/99.5%/2.5%/2.5%/，这样的原因是因为deep learning中，数据量足够大而且deep learning的学习能力很强，大家一定注意这一点。当然，如果实在没有test set，但是有dev set也是可以接受的。</p>\n<h2 id=\"Bias-and-variance\"><a href=\"#Bias-and-variance\" class=\"headerlink\" title=\"Bias and variance\"></a>Bias and variance</h2><h3 id=\"什么是bias和variance\"><a href=\"#什么是bias和variance\" class=\"headerlink\" title=\"什么是bias和variance\"></a>什么是bias和variance</h3><p>bias &amp; variance是machine learning 领域一个经典的辩证问题，在Ng经典的CS229中就重点的讲述过，具体的定义我不太想给出了，后续有时间可以专门写一篇，后面会给出一些资料链接。我们简单的看一幅图<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/6/6-5.png\" alt=\"\"><br>左图就是一个典型的high bias situation，模型没有办法很好的拟合数据，这也就是我们常说的under fitting，右图则是典型的high variance situation，模型过分的拟合了training set，这就是我们最需要防范的over fitting.当然，中间的则是比较理想的状况。</p>\n<h3 id=\"Solution\"><a href=\"#Solution\" class=\"headerlink\" title=\"Solution\"></a>Solution</h3><p>在实际的工作中，我们应该怎么分析自己模型的bias和variance情况呢，Ng给了我们一个流程图，如下：<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/6/6-2.png\" alt=\"\"><br>首先检验是否存在high bias 情况，具体方法是在training set 和 dev set上计算error，对比training error和dev error，如果两者都很高，那么就是high bias，如果training error很小而dev error很高，那么一定是high variance，如果两者都很大，那么就是最差的情况了既high bias又high variance</p>\n<p>对于high bias，我们可以通过更复杂的神经网络、更长的训练时间，更强的网络结构来解决这个问题；<br>对于high variance，我们可以通过更多的数据，regularization的方法来解决。</p>\n<h2 id=\"Regularization\"><a href=\"#Regularization\" class=\"headerlink\" title=\"Regularization\"></a>Regularization</h2><h3 id=\"L1-amp-L2-regularization\"><a href=\"#L1-amp-L2-regularization\" class=\"headerlink\" title=\"L1&amp;L2 regularization\"></a>L1&amp;L2 regularization</h3><p>这部分内容我就不多说了，我之前专门详细深入的讲述过L1和L2 regularization，大家可以去看一看。</p>\n<p>唯一需要明确的一点是，在加入L1或者L2 regularization之后，在观测cost function convergence 的时候，<strong>一定要带上regularization item</strong>，否则结果是很难看到convergence的，这和regularization性质有很大的关系。 </p>\n<h3 id=\"Dropout\"><a href=\"#Dropout\" class=\"headerlink\" title=\"Dropout\"></a>Dropout</h3><p>Dropout是neural network中一种经典的regularization方法，经典到什么程度呢，我当年毕设课题中都用到了这个方法，而且效果超赞</p>\n<p>Dropout方法的实质是<strong>按比例随机隐藏</strong>掉neural network中layer里的某些units，也就是说，再一次epoch中，只有一部分的units对应的weights和bias会得到更新，而下一次epoch中，则是另一部分units对应的weights和bias得到更新，如下图<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/6/6-3.png\" alt=\"\"><br>那么为什么Dropout可以实现regularization效果呢，Ng告诉我们：</p>\n<blockquote>\n<p>Intuition:Can’t rely on any one feature, so have to spread out weights</p>\n</blockquote>\n<p>如何理解呢？加入dropout后，每个unit对应的weights和bias不能完全依赖上层units，因为他并不是每一次epoch都可以work on，因此在学习的过程中，见笑了over fitting的风险。实际上，dropout可以产生shrink weights的效果，和L2 regularization相似，因此也是一种regularization方法。</p>\n<p>但是，dropout和L2 regularization唯一的区别在于，他很难给出一个regularization item，所以你没有办法画出cost function convergence的轨迹。</p>\n<h3 id=\"Other-methods\"><a href=\"#Other-methods\" class=\"headerlink\" title=\"Other methods\"></a>Other methods</h3><p>除了经典的L1、L2 regularization和dropout方法，还有一些防止over fitting的方法，例如图像处理中，我们可以用data augmentation，旋转，翻转，加噪声等方法。</p>\n<p>还有一个early stopping方法，我们都知道，随着training 的epoch增多，模型对training set拟合会越来越好，随之带来的问题就是可能over fitting，我们可以通过early stopping，让模型在没有产生over fitting的时候停下来，效果可能会更好。</p>\n<h2 id=\"Exploding-vanishing-gradient\"><a href=\"#Exploding-vanishing-gradient\" class=\"headerlink\" title=\"Exploding/vanishing gradient\"></a>Exploding/vanishing gradient</h2><h3 id=\"什么是exploding-vanishing-gradient\"><a href=\"#什么是exploding-vanishing-gradient\" class=\"headerlink\" title=\"什么是exploding/vanishing gradient\"></a>什么是exploding/vanishing gradient</h3><p>对于deep learning，曾经最为棘手的问题就是exploding/vanishing gradient，甚至是限制deep learning发展的瓶颈，我们来一起看看。<br> 假设我们有一个<strong>比较深</strong>的neural network，假设一共有\\(l\\)层，对应的weights是\\(W^{[1]}\\)到\\(W^{[l]}\\)，bias是\\(b^{[1]}\\)到\\(b^{[l]}\\)，我们为了计算方便，假设bias均为0，active function为\\(g(z)=z\\)，那么，\\(y\\)就等于<br>$$\\hat{y}=W^{[l]}W^{[l-1]}W^{[l-2]} \\cdots W^{[3]}W^{[2]}W^{[1]}X$$<br>大家感兴趣的话可以验证一下，很简单的。</p>\n<p>那么现在问题来了，当\\(l\\)很大的情况下，如果\\(W\\)元素都大于1，那么最后的结果就会非常非常大，甚至到无限大，这种情况叫exploding gradient；相应的，如果\\(W\\)元素都小于1，那么最后的结果就会特别小，甚至为零，这就是vanishing gradient.</p>\n<h3 id=\"Solution-1\"><a href=\"#Solution-1\" class=\"headerlink\" title=\"Solution\"></a>Solution</h3><p>对于上面的问题，我们一般在weights初始化的时候做一些工作来解决可能出现的exploding or vanishing gradient。我们可以直观的理解一下，对于active function\\(g(z)\\)，假设bias为零，<br>$$z=w_{1}x_1 +w_{2}x_2+ \\cdots +w_{n}x_n$$<br>我们要我们可以看到，\\(n\\)的增大，\\(w\\)会变小，我们让\\(w\\)始终保持以0为mean，1为varance的Gaussian distribution下，就可以很好的控制\\(w\\)的大小，那么我们可以看到，\\(var(w)= \\frac{1}{n^{l-1}}\\)</p>\n<p>在Ng的建议中，如果active function是sigmoid，我们一般取\\(var(w)= \\frac{1}{n^{l-1}}\\)，如果是reLu，我们取\\(var(w)= \\frac{2}{n^{l-1}}\\)，对于tanh，\\(var(w)= \\frac{1}{n^{l-1}}\\)或者\\(var(w)= \\frac{2}{n^{l-1}+n^{l}}\\)，这样可以很好的避免exploding or vanishing gradient.</p>\n<h2 id=\"Gradient-checking\"><a href=\"#Gradient-checking\" class=\"headerlink\" title=\"Gradient checking\"></a>Gradient checking</h2><h3 id=\"Gradient-approximation\"><a href=\"#Gradient-approximation\" class=\"headerlink\" title=\"Gradient approximation\"></a>Gradient approximation</h3><p>在调试neural network的时候，我们会经常做gradient check的工作，以确定整个network正常的运行，Ng在这里建议我们使用双边逼近的方法去做gradient check，这里我不做太多描述，主要上一张图：<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/6/6-6.png\" alt=\"\"><br>通常来说，双边逼近的方法获得结果更加准确。</p>\n<h3 id=\"Gradient-checking-notes\"><a href=\"#Gradient-checking-notes\" class=\"headerlink\" title=\"Gradient checking notes\"></a>Gradient checking notes</h3><blockquote>\n<ol>\n<li>Don’t use in training-only to debug(too slow)</li>\n<li>If algorithm fails grad check, look at components to try to identify bug</li>\n<li>Remeber regularization</li>\n<li>Dosen’t wrok with dropout</li>\n<li>Run at random initialzation; perhaps again after some training.</li>\n</ol>\n</blockquote>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ul>\n<li><a href=\"https://www.coursera.org/specializations/deep-learning\" target=\"_blank\" rel=\"noopener\">Deep learning-Coursera Andrew Ng</a></li>\n<li><a href=\"https://mooc.study.163.com/course/deeplearning_ai-2001281003#/info\" target=\"_blank\" rel=\"noopener\">Deep learning-网易云课堂 Andrew Ng</a></li>\n<li><a href=\"https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff\" target=\"_blank\" rel=\"noopener\">Bias and variance</a></li>\n</ul>"},{"title":"Learning Notes-Deep Learning, course2, week3","date":"2017-09-30T07:44:25.000Z","_content":"不知不觉来到第三周的课程了，大家加油！这周的主要内容是hyperparameter selection和batch normal的问题，我们一起来看看这一周的内容！\n<!--more-->\n## Hyperparameter selection\nHyperparameter selection在machine learning中是一个非常重要的优化过程，例如gradient descent中的learning rate \\\\(\\alpha\\\\)就是关乎算法结果的重要hyperparameter，那么我们应该怎么去选择呢？Ng给出了两个建议：\n* 构建多个hyperparameter交叉，随机选择大小，选择效果较好的范围，继续随机选择hyperparameter大小，观察结果。\n* 选择参数的时候分段选择，并且使用log分段，例如在0.0001到1之间选择，将数轴分成0.0001，0.001，0.01，0.1和1，这样选择出的结果更好\n\n对于整体模型的hyperparameter selection，Ng也出了建议，那就是babysitting和parallel方法，一种是对一个模型多次调整，一种是同时启动多个不同hyperparameter的模型，最后取效果最好的。\n\n两种方法殊途同归，可以根据自己的具体情况做出选择。\n## Batch norm\n### Normalization\n相信大家都听说过大名鼎鼎的normalization吧，这是一种很棒的数据预处理的方法，它可以很好的提升数据处理（例如gradient descent）的速度和效果，在引入batch norm之前，我也稍微提一下normalization，下面上公式：\n\n对于输入数据来说，我们可以按以下方法来normalize\n$$ \\mu = \\frac{1}{m} \\sum_i x^{(i)}$$\n$$X = X- \\mu$$\n$$ \\sigma^2 = \\frac{1}{m} \\sum_i (x^{(i)})^2 $$\n$$ X = X/ \\sigma ^2$$\n这样，我们就把输入数据转化成了符合期望为0，方差为1的Gaussian distribution的数据。\n\n当然，这只是normaliztion中的一种方法，也是被称作z-score方法。\n### Batch norm\n上面说的normalization方法可以推广到neural networks中，对于nerual networks中的某一个layer来说，可以看做是一个孤立的计算过程，在这个过程中，我们可以引入normalization，对于\\\\(z^{(i)}\\\\)来说：\n$$ \\mu = \\frac{1}{m} \\sum_i z^{(i)}$$\n$$ \\sigma ^2= \\frac{1}{m} \\sum_i (z^{(i)}- \\mu)^2 $$\n$$z^{(i)}_{norm}= \\frac{z^{(i)}- \\mu}{ \\sqrt{ \\sigma^2 + \\epsilon}}$$\n$$z^{N(i)}= \\gamma z^{(i)}_{norm} + \\beta$$\n然后我们用最终的\\\\(z^{N\\[l](i)}\\\\)来替换\\\\(z^{\\[l](i)}\\\\) 就可以，其中\\\\( \\gamma\\\\)和\\\\(\\beta\\\\)是两个parameter，可以通过gradient descent来更新，这两个parameter存在的意义，就是可以调整normalization映射的Gaussian distribution，而不是统统映射到Normal distribution，值得注意的是，\\\\(\\epsilon\\\\)是一个很小的数，用来避免分母分0的情况。\n\n如果\\\\(\\gamma = \\sqrt{ \\sigma^2 + \\epsilon}\\\\)且\\\\( \\beta = \\mu\\\\)的话，那么其实\\\\(z^{N(i)}=z^(i)\\\\)的，大家可以算算，这种情况下，就是相当于没做normalization.\n### Batch norm on neural networks\n对于neural networks，输入\\\\(X\\\\)通过parameter\\\\(w^{[1]}\\\\)和\\\\(b^{[1]}\\\\)得到\\\\(z^{[1]}\\\\)，通过\\\\(\\beta\\\\)和\\\\(\\gamma\\\\)获得\\\\(z^{N[1]}\\\\)，经过active function后获得\\\\(a^{[1]}\\\\)，通过\\\\(w^{[2]}\\\\)和\\\\(b^{[2]}\\\\)获得\\\\(z^{[2]}\\\\)，如此下去，一直到最后的输出层，完成forward propagation.\n\n在整个过程中，一共有四个parameters，分别是\\\\(w^{[l]}\\\\)，\\\\(b^{[l]}\\\\)，\\\\( \\beta^{[l]}\\\\)，\\\\( \\gamma^{[l]}\\\\)，我们都知道：\n$$z^{[l]}=w^{[l]}a^{[l-1]}+b^{[l]}$$\n但是，我们在做batch normal的时候，首先会把\\\\(z^{[l]}\\\\)映射到期望为1方差为0的Gaussian distribution上，这就意味着\\\\(b^{[l]}\\\\)是可以忽略掉的，因为即使保留，在batch normal的时候也会被减去，因此，我们的parameter只有三个，即：\\\\(w^{[l]}\\\\)，\\\\( \\beta^{[l]}\\\\)，\\\\( \\gamma^{[l]}\\\\)\n\n在backforward的时候，我们和普通的neural networks一样，只是可以不用再去计算\\\\(db\\\\)\n### Solve covariate shift\n什么是covariate shift？简单的理解，就是模型需要随着样本的变化而变化，Ng举的例子就很直观，在猫脸试验中，假设training set里都是黑猫，这样获得的模型，对于花猫识别就是不适用的，这就叫covariate shift. 其实，batch norm可以改善neural networks效果的原因，就可以理解为solve covariate shift的过程。\n\nOK，我们来详细看看原因，假设我们有一个如图的neural networks：\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/8/8-1.png)\n在标示出的位置，有parameter\\\\(w^{[3]}\\\\)和\\\\(b^{[3]}\\\\)，如果我们盖住前面的部分，那么我们将获得如图的neural networks\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/8/8-2.png)\n对于neural networks来说，相当于获得了黑箱输出的\\\\(a^{[2]}\\\\)，而\\\\(a^{[2]}\\\\)的值其实并不是是固定的，每一次iteration后都有不一样的\\\\(a^{[2]}\\\\)，这就产生了covariate shift问题。\n\n但是，batch norm可以将\\\\(a^{[2]}\\\\)的期望和方差限制到\\\\(\\beta\\\\)和\\\\(gamma\\\\)控制的范围内，以此**极大限度**的缓解了covariate shift现象。\n\n另外，batch norm还可以有一些regularization的作用，由于每次mini-batch gradient descent中batch norm作用的sample不一样，类似于dropout的效果，会给对应layer加入一些噪声，以此产生一些regularization的效果。但是，我们一般不会把batch norm列入regularization范畴内。\n\n## Reference\n* [Deep learning-Coursera Andrew Ng](https://www.coursera.org/specializations/deep-learning)\n* [Deep learning-网易云课堂 Andrew Ng](https://mooc.study.163.com/course/deeplearning_ai-2001281003#/info)\n","source":"_posts/course-deep-learning-course2-week3.md","raw":"---\ntitle: Learning Notes-Deep Learning, course2, week3\ndate: 2017-09-30 15:44:25\ntags: \n\t- hyperparameter\n\t- batch norm\n\t- covariate shift\ncategories: learning notes\n---\n不知不觉来到第三周的课程了，大家加油！这周的主要内容是hyperparameter selection和batch normal的问题，我们一起来看看这一周的内容！\n<!--more-->\n## Hyperparameter selection\nHyperparameter selection在machine learning中是一个非常重要的优化过程，例如gradient descent中的learning rate \\\\(\\alpha\\\\)就是关乎算法结果的重要hyperparameter，那么我们应该怎么去选择呢？Ng给出了两个建议：\n* 构建多个hyperparameter交叉，随机选择大小，选择效果较好的范围，继续随机选择hyperparameter大小，观察结果。\n* 选择参数的时候分段选择，并且使用log分段，例如在0.0001到1之间选择，将数轴分成0.0001，0.001，0.01，0.1和1，这样选择出的结果更好\n\n对于整体模型的hyperparameter selection，Ng也出了建议，那就是babysitting和parallel方法，一种是对一个模型多次调整，一种是同时启动多个不同hyperparameter的模型，最后取效果最好的。\n\n两种方法殊途同归，可以根据自己的具体情况做出选择。\n## Batch norm\n### Normalization\n相信大家都听说过大名鼎鼎的normalization吧，这是一种很棒的数据预处理的方法，它可以很好的提升数据处理（例如gradient descent）的速度和效果，在引入batch norm之前，我也稍微提一下normalization，下面上公式：\n\n对于输入数据来说，我们可以按以下方法来normalize\n$$ \\mu = \\frac{1}{m} \\sum_i x^{(i)}$$\n$$X = X- \\mu$$\n$$ \\sigma^2 = \\frac{1}{m} \\sum_i (x^{(i)})^2 $$\n$$ X = X/ \\sigma ^2$$\n这样，我们就把输入数据转化成了符合期望为0，方差为1的Gaussian distribution的数据。\n\n当然，这只是normaliztion中的一种方法，也是被称作z-score方法。\n### Batch norm\n上面说的normalization方法可以推广到neural networks中，对于nerual networks中的某一个layer来说，可以看做是一个孤立的计算过程，在这个过程中，我们可以引入normalization，对于\\\\(z^{(i)}\\\\)来说：\n$$ \\mu = \\frac{1}{m} \\sum_i z^{(i)}$$\n$$ \\sigma ^2= \\frac{1}{m} \\sum_i (z^{(i)}- \\mu)^2 $$\n$$z^{(i)}_{norm}= \\frac{z^{(i)}- \\mu}{ \\sqrt{ \\sigma^2 + \\epsilon}}$$\n$$z^{N(i)}= \\gamma z^{(i)}_{norm} + \\beta$$\n然后我们用最终的\\\\(z^{N\\[l](i)}\\\\)来替换\\\\(z^{\\[l](i)}\\\\) 就可以，其中\\\\( \\gamma\\\\)和\\\\(\\beta\\\\)是两个parameter，可以通过gradient descent来更新，这两个parameter存在的意义，就是可以调整normalization映射的Gaussian distribution，而不是统统映射到Normal distribution，值得注意的是，\\\\(\\epsilon\\\\)是一个很小的数，用来避免分母分0的情况。\n\n如果\\\\(\\gamma = \\sqrt{ \\sigma^2 + \\epsilon}\\\\)且\\\\( \\beta = \\mu\\\\)的话，那么其实\\\\(z^{N(i)}=z^(i)\\\\)的，大家可以算算，这种情况下，就是相当于没做normalization.\n### Batch norm on neural networks\n对于neural networks，输入\\\\(X\\\\)通过parameter\\\\(w^{[1]}\\\\)和\\\\(b^{[1]}\\\\)得到\\\\(z^{[1]}\\\\)，通过\\\\(\\beta\\\\)和\\\\(\\gamma\\\\)获得\\\\(z^{N[1]}\\\\)，经过active function后获得\\\\(a^{[1]}\\\\)，通过\\\\(w^{[2]}\\\\)和\\\\(b^{[2]}\\\\)获得\\\\(z^{[2]}\\\\)，如此下去，一直到最后的输出层，完成forward propagation.\n\n在整个过程中，一共有四个parameters，分别是\\\\(w^{[l]}\\\\)，\\\\(b^{[l]}\\\\)，\\\\( \\beta^{[l]}\\\\)，\\\\( \\gamma^{[l]}\\\\)，我们都知道：\n$$z^{[l]}=w^{[l]}a^{[l-1]}+b^{[l]}$$\n但是，我们在做batch normal的时候，首先会把\\\\(z^{[l]}\\\\)映射到期望为1方差为0的Gaussian distribution上，这就意味着\\\\(b^{[l]}\\\\)是可以忽略掉的，因为即使保留，在batch normal的时候也会被减去，因此，我们的parameter只有三个，即：\\\\(w^{[l]}\\\\)，\\\\( \\beta^{[l]}\\\\)，\\\\( \\gamma^{[l]}\\\\)\n\n在backforward的时候，我们和普通的neural networks一样，只是可以不用再去计算\\\\(db\\\\)\n### Solve covariate shift\n什么是covariate shift？简单的理解，就是模型需要随着样本的变化而变化，Ng举的例子就很直观，在猫脸试验中，假设training set里都是黑猫，这样获得的模型，对于花猫识别就是不适用的，这就叫covariate shift. 其实，batch norm可以改善neural networks效果的原因，就可以理解为solve covariate shift的过程。\n\nOK，我们来详细看看原因，假设我们有一个如图的neural networks：\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/8/8-1.png)\n在标示出的位置，有parameter\\\\(w^{[3]}\\\\)和\\\\(b^{[3]}\\\\)，如果我们盖住前面的部分，那么我们将获得如图的neural networks\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/8/8-2.png)\n对于neural networks来说，相当于获得了黑箱输出的\\\\(a^{[2]}\\\\)，而\\\\(a^{[2]}\\\\)的值其实并不是是固定的，每一次iteration后都有不一样的\\\\(a^{[2]}\\\\)，这就产生了covariate shift问题。\n\n但是，batch norm可以将\\\\(a^{[2]}\\\\)的期望和方差限制到\\\\(\\beta\\\\)和\\\\(gamma\\\\)控制的范围内，以此**极大限度**的缓解了covariate shift现象。\n\n另外，batch norm还可以有一些regularization的作用，由于每次mini-batch gradient descent中batch norm作用的sample不一样，类似于dropout的效果，会给对应layer加入一些噪声，以此产生一些regularization的效果。但是，我们一般不会把batch norm列入regularization范畴内。\n\n## Reference\n* [Deep learning-Coursera Andrew Ng](https://www.coursera.org/specializations/deep-learning)\n* [Deep learning-网易云课堂 Andrew Ng](https://mooc.study.163.com/course/deeplearning_ai-2001281003#/info)\n","slug":"course-deep-learning-course2-week3","published":1,"updated":"2020-05-10T06:50:12.528Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cka0wnkvx0001qxotuhlk6rrc","content":"<p>不知不觉来到第三周的课程了，大家加油！这周的主要内容是hyperparameter selection和batch normal的问题，我们一起来看看这一周的内容！<br><a id=\"more\"></a></p>\n<h2 id=\"Hyperparameter-selection\"><a href=\"#Hyperparameter-selection\" class=\"headerlink\" title=\"Hyperparameter selection\"></a>Hyperparameter selection</h2><p>Hyperparameter selection在machine learning中是一个非常重要的优化过程，例如gradient descent中的learning rate \\(\\alpha\\)就是关乎算法结果的重要hyperparameter，那么我们应该怎么去选择呢？Ng给出了两个建议：</p>\n<ul>\n<li>构建多个hyperparameter交叉，随机选择大小，选择效果较好的范围，继续随机选择hyperparameter大小，观察结果。</li>\n<li>选择参数的时候分段选择，并且使用log分段，例如在0.0001到1之间选择，将数轴分成0.0001，0.001，0.01，0.1和1，这样选择出的结果更好</li>\n</ul>\n<p>对于整体模型的hyperparameter selection，Ng也出了建议，那就是babysitting和parallel方法，一种是对一个模型多次调整，一种是同时启动多个不同hyperparameter的模型，最后取效果最好的。</p>\n<p>两种方法殊途同归，可以根据自己的具体情况做出选择。</p>\n<h2 id=\"Batch-norm\"><a href=\"#Batch-norm\" class=\"headerlink\" title=\"Batch norm\"></a>Batch norm</h2><h3 id=\"Normalization\"><a href=\"#Normalization\" class=\"headerlink\" title=\"Normalization\"></a>Normalization</h3><p>相信大家都听说过大名鼎鼎的normalization吧，这是一种很棒的数据预处理的方法，它可以很好的提升数据处理（例如gradient descent）的速度和效果，在引入batch norm之前，我也稍微提一下normalization，下面上公式：</p>\n<p>对于输入数据来说，我们可以按以下方法来normalize<br>$$ \\mu = \\frac{1}{m} \\sum_i x^{(i)}$$<br>$$X = X- \\mu$$<br>$$ \\sigma^2 = \\frac{1}{m} \\sum_i (x^{(i)})^2 $$<br>$$ X = X/ \\sigma ^2$$<br>这样，我们就把输入数据转化成了符合期望为0，方差为1的Gaussian distribution的数据。</p>\n<p>当然，这只是normaliztion中的一种方法，也是被称作z-score方法。</p>\n<h3 id=\"Batch-norm-1\"><a href=\"#Batch-norm-1\" class=\"headerlink\" title=\"Batch norm\"></a>Batch norm</h3><p>上面说的normalization方法可以推广到neural networks中，对于nerual networks中的某一个layer来说，可以看做是一个孤立的计算过程，在这个过程中，我们可以引入normalization，对于\\(z^{(i)}\\)来说：<br>$$ \\mu = \\frac{1}{m} \\sum_i z^{(i)}$$<br>$$ \\sigma ^2= \\frac{1}{m} \\sum_i (z^{(i)}- \\mu)^2 $$<br>$$z^{(i)}<em>{norm}= \\frac{z^{(i)}- \\mu}{ \\sqrt{ \\sigma^2 + \\epsilon}}$$<br>$$z^{N(i)}= \\gamma z^{(i)}</em>{norm} + \\beta$$<br>然后我们用最终的\\(z^{N[l](i)}\\)来替换\\(z^{[l](i)}\\) 就可以，其中\\( \\gamma\\)和\\(\\beta\\)是两个parameter，可以通过gradient descent来更新，这两个parameter存在的意义，就是可以调整normalization映射的Gaussian distribution，而不是统统映射到Normal distribution，值得注意的是，\\(\\epsilon\\)是一个很小的数，用来避免分母分0的情况。</p>\n<p>如果\\(\\gamma = \\sqrt{ \\sigma^2 + \\epsilon}\\)且\\( \\beta = \\mu\\)的话，那么其实\\(z^{N(i)}=z^(i)\\)的，大家可以算算，这种情况下，就是相当于没做normalization.</p>\n<h3 id=\"Batch-norm-on-neural-networks\"><a href=\"#Batch-norm-on-neural-networks\" class=\"headerlink\" title=\"Batch norm on neural networks\"></a>Batch norm on neural networks</h3><p>对于neural networks，输入\\(X\\)通过parameter\\(w^{[1]}\\)和\\(b^{[1]}\\)得到\\(z^{[1]}\\)，通过\\(\\beta\\)和\\(\\gamma\\)获得\\(z^{N[1]}\\)，经过active function后获得\\(a^{[1]}\\)，通过\\(w^{[2]}\\)和\\(b^{[2]}\\)获得\\(z^{[2]}\\)，如此下去，一直到最后的输出层，完成forward propagation.</p>\n<p>在整个过程中，一共有四个parameters，分别是\\(w^{[l]}\\)，\\(b^{[l]}\\)，\\( \\beta^{[l]}\\)，\\( \\gamma^{[l]}\\)，我们都知道：<br>$$z^{[l]}=w^{[l]}a^{[l-1]}+b^{[l]}$$<br>但是，我们在做batch normal的时候，首先会把\\(z^{[l]}\\)映射到期望为1方差为0的Gaussian distribution上，这就意味着\\(b^{[l]}\\)是可以忽略掉的，因为即使保留，在batch normal的时候也会被减去，因此，我们的parameter只有三个，即：\\(w^{[l]}\\)，\\( \\beta^{[l]}\\)，\\( \\gamma^{[l]}\\)</p>\n<p>在backforward的时候，我们和普通的neural networks一样，只是可以不用再去计算\\(db\\)</p>\n<h3 id=\"Solve-covariate-shift\"><a href=\"#Solve-covariate-shift\" class=\"headerlink\" title=\"Solve covariate shift\"></a>Solve covariate shift</h3><p>什么是covariate shift？简单的理解，就是模型需要随着样本的变化而变化，Ng举的例子就很直观，在猫脸试验中，假设training set里都是黑猫，这样获得的模型，对于花猫识别就是不适用的，这就叫covariate shift. 其实，batch norm可以改善neural networks效果的原因，就可以理解为solve covariate shift的过程。</p>\n<p>OK，我们来详细看看原因，假设我们有一个如图的neural networks：<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/8/8-1.png\" alt=\"\"><br>在标示出的位置，有parameter\\(w^{[3]}\\)和\\(b^{[3]}\\)，如果我们盖住前面的部分，那么我们将获得如图的neural networks<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/8/8-2.png\" alt=\"\"><br>对于neural networks来说，相当于获得了黑箱输出的\\(a^{[2]}\\)，而\\(a^{[2]}\\)的值其实并不是是固定的，每一次iteration后都有不一样的\\(a^{[2]}\\)，这就产生了covariate shift问题。</p>\n<p>但是，batch norm可以将\\(a^{[2]}\\)的期望和方差限制到\\(\\beta\\)和\\(gamma\\)控制的范围内，以此<strong>极大限度</strong>的缓解了covariate shift现象。</p>\n<p>另外，batch norm还可以有一些regularization的作用，由于每次mini-batch gradient descent中batch norm作用的sample不一样，类似于dropout的效果，会给对应layer加入一些噪声，以此产生一些regularization的效果。但是，我们一般不会把batch norm列入regularization范畴内。</p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ul>\n<li><a href=\"https://www.coursera.org/specializations/deep-learning\" target=\"_blank\" rel=\"noopener\">Deep learning-Coursera Andrew Ng</a></li>\n<li><a href=\"https://mooc.study.163.com/course/deeplearning_ai-2001281003#/info\" target=\"_blank\" rel=\"noopener\">Deep learning-网易云课堂 Andrew Ng</a></li>\n</ul>\n","site":{"data":{}},"_categories":[{"name":"learning notes","path":"categories/learning-notes/"}],"_tags":[{"name":"hyperparameter","path":"tags/hyperparameter/"},{"name":"batch norm","path":"tags/batch-norm/"},{"name":"covariate shift","path":"tags/covariate-shift/"}],"excerpt":"<p>不知不觉来到第三周的课程了，大家加油！这周的主要内容是hyperparameter selection和batch normal的问题，我们一起来看看这一周的内容！<br></p>","more":"</p>\n<h2 id=\"Hyperparameter-selection\"><a href=\"#Hyperparameter-selection\" class=\"headerlink\" title=\"Hyperparameter selection\"></a>Hyperparameter selection</h2><p>Hyperparameter selection在machine learning中是一个非常重要的优化过程，例如gradient descent中的learning rate \\(\\alpha\\)就是关乎算法结果的重要hyperparameter，那么我们应该怎么去选择呢？Ng给出了两个建议：</p>\n<ul>\n<li>构建多个hyperparameter交叉，随机选择大小，选择效果较好的范围，继续随机选择hyperparameter大小，观察结果。</li>\n<li>选择参数的时候分段选择，并且使用log分段，例如在0.0001到1之间选择，将数轴分成0.0001，0.001，0.01，0.1和1，这样选择出的结果更好</li>\n</ul>\n<p>对于整体模型的hyperparameter selection，Ng也出了建议，那就是babysitting和parallel方法，一种是对一个模型多次调整，一种是同时启动多个不同hyperparameter的模型，最后取效果最好的。</p>\n<p>两种方法殊途同归，可以根据自己的具体情况做出选择。</p>\n<h2 id=\"Batch-norm\"><a href=\"#Batch-norm\" class=\"headerlink\" title=\"Batch norm\"></a>Batch norm</h2><h3 id=\"Normalization\"><a href=\"#Normalization\" class=\"headerlink\" title=\"Normalization\"></a>Normalization</h3><p>相信大家都听说过大名鼎鼎的normalization吧，这是一种很棒的数据预处理的方法，它可以很好的提升数据处理（例如gradient descent）的速度和效果，在引入batch norm之前，我也稍微提一下normalization，下面上公式：</p>\n<p>对于输入数据来说，我们可以按以下方法来normalize<br>$$ \\mu = \\frac{1}{m} \\sum_i x^{(i)}$$<br>$$X = X- \\mu$$<br>$$ \\sigma^2 = \\frac{1}{m} \\sum_i (x^{(i)})^2 $$<br>$$ X = X/ \\sigma ^2$$<br>这样，我们就把输入数据转化成了符合期望为0，方差为1的Gaussian distribution的数据。</p>\n<p>当然，这只是normaliztion中的一种方法，也是被称作z-score方法。</p>\n<h3 id=\"Batch-norm-1\"><a href=\"#Batch-norm-1\" class=\"headerlink\" title=\"Batch norm\"></a>Batch norm</h3><p>上面说的normalization方法可以推广到neural networks中，对于nerual networks中的某一个layer来说，可以看做是一个孤立的计算过程，在这个过程中，我们可以引入normalization，对于\\(z^{(i)}\\)来说：<br>$$ \\mu = \\frac{1}{m} \\sum_i z^{(i)}$$<br>$$ \\sigma ^2= \\frac{1}{m} \\sum_i (z^{(i)}- \\mu)^2 $$<br>$$z^{(i)}<em>{norm}= \\frac{z^{(i)}- \\mu}{ \\sqrt{ \\sigma^2 + \\epsilon}}$$<br>$$z^{N(i)}= \\gamma z^{(i)}</em>{norm} + \\beta$$<br>然后我们用最终的\\(z^{N[l](i)}\\)来替换\\(z^{[l](i)}\\) 就可以，其中\\( \\gamma\\)和\\(\\beta\\)是两个parameter，可以通过gradient descent来更新，这两个parameter存在的意义，就是可以调整normalization映射的Gaussian distribution，而不是统统映射到Normal distribution，值得注意的是，\\(\\epsilon\\)是一个很小的数，用来避免分母分0的情况。</p>\n<p>如果\\(\\gamma = \\sqrt{ \\sigma^2 + \\epsilon}\\)且\\( \\beta = \\mu\\)的话，那么其实\\(z^{N(i)}=z^(i)\\)的，大家可以算算，这种情况下，就是相当于没做normalization.</p>\n<h3 id=\"Batch-norm-on-neural-networks\"><a href=\"#Batch-norm-on-neural-networks\" class=\"headerlink\" title=\"Batch norm on neural networks\"></a>Batch norm on neural networks</h3><p>对于neural networks，输入\\(X\\)通过parameter\\(w^{[1]}\\)和\\(b^{[1]}\\)得到\\(z^{[1]}\\)，通过\\(\\beta\\)和\\(\\gamma\\)获得\\(z^{N[1]}\\)，经过active function后获得\\(a^{[1]}\\)，通过\\(w^{[2]}\\)和\\(b^{[2]}\\)获得\\(z^{[2]}\\)，如此下去，一直到最后的输出层，完成forward propagation.</p>\n<p>在整个过程中，一共有四个parameters，分别是\\(w^{[l]}\\)，\\(b^{[l]}\\)，\\( \\beta^{[l]}\\)，\\( \\gamma^{[l]}\\)，我们都知道：<br>$$z^{[l]}=w^{[l]}a^{[l-1]}+b^{[l]}$$<br>但是，我们在做batch normal的时候，首先会把\\(z^{[l]}\\)映射到期望为1方差为0的Gaussian distribution上，这就意味着\\(b^{[l]}\\)是可以忽略掉的，因为即使保留，在batch normal的时候也会被减去，因此，我们的parameter只有三个，即：\\(w^{[l]}\\)，\\( \\beta^{[l]}\\)，\\( \\gamma^{[l]}\\)</p>\n<p>在backforward的时候，我们和普通的neural networks一样，只是可以不用再去计算\\(db\\)</p>\n<h3 id=\"Solve-covariate-shift\"><a href=\"#Solve-covariate-shift\" class=\"headerlink\" title=\"Solve covariate shift\"></a>Solve covariate shift</h3><p>什么是covariate shift？简单的理解，就是模型需要随着样本的变化而变化，Ng举的例子就很直观，在猫脸试验中，假设training set里都是黑猫，这样获得的模型，对于花猫识别就是不适用的，这就叫covariate shift. 其实，batch norm可以改善neural networks效果的原因，就可以理解为solve covariate shift的过程。</p>\n<p>OK，我们来详细看看原因，假设我们有一个如图的neural networks：<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/8/8-1.png\" alt=\"\"><br>在标示出的位置，有parameter\\(w^{[3]}\\)和\\(b^{[3]}\\)，如果我们盖住前面的部分，那么我们将获得如图的neural networks<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/8/8-2.png\" alt=\"\"><br>对于neural networks来说，相当于获得了黑箱输出的\\(a^{[2]}\\)，而\\(a^{[2]}\\)的值其实并不是是固定的，每一次iteration后都有不一样的\\(a^{[2]}\\)，这就产生了covariate shift问题。</p>\n<p>但是，batch norm可以将\\(a^{[2]}\\)的期望和方差限制到\\(\\beta\\)和\\(gamma\\)控制的范围内，以此<strong>极大限度</strong>的缓解了covariate shift现象。</p>\n<p>另外，batch norm还可以有一些regularization的作用，由于每次mini-batch gradient descent中batch norm作用的sample不一样，类似于dropout的效果，会给对应layer加入一些噪声，以此产生一些regularization的效果。但是，我们一般不会把batch norm列入regularization范畴内。</p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ul>\n<li><a href=\"https://www.coursera.org/specializations/deep-learning\" target=\"_blank\" rel=\"noopener\">Deep learning-Coursera Andrew Ng</a></li>\n<li><a href=\"https://mooc.study.163.com/course/deeplearning_ai-2001281003#/info\" target=\"_blank\" rel=\"noopener\">Deep learning-网易云课堂 Andrew Ng</a></li>\n</ul>"},{"title":"Learning Notes-Deep Learning, course2, week2","date":"2017-09-27T13:38:35.000Z","_content":"大家好，课程来到了第二周，这周主要是一些优化方法，使得整个neural networks可以更快更好的工作，我们一起来recap一下。\n<!--more-->\n## Mini-batch gradient descent\n这一节我就不打算写了，比较基础，其实mini-batch gradient descent是batch gradient和stochastic gradient descent综合后的结果，一方面解决了batch gradient计算量大，容易converge到local minimum的问题，也解决了stochastic gradient descent epoch太多的弊端，是现在gradient descent使用最广泛的方法。\n\n对于mini-batch gradient descent，Ng建议batch大小取决于数据量多少，在小数据量上完全没有必要做mini-batch，直接使用batch gradient descent就可以，对于大数据量的情况，最好选择2的乘方，如64,128,256,512来作为batch size. 当然，batch size也要满足CPU和GPU的内存大小。\n## Exponentially weighted averages\n### Exponentially weighted averages\nExponentially weighted averages，也被称为moving averages，是一种综合历史数据的加权平均方法，课程中Ng用了伦敦一年的气温变化曲线作为例子，对于固有变量\\\\(\\theta\\\\)来说，我们要求的平均值\\\\(v\\\\)应该是\n$$v_0 = 0$$ \n$$v_1 = \\beta v_0 + (1- \\beta) \\theta_1 $$\n$$v_2 = \\beta v_1 + (1- \\beta) \\theta_2$$\n$$v_3= \\beta v_2 + (1- \\beta) \\theta_3$$\n$$\\cdots$$\n$$v_t= \\beta v_{t-1} + (1- \\beta) \\theta_t$$\n其中\\\\(\\beta\\\\)是一个因子，它决定了moving averages大约向前平均了\\\\(\\frac{1}{1- \\beta}\\\\)个\\\\(\\theta\\\\)值，例如\\\\(\\beta = 0.9\\\\)，那么大约向前平均了10个值，并且是向前按指数衰减加权获得的平均值。\n### Bias correction\n在exponentially weighted averages中，有一个问题很尖锐，那就是在最初的求解过程中，由于\\\\(v_0=0\\\\)，导致前面的数字结果距离正确结果较小，如下图所示，紫色曲线是获得的结果，而在起始位置的值明显是偏小的。\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/7/7-1.png)\n此时，我们引入bias correction，原理也很简单，就是此处我们不使用\\\\(v_t\\\\)作为最终的结果，而是使用\\\\( \\frac{v_t}{1- \\beta^{t}}\\\\)作为最后的结果，通过bias correction，我们会获得绿色的曲线。\n\n我们可以看到，起始绿色钱和紫色曲线在最后基本没有差别，几乎重合，但是在曲线开始的时候，绿色曲线比紫色曲线更加逼近真实情况，因此，Ng给我们以下建议：\n* 当我们不关注moving averages initial value大小的时候，我们可以不使用bias correction\n* Bias correction对于initial value效果更好\n\n## Gradient descent optimization\n### momentum\n在gradient descent中，我们经常会遇到一种情况，如图所示：\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/7/7-2.png)\n在水平方向上，我们希望更快的下降，而在垂直方向上我们希望更小的下降速率，以避免过多的iteration，针对这种情况，我们讲moving averages的思想带入进来，这就是momentum方法。\n\n在momentum中，我们的每次迭代中：\n$$v_{dW}= \\beta v_{dW}+(1- \\beta)dW$$\n$$v_{db}= \\beta v_{db}+(1- \\beta)db$$\n$$W:=W- \\alpha v_{dW}$$\n$$b:=b - \\alpha v_{db}$$\n在很多的文献中，上面的\\\\(1- \\beta\\\\)项被省略掉了，这样做只是将等式等量做了缩放，并不影响实际的效果，Ng表示，两种方式的momentum几乎没有差别，大家可以放心使用。\n\n实际上，momentum可以理解为将数次之前迭代过程中的gradient变化也带入到了这次迭代过程中，我个人认为就是带入了一种gradient变化的趋势，这样可以更好的控制gradient descent的方向和大小。例如上图中的纵向方向中，加入momentum后可以轻松的将纵向梯度正负抵消到近似0，这样就可以减少在gradient descent在纵向的反复迭代，因为，那既是无用功，也是我们不愿意看到的情况。\n\n另外，Ng给我们了一个\\\\(\\beta\\\\)的理想取值，既0.9，这个值大约取了前10次迭代结果的moving averages。另外，Ng表示，在momen中很少使用bias correction，因为10次迭代之后，这种问题很快就会消除，而一般的gradient descent，iteration次数远远大于10次。\n### RMSprop\n除了momentum，还有一些类似的方法，例如大名鼎鼎的RMSprop(root means square prop)，在这个方法中，主要体现了square的应用，我们来看看，在每次的迭代中：\n$$S_{dW}= \\beta S_{dW} + (1- \\beta)(dW)^2$$\n$$S_{db}= \\beta S_{db}+(1- \\beta)(db)^2$$\n$$W:=W- \\alpha \\frac{dW}{\\sqrt {S_{dW}}+ \\epsilon}$$\n$$b:=b- \\alpha \\frac{db}{\\sqrt {S_{db}} + \\epsilon}$$\n\\\\(\\epsilon\\\\)是为了防止分母为0的一个item，取值建议为\\\\(10^{-8}\\\\)\n\n假设在gradient descent中，\\\\(W\\\\)下降速率太低，也就是\\\\(dW\\\\)太小，那么在RMSprop的迭代中，\\\\(dW\\\\)将会除以一个很小的值\\\\( \\sqrt{S_{dW}}\\\\)，也就是\\\\(W\\\\)将会减去一个较大的值，反之亦然。\n\n通过这种方式，我们缓解了上图所示的情况，改善了gradient descent的合理性，同时可以使用更大的\\\\(\\alpha\\\\)去实现更快的gradient descent.\n### Adam\n我们看到了momentum和RMSprop优化方法的厉害之处，现在Adam方法横空出世，他融合了momentum和RMSprop，他是如何融合的呢，我们把momentum中的\\\\( \\beta\\\\)命名为\\\\( \\beta\\_1\\\\)，把RMSprop中的\\\\( \\beta\\\\)命名为\\\\( \\beta\\_2\\\\)，在第\\\\(t\\\\)次迭代中：\n$$v_{dW}= \\beta_1 v_{dW}+(1- \\beta_1)dW, \\qquad v_{db}= \\beta_1 v_{db}+(1- \\beta_1)db$$\n$$s_{dW}= \\beta_2 s_{dW}+(1- \\beta_2)(dW)^2, \\qquad s_{db}= \\beta_2 s_{db}+(1- \\beta_2)(db)^2$$\n加上bias correction后\n$$v^{corrected}_{dW}= \\frac{v_{dW}}{1- \\beta^t_1}, \\qquad v^{corrected}_{db}= \\frac{v_{db}}{1- \\beta^t_1}$$\n$$s^{corrected}_{dW}= \\frac{s_{dW}}{1- \\beta^t_2}, \\qquad s^{corrected}_{db}= \\frac{s_{db}}{1- \\beta^t_2}$$\n$$W:=W- \\alpha \\frac{v^{corrected}_{dW}}{ \\sqrt{s^{corrected}_{dW}}+ \\epsilon}$$\n$$W:=W- \\alpha \\frac{v^{corrected}_{db}}{ \\sqrt{s^{corrected}_{db}}+ \\epsilon}$$\n其中，\\\\(\\epsilon\\\\)是为了防止分母为0的一个item，取值建议为\\\\(10^{-8}\\\\)，\\\\(\\beta\\_1\\\\)建议取值0.9，\\\\(\\beta\\_2\\\\)建议取值0.999。\n## Learning rate decay\n在gradient descent中，随着迭代的深度，越来越靠近minimum，我们需要更小的learning rate，以避免越过minimum，常用的learning decay方法有：\n$$\\alpha = \\frac{1}{1+decayRate*epochNum} * \\alpha_0$$\n$$\\alpha = 0.95^{epochNum} * \\alpha_0$$\n$$\\alpha = \\frac{k}{\\sqrt{epochNum}}* \\alpha_0$$\n这些方法都可以让\\\\(\\alpha\\\\)随着迭代次数增加而慢慢变小，可以更好的逼近minimum.\n## Reference\n* [Deep learning-Coursera Andrew Ng](https://www.coursera.org/specializations/deep-learning)\n* [Deep learning-网易云课堂 Andrew Ng](https://mooc.study.163.com/course/deeplearning_ai-2001281003#/info)\n* [An overview of gradient descent optimization algorithms ](http://ruder.io/optimizing-gradient-descent/)\n","source":"_posts/course-deep-learning-course2-week2.md","raw":"---\ntitle: Learning Notes-Deep Learning, course2, week2\ndate: 2017-09-27 21:38:35\ntags:\n\t- gradient descent\n\t- moving averages\ncategories: learning notes\n---\n大家好，课程来到了第二周，这周主要是一些优化方法，使得整个neural networks可以更快更好的工作，我们一起来recap一下。\n<!--more-->\n## Mini-batch gradient descent\n这一节我就不打算写了，比较基础，其实mini-batch gradient descent是batch gradient和stochastic gradient descent综合后的结果，一方面解决了batch gradient计算量大，容易converge到local minimum的问题，也解决了stochastic gradient descent epoch太多的弊端，是现在gradient descent使用最广泛的方法。\n\n对于mini-batch gradient descent，Ng建议batch大小取决于数据量多少，在小数据量上完全没有必要做mini-batch，直接使用batch gradient descent就可以，对于大数据量的情况，最好选择2的乘方，如64,128,256,512来作为batch size. 当然，batch size也要满足CPU和GPU的内存大小。\n## Exponentially weighted averages\n### Exponentially weighted averages\nExponentially weighted averages，也被称为moving averages，是一种综合历史数据的加权平均方法，课程中Ng用了伦敦一年的气温变化曲线作为例子，对于固有变量\\\\(\\theta\\\\)来说，我们要求的平均值\\\\(v\\\\)应该是\n$$v_0 = 0$$ \n$$v_1 = \\beta v_0 + (1- \\beta) \\theta_1 $$\n$$v_2 = \\beta v_1 + (1- \\beta) \\theta_2$$\n$$v_3= \\beta v_2 + (1- \\beta) \\theta_3$$\n$$\\cdots$$\n$$v_t= \\beta v_{t-1} + (1- \\beta) \\theta_t$$\n其中\\\\(\\beta\\\\)是一个因子，它决定了moving averages大约向前平均了\\\\(\\frac{1}{1- \\beta}\\\\)个\\\\(\\theta\\\\)值，例如\\\\(\\beta = 0.9\\\\)，那么大约向前平均了10个值，并且是向前按指数衰减加权获得的平均值。\n### Bias correction\n在exponentially weighted averages中，有一个问题很尖锐，那就是在最初的求解过程中，由于\\\\(v_0=0\\\\)，导致前面的数字结果距离正确结果较小，如下图所示，紫色曲线是获得的结果，而在起始位置的值明显是偏小的。\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/7/7-1.png)\n此时，我们引入bias correction，原理也很简单，就是此处我们不使用\\\\(v_t\\\\)作为最终的结果，而是使用\\\\( \\frac{v_t}{1- \\beta^{t}}\\\\)作为最后的结果，通过bias correction，我们会获得绿色的曲线。\n\n我们可以看到，起始绿色钱和紫色曲线在最后基本没有差别，几乎重合，但是在曲线开始的时候，绿色曲线比紫色曲线更加逼近真实情况，因此，Ng给我们以下建议：\n* 当我们不关注moving averages initial value大小的时候，我们可以不使用bias correction\n* Bias correction对于initial value效果更好\n\n## Gradient descent optimization\n### momentum\n在gradient descent中，我们经常会遇到一种情况，如图所示：\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/7/7-2.png)\n在水平方向上，我们希望更快的下降，而在垂直方向上我们希望更小的下降速率，以避免过多的iteration，针对这种情况，我们讲moving averages的思想带入进来，这就是momentum方法。\n\n在momentum中，我们的每次迭代中：\n$$v_{dW}= \\beta v_{dW}+(1- \\beta)dW$$\n$$v_{db}= \\beta v_{db}+(1- \\beta)db$$\n$$W:=W- \\alpha v_{dW}$$\n$$b:=b - \\alpha v_{db}$$\n在很多的文献中，上面的\\\\(1- \\beta\\\\)项被省略掉了，这样做只是将等式等量做了缩放，并不影响实际的效果，Ng表示，两种方式的momentum几乎没有差别，大家可以放心使用。\n\n实际上，momentum可以理解为将数次之前迭代过程中的gradient变化也带入到了这次迭代过程中，我个人认为就是带入了一种gradient变化的趋势，这样可以更好的控制gradient descent的方向和大小。例如上图中的纵向方向中，加入momentum后可以轻松的将纵向梯度正负抵消到近似0，这样就可以减少在gradient descent在纵向的反复迭代，因为，那既是无用功，也是我们不愿意看到的情况。\n\n另外，Ng给我们了一个\\\\(\\beta\\\\)的理想取值，既0.9，这个值大约取了前10次迭代结果的moving averages。另外，Ng表示，在momen中很少使用bias correction，因为10次迭代之后，这种问题很快就会消除，而一般的gradient descent，iteration次数远远大于10次。\n### RMSprop\n除了momentum，还有一些类似的方法，例如大名鼎鼎的RMSprop(root means square prop)，在这个方法中，主要体现了square的应用，我们来看看，在每次的迭代中：\n$$S_{dW}= \\beta S_{dW} + (1- \\beta)(dW)^2$$\n$$S_{db}= \\beta S_{db}+(1- \\beta)(db)^2$$\n$$W:=W- \\alpha \\frac{dW}{\\sqrt {S_{dW}}+ \\epsilon}$$\n$$b:=b- \\alpha \\frac{db}{\\sqrt {S_{db}} + \\epsilon}$$\n\\\\(\\epsilon\\\\)是为了防止分母为0的一个item，取值建议为\\\\(10^{-8}\\\\)\n\n假设在gradient descent中，\\\\(W\\\\)下降速率太低，也就是\\\\(dW\\\\)太小，那么在RMSprop的迭代中，\\\\(dW\\\\)将会除以一个很小的值\\\\( \\sqrt{S_{dW}}\\\\)，也就是\\\\(W\\\\)将会减去一个较大的值，反之亦然。\n\n通过这种方式，我们缓解了上图所示的情况，改善了gradient descent的合理性，同时可以使用更大的\\\\(\\alpha\\\\)去实现更快的gradient descent.\n### Adam\n我们看到了momentum和RMSprop优化方法的厉害之处，现在Adam方法横空出世，他融合了momentum和RMSprop，他是如何融合的呢，我们把momentum中的\\\\( \\beta\\\\)命名为\\\\( \\beta\\_1\\\\)，把RMSprop中的\\\\( \\beta\\\\)命名为\\\\( \\beta\\_2\\\\)，在第\\\\(t\\\\)次迭代中：\n$$v_{dW}= \\beta_1 v_{dW}+(1- \\beta_1)dW, \\qquad v_{db}= \\beta_1 v_{db}+(1- \\beta_1)db$$\n$$s_{dW}= \\beta_2 s_{dW}+(1- \\beta_2)(dW)^2, \\qquad s_{db}= \\beta_2 s_{db}+(1- \\beta_2)(db)^2$$\n加上bias correction后\n$$v^{corrected}_{dW}= \\frac{v_{dW}}{1- \\beta^t_1}, \\qquad v^{corrected}_{db}= \\frac{v_{db}}{1- \\beta^t_1}$$\n$$s^{corrected}_{dW}= \\frac{s_{dW}}{1- \\beta^t_2}, \\qquad s^{corrected}_{db}= \\frac{s_{db}}{1- \\beta^t_2}$$\n$$W:=W- \\alpha \\frac{v^{corrected}_{dW}}{ \\sqrt{s^{corrected}_{dW}}+ \\epsilon}$$\n$$W:=W- \\alpha \\frac{v^{corrected}_{db}}{ \\sqrt{s^{corrected}_{db}}+ \\epsilon}$$\n其中，\\\\(\\epsilon\\\\)是为了防止分母为0的一个item，取值建议为\\\\(10^{-8}\\\\)，\\\\(\\beta\\_1\\\\)建议取值0.9，\\\\(\\beta\\_2\\\\)建议取值0.999。\n## Learning rate decay\n在gradient descent中，随着迭代的深度，越来越靠近minimum，我们需要更小的learning rate，以避免越过minimum，常用的learning decay方法有：\n$$\\alpha = \\frac{1}{1+decayRate*epochNum} * \\alpha_0$$\n$$\\alpha = 0.95^{epochNum} * \\alpha_0$$\n$$\\alpha = \\frac{k}{\\sqrt{epochNum}}* \\alpha_0$$\n这些方法都可以让\\\\(\\alpha\\\\)随着迭代次数增加而慢慢变小，可以更好的逼近minimum.\n## Reference\n* [Deep learning-Coursera Andrew Ng](https://www.coursera.org/specializations/deep-learning)\n* [Deep learning-网易云课堂 Andrew Ng](https://mooc.study.163.com/course/deeplearning_ai-2001281003#/info)\n* [An overview of gradient descent optimization algorithms ](http://ruder.io/optimizing-gradient-descent/)\n","slug":"course-deep-learning-course2-week2","published":1,"updated":"2020-05-10T06:50:12.528Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cka0wnkw00004qxotllo8f3to","content":"<p>大家好，课程来到了第二周，这周主要是一些优化方法，使得整个neural networks可以更快更好的工作，我们一起来recap一下。<br><a id=\"more\"></a></p>\n<h2 id=\"Mini-batch-gradient-descent\"><a href=\"#Mini-batch-gradient-descent\" class=\"headerlink\" title=\"Mini-batch gradient descent\"></a>Mini-batch gradient descent</h2><p>这一节我就不打算写了，比较基础，其实mini-batch gradient descent是batch gradient和stochastic gradient descent综合后的结果，一方面解决了batch gradient计算量大，容易converge到local minimum的问题，也解决了stochastic gradient descent epoch太多的弊端，是现在gradient descent使用最广泛的方法。</p>\n<p>对于mini-batch gradient descent，Ng建议batch大小取决于数据量多少，在小数据量上完全没有必要做mini-batch，直接使用batch gradient descent就可以，对于大数据量的情况，最好选择2的乘方，如64,128,256,512来作为batch size. 当然，batch size也要满足CPU和GPU的内存大小。</p>\n<h2 id=\"Exponentially-weighted-averages\"><a href=\"#Exponentially-weighted-averages\" class=\"headerlink\" title=\"Exponentially weighted averages\"></a>Exponentially weighted averages</h2><h3 id=\"Exponentially-weighted-averages-1\"><a href=\"#Exponentially-weighted-averages-1\" class=\"headerlink\" title=\"Exponentially weighted averages\"></a>Exponentially weighted averages</h3><p>Exponentially weighted averages，也被称为moving averages，是一种综合历史数据的加权平均方法，课程中Ng用了伦敦一年的气温变化曲线作为例子，对于固有变量\\(\\theta\\)来说，我们要求的平均值\\(v\\)应该是<br>$$v_0 = 0$$<br>$$v_1 = \\beta v_0 + (1- \\beta) \\theta_1 $$<br>$$v_2 = \\beta v_1 + (1- \\beta) \\theta_2$$<br>$$v_3= \\beta v_2 + (1- \\beta) \\theta_3$$<br>$$\\cdots$$<br>$$v_t= \\beta v_{t-1} + (1- \\beta) \\theta_t$$<br>其中\\(\\beta\\)是一个因子，它决定了moving averages大约向前平均了\\(\\frac{1}{1- \\beta}\\)个\\(\\theta\\)值，例如\\(\\beta = 0.9\\)，那么大约向前平均了10个值，并且是向前按指数衰减加权获得的平均值。</p>\n<h3 id=\"Bias-correction\"><a href=\"#Bias-correction\" class=\"headerlink\" title=\"Bias correction\"></a>Bias correction</h3><p>在exponentially weighted averages中，有一个问题很尖锐，那就是在最初的求解过程中，由于\\(v_0=0\\)，导致前面的数字结果距离正确结果较小，如下图所示，紫色曲线是获得的结果，而在起始位置的值明显是偏小的。<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/7/7-1.png\" alt=\"\"><br>此时，我们引入bias correction，原理也很简单，就是此处我们不使用\\(v_t\\)作为最终的结果，而是使用\\( \\frac{v_t}{1- \\beta^{t}}\\)作为最后的结果，通过bias correction，我们会获得绿色的曲线。</p>\n<p>我们可以看到，起始绿色钱和紫色曲线在最后基本没有差别，几乎重合，但是在曲线开始的时候，绿色曲线比紫色曲线更加逼近真实情况，因此，Ng给我们以下建议：</p>\n<ul>\n<li>当我们不关注moving averages initial value大小的时候，我们可以不使用bias correction</li>\n<li>Bias correction对于initial value效果更好</li>\n</ul>\n<h2 id=\"Gradient-descent-optimization\"><a href=\"#Gradient-descent-optimization\" class=\"headerlink\" title=\"Gradient descent optimization\"></a>Gradient descent optimization</h2><h3 id=\"momentum\"><a href=\"#momentum\" class=\"headerlink\" title=\"momentum\"></a>momentum</h3><p>在gradient descent中，我们经常会遇到一种情况，如图所示：<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/7/7-2.png\" alt=\"\"><br>在水平方向上，我们希望更快的下降，而在垂直方向上我们希望更小的下降速率，以避免过多的iteration，针对这种情况，我们讲moving averages的思想带入进来，这就是momentum方法。</p>\n<p>在momentum中，我们的每次迭代中：<br>$$v_{dW}= \\beta v_{dW}+(1- \\beta)dW$$<br>$$v_{db}= \\beta v_{db}+(1- \\beta)db$$<br>$$W:=W- \\alpha v_{dW}$$<br>$$b:=b - \\alpha v_{db}$$<br>在很多的文献中，上面的\\(1- \\beta\\)项被省略掉了，这样做只是将等式等量做了缩放，并不影响实际的效果，Ng表示，两种方式的momentum几乎没有差别，大家可以放心使用。</p>\n<p>实际上，momentum可以理解为将数次之前迭代过程中的gradient变化也带入到了这次迭代过程中，我个人认为就是带入了一种gradient变化的趋势，这样可以更好的控制gradient descent的方向和大小。例如上图中的纵向方向中，加入momentum后可以轻松的将纵向梯度正负抵消到近似0，这样就可以减少在gradient descent在纵向的反复迭代，因为，那既是无用功，也是我们不愿意看到的情况。</p>\n<p>另外，Ng给我们了一个\\(\\beta\\)的理想取值，既0.9，这个值大约取了前10次迭代结果的moving averages。另外，Ng表示，在momen中很少使用bias correction，因为10次迭代之后，这种问题很快就会消除，而一般的gradient descent，iteration次数远远大于10次。</p>\n<h3 id=\"RMSprop\"><a href=\"#RMSprop\" class=\"headerlink\" title=\"RMSprop\"></a>RMSprop</h3><p>除了momentum，还有一些类似的方法，例如大名鼎鼎的RMSprop(root means square prop)，在这个方法中，主要体现了square的应用，我们来看看，在每次的迭代中：<br>$$S_{dW}= \\beta S_{dW} + (1- \\beta)(dW)^2$$<br>$$S_{db}= \\beta S_{db}+(1- \\beta)(db)^2$$<br>$$W:=W- \\alpha \\frac{dW}{\\sqrt {S_{dW}}+ \\epsilon}$$<br>$$b:=b- \\alpha \\frac{db}{\\sqrt {S_{db}} + \\epsilon}$$<br>\\(\\epsilon\\)是为了防止分母为0的一个item，取值建议为\\(10^{-8}\\)</p>\n<p>假设在gradient descent中，\\(W\\)下降速率太低，也就是\\(dW\\)太小，那么在RMSprop的迭代中，\\(dW\\)将会除以一个很小的值\\( \\sqrt{S_{dW}}\\)，也就是\\(W\\)将会减去一个较大的值，反之亦然。</p>\n<p>通过这种方式，我们缓解了上图所示的情况，改善了gradient descent的合理性，同时可以使用更大的\\(\\alpha\\)去实现更快的gradient descent.</p>\n<h3 id=\"Adam\"><a href=\"#Adam\" class=\"headerlink\" title=\"Adam\"></a>Adam</h3><p>我们看到了momentum和RMSprop优化方法的厉害之处，现在Adam方法横空出世，他融合了momentum和RMSprop，他是如何融合的呢，我们把momentum中的\\( \\beta\\)命名为\\( \\beta_1\\)，把RMSprop中的\\( \\beta\\)命名为\\( \\beta_2\\)，在第\\(t\\)次迭代中：<br>$$v_{dW}= \\beta_1 v_{dW}+(1- \\beta_1)dW, \\qquad v_{db}= \\beta_1 v_{db}+(1- \\beta_1)db$$<br>$$s_{dW}= \\beta_2 s_{dW}+(1- \\beta_2)(dW)^2, \\qquad s_{db}= \\beta_2 s_{db}+(1- \\beta_2)(db)^2$$<br>加上bias correction后<br>$$v^{corrected}<em>{dW}= \\frac{v</em>{dW}}{1- \\beta^t_1}, \\qquad v^{corrected}<em>{db}= \\frac{v</em>{db}}{1- \\beta^t_1}$$<br>$$s^{corrected}<em>{dW}= \\frac{s</em>{dW}}{1- \\beta^t_2}, \\qquad s^{corrected}<em>{db}= \\frac{s</em>{db}}{1- \\beta^t_2}$$<br>$$W:=W- \\alpha \\frac{v^{corrected}<em>{dW}}{ \\sqrt{s^{corrected}</em>{dW}}+ \\epsilon}$$<br>$$W:=W- \\alpha \\frac{v^{corrected}<em>{db}}{ \\sqrt{s^{corrected}</em>{db}}+ \\epsilon}$$<br>其中，\\(\\epsilon\\)是为了防止分母为0的一个item，取值建议为\\(10^{-8}\\)，\\(\\beta_1\\)建议取值0.9，\\(\\beta_2\\)建议取值0.999。</p>\n<h2 id=\"Learning-rate-decay\"><a href=\"#Learning-rate-decay\" class=\"headerlink\" title=\"Learning rate decay\"></a>Learning rate decay</h2><p>在gradient descent中，随着迭代的深度，越来越靠近minimum，我们需要更小的learning rate，以避免越过minimum，常用的learning decay方法有：<br>$$\\alpha = \\frac{1}{1+decayRate<em>epochNum} </em> \\alpha_0$$<br>$$\\alpha = 0.95^{epochNum} <em> \\alpha_0$$<br>$$\\alpha = \\frac{k}{\\sqrt{epochNum}}</em> \\alpha_0$$<br>这些方法都可以让\\(\\alpha\\)随着迭代次数增加而慢慢变小，可以更好的逼近minimum.</p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ul>\n<li><a href=\"https://www.coursera.org/specializations/deep-learning\" target=\"_blank\" rel=\"noopener\">Deep learning-Coursera Andrew Ng</a></li>\n<li><a href=\"https://mooc.study.163.com/course/deeplearning_ai-2001281003#/info\" target=\"_blank\" rel=\"noopener\">Deep learning-网易云课堂 Andrew Ng</a></li>\n<li><a href=\"http://ruder.io/optimizing-gradient-descent/\" target=\"_blank\" rel=\"noopener\">An overview of gradient descent optimization algorithms </a></li>\n</ul>\n","site":{"data":{}},"_categories":[{"name":"learning notes","path":"categories/learning-notes/"}],"_tags":[{"name":"gradient descent","path":"tags/gradient-descent/"},{"name":"moving averages","path":"tags/moving-averages/"}],"excerpt":"<p>大家好，课程来到了第二周，这周主要是一些优化方法，使得整个neural networks可以更快更好的工作，我们一起来recap一下。<br></p>","more":"</p>\n<h2 id=\"Mini-batch-gradient-descent\"><a href=\"#Mini-batch-gradient-descent\" class=\"headerlink\" title=\"Mini-batch gradient descent\"></a>Mini-batch gradient descent</h2><p>这一节我就不打算写了，比较基础，其实mini-batch gradient descent是batch gradient和stochastic gradient descent综合后的结果，一方面解决了batch gradient计算量大，容易converge到local minimum的问题，也解决了stochastic gradient descent epoch太多的弊端，是现在gradient descent使用最广泛的方法。</p>\n<p>对于mini-batch gradient descent，Ng建议batch大小取决于数据量多少，在小数据量上完全没有必要做mini-batch，直接使用batch gradient descent就可以，对于大数据量的情况，最好选择2的乘方，如64,128,256,512来作为batch size. 当然，batch size也要满足CPU和GPU的内存大小。</p>\n<h2 id=\"Exponentially-weighted-averages\"><a href=\"#Exponentially-weighted-averages\" class=\"headerlink\" title=\"Exponentially weighted averages\"></a>Exponentially weighted averages</h2><h3 id=\"Exponentially-weighted-averages-1\"><a href=\"#Exponentially-weighted-averages-1\" class=\"headerlink\" title=\"Exponentially weighted averages\"></a>Exponentially weighted averages</h3><p>Exponentially weighted averages，也被称为moving averages，是一种综合历史数据的加权平均方法，课程中Ng用了伦敦一年的气温变化曲线作为例子，对于固有变量\\(\\theta\\)来说，我们要求的平均值\\(v\\)应该是<br>$$v_0 = 0$$<br>$$v_1 = \\beta v_0 + (1- \\beta) \\theta_1 $$<br>$$v_2 = \\beta v_1 + (1- \\beta) \\theta_2$$<br>$$v_3= \\beta v_2 + (1- \\beta) \\theta_3$$<br>$$\\cdots$$<br>$$v_t= \\beta v_{t-1} + (1- \\beta) \\theta_t$$<br>其中\\(\\beta\\)是一个因子，它决定了moving averages大约向前平均了\\(\\frac{1}{1- \\beta}\\)个\\(\\theta\\)值，例如\\(\\beta = 0.9\\)，那么大约向前平均了10个值，并且是向前按指数衰减加权获得的平均值。</p>\n<h3 id=\"Bias-correction\"><a href=\"#Bias-correction\" class=\"headerlink\" title=\"Bias correction\"></a>Bias correction</h3><p>在exponentially weighted averages中，有一个问题很尖锐，那就是在最初的求解过程中，由于\\(v_0=0\\)，导致前面的数字结果距离正确结果较小，如下图所示，紫色曲线是获得的结果，而在起始位置的值明显是偏小的。<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/7/7-1.png\" alt=\"\"><br>此时，我们引入bias correction，原理也很简单，就是此处我们不使用\\(v_t\\)作为最终的结果，而是使用\\( \\frac{v_t}{1- \\beta^{t}}\\)作为最后的结果，通过bias correction，我们会获得绿色的曲线。</p>\n<p>我们可以看到，起始绿色钱和紫色曲线在最后基本没有差别，几乎重合，但是在曲线开始的时候，绿色曲线比紫色曲线更加逼近真实情况，因此，Ng给我们以下建议：</p>\n<ul>\n<li>当我们不关注moving averages initial value大小的时候，我们可以不使用bias correction</li>\n<li>Bias correction对于initial value效果更好</li>\n</ul>\n<h2 id=\"Gradient-descent-optimization\"><a href=\"#Gradient-descent-optimization\" class=\"headerlink\" title=\"Gradient descent optimization\"></a>Gradient descent optimization</h2><h3 id=\"momentum\"><a href=\"#momentum\" class=\"headerlink\" title=\"momentum\"></a>momentum</h3><p>在gradient descent中，我们经常会遇到一种情况，如图所示：<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/7/7-2.png\" alt=\"\"><br>在水平方向上，我们希望更快的下降，而在垂直方向上我们希望更小的下降速率，以避免过多的iteration，针对这种情况，我们讲moving averages的思想带入进来，这就是momentum方法。</p>\n<p>在momentum中，我们的每次迭代中：<br>$$v_{dW}= \\beta v_{dW}+(1- \\beta)dW$$<br>$$v_{db}= \\beta v_{db}+(1- \\beta)db$$<br>$$W:=W- \\alpha v_{dW}$$<br>$$b:=b - \\alpha v_{db}$$<br>在很多的文献中，上面的\\(1- \\beta\\)项被省略掉了，这样做只是将等式等量做了缩放，并不影响实际的效果，Ng表示，两种方式的momentum几乎没有差别，大家可以放心使用。</p>\n<p>实际上，momentum可以理解为将数次之前迭代过程中的gradient变化也带入到了这次迭代过程中，我个人认为就是带入了一种gradient变化的趋势，这样可以更好的控制gradient descent的方向和大小。例如上图中的纵向方向中，加入momentum后可以轻松的将纵向梯度正负抵消到近似0，这样就可以减少在gradient descent在纵向的反复迭代，因为，那既是无用功，也是我们不愿意看到的情况。</p>\n<p>另外，Ng给我们了一个\\(\\beta\\)的理想取值，既0.9，这个值大约取了前10次迭代结果的moving averages。另外，Ng表示，在momen中很少使用bias correction，因为10次迭代之后，这种问题很快就会消除，而一般的gradient descent，iteration次数远远大于10次。</p>\n<h3 id=\"RMSprop\"><a href=\"#RMSprop\" class=\"headerlink\" title=\"RMSprop\"></a>RMSprop</h3><p>除了momentum，还有一些类似的方法，例如大名鼎鼎的RMSprop(root means square prop)，在这个方法中，主要体现了square的应用，我们来看看，在每次的迭代中：<br>$$S_{dW}= \\beta S_{dW} + (1- \\beta)(dW)^2$$<br>$$S_{db}= \\beta S_{db}+(1- \\beta)(db)^2$$<br>$$W:=W- \\alpha \\frac{dW}{\\sqrt {S_{dW}}+ \\epsilon}$$<br>$$b:=b- \\alpha \\frac{db}{\\sqrt {S_{db}} + \\epsilon}$$<br>\\(\\epsilon\\)是为了防止分母为0的一个item，取值建议为\\(10^{-8}\\)</p>\n<p>假设在gradient descent中，\\(W\\)下降速率太低，也就是\\(dW\\)太小，那么在RMSprop的迭代中，\\(dW\\)将会除以一个很小的值\\( \\sqrt{S_{dW}}\\)，也就是\\(W\\)将会减去一个较大的值，反之亦然。</p>\n<p>通过这种方式，我们缓解了上图所示的情况，改善了gradient descent的合理性，同时可以使用更大的\\(\\alpha\\)去实现更快的gradient descent.</p>\n<h3 id=\"Adam\"><a href=\"#Adam\" class=\"headerlink\" title=\"Adam\"></a>Adam</h3><p>我们看到了momentum和RMSprop优化方法的厉害之处，现在Adam方法横空出世，他融合了momentum和RMSprop，他是如何融合的呢，我们把momentum中的\\( \\beta\\)命名为\\( \\beta_1\\)，把RMSprop中的\\( \\beta\\)命名为\\( \\beta_2\\)，在第\\(t\\)次迭代中：<br>$$v_{dW}= \\beta_1 v_{dW}+(1- \\beta_1)dW, \\qquad v_{db}= \\beta_1 v_{db}+(1- \\beta_1)db$$<br>$$s_{dW}= \\beta_2 s_{dW}+(1- \\beta_2)(dW)^2, \\qquad s_{db}= \\beta_2 s_{db}+(1- \\beta_2)(db)^2$$<br>加上bias correction后<br>$$v^{corrected}<em>{dW}= \\frac{v</em>{dW}}{1- \\beta^t_1}, \\qquad v^{corrected}<em>{db}= \\frac{v</em>{db}}{1- \\beta^t_1}$$<br>$$s^{corrected}<em>{dW}= \\frac{s</em>{dW}}{1- \\beta^t_2}, \\qquad s^{corrected}<em>{db}= \\frac{s</em>{db}}{1- \\beta^t_2}$$<br>$$W:=W- \\alpha \\frac{v^{corrected}<em>{dW}}{ \\sqrt{s^{corrected}</em>{dW}}+ \\epsilon}$$<br>$$W:=W- \\alpha \\frac{v^{corrected}<em>{db}}{ \\sqrt{s^{corrected}</em>{db}}+ \\epsilon}$$<br>其中，\\(\\epsilon\\)是为了防止分母为0的一个item，取值建议为\\(10^{-8}\\)，\\(\\beta_1\\)建议取值0.9，\\(\\beta_2\\)建议取值0.999。</p>\n<h2 id=\"Learning-rate-decay\"><a href=\"#Learning-rate-decay\" class=\"headerlink\" title=\"Learning rate decay\"></a>Learning rate decay</h2><p>在gradient descent中，随着迭代的深度，越来越靠近minimum，我们需要更小的learning rate，以避免越过minimum，常用的learning decay方法有：<br>$$\\alpha = \\frac{1}{1+decayRate<em>epochNum} </em> \\alpha_0$$<br>$$\\alpha = 0.95^{epochNum} <em> \\alpha_0$$<br>$$\\alpha = \\frac{k}{\\sqrt{epochNum}}</em> \\alpha_0$$<br>这些方法都可以让\\(\\alpha\\)随着迭代次数增加而慢慢变小，可以更好的逼近minimum.</p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ul>\n<li><a href=\"https://www.coursera.org/specializations/deep-learning\" target=\"_blank\" rel=\"noopener\">Deep learning-Coursera Andrew Ng</a></li>\n<li><a href=\"https://mooc.study.163.com/course/deeplearning_ai-2001281003#/info\" target=\"_blank\" rel=\"noopener\">Deep learning-网易云课堂 Andrew Ng</a></li>\n<li><a href=\"http://ruder.io/optimizing-gradient-descent/\" target=\"_blank\" rel=\"noopener\">An overview of gradient descent optimization algorithms </a></li>\n</ul>"},{"title":"Learning Notes-Deep Learning, course3, week2","date":"2017-10-18T13:28:12.000Z","_content":"Hi all，course3来到了week2，本周的课程依然主要是关于一些learning strategy，这些方法相当实用。虽然不是什么具体的算法，但都都是Ng在科研和工作中积累下来的宝贵经验，对于实际问题十分有效。\n\n我们一起来看看。\n<!--more-->\n## Error analysis\n### Carry out error analysis\n按照通常的流程，在进行training过程后，我们在dev set会进行模型的测试，如果dev error比training error大很多的话，我们应该去排查问题的症结所在呢？Ng给出了solution\n\n例如在cat recognition中，我们发现错分的sample有很多dog图像，还有很多猫科动物的图像，还有一些是模糊的cat图像。于是我们自然而然的想到三种解决方案：\n* 解决狗错分为猫的问题\n* 解决猫科动物被错分成猫的问题\n* 提升模糊图像被误分的问题\n\n可是由于我们精力和时间都有限，需要找出误分最主要的问题，因此我们要做的，是把所有错分的图像罗列出来，或者随机抽样一定的图像，分析每种错误它有多少，占错分图像多少比例。我们来看截图\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/10/10-1.png)\n每一个错分的图像都会进行标签化的统计，最后通过统计每一个标签，找出影响错分最严重的因素，作为我们的改进方向。\n### Clean up incorrectly labeled data\n在常见的错误中，错误的label是一种很常见的问题，这种问题往往来自于标注时候，错误的label会对training造成误导。\n\n首先，对于training set，来说，incorrectly labeled data应该怎么处理？首先，Ng告诉了我们一个性质：\n\n> DL algorithms are quite robust to random errors in the training set\n\nDL因为其自身的robust性质，当training set中有少许的，随机产生的incorrectly labeled data时，效果并不会有多差，我们完全不需要去管他。但是，当这incorrectly labeled data很多时就不行了，因为它们带来的是systematic errors，极端的想，如果把所有的白狗都错误的标注成了猫，那么这个cat recognition系统一定不会好，因为它一定会把白色的狗判断成为猫。\n\n再来看看dev/test set中的incorrectly labeled data，对于这个问题，我们要做的是，评估incorrectly labeled data对dev error带来了多少贡献，解决的过程也是类似的，来看截图：\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/10/10-2.png)\n我们把incorrectly labeled也作为一个要素或标签，放在错分图像分析的过程中，\n看看最终的统计结果，再决定incorrectly labeled data是不是影响dev error的主要原因，是否值得我们去fix it up.\n\n最后，关于correcting incorrect dev/test set example，Ng给出了一些建议：\n> Apply same proces to your dev and test sets to make sure thry continue to come from the same distribution.\n\n在修正的过程中，一定要保证dev和test set同时被修正，如果他们不再符合同一distribution，那么会对于后续的评价带来一些问题。\n\n> Consider examining examples your algorithm got right as well as ones it got wrong.\n\n我们在更正的时候，不能只是看被错分的图像，对于被正确分类的，也有可能存在incorrect labeled 的情况。\n\n> Tran and dev/test data may now come from slightly different distribution\n\n正如刚才讲的，DL对于training有一定程度的robust性，incorrect labeled data可能不会对training set带来这些问题，在这种情况下，我们可以不用去更正training set，这种情况，我们是可以接受的。\n\n### Build up quickly and iterate\n最后Ng用一个speech recognition作为例子，我们首先要分析出可能影响效果的一些因素：\n* Noisy background\n* Accented speech\n* Far from microphone\n* Young children's speech\n\n...\n针对这些问题，我们该如何构造我们的模型呢，Ng给出了建议\n* Set up dev/test set and metric\n* Build initial system quickly\n* Use bias/variance analysis & error analysis to prioritize next steps.\n\n总而言之，guideline是\n> Build your first system quickly, then iterate.\n\n## Mismatched training and dev/test data\n### Training and testing on different distributions\n之前我们再三强调过一个尖锐的问题，那就是training/dev/test set一定要在同一个distribution下，但是实际上，愿望总是美好的而现实很残酷，我们总是会面对一些training and testing on different distribution问题。\n\n例如在猫识别的任务中，我们需要将这个模型部署到手机app上，我们手上的数据只有10k是从手机拍摄获得的，而有200k的数据是从网络上获得的，这两种图像显然不属于同一distribution，我们应该怎么办？\n\n首先来看option1，我们将所有的210k数据充分混合在一起，其中205k作为training set，2.5k作为dev，2.5作为test set。这样看起来是一个很不错的方法，但是，确实很不好的一个方法，为什么这么说呢？\n\n在整个过程中，dev/test set其实扮演了一个非常重要的角色，它决定了我们的target，也就是整体的优化方向。在这个例子中，我们要优化的方向是app上的图像，而这种data set分割方法和我们的task target并不符合，因此并不优秀。\n\n我们再来看option2，我们将200k的来自网络的图片全部放入training set，然后将10k的app数据，5k放入training set，2.5k作为dev，2.5作为test，这样做的话，dev/test决定的target 和我们的task target是一致的，所以长远来看，虽然option2的training/dev set并不是同一distribution，但是从长远看它的效果还是很不错的。\n### Bias & variance with mismatched data distribution\n在training/dev/test set符合同一distribution的时候，我们通过比较training error和dev error就可以定性是否存在high variance的问题。但是，当training set和dev set不符合同一distribution的时候，这个判断就显得有些困难了。我们应该怎么处理呢？\n\n这时候，我们可以从training set中取出一小部分数据，命名为training-dev set，这部分数据将不再进行training，而是作为评判training效果的一个set，此时我们就有了training error，training-dev error和dev error三个error，再结合human error，training error和training-dev error之间的差值可以反映出模型是否有high bias或者variance，这样可以更科学的来评判模型效果。相应的，training-dev error和dev error相差越多，data mismatch的程度越大。\n### Addressing data mismatch\n我们如何addressing data mismatch呢，首先我们来看看Ng的两条guideline：\n* Carry out manual error analysis to try to understand difference between training and dev/test sets\n* Make training data more similar;  or collect more data similar to dev/test sets\n\n理解一下，首先我们要通过人工的analysis去分析出造成training set和dev set之间distribution不同的原因，比如语音识别中的有无汽车噪声等等；然后我们需要根据这些差别，让training set和dev set更加的相似，甚至相通。\n\n但是要注意的是，我们在这个过程中，要避免出现overfitting的情况出现，例如Ng举出的例子，在识别车内的人声过程中，我们可以通过人工的合成汽车声音与人的声音让training set和dev set更加的相似，但是如果我们的只用一段汽车噪音循环往复的去做合成，例如吧1min的汽车噪声循环的合成到1h的人声中，那结果一定是不尽如人意的，因为出现了overfitting.\n## Transfer learning\n下面我们一起来看看大名鼎鼎的transfer learning，所谓transfer，就是存在一种从A到B的转换，而且这种情况往往是B的数据量很少，需要通过A来做一个pre-training过程。假设我们有如下的neural networks\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/10/10-3.png)\n假设这个我们使用这个neural networks训练了一个image recognition模型，在训练完成后，我们将最后的output，以及output对应的的\\\\(w\\\\)和\\\\(b\\\\)也删除，更换成例如放射数据再进行训练，如下图：\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/10/10-4.png)\n我们不仅仅可以把output层更换成一个新的output层，还可以将output层更换成几个新层。我们甚至可以将transfer之前的训练认为是一种pre-training，但是transfer training需要有几个条件：\n* Task A and B have the same input x.\n* You have a lot more data for Task A than Task B.\n* Low level features from A could be helpful for learning B.\n\n## Multi-task learning\n现在假设我们有一个自动驾驶的场景，我们需要从视频中识别行人、车辆、停车标志和红绿灯，按照常理来说，我们可以单独的构建4个模型。但是，这4个模型的特征场景都是很相似的，构建4个模型稍微有一些浪费，于是我们可以把这四个任务合并在一起，这就是Multi-task learning.\n\n在这里我们的标签\\\\(y\\\\)，就不再是一个m×1的矩阵了，而是一个m×4的矩阵，对于multi-task来说，在以下情况下是可行的：\n* Training on a set of tasks that could benefit from having shared lower-level features.\n* Usually: Amount of data you have for eachtask is quite similar.\n* Can train a big enough neural network to do well on all the tasks.\n\n## End to end learning\nEnd to end learning是随着DL兴起后而产生的一种learning方式，在end2end中，我们不再关注一些中间的步骤，例如feature selection或者image processing，我们只是把原始的数据和最后的结果告诉DL，它就可以自主的完成这个任务。\n\n当然end2end 也是有一些优势和劣势的，我们来看一下：\nPros：\n* Let the data speak.\n* Less hand-desgining of components needed.\n\nCons:\n* May need large amount of data.\n* Excludes potentially userful hand-designed components.\n\n总之，对于end2end来说，大数据量，一定是最重要的因素，基于这一点，我们才可以摆脱传统的中间步骤，彻底实现end to end learning.\n\n## Reference\n* [Deep learning-Coursera Andrew Ng](https://www.coursera.org/specializations/deep-learning)\n* [Deep learning-网易云课堂 Andrew Ng](https://mooc.study.163.com/course/deeplearning_ai-2001281003#/info)\n","source":"_posts/course-deep-learning-course3-week2.md","raw":"---\ntitle: Learning Notes-Deep Learning, course3, week2\ndate: 2017-10-18 21:28:12\ntags: \n\t- learning strategy\n\t- transfer learning\n\t- multi-task learning\t\ncategories: learning notes\n---\nHi all，course3来到了week2，本周的课程依然主要是关于一些learning strategy，这些方法相当实用。虽然不是什么具体的算法，但都都是Ng在科研和工作中积累下来的宝贵经验，对于实际问题十分有效。\n\n我们一起来看看。\n<!--more-->\n## Error analysis\n### Carry out error analysis\n按照通常的流程，在进行training过程后，我们在dev set会进行模型的测试，如果dev error比training error大很多的话，我们应该去排查问题的症结所在呢？Ng给出了solution\n\n例如在cat recognition中，我们发现错分的sample有很多dog图像，还有很多猫科动物的图像，还有一些是模糊的cat图像。于是我们自然而然的想到三种解决方案：\n* 解决狗错分为猫的问题\n* 解决猫科动物被错分成猫的问题\n* 提升模糊图像被误分的问题\n\n可是由于我们精力和时间都有限，需要找出误分最主要的问题，因此我们要做的，是把所有错分的图像罗列出来，或者随机抽样一定的图像，分析每种错误它有多少，占错分图像多少比例。我们来看截图\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/10/10-1.png)\n每一个错分的图像都会进行标签化的统计，最后通过统计每一个标签，找出影响错分最严重的因素，作为我们的改进方向。\n### Clean up incorrectly labeled data\n在常见的错误中，错误的label是一种很常见的问题，这种问题往往来自于标注时候，错误的label会对training造成误导。\n\n首先，对于training set，来说，incorrectly labeled data应该怎么处理？首先，Ng告诉了我们一个性质：\n\n> DL algorithms are quite robust to random errors in the training set\n\nDL因为其自身的robust性质，当training set中有少许的，随机产生的incorrectly labeled data时，效果并不会有多差，我们完全不需要去管他。但是，当这incorrectly labeled data很多时就不行了，因为它们带来的是systematic errors，极端的想，如果把所有的白狗都错误的标注成了猫，那么这个cat recognition系统一定不会好，因为它一定会把白色的狗判断成为猫。\n\n再来看看dev/test set中的incorrectly labeled data，对于这个问题，我们要做的是，评估incorrectly labeled data对dev error带来了多少贡献，解决的过程也是类似的，来看截图：\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/10/10-2.png)\n我们把incorrectly labeled也作为一个要素或标签，放在错分图像分析的过程中，\n看看最终的统计结果，再决定incorrectly labeled data是不是影响dev error的主要原因，是否值得我们去fix it up.\n\n最后，关于correcting incorrect dev/test set example，Ng给出了一些建议：\n> Apply same proces to your dev and test sets to make sure thry continue to come from the same distribution.\n\n在修正的过程中，一定要保证dev和test set同时被修正，如果他们不再符合同一distribution，那么会对于后续的评价带来一些问题。\n\n> Consider examining examples your algorithm got right as well as ones it got wrong.\n\n我们在更正的时候，不能只是看被错分的图像，对于被正确分类的，也有可能存在incorrect labeled 的情况。\n\n> Tran and dev/test data may now come from slightly different distribution\n\n正如刚才讲的，DL对于training有一定程度的robust性，incorrect labeled data可能不会对training set带来这些问题，在这种情况下，我们可以不用去更正training set，这种情况，我们是可以接受的。\n\n### Build up quickly and iterate\n最后Ng用一个speech recognition作为例子，我们首先要分析出可能影响效果的一些因素：\n* Noisy background\n* Accented speech\n* Far from microphone\n* Young children's speech\n\n...\n针对这些问题，我们该如何构造我们的模型呢，Ng给出了建议\n* Set up dev/test set and metric\n* Build initial system quickly\n* Use bias/variance analysis & error analysis to prioritize next steps.\n\n总而言之，guideline是\n> Build your first system quickly, then iterate.\n\n## Mismatched training and dev/test data\n### Training and testing on different distributions\n之前我们再三强调过一个尖锐的问题，那就是training/dev/test set一定要在同一个distribution下，但是实际上，愿望总是美好的而现实很残酷，我们总是会面对一些training and testing on different distribution问题。\n\n例如在猫识别的任务中，我们需要将这个模型部署到手机app上，我们手上的数据只有10k是从手机拍摄获得的，而有200k的数据是从网络上获得的，这两种图像显然不属于同一distribution，我们应该怎么办？\n\n首先来看option1，我们将所有的210k数据充分混合在一起，其中205k作为training set，2.5k作为dev，2.5作为test set。这样看起来是一个很不错的方法，但是，确实很不好的一个方法，为什么这么说呢？\n\n在整个过程中，dev/test set其实扮演了一个非常重要的角色，它决定了我们的target，也就是整体的优化方向。在这个例子中，我们要优化的方向是app上的图像，而这种data set分割方法和我们的task target并不符合，因此并不优秀。\n\n我们再来看option2，我们将200k的来自网络的图片全部放入training set，然后将10k的app数据，5k放入training set，2.5k作为dev，2.5作为test，这样做的话，dev/test决定的target 和我们的task target是一致的，所以长远来看，虽然option2的training/dev set并不是同一distribution，但是从长远看它的效果还是很不错的。\n### Bias & variance with mismatched data distribution\n在training/dev/test set符合同一distribution的时候，我们通过比较training error和dev error就可以定性是否存在high variance的问题。但是，当training set和dev set不符合同一distribution的时候，这个判断就显得有些困难了。我们应该怎么处理呢？\n\n这时候，我们可以从training set中取出一小部分数据，命名为training-dev set，这部分数据将不再进行training，而是作为评判training效果的一个set，此时我们就有了training error，training-dev error和dev error三个error，再结合human error，training error和training-dev error之间的差值可以反映出模型是否有high bias或者variance，这样可以更科学的来评判模型效果。相应的，training-dev error和dev error相差越多，data mismatch的程度越大。\n### Addressing data mismatch\n我们如何addressing data mismatch呢，首先我们来看看Ng的两条guideline：\n* Carry out manual error analysis to try to understand difference between training and dev/test sets\n* Make training data more similar;  or collect more data similar to dev/test sets\n\n理解一下，首先我们要通过人工的analysis去分析出造成training set和dev set之间distribution不同的原因，比如语音识别中的有无汽车噪声等等；然后我们需要根据这些差别，让training set和dev set更加的相似，甚至相通。\n\n但是要注意的是，我们在这个过程中，要避免出现overfitting的情况出现，例如Ng举出的例子，在识别车内的人声过程中，我们可以通过人工的合成汽车声音与人的声音让training set和dev set更加的相似，但是如果我们的只用一段汽车噪音循环往复的去做合成，例如吧1min的汽车噪声循环的合成到1h的人声中，那结果一定是不尽如人意的，因为出现了overfitting.\n## Transfer learning\n下面我们一起来看看大名鼎鼎的transfer learning，所谓transfer，就是存在一种从A到B的转换，而且这种情况往往是B的数据量很少，需要通过A来做一个pre-training过程。假设我们有如下的neural networks\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/10/10-3.png)\n假设这个我们使用这个neural networks训练了一个image recognition模型，在训练完成后，我们将最后的output，以及output对应的的\\\\(w\\\\)和\\\\(b\\\\)也删除，更换成例如放射数据再进行训练，如下图：\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/10/10-4.png)\n我们不仅仅可以把output层更换成一个新的output层，还可以将output层更换成几个新层。我们甚至可以将transfer之前的训练认为是一种pre-training，但是transfer training需要有几个条件：\n* Task A and B have the same input x.\n* You have a lot more data for Task A than Task B.\n* Low level features from A could be helpful for learning B.\n\n## Multi-task learning\n现在假设我们有一个自动驾驶的场景，我们需要从视频中识别行人、车辆、停车标志和红绿灯，按照常理来说，我们可以单独的构建4个模型。但是，这4个模型的特征场景都是很相似的，构建4个模型稍微有一些浪费，于是我们可以把这四个任务合并在一起，这就是Multi-task learning.\n\n在这里我们的标签\\\\(y\\\\)，就不再是一个m×1的矩阵了，而是一个m×4的矩阵，对于multi-task来说，在以下情况下是可行的：\n* Training on a set of tasks that could benefit from having shared lower-level features.\n* Usually: Amount of data you have for eachtask is quite similar.\n* Can train a big enough neural network to do well on all the tasks.\n\n## End to end learning\nEnd to end learning是随着DL兴起后而产生的一种learning方式，在end2end中，我们不再关注一些中间的步骤，例如feature selection或者image processing，我们只是把原始的数据和最后的结果告诉DL，它就可以自主的完成这个任务。\n\n当然end2end 也是有一些优势和劣势的，我们来看一下：\nPros：\n* Let the data speak.\n* Less hand-desgining of components needed.\n\nCons:\n* May need large amount of data.\n* Excludes potentially userful hand-designed components.\n\n总之，对于end2end来说，大数据量，一定是最重要的因素，基于这一点，我们才可以摆脱传统的中间步骤，彻底实现end to end learning.\n\n## Reference\n* [Deep learning-Coursera Andrew Ng](https://www.coursera.org/specializations/deep-learning)\n* [Deep learning-网易云课堂 Andrew Ng](https://mooc.study.163.com/course/deeplearning_ai-2001281003#/info)\n","slug":"course-deep-learning-course3-week2","published":1,"updated":"2020-05-10T06:50:12.528Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cka0wnkw10005qxotquuk2g90","content":"<p>Hi all，course3来到了week2，本周的课程依然主要是关于一些learning strategy，这些方法相当实用。虽然不是什么具体的算法，但都都是Ng在科研和工作中积累下来的宝贵经验，对于实际问题十分有效。</p>\n<p>我们一起来看看。<br><a id=\"more\"></a></p>\n<h2 id=\"Error-analysis\"><a href=\"#Error-analysis\" class=\"headerlink\" title=\"Error analysis\"></a>Error analysis</h2><h3 id=\"Carry-out-error-analysis\"><a href=\"#Carry-out-error-analysis\" class=\"headerlink\" title=\"Carry out error analysis\"></a>Carry out error analysis</h3><p>按照通常的流程，在进行training过程后，我们在dev set会进行模型的测试，如果dev error比training error大很多的话，我们应该去排查问题的症结所在呢？Ng给出了solution</p>\n<p>例如在cat recognition中，我们发现错分的sample有很多dog图像，还有很多猫科动物的图像，还有一些是模糊的cat图像。于是我们自然而然的想到三种解决方案：</p>\n<ul>\n<li>解决狗错分为猫的问题</li>\n<li>解决猫科动物被错分成猫的问题</li>\n<li>提升模糊图像被误分的问题</li>\n</ul>\n<p>可是由于我们精力和时间都有限，需要找出误分最主要的问题，因此我们要做的，是把所有错分的图像罗列出来，或者随机抽样一定的图像，分析每种错误它有多少，占错分图像多少比例。我们来看截图<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/10/10-1.png\" alt=\"\"><br>每一个错分的图像都会进行标签化的统计，最后通过统计每一个标签，找出影响错分最严重的因素，作为我们的改进方向。</p>\n<h3 id=\"Clean-up-incorrectly-labeled-data\"><a href=\"#Clean-up-incorrectly-labeled-data\" class=\"headerlink\" title=\"Clean up incorrectly labeled data\"></a>Clean up incorrectly labeled data</h3><p>在常见的错误中，错误的label是一种很常见的问题，这种问题往往来自于标注时候，错误的label会对training造成误导。</p>\n<p>首先，对于training set，来说，incorrectly labeled data应该怎么处理？首先，Ng告诉了我们一个性质：</p>\n<blockquote>\n<p>DL algorithms are quite robust to random errors in the training set</p>\n</blockquote>\n<p>DL因为其自身的robust性质，当training set中有少许的，随机产生的incorrectly labeled data时，效果并不会有多差，我们完全不需要去管他。但是，当这incorrectly labeled data很多时就不行了，因为它们带来的是systematic errors，极端的想，如果把所有的白狗都错误的标注成了猫，那么这个cat recognition系统一定不会好，因为它一定会把白色的狗判断成为猫。</p>\n<p>再来看看dev/test set中的incorrectly labeled data，对于这个问题，我们要做的是，评估incorrectly labeled data对dev error带来了多少贡献，解决的过程也是类似的，来看截图：<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/10/10-2.png\" alt=\"\"><br>我们把incorrectly labeled也作为一个要素或标签，放在错分图像分析的过程中，<br>看看最终的统计结果，再决定incorrectly labeled data是不是影响dev error的主要原因，是否值得我们去fix it up.</p>\n<p>最后，关于correcting incorrect dev/test set example，Ng给出了一些建议：</p>\n<blockquote>\n<p>Apply same proces to your dev and test sets to make sure thry continue to come from the same distribution.</p>\n</blockquote>\n<p>在修正的过程中，一定要保证dev和test set同时被修正，如果他们不再符合同一distribution，那么会对于后续的评价带来一些问题。</p>\n<blockquote>\n<p>Consider examining examples your algorithm got right as well as ones it got wrong.</p>\n</blockquote>\n<p>我们在更正的时候，不能只是看被错分的图像，对于被正确分类的，也有可能存在incorrect labeled 的情况。</p>\n<blockquote>\n<p>Tran and dev/test data may now come from slightly different distribution</p>\n</blockquote>\n<p>正如刚才讲的，DL对于training有一定程度的robust性，incorrect labeled data可能不会对training set带来这些问题，在这种情况下，我们可以不用去更正training set，这种情况，我们是可以接受的。</p>\n<h3 id=\"Build-up-quickly-and-iterate\"><a href=\"#Build-up-quickly-and-iterate\" class=\"headerlink\" title=\"Build up quickly and iterate\"></a>Build up quickly and iterate</h3><p>最后Ng用一个speech recognition作为例子，我们首先要分析出可能影响效果的一些因素：</p>\n<ul>\n<li>Noisy background</li>\n<li>Accented speech</li>\n<li>Far from microphone</li>\n<li>Young children’s speech</li>\n</ul>\n<p>…<br>针对这些问题，我们该如何构造我们的模型呢，Ng给出了建议</p>\n<ul>\n<li>Set up dev/test set and metric</li>\n<li>Build initial system quickly</li>\n<li>Use bias/variance analysis &amp; error analysis to prioritize next steps.</li>\n</ul>\n<p>总而言之，guideline是</p>\n<blockquote>\n<p>Build your first system quickly, then iterate.</p>\n</blockquote>\n<h2 id=\"Mismatched-training-and-dev-test-data\"><a href=\"#Mismatched-training-and-dev-test-data\" class=\"headerlink\" title=\"Mismatched training and dev/test data\"></a>Mismatched training and dev/test data</h2><h3 id=\"Training-and-testing-on-different-distributions\"><a href=\"#Training-and-testing-on-different-distributions\" class=\"headerlink\" title=\"Training and testing on different distributions\"></a>Training and testing on different distributions</h3><p>之前我们再三强调过一个尖锐的问题，那就是training/dev/test set一定要在同一个distribution下，但是实际上，愿望总是美好的而现实很残酷，我们总是会面对一些training and testing on different distribution问题。</p>\n<p>例如在猫识别的任务中，我们需要将这个模型部署到手机app上，我们手上的数据只有10k是从手机拍摄获得的，而有200k的数据是从网络上获得的，这两种图像显然不属于同一distribution，我们应该怎么办？</p>\n<p>首先来看option1，我们将所有的210k数据充分混合在一起，其中205k作为training set，2.5k作为dev，2.5作为test set。这样看起来是一个很不错的方法，但是，确实很不好的一个方法，为什么这么说呢？</p>\n<p>在整个过程中，dev/test set其实扮演了一个非常重要的角色，它决定了我们的target，也就是整体的优化方向。在这个例子中，我们要优化的方向是app上的图像，而这种data set分割方法和我们的task target并不符合，因此并不优秀。</p>\n<p>我们再来看option2，我们将200k的来自网络的图片全部放入training set，然后将10k的app数据，5k放入training set，2.5k作为dev，2.5作为test，这样做的话，dev/test决定的target 和我们的task target是一致的，所以长远来看，虽然option2的training/dev set并不是同一distribution，但是从长远看它的效果还是很不错的。</p>\n<h3 id=\"Bias-amp-variance-with-mismatched-data-distribution\"><a href=\"#Bias-amp-variance-with-mismatched-data-distribution\" class=\"headerlink\" title=\"Bias &amp; variance with mismatched data distribution\"></a>Bias &amp; variance with mismatched data distribution</h3><p>在training/dev/test set符合同一distribution的时候，我们通过比较training error和dev error就可以定性是否存在high variance的问题。但是，当training set和dev set不符合同一distribution的时候，这个判断就显得有些困难了。我们应该怎么处理呢？</p>\n<p>这时候，我们可以从training set中取出一小部分数据，命名为training-dev set，这部分数据将不再进行training，而是作为评判training效果的一个set，此时我们就有了training error，training-dev error和dev error三个error，再结合human error，training error和training-dev error之间的差值可以反映出模型是否有high bias或者variance，这样可以更科学的来评判模型效果。相应的，training-dev error和dev error相差越多，data mismatch的程度越大。</p>\n<h3 id=\"Addressing-data-mismatch\"><a href=\"#Addressing-data-mismatch\" class=\"headerlink\" title=\"Addressing data mismatch\"></a>Addressing data mismatch</h3><p>我们如何addressing data mismatch呢，首先我们来看看Ng的两条guideline：</p>\n<ul>\n<li>Carry out manual error analysis to try to understand difference between training and dev/test sets</li>\n<li>Make training data more similar;  or collect more data similar to dev/test sets</li>\n</ul>\n<p>理解一下，首先我们要通过人工的analysis去分析出造成training set和dev set之间distribution不同的原因，比如语音识别中的有无汽车噪声等等；然后我们需要根据这些差别，让training set和dev set更加的相似，甚至相通。</p>\n<p>但是要注意的是，我们在这个过程中，要避免出现overfitting的情况出现，例如Ng举出的例子，在识别车内的人声过程中，我们可以通过人工的合成汽车声音与人的声音让training set和dev set更加的相似，但是如果我们的只用一段汽车噪音循环往复的去做合成，例如吧1min的汽车噪声循环的合成到1h的人声中，那结果一定是不尽如人意的，因为出现了overfitting.</p>\n<h2 id=\"Transfer-learning\"><a href=\"#Transfer-learning\" class=\"headerlink\" title=\"Transfer learning\"></a>Transfer learning</h2><p>下面我们一起来看看大名鼎鼎的transfer learning，所谓transfer，就是存在一种从A到B的转换，而且这种情况往往是B的数据量很少，需要通过A来做一个pre-training过程。假设我们有如下的neural networks<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/10/10-3.png\" alt=\"\"><br>假设这个我们使用这个neural networks训练了一个image recognition模型，在训练完成后，我们将最后的output，以及output对应的的\\(w\\)和\\(b\\)也删除，更换成例如放射数据再进行训练，如下图：<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/10/10-4.png\" alt=\"\"><br>我们不仅仅可以把output层更换成一个新的output层，还可以将output层更换成几个新层。我们甚至可以将transfer之前的训练认为是一种pre-training，但是transfer training需要有几个条件：</p>\n<ul>\n<li>Task A and B have the same input x.</li>\n<li>You have a lot more data for Task A than Task B.</li>\n<li>Low level features from A could be helpful for learning B.</li>\n</ul>\n<h2 id=\"Multi-task-learning\"><a href=\"#Multi-task-learning\" class=\"headerlink\" title=\"Multi-task learning\"></a>Multi-task learning</h2><p>现在假设我们有一个自动驾驶的场景，我们需要从视频中识别行人、车辆、停车标志和红绿灯，按照常理来说，我们可以单独的构建4个模型。但是，这4个模型的特征场景都是很相似的，构建4个模型稍微有一些浪费，于是我们可以把这四个任务合并在一起，这就是Multi-task learning.</p>\n<p>在这里我们的标签\\(y\\)，就不再是一个m×1的矩阵了，而是一个m×4的矩阵，对于multi-task来说，在以下情况下是可行的：</p>\n<ul>\n<li>Training on a set of tasks that could benefit from having shared lower-level features.</li>\n<li>Usually: Amount of data you have for eachtask is quite similar.</li>\n<li>Can train a big enough neural network to do well on all the tasks.</li>\n</ul>\n<h2 id=\"End-to-end-learning\"><a href=\"#End-to-end-learning\" class=\"headerlink\" title=\"End to end learning\"></a>End to end learning</h2><p>End to end learning是随着DL兴起后而产生的一种learning方式，在end2end中，我们不再关注一些中间的步骤，例如feature selection或者image processing，我们只是把原始的数据和最后的结果告诉DL，它就可以自主的完成这个任务。</p>\n<p>当然end2end 也是有一些优势和劣势的，我们来看一下：<br>Pros：</p>\n<ul>\n<li>Let the data speak.</li>\n<li>Less hand-desgining of components needed.</li>\n</ul>\n<p>Cons:</p>\n<ul>\n<li>May need large amount of data.</li>\n<li>Excludes potentially userful hand-designed components.</li>\n</ul>\n<p>总之，对于end2end来说，大数据量，一定是最重要的因素，基于这一点，我们才可以摆脱传统的中间步骤，彻底实现end to end learning.</p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ul>\n<li><a href=\"https://www.coursera.org/specializations/deep-learning\" target=\"_blank\" rel=\"noopener\">Deep learning-Coursera Andrew Ng</a></li>\n<li><a href=\"https://mooc.study.163.com/course/deeplearning_ai-2001281003#/info\" target=\"_blank\" rel=\"noopener\">Deep learning-网易云课堂 Andrew Ng</a></li>\n</ul>\n","site":{"data":{}},"_categories":[{"name":"learning notes","path":"categories/learning-notes/"}],"_tags":[{"name":"learning strategy","path":"tags/learning-strategy/"},{"name":"transfer learning","path":"tags/transfer-learning/"},{"name":"multi-task learning","path":"tags/multi-task-learning/"}],"excerpt":"<p>Hi all，course3来到了week2，本周的课程依然主要是关于一些learning strategy，这些方法相当实用。虽然不是什么具体的算法，但都都是Ng在科研和工作中积累下来的宝贵经验，对于实际问题十分有效。</p>\n<p>我们一起来看看。<br></p>","more":"</p>\n<h2 id=\"Error-analysis\"><a href=\"#Error-analysis\" class=\"headerlink\" title=\"Error analysis\"></a>Error analysis</h2><h3 id=\"Carry-out-error-analysis\"><a href=\"#Carry-out-error-analysis\" class=\"headerlink\" title=\"Carry out error analysis\"></a>Carry out error analysis</h3><p>按照通常的流程，在进行training过程后，我们在dev set会进行模型的测试，如果dev error比training error大很多的话，我们应该去排查问题的症结所在呢？Ng给出了solution</p>\n<p>例如在cat recognition中，我们发现错分的sample有很多dog图像，还有很多猫科动物的图像，还有一些是模糊的cat图像。于是我们自然而然的想到三种解决方案：</p>\n<ul>\n<li>解决狗错分为猫的问题</li>\n<li>解决猫科动物被错分成猫的问题</li>\n<li>提升模糊图像被误分的问题</li>\n</ul>\n<p>可是由于我们精力和时间都有限，需要找出误分最主要的问题，因此我们要做的，是把所有错分的图像罗列出来，或者随机抽样一定的图像，分析每种错误它有多少，占错分图像多少比例。我们来看截图<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/10/10-1.png\" alt=\"\"><br>每一个错分的图像都会进行标签化的统计，最后通过统计每一个标签，找出影响错分最严重的因素，作为我们的改进方向。</p>\n<h3 id=\"Clean-up-incorrectly-labeled-data\"><a href=\"#Clean-up-incorrectly-labeled-data\" class=\"headerlink\" title=\"Clean up incorrectly labeled data\"></a>Clean up incorrectly labeled data</h3><p>在常见的错误中，错误的label是一种很常见的问题，这种问题往往来自于标注时候，错误的label会对training造成误导。</p>\n<p>首先，对于training set，来说，incorrectly labeled data应该怎么处理？首先，Ng告诉了我们一个性质：</p>\n<blockquote>\n<p>DL algorithms are quite robust to random errors in the training set</p>\n</blockquote>\n<p>DL因为其自身的robust性质，当training set中有少许的，随机产生的incorrectly labeled data时，效果并不会有多差，我们完全不需要去管他。但是，当这incorrectly labeled data很多时就不行了，因为它们带来的是systematic errors，极端的想，如果把所有的白狗都错误的标注成了猫，那么这个cat recognition系统一定不会好，因为它一定会把白色的狗判断成为猫。</p>\n<p>再来看看dev/test set中的incorrectly labeled data，对于这个问题，我们要做的是，评估incorrectly labeled data对dev error带来了多少贡献，解决的过程也是类似的，来看截图：<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/10/10-2.png\" alt=\"\"><br>我们把incorrectly labeled也作为一个要素或标签，放在错分图像分析的过程中，<br>看看最终的统计结果，再决定incorrectly labeled data是不是影响dev error的主要原因，是否值得我们去fix it up.</p>\n<p>最后，关于correcting incorrect dev/test set example，Ng给出了一些建议：</p>\n<blockquote>\n<p>Apply same proces to your dev and test sets to make sure thry continue to come from the same distribution.</p>\n</blockquote>\n<p>在修正的过程中，一定要保证dev和test set同时被修正，如果他们不再符合同一distribution，那么会对于后续的评价带来一些问题。</p>\n<blockquote>\n<p>Consider examining examples your algorithm got right as well as ones it got wrong.</p>\n</blockquote>\n<p>我们在更正的时候，不能只是看被错分的图像，对于被正确分类的，也有可能存在incorrect labeled 的情况。</p>\n<blockquote>\n<p>Tran and dev/test data may now come from slightly different distribution</p>\n</blockquote>\n<p>正如刚才讲的，DL对于training有一定程度的robust性，incorrect labeled data可能不会对training set带来这些问题，在这种情况下，我们可以不用去更正training set，这种情况，我们是可以接受的。</p>\n<h3 id=\"Build-up-quickly-and-iterate\"><a href=\"#Build-up-quickly-and-iterate\" class=\"headerlink\" title=\"Build up quickly and iterate\"></a>Build up quickly and iterate</h3><p>最后Ng用一个speech recognition作为例子，我们首先要分析出可能影响效果的一些因素：</p>\n<ul>\n<li>Noisy background</li>\n<li>Accented speech</li>\n<li>Far from microphone</li>\n<li>Young children’s speech</li>\n</ul>\n<p>…<br>针对这些问题，我们该如何构造我们的模型呢，Ng给出了建议</p>\n<ul>\n<li>Set up dev/test set and metric</li>\n<li>Build initial system quickly</li>\n<li>Use bias/variance analysis &amp; error analysis to prioritize next steps.</li>\n</ul>\n<p>总而言之，guideline是</p>\n<blockquote>\n<p>Build your first system quickly, then iterate.</p>\n</blockquote>\n<h2 id=\"Mismatched-training-and-dev-test-data\"><a href=\"#Mismatched-training-and-dev-test-data\" class=\"headerlink\" title=\"Mismatched training and dev/test data\"></a>Mismatched training and dev/test data</h2><h3 id=\"Training-and-testing-on-different-distributions\"><a href=\"#Training-and-testing-on-different-distributions\" class=\"headerlink\" title=\"Training and testing on different distributions\"></a>Training and testing on different distributions</h3><p>之前我们再三强调过一个尖锐的问题，那就是training/dev/test set一定要在同一个distribution下，但是实际上，愿望总是美好的而现实很残酷，我们总是会面对一些training and testing on different distribution问题。</p>\n<p>例如在猫识别的任务中，我们需要将这个模型部署到手机app上，我们手上的数据只有10k是从手机拍摄获得的，而有200k的数据是从网络上获得的，这两种图像显然不属于同一distribution，我们应该怎么办？</p>\n<p>首先来看option1，我们将所有的210k数据充分混合在一起，其中205k作为training set，2.5k作为dev，2.5作为test set。这样看起来是一个很不错的方法，但是，确实很不好的一个方法，为什么这么说呢？</p>\n<p>在整个过程中，dev/test set其实扮演了一个非常重要的角色，它决定了我们的target，也就是整体的优化方向。在这个例子中，我们要优化的方向是app上的图像，而这种data set分割方法和我们的task target并不符合，因此并不优秀。</p>\n<p>我们再来看option2，我们将200k的来自网络的图片全部放入training set，然后将10k的app数据，5k放入training set，2.5k作为dev，2.5作为test，这样做的话，dev/test决定的target 和我们的task target是一致的，所以长远来看，虽然option2的training/dev set并不是同一distribution，但是从长远看它的效果还是很不错的。</p>\n<h3 id=\"Bias-amp-variance-with-mismatched-data-distribution\"><a href=\"#Bias-amp-variance-with-mismatched-data-distribution\" class=\"headerlink\" title=\"Bias &amp; variance with mismatched data distribution\"></a>Bias &amp; variance with mismatched data distribution</h3><p>在training/dev/test set符合同一distribution的时候，我们通过比较training error和dev error就可以定性是否存在high variance的问题。但是，当training set和dev set不符合同一distribution的时候，这个判断就显得有些困难了。我们应该怎么处理呢？</p>\n<p>这时候，我们可以从training set中取出一小部分数据，命名为training-dev set，这部分数据将不再进行training，而是作为评判training效果的一个set，此时我们就有了training error，training-dev error和dev error三个error，再结合human error，training error和training-dev error之间的差值可以反映出模型是否有high bias或者variance，这样可以更科学的来评判模型效果。相应的，training-dev error和dev error相差越多，data mismatch的程度越大。</p>\n<h3 id=\"Addressing-data-mismatch\"><a href=\"#Addressing-data-mismatch\" class=\"headerlink\" title=\"Addressing data mismatch\"></a>Addressing data mismatch</h3><p>我们如何addressing data mismatch呢，首先我们来看看Ng的两条guideline：</p>\n<ul>\n<li>Carry out manual error analysis to try to understand difference between training and dev/test sets</li>\n<li>Make training data more similar;  or collect more data similar to dev/test sets</li>\n</ul>\n<p>理解一下，首先我们要通过人工的analysis去分析出造成training set和dev set之间distribution不同的原因，比如语音识别中的有无汽车噪声等等；然后我们需要根据这些差别，让training set和dev set更加的相似，甚至相通。</p>\n<p>但是要注意的是，我们在这个过程中，要避免出现overfitting的情况出现，例如Ng举出的例子，在识别车内的人声过程中，我们可以通过人工的合成汽车声音与人的声音让training set和dev set更加的相似，但是如果我们的只用一段汽车噪音循环往复的去做合成，例如吧1min的汽车噪声循环的合成到1h的人声中，那结果一定是不尽如人意的，因为出现了overfitting.</p>\n<h2 id=\"Transfer-learning\"><a href=\"#Transfer-learning\" class=\"headerlink\" title=\"Transfer learning\"></a>Transfer learning</h2><p>下面我们一起来看看大名鼎鼎的transfer learning，所谓transfer，就是存在一种从A到B的转换，而且这种情况往往是B的数据量很少，需要通过A来做一个pre-training过程。假设我们有如下的neural networks<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/10/10-3.png\" alt=\"\"><br>假设这个我们使用这个neural networks训练了一个image recognition模型，在训练完成后，我们将最后的output，以及output对应的的\\(w\\)和\\(b\\)也删除，更换成例如放射数据再进行训练，如下图：<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/10/10-4.png\" alt=\"\"><br>我们不仅仅可以把output层更换成一个新的output层，还可以将output层更换成几个新层。我们甚至可以将transfer之前的训练认为是一种pre-training，但是transfer training需要有几个条件：</p>\n<ul>\n<li>Task A and B have the same input x.</li>\n<li>You have a lot more data for Task A than Task B.</li>\n<li>Low level features from A could be helpful for learning B.</li>\n</ul>\n<h2 id=\"Multi-task-learning\"><a href=\"#Multi-task-learning\" class=\"headerlink\" title=\"Multi-task learning\"></a>Multi-task learning</h2><p>现在假设我们有一个自动驾驶的场景，我们需要从视频中识别行人、车辆、停车标志和红绿灯，按照常理来说，我们可以单独的构建4个模型。但是，这4个模型的特征场景都是很相似的，构建4个模型稍微有一些浪费，于是我们可以把这四个任务合并在一起，这就是Multi-task learning.</p>\n<p>在这里我们的标签\\(y\\)，就不再是一个m×1的矩阵了，而是一个m×4的矩阵，对于multi-task来说，在以下情况下是可行的：</p>\n<ul>\n<li>Training on a set of tasks that could benefit from having shared lower-level features.</li>\n<li>Usually: Amount of data you have for eachtask is quite similar.</li>\n<li>Can train a big enough neural network to do well on all the tasks.</li>\n</ul>\n<h2 id=\"End-to-end-learning\"><a href=\"#End-to-end-learning\" class=\"headerlink\" title=\"End to end learning\"></a>End to end learning</h2><p>End to end learning是随着DL兴起后而产生的一种learning方式，在end2end中，我们不再关注一些中间的步骤，例如feature selection或者image processing，我们只是把原始的数据和最后的结果告诉DL，它就可以自主的完成这个任务。</p>\n<p>当然end2end 也是有一些优势和劣势的，我们来看一下：<br>Pros：</p>\n<ul>\n<li>Let the data speak.</li>\n<li>Less hand-desgining of components needed.</li>\n</ul>\n<p>Cons:</p>\n<ul>\n<li>May need large amount of data.</li>\n<li>Excludes potentially userful hand-designed components.</li>\n</ul>\n<p>总之，对于end2end来说，大数据量，一定是最重要的因素，基于这一点，我们才可以摆脱传统的中间步骤，彻底实现end to end learning.</p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ul>\n<li><a href=\"https://www.coursera.org/specializations/deep-learning\" target=\"_blank\" rel=\"noopener\">Deep learning-Coursera Andrew Ng</a></li>\n<li><a href=\"https://mooc.study.163.com/course/deeplearning_ai-2001281003#/info\" target=\"_blank\" rel=\"noopener\">Deep learning-网易云课堂 Andrew Ng</a></li>\n</ul>"},{"title":"Learning Notes-Deep Learning, course3, week1","date":"2017-10-12T04:34:05.000Z","_content":"课程3主要讲的是deep learning中的一些strategy，这些strategy可以帮助我们快速的分析模型所存在的问题，避免我们的优化方向有偏差而导致的人力以及时间的浪费，这一点对于团队尤其重要。\n\n我们一起来recap一下week1的课程\n<!--more-->\n## Orthogonalization\n对于ML task来说，有众多因素影响最终的效果，这些因素相互犬牙交错，因此我们在提升模型效果的时候，一定要把所有的因素orthogonalization一下，Ng举的例子就很形象，就像显示器的控制按钮一样，每个按钮各司其职，一个控制高度，一个控制宽度，一个控制大小，一个控制梯度，通过各自调整每一个按钮，我们可以很好的完成画面调整。\n\n对于orthogonalization优化模型，Ng给出了4方面的建议：\n* Fit training set well in cost function\n* Fit development set well on cost function\n* Fit test set well on cost function\n* Performs well in real world\n\n我们详细来看看这四条：\n\n对于第一条，首先模型必须要对于training set有良好的拟合效果，如果这点达不到的话，模型一定是**high bias**，也就是**under fitting**了，那么我们必须尝试通过more complex的模型，bigger neural networks或者是longer training time去更充分的拟合training set.\n\n对于第二条，在很好的拟合training set的前提下，我们就要看看development set的效果了，如果对于development set fit效果不好的话，那基本上就是**high varience**，也就是**over fitting**的问题了，这时候，regularization或者more training data可以解决解决这个问题。\n\n对于第三条，在符合上两条的前提下，如果模型在test set上表现不佳，我们就需要更大的development set去涵盖更多的情况，并通过扩充后的development set重复第二条的检验\n\n对于最后一条，在符合上三条的前提下，如果模型在real world中表现不佳，那么很大程度上是因为我们的development 和test set与real world相差比较多，比如猫脸检测中，我们的data set都是高清的图像，但是real world 中，都是像素很低的图像。因此，我们需要让development 和test set更接近与real world，然后重复上面的步骤。\n\n通过这四个步骤，我们就可以通过orthogonalization来完成模型的调整\n\n## Metric\n### Single number evaluation\nMetric无疑是ML task中很重要的环节，通过metric，我们可以评估不同模型之间的优良差异，并且可以选择出最理想的模型。\n\n但是metric指标琳郎满目，例如对于两个模型，模型A的precision高于B的，但是A的recall又低于B，这时候就不太好评价两个模型，在这种情况下，我们需要采用单一的数字评价指标，例如我们可以用F1-score来进行评估，single number evaluation metric是我们做metrics时一定要注意的\n### Satisficing   and optimizing\n在某些情况下，例如我们不仅仅要求模型的指标，还对其他的，例如模型时间会有要求，如果一个模型有很高的模型accuracy，但是却很耗费时间，那是我们不能接受的，如下图例子：\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/9/9-1.png)\n图中的accuracy是optimizing metric，通常更高的accuracy就代表这classifier更加的优秀；但是，这里还有一个必须低于100ms 的running time作为satisficing metric，通常来说，如果我们有\\\\(N\\\\)个metrics，那么我们的optimizing metric必须只有一个，剩下的\\\\(N-1\\\\)metrics 都是satisficing metrics，只要以threshold形式进行限定就可以了。\n## Data set\n### Distributions\n对于training set，dev set和test set来说，所有data一定要保证服从同一个data distribution，例如猫脸实验，如果training set是高清大图而dev set是模糊图像，那么最终一定很难获得理想的metric.\n\n最重要的是，你所构建模型的data，一定和模型应用场景的data在同样的distribution下，Ng给出的guideline是\n> Choose a development set and test set to reflect data you expect to get in  the future and consider important to do well.\n\n### Size of data set\n传统的ML task中，dataset的分布一般如下图所示：\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/9/9-2.png)\n但是在big data时代，一般采用下图：\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/9/9-3.png)\nNg同样给出了guideline：\n* Set uop the size of test set to give a high confidence in the overall performance of the system.\n* Test set helps evaluate the proformance of the final classifier which could be less than 30% of the whole data set\n* The development set has to be big enough to evaluate different ideas\n\n## Change data set and metric\n### Change data set \n这个问题的原因，其实就是data set不在同一distribution下的问题，如果模型在dev和test set 上都有很好的表现和metric，但是在real world中效果并不好，那么我们要做的一定就是改变data set，让dev/test set与real world在同一distribution下\n### Change metric\n我们使用ML来解决现实中的问题时，metric也不是一成不变的，需要根据具体的情况做出一些改变，例如，色情图像识别中，我们误讲非色情识别维色情图像，是可以接受的，但是将色情图像识别成非色情图像则是不可接受的，相似的，风控系统中，将风险用户分类为正常用户的错误，比把正常用户分类为风险用户的错误要严重很多，因此，在类似的场景下，我们的metric需要随着业务场景做出一些改变。\n\n正常情况下，我们计算的模型error是：\n$$Error = \\frac{1}{m_{dev}} \\sum^{m_{dev}}_{i=1} \\mathcal \\{ \\hat{y}^{(i)} \\neq y^{(i)} \\}$$\n但是在上述场景中，我们需要加入一个权重，来对两种不同错误加以区分\n$$ w^{(i)}=\\left\\{\n\\begin{aligned}\n1 \\quad x^{(i)}notpron \\\\\n10 \\quad x^{(i)} pron \\\\\n\\end{aligned}\n\\right.\n$$\n$$Error = \\frac{1}{ \\sum w^{(i)}} \\sum^{m_{dev}}_{i=1} w^{(i)} \\mathcal \\{ \\hat{y}^{(i)} \\neq y^{(i)} \\}$$\n这样就把两种不同的问题区分开了。\n## Improve model performance\n模型performance的提升是模型的核心问题，我们如何确定模型调整的大体方向，Ng给出了如下的图\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/9/9-4.png)\n我们默认human-level是很接近理论误差，也就是Bayes error，我们需要比较human-level，training error和dev error这三者之间的关系，human-level和training error之间的差值更大的话，我们就需要去减小bias，反之，我们需要去减少variance，具体的方法，还是我们之前的老套路。\n\n## Reference\n* [Deep learning-Coursera Andrew Ng](https://www.coursera.org/specializations/deep-learning)\n* [Deep learning-网易云课堂 Andrew Ng](https://mooc.study.163.com/course/deeplearning_ai-2001281003#/info)\n","source":"_posts/course-deep-learning-course3-week1.md","raw":"---\ntitle: Learning Notes-Deep Learning, course3, week1\ndate: 2017-10-12 12:34:05\ntags: \n\t- learning strategy\n\t- orthogonalization\ncategories: learning notes\n---\n课程3主要讲的是deep learning中的一些strategy，这些strategy可以帮助我们快速的分析模型所存在的问题，避免我们的优化方向有偏差而导致的人力以及时间的浪费，这一点对于团队尤其重要。\n\n我们一起来recap一下week1的课程\n<!--more-->\n## Orthogonalization\n对于ML task来说，有众多因素影响最终的效果，这些因素相互犬牙交错，因此我们在提升模型效果的时候，一定要把所有的因素orthogonalization一下，Ng举的例子就很形象，就像显示器的控制按钮一样，每个按钮各司其职，一个控制高度，一个控制宽度，一个控制大小，一个控制梯度，通过各自调整每一个按钮，我们可以很好的完成画面调整。\n\n对于orthogonalization优化模型，Ng给出了4方面的建议：\n* Fit training set well in cost function\n* Fit development set well on cost function\n* Fit test set well on cost function\n* Performs well in real world\n\n我们详细来看看这四条：\n\n对于第一条，首先模型必须要对于training set有良好的拟合效果，如果这点达不到的话，模型一定是**high bias**，也就是**under fitting**了，那么我们必须尝试通过more complex的模型，bigger neural networks或者是longer training time去更充分的拟合training set.\n\n对于第二条，在很好的拟合training set的前提下，我们就要看看development set的效果了，如果对于development set fit效果不好的话，那基本上就是**high varience**，也就是**over fitting**的问题了，这时候，regularization或者more training data可以解决解决这个问题。\n\n对于第三条，在符合上两条的前提下，如果模型在test set上表现不佳，我们就需要更大的development set去涵盖更多的情况，并通过扩充后的development set重复第二条的检验\n\n对于最后一条，在符合上三条的前提下，如果模型在real world中表现不佳，那么很大程度上是因为我们的development 和test set与real world相差比较多，比如猫脸检测中，我们的data set都是高清的图像，但是real world 中，都是像素很低的图像。因此，我们需要让development 和test set更接近与real world，然后重复上面的步骤。\n\n通过这四个步骤，我们就可以通过orthogonalization来完成模型的调整\n\n## Metric\n### Single number evaluation\nMetric无疑是ML task中很重要的环节，通过metric，我们可以评估不同模型之间的优良差异，并且可以选择出最理想的模型。\n\n但是metric指标琳郎满目，例如对于两个模型，模型A的precision高于B的，但是A的recall又低于B，这时候就不太好评价两个模型，在这种情况下，我们需要采用单一的数字评价指标，例如我们可以用F1-score来进行评估，single number evaluation metric是我们做metrics时一定要注意的\n### Satisficing   and optimizing\n在某些情况下，例如我们不仅仅要求模型的指标，还对其他的，例如模型时间会有要求，如果一个模型有很高的模型accuracy，但是却很耗费时间，那是我们不能接受的，如下图例子：\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/9/9-1.png)\n图中的accuracy是optimizing metric，通常更高的accuracy就代表这classifier更加的优秀；但是，这里还有一个必须低于100ms 的running time作为satisficing metric，通常来说，如果我们有\\\\(N\\\\)个metrics，那么我们的optimizing metric必须只有一个，剩下的\\\\(N-1\\\\)metrics 都是satisficing metrics，只要以threshold形式进行限定就可以了。\n## Data set\n### Distributions\n对于training set，dev set和test set来说，所有data一定要保证服从同一个data distribution，例如猫脸实验，如果training set是高清大图而dev set是模糊图像，那么最终一定很难获得理想的metric.\n\n最重要的是，你所构建模型的data，一定和模型应用场景的data在同样的distribution下，Ng给出的guideline是\n> Choose a development set and test set to reflect data you expect to get in  the future and consider important to do well.\n\n### Size of data set\n传统的ML task中，dataset的分布一般如下图所示：\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/9/9-2.png)\n但是在big data时代，一般采用下图：\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/9/9-3.png)\nNg同样给出了guideline：\n* Set uop the size of test set to give a high confidence in the overall performance of the system.\n* Test set helps evaluate the proformance of the final classifier which could be less than 30% of the whole data set\n* The development set has to be big enough to evaluate different ideas\n\n## Change data set and metric\n### Change data set \n这个问题的原因，其实就是data set不在同一distribution下的问题，如果模型在dev和test set 上都有很好的表现和metric，但是在real world中效果并不好，那么我们要做的一定就是改变data set，让dev/test set与real world在同一distribution下\n### Change metric\n我们使用ML来解决现实中的问题时，metric也不是一成不变的，需要根据具体的情况做出一些改变，例如，色情图像识别中，我们误讲非色情识别维色情图像，是可以接受的，但是将色情图像识别成非色情图像则是不可接受的，相似的，风控系统中，将风险用户分类为正常用户的错误，比把正常用户分类为风险用户的错误要严重很多，因此，在类似的场景下，我们的metric需要随着业务场景做出一些改变。\n\n正常情况下，我们计算的模型error是：\n$$Error = \\frac{1}{m_{dev}} \\sum^{m_{dev}}_{i=1} \\mathcal \\{ \\hat{y}^{(i)} \\neq y^{(i)} \\}$$\n但是在上述场景中，我们需要加入一个权重，来对两种不同错误加以区分\n$$ w^{(i)}=\\left\\{\n\\begin{aligned}\n1 \\quad x^{(i)}notpron \\\\\n10 \\quad x^{(i)} pron \\\\\n\\end{aligned}\n\\right.\n$$\n$$Error = \\frac{1}{ \\sum w^{(i)}} \\sum^{m_{dev}}_{i=1} w^{(i)} \\mathcal \\{ \\hat{y}^{(i)} \\neq y^{(i)} \\}$$\n这样就把两种不同的问题区分开了。\n## Improve model performance\n模型performance的提升是模型的核心问题，我们如何确定模型调整的大体方向，Ng给出了如下的图\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/9/9-4.png)\n我们默认human-level是很接近理论误差，也就是Bayes error，我们需要比较human-level，training error和dev error这三者之间的关系，human-level和training error之间的差值更大的话，我们就需要去减小bias，反之，我们需要去减少variance，具体的方法，还是我们之前的老套路。\n\n## Reference\n* [Deep learning-Coursera Andrew Ng](https://www.coursera.org/specializations/deep-learning)\n* [Deep learning-网易云课堂 Andrew Ng](https://mooc.study.163.com/course/deeplearning_ai-2001281003#/info)\n","slug":"course-deep-learning-course3-week1","published":1,"updated":"2020-05-10T06:50:12.528Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cka0wnkw20006qxot88x4of75","content":"<p>课程3主要讲的是deep learning中的一些strategy，这些strategy可以帮助我们快速的分析模型所存在的问题，避免我们的优化方向有偏差而导致的人力以及时间的浪费，这一点对于团队尤其重要。</p>\n<p>我们一起来recap一下week1的课程<br><a id=\"more\"></a></p>\n<h2 id=\"Orthogonalization\"><a href=\"#Orthogonalization\" class=\"headerlink\" title=\"Orthogonalization\"></a>Orthogonalization</h2><p>对于ML task来说，有众多因素影响最终的效果，这些因素相互犬牙交错，因此我们在提升模型效果的时候，一定要把所有的因素orthogonalization一下，Ng举的例子就很形象，就像显示器的控制按钮一样，每个按钮各司其职，一个控制高度，一个控制宽度，一个控制大小，一个控制梯度，通过各自调整每一个按钮，我们可以很好的完成画面调整。</p>\n<p>对于orthogonalization优化模型，Ng给出了4方面的建议：</p>\n<ul>\n<li>Fit training set well in cost function</li>\n<li>Fit development set well on cost function</li>\n<li>Fit test set well on cost function</li>\n<li>Performs well in real world</li>\n</ul>\n<p>我们详细来看看这四条：</p>\n<p>对于第一条，首先模型必须要对于training set有良好的拟合效果，如果这点达不到的话，模型一定是<strong>high bias</strong>，也就是<strong>under fitting</strong>了，那么我们必须尝试通过more complex的模型，bigger neural networks或者是longer training time去更充分的拟合training set.</p>\n<p>对于第二条，在很好的拟合training set的前提下，我们就要看看development set的效果了，如果对于development set fit效果不好的话，那基本上就是<strong>high varience</strong>，也就是<strong>over fitting</strong>的问题了，这时候，regularization或者more training data可以解决解决这个问题。</p>\n<p>对于第三条，在符合上两条的前提下，如果模型在test set上表现不佳，我们就需要更大的development set去涵盖更多的情况，并通过扩充后的development set重复第二条的检验</p>\n<p>对于最后一条，在符合上三条的前提下，如果模型在real world中表现不佳，那么很大程度上是因为我们的development 和test set与real world相差比较多，比如猫脸检测中，我们的data set都是高清的图像，但是real world 中，都是像素很低的图像。因此，我们需要让development 和test set更接近与real world，然后重复上面的步骤。</p>\n<p>通过这四个步骤，我们就可以通过orthogonalization来完成模型的调整</p>\n<h2 id=\"Metric\"><a href=\"#Metric\" class=\"headerlink\" title=\"Metric\"></a>Metric</h2><h3 id=\"Single-number-evaluation\"><a href=\"#Single-number-evaluation\" class=\"headerlink\" title=\"Single number evaluation\"></a>Single number evaluation</h3><p>Metric无疑是ML task中很重要的环节，通过metric，我们可以评估不同模型之间的优良差异，并且可以选择出最理想的模型。</p>\n<p>但是metric指标琳郎满目，例如对于两个模型，模型A的precision高于B的，但是A的recall又低于B，这时候就不太好评价两个模型，在这种情况下，我们需要采用单一的数字评价指标，例如我们可以用F1-score来进行评估，single number evaluation metric是我们做metrics时一定要注意的</p>\n<h3 id=\"Satisficing-and-optimizing\"><a href=\"#Satisficing-and-optimizing\" class=\"headerlink\" title=\"Satisficing   and optimizing\"></a>Satisficing   and optimizing</h3><p>在某些情况下，例如我们不仅仅要求模型的指标，还对其他的，例如模型时间会有要求，如果一个模型有很高的模型accuracy，但是却很耗费时间，那是我们不能接受的，如下图例子：<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/9/9-1.png\" alt=\"\"><br>图中的accuracy是optimizing metric，通常更高的accuracy就代表这classifier更加的优秀；但是，这里还有一个必须低于100ms 的running time作为satisficing metric，通常来说，如果我们有\\(N\\)个metrics，那么我们的optimizing metric必须只有一个，剩下的\\(N-1\\)metrics 都是satisficing metrics，只要以threshold形式进行限定就可以了。</p>\n<h2 id=\"Data-set\"><a href=\"#Data-set\" class=\"headerlink\" title=\"Data set\"></a>Data set</h2><h3 id=\"Distributions\"><a href=\"#Distributions\" class=\"headerlink\" title=\"Distributions\"></a>Distributions</h3><p>对于training set，dev set和test set来说，所有data一定要保证服从同一个data distribution，例如猫脸实验，如果training set是高清大图而dev set是模糊图像，那么最终一定很难获得理想的metric.</p>\n<p>最重要的是，你所构建模型的data，一定和模型应用场景的data在同样的distribution下，Ng给出的guideline是</p>\n<blockquote>\n<p>Choose a development set and test set to reflect data you expect to get in  the future and consider important to do well.</p>\n</blockquote>\n<h3 id=\"Size-of-data-set\"><a href=\"#Size-of-data-set\" class=\"headerlink\" title=\"Size of data set\"></a>Size of data set</h3><p>传统的ML task中，dataset的分布一般如下图所示：<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/9/9-2.png\" alt=\"\"><br>但是在big data时代，一般采用下图：<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/9/9-3.png\" alt=\"\"><br>Ng同样给出了guideline：</p>\n<ul>\n<li>Set uop the size of test set to give a high confidence in the overall performance of the system.</li>\n<li>Test set helps evaluate the proformance of the final classifier which could be less than 30% of the whole data set</li>\n<li>The development set has to be big enough to evaluate different ideas</li>\n</ul>\n<h2 id=\"Change-data-set-and-metric\"><a href=\"#Change-data-set-and-metric\" class=\"headerlink\" title=\"Change data set and metric\"></a>Change data set and metric</h2><h3 id=\"Change-data-set\"><a href=\"#Change-data-set\" class=\"headerlink\" title=\"Change data set\"></a>Change data set</h3><p>这个问题的原因，其实就是data set不在同一distribution下的问题，如果模型在dev和test set 上都有很好的表现和metric，但是在real world中效果并不好，那么我们要做的一定就是改变data set，让dev/test set与real world在同一distribution下</p>\n<h3 id=\"Change-metric\"><a href=\"#Change-metric\" class=\"headerlink\" title=\"Change metric\"></a>Change metric</h3><p>我们使用ML来解决现实中的问题时，metric也不是一成不变的，需要根据具体的情况做出一些改变，例如，色情图像识别中，我们误讲非色情识别维色情图像，是可以接受的，但是将色情图像识别成非色情图像则是不可接受的，相似的，风控系统中，将风险用户分类为正常用户的错误，比把正常用户分类为风险用户的错误要严重很多，因此，在类似的场景下，我们的metric需要随着业务场景做出一些改变。</p>\n<p>正常情况下，我们计算的模型error是：<br>$$Error = \\frac{1}{m_{dev}} \\sum^{m_{dev}}<em>{i=1} \\mathcal { \\hat{y}^{(i)} \\neq y^{(i)} }$$<br>但是在上述场景中，我们需要加入一个权重，来对两种不同错误加以区分<br>$$ w^{(i)}=\\left{<br>\\begin{aligned}<br>1 \\quad x^{(i)}notpron \\<br>10 \\quad x^{(i)} pron \\<br>\\end{aligned}<br>\\right.<br>$$<br>$$Error = \\frac{1}{ \\sum w^{(i)}} \\sum^{m</em>{dev}}_{i=1} w^{(i)} \\mathcal { \\hat{y}^{(i)} \\neq y^{(i)} }$$<br>这样就把两种不同的问题区分开了。</p>\n<h2 id=\"Improve-model-performance\"><a href=\"#Improve-model-performance\" class=\"headerlink\" title=\"Improve model performance\"></a>Improve model performance</h2><p>模型performance的提升是模型的核心问题，我们如何确定模型调整的大体方向，Ng给出了如下的图<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/9/9-4.png\" alt=\"\"><br>我们默认human-level是很接近理论误差，也就是Bayes error，我们需要比较human-level，training error和dev error这三者之间的关系，human-level和training error之间的差值更大的话，我们就需要去减小bias，反之，我们需要去减少variance，具体的方法，还是我们之前的老套路。</p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ul>\n<li><a href=\"https://www.coursera.org/specializations/deep-learning\" target=\"_blank\" rel=\"noopener\">Deep learning-Coursera Andrew Ng</a></li>\n<li><a href=\"https://mooc.study.163.com/course/deeplearning_ai-2001281003#/info\" target=\"_blank\" rel=\"noopener\">Deep learning-网易云课堂 Andrew Ng</a></li>\n</ul>\n","site":{"data":{}},"_categories":[{"name":"learning notes","path":"categories/learning-notes/"}],"_tags":[{"name":"learning strategy","path":"tags/learning-strategy/"},{"name":"orthogonalization","path":"tags/orthogonalization/"}],"excerpt":"<p>课程3主要讲的是deep learning中的一些strategy，这些strategy可以帮助我们快速的分析模型所存在的问题，避免我们的优化方向有偏差而导致的人力以及时间的浪费，这一点对于团队尤其重要。</p>\n<p>我们一起来recap一下week1的课程<br></p>","more":"</p>\n<h2 id=\"Orthogonalization\"><a href=\"#Orthogonalization\" class=\"headerlink\" title=\"Orthogonalization\"></a>Orthogonalization</h2><p>对于ML task来说，有众多因素影响最终的效果，这些因素相互犬牙交错，因此我们在提升模型效果的时候，一定要把所有的因素orthogonalization一下，Ng举的例子就很形象，就像显示器的控制按钮一样，每个按钮各司其职，一个控制高度，一个控制宽度，一个控制大小，一个控制梯度，通过各自调整每一个按钮，我们可以很好的完成画面调整。</p>\n<p>对于orthogonalization优化模型，Ng给出了4方面的建议：</p>\n<ul>\n<li>Fit training set well in cost function</li>\n<li>Fit development set well on cost function</li>\n<li>Fit test set well on cost function</li>\n<li>Performs well in real world</li>\n</ul>\n<p>我们详细来看看这四条：</p>\n<p>对于第一条，首先模型必须要对于training set有良好的拟合效果，如果这点达不到的话，模型一定是<strong>high bias</strong>，也就是<strong>under fitting</strong>了，那么我们必须尝试通过more complex的模型，bigger neural networks或者是longer training time去更充分的拟合training set.</p>\n<p>对于第二条，在很好的拟合training set的前提下，我们就要看看development set的效果了，如果对于development set fit效果不好的话，那基本上就是<strong>high varience</strong>，也就是<strong>over fitting</strong>的问题了，这时候，regularization或者more training data可以解决解决这个问题。</p>\n<p>对于第三条，在符合上两条的前提下，如果模型在test set上表现不佳，我们就需要更大的development set去涵盖更多的情况，并通过扩充后的development set重复第二条的检验</p>\n<p>对于最后一条，在符合上三条的前提下，如果模型在real world中表现不佳，那么很大程度上是因为我们的development 和test set与real world相差比较多，比如猫脸检测中，我们的data set都是高清的图像，但是real world 中，都是像素很低的图像。因此，我们需要让development 和test set更接近与real world，然后重复上面的步骤。</p>\n<p>通过这四个步骤，我们就可以通过orthogonalization来完成模型的调整</p>\n<h2 id=\"Metric\"><a href=\"#Metric\" class=\"headerlink\" title=\"Metric\"></a>Metric</h2><h3 id=\"Single-number-evaluation\"><a href=\"#Single-number-evaluation\" class=\"headerlink\" title=\"Single number evaluation\"></a>Single number evaluation</h3><p>Metric无疑是ML task中很重要的环节，通过metric，我们可以评估不同模型之间的优良差异，并且可以选择出最理想的模型。</p>\n<p>但是metric指标琳郎满目，例如对于两个模型，模型A的precision高于B的，但是A的recall又低于B，这时候就不太好评价两个模型，在这种情况下，我们需要采用单一的数字评价指标，例如我们可以用F1-score来进行评估，single number evaluation metric是我们做metrics时一定要注意的</p>\n<h3 id=\"Satisficing-and-optimizing\"><a href=\"#Satisficing-and-optimizing\" class=\"headerlink\" title=\"Satisficing   and optimizing\"></a>Satisficing   and optimizing</h3><p>在某些情况下，例如我们不仅仅要求模型的指标，还对其他的，例如模型时间会有要求，如果一个模型有很高的模型accuracy，但是却很耗费时间，那是我们不能接受的，如下图例子：<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/9/9-1.png\" alt=\"\"><br>图中的accuracy是optimizing metric，通常更高的accuracy就代表这classifier更加的优秀；但是，这里还有一个必须低于100ms 的running time作为satisficing metric，通常来说，如果我们有\\(N\\)个metrics，那么我们的optimizing metric必须只有一个，剩下的\\(N-1\\)metrics 都是satisficing metrics，只要以threshold形式进行限定就可以了。</p>\n<h2 id=\"Data-set\"><a href=\"#Data-set\" class=\"headerlink\" title=\"Data set\"></a>Data set</h2><h3 id=\"Distributions\"><a href=\"#Distributions\" class=\"headerlink\" title=\"Distributions\"></a>Distributions</h3><p>对于training set，dev set和test set来说，所有data一定要保证服从同一个data distribution，例如猫脸实验，如果training set是高清大图而dev set是模糊图像，那么最终一定很难获得理想的metric.</p>\n<p>最重要的是，你所构建模型的data，一定和模型应用场景的data在同样的distribution下，Ng给出的guideline是</p>\n<blockquote>\n<p>Choose a development set and test set to reflect data you expect to get in  the future and consider important to do well.</p>\n</blockquote>\n<h3 id=\"Size-of-data-set\"><a href=\"#Size-of-data-set\" class=\"headerlink\" title=\"Size of data set\"></a>Size of data set</h3><p>传统的ML task中，dataset的分布一般如下图所示：<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/9/9-2.png\" alt=\"\"><br>但是在big data时代，一般采用下图：<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/9/9-3.png\" alt=\"\"><br>Ng同样给出了guideline：</p>\n<ul>\n<li>Set uop the size of test set to give a high confidence in the overall performance of the system.</li>\n<li>Test set helps evaluate the proformance of the final classifier which could be less than 30% of the whole data set</li>\n<li>The development set has to be big enough to evaluate different ideas</li>\n</ul>\n<h2 id=\"Change-data-set-and-metric\"><a href=\"#Change-data-set-and-metric\" class=\"headerlink\" title=\"Change data set and metric\"></a>Change data set and metric</h2><h3 id=\"Change-data-set\"><a href=\"#Change-data-set\" class=\"headerlink\" title=\"Change data set\"></a>Change data set</h3><p>这个问题的原因，其实就是data set不在同一distribution下的问题，如果模型在dev和test set 上都有很好的表现和metric，但是在real world中效果并不好，那么我们要做的一定就是改变data set，让dev/test set与real world在同一distribution下</p>\n<h3 id=\"Change-metric\"><a href=\"#Change-metric\" class=\"headerlink\" title=\"Change metric\"></a>Change metric</h3><p>我们使用ML来解决现实中的问题时，metric也不是一成不变的，需要根据具体的情况做出一些改变，例如，色情图像识别中，我们误讲非色情识别维色情图像，是可以接受的，但是将色情图像识别成非色情图像则是不可接受的，相似的，风控系统中，将风险用户分类为正常用户的错误，比把正常用户分类为风险用户的错误要严重很多，因此，在类似的场景下，我们的metric需要随着业务场景做出一些改变。</p>\n<p>正常情况下，我们计算的模型error是：<br>$$Error = \\frac{1}{m_{dev}} \\sum^{m_{dev}}<em>{i=1} \\mathcal { \\hat{y}^{(i)} \\neq y^{(i)} }$$<br>但是在上述场景中，我们需要加入一个权重，来对两种不同错误加以区分<br>$$ w^{(i)}=\\left{<br>\\begin{aligned}<br>1 \\quad x^{(i)}notpron \\<br>10 \\quad x^{(i)} pron \\<br>\\end{aligned}<br>\\right.<br>$$<br>$$Error = \\frac{1}{ \\sum w^{(i)}} \\sum^{m</em>{dev}}_{i=1} w^{(i)} \\mathcal { \\hat{y}^{(i)} \\neq y^{(i)} }$$<br>这样就把两种不同的问题区分开了。</p>\n<h2 id=\"Improve-model-performance\"><a href=\"#Improve-model-performance\" class=\"headerlink\" title=\"Improve model performance\"></a>Improve model performance</h2><p>模型performance的提升是模型的核心问题，我们如何确定模型调整的大体方向，Ng给出了如下的图<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/9/9-4.png\" alt=\"\"><br>我们默认human-level是很接近理论误差，也就是Bayes error，我们需要比较human-level，training error和dev error这三者之间的关系，human-level和training error之间的差值更大的话，我们就需要去减小bias，反之，我们需要去减少variance，具体的方法，还是我们之前的老套路。</p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ul>\n<li><a href=\"https://www.coursera.org/specializations/deep-learning\" target=\"_blank\" rel=\"noopener\">Deep learning-Coursera Andrew Ng</a></li>\n<li><a href=\"https://mooc.study.163.com/course/deeplearning_ai-2001281003#/info\" target=\"_blank\" rel=\"noopener\">Deep learning-网易云课堂 Andrew Ng</a></li>\n</ul>"},{"title":"Learning Notes-Deep Learning, course4, week1","date":"2017-11-26T12:30:47.000Z","_content":"Hi, all. 最近开始休假了，可以有空继续自己的学习，一方面补一补前面的作业，一方面继续自己的学习，今天我们来到了course4，也就是convolutional neural networks 的内容。我们一起来看看！\n<!--more-->\n## Convolution\n在课程中，Ng从edge detection的角度来给大家讲了讲convolution，因为本人是image processing出身，所以认为Ng在这里讲的还是很浅显易懂的，我就不再专门的markdown。主要来看看convolution中的一些技巧。\n### Padding\n我们都知道，在最纯粹的convolution中，我们假设原image尺寸是\\\\(n \\*n\\\\)，convolution filter尺寸是\\\\(f \\*f\\\\)，那么最终的结果image尺寸应该是\\\\( (n-f+1) \\*(n-f+1)\\\\)，也就是说，结果的尺寸变小了。如果想让输出image的尺寸不发生改变，那么我们就要使用大名鼎鼎的padding了。\n\nPadding其实就是表示，在原始image中，向外扩大多少尺寸，一般我们会使用简单复制相邻元素值的方法进行扩充。假设对于一个\\\\(6 \\*6\\\\)的原始image，采用\\\\(3 \\*3\\\\)的filter，加上\\\\\\(p=1\\\\)的padding，那么原始图像尺寸变成了\\\\(8 \\* 8\\\\)，结果变成了\\\\(6 \\*6\\\\)，原始image和结果image一模一样！于是加入了padding的convolution公式就成了\\\\( (n+2p-f+1) \\*(n+2p-f+1)\\\\).\n\n在这里Ng引入了两个概念，valid和same convolutions，所谓valid convolution，就是没有padding 的convolution；所谓same convolution，就是输入输出的尺寸完全一样。\n\n### Stride\n继padding之后，还有一个很重要的参数，就是步长stride，步长stride决定了filter做convolution时候的步长，如果stride=1，那么filter就会挨着计算，如果stride=2，那么就会跳跃这进行计算。\n\n总结一下，假设原image尺寸是\\\\(n \\*n\\\\)，convolution filter尺寸是\\\\(f \\*f\\\\)，padding值是\\\\(p\\\\)，stride值是\\\\(s\\\\)那么最终的结果image尺寸应该是\\\\( ( \\frac {n+2p-f}{s}+1) \\*( \\frac {n+2p-f}{s}+1)\\\\)，如果除不尽的话，我们选择向下取整，也就是不足以做convolution的区域，我们选择放弃。\n\n##Convolution over Volume\n对于一般的图像处理，我们使用的都是RGB图像，我们都知道，RGB图像有三个channel，这种情况下，convolution应该如何做，我们来看下面的图：\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/13/13-1.png)\n假设我们的图像是6×6×3，也就是hight×width×channel(depth)，因此对应的filter也要有3的channel(depth)，最后可以得到一个4×4的结果。\n\n当然，我们可以采用不止一个filter，如图\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/13/13-2.png)\n我们加入了两个不同的filter，他们的大小都是3×3×3，于是最终的结果就是4×4×2，请注意：结果的channel数目取决于filter的个数，而和输入的channel没有任何关系。\n\n## CNN\n### Convolution Layer\n下面我们来看看CNN网络中的一个layer的工作原理是什么，首先来看截图：\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/13/13-3.png)\n这张图十分复杂，我们一起仔细看看这张图，对于一个6×6×3的RGB图像，我们用了两个3×3×3的filter，我们可以把输入image看做\\\\(x\\\\)，也就是\\\\(a ^{[0]}\\\\)，filter看做\\\\(w ^{[1]}\\\\)，得到的结果就是\\\\(w ^{[1]}a ^{[0]}\\\\)，我们再加上一个bias项\\\\(b^{[1]}\\\\)，那么就获得了一个liner output\\\\(w ^{[1]}a ^{[0]}+b^{[1]}\\\\)，我们再使用一个non-liner function例如ReLU，如此获得一个4×4×2的output。如此就是CNN的一个layer.\n\n如此我们可以看到，CNN和之前的DNN实质上都存在一种liner function到non-liner function的转化，通过non-liner function去classify线性不可分的data，另外，在CNN中，每一个filter就可以获得一个不同的feature，多个filter可以让我们从多个角度去classify data.\n\n另外，相比较于fully connected 的DNN，CNN所需要的parameters也少了很多，这一点值得我们注意。\n\n### Pooling\nPooling原理还是很简单的，我们来看一张图：\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/13/13-4.png)\n首先我们来看看max pooling，如图，我们取filter尺寸\\\\(f=2\\\\)，stride大小\\\\(s=2\\\\)，对于一个filter中的元素，我们取max作为输出；相对应的，如果我们取average，那么就成了average pooling，pooling中的hyperparameter只有filter尺寸\\\\(f\\\\)和stride大小\\\\(s\\\\)，值得注意的是，pooling过程中不存在学习过程，no parameters to learn!\n\n### Fully Connected layer\nFully connected layer在CNN其实很简单，我们只需要将input展开，按照DNN的方法进行fully connected就可以了。\n\n一般情况下，我们认为有prameter变化的才算一层，因此我们不认为pooling是一个layer，我们举个一个最简单的CNN做例子：CONV-POOL-CONV-POOL-FC-FC-Softmax，我们这就是一个简单的5层的CNN，在下周的课程中我们可以看到一些经典的CNN框架，这里就不再复述。\n\n## Why Convolutions\n关于这个问题，Ng给出了两个意见，我们一起看看：\n> **Parameter sharing**: A feature detector (such as a vertical edge detetor) that's useful in one part of image is probably useful in another part of the image.\n**Sparsity of connections**: In  each layer, each output value depends only on a small number of inputs.\n\n## Reference\n* [Deep learning-Coursera Andrew Ng](hhttps://www.coursera.org/learn/convolutional-neural-networks)\n* [Deep learning-网易云课堂 Andrew Ng](https://mooc.study.163.com/course/2001281004#/info)\n","source":"_posts/course-deep-learning-course4-week1.md","raw":"---\ntitle: Learning Notes-Deep Learning, course4, week1\ndate: 2017-11-26 20:30:47\ntags: CNN\ncategories: learning notes\n---\nHi, all. 最近开始休假了，可以有空继续自己的学习，一方面补一补前面的作业，一方面继续自己的学习，今天我们来到了course4，也就是convolutional neural networks 的内容。我们一起来看看！\n<!--more-->\n## Convolution\n在课程中，Ng从edge detection的角度来给大家讲了讲convolution，因为本人是image processing出身，所以认为Ng在这里讲的还是很浅显易懂的，我就不再专门的markdown。主要来看看convolution中的一些技巧。\n### Padding\n我们都知道，在最纯粹的convolution中，我们假设原image尺寸是\\\\(n \\*n\\\\)，convolution filter尺寸是\\\\(f \\*f\\\\)，那么最终的结果image尺寸应该是\\\\( (n-f+1) \\*(n-f+1)\\\\)，也就是说，结果的尺寸变小了。如果想让输出image的尺寸不发生改变，那么我们就要使用大名鼎鼎的padding了。\n\nPadding其实就是表示，在原始image中，向外扩大多少尺寸，一般我们会使用简单复制相邻元素值的方法进行扩充。假设对于一个\\\\(6 \\*6\\\\)的原始image，采用\\\\(3 \\*3\\\\)的filter，加上\\\\\\(p=1\\\\)的padding，那么原始图像尺寸变成了\\\\(8 \\* 8\\\\)，结果变成了\\\\(6 \\*6\\\\)，原始image和结果image一模一样！于是加入了padding的convolution公式就成了\\\\( (n+2p-f+1) \\*(n+2p-f+1)\\\\).\n\n在这里Ng引入了两个概念，valid和same convolutions，所谓valid convolution，就是没有padding 的convolution；所谓same convolution，就是输入输出的尺寸完全一样。\n\n### Stride\n继padding之后，还有一个很重要的参数，就是步长stride，步长stride决定了filter做convolution时候的步长，如果stride=1，那么filter就会挨着计算，如果stride=2，那么就会跳跃这进行计算。\n\n总结一下，假设原image尺寸是\\\\(n \\*n\\\\)，convolution filter尺寸是\\\\(f \\*f\\\\)，padding值是\\\\(p\\\\)，stride值是\\\\(s\\\\)那么最终的结果image尺寸应该是\\\\( ( \\frac {n+2p-f}{s}+1) \\*( \\frac {n+2p-f}{s}+1)\\\\)，如果除不尽的话，我们选择向下取整，也就是不足以做convolution的区域，我们选择放弃。\n\n##Convolution over Volume\n对于一般的图像处理，我们使用的都是RGB图像，我们都知道，RGB图像有三个channel，这种情况下，convolution应该如何做，我们来看下面的图：\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/13/13-1.png)\n假设我们的图像是6×6×3，也就是hight×width×channel(depth)，因此对应的filter也要有3的channel(depth)，最后可以得到一个4×4的结果。\n\n当然，我们可以采用不止一个filter，如图\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/13/13-2.png)\n我们加入了两个不同的filter，他们的大小都是3×3×3，于是最终的结果就是4×4×2，请注意：结果的channel数目取决于filter的个数，而和输入的channel没有任何关系。\n\n## CNN\n### Convolution Layer\n下面我们来看看CNN网络中的一个layer的工作原理是什么，首先来看截图：\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/13/13-3.png)\n这张图十分复杂，我们一起仔细看看这张图，对于一个6×6×3的RGB图像，我们用了两个3×3×3的filter，我们可以把输入image看做\\\\(x\\\\)，也就是\\\\(a ^{[0]}\\\\)，filter看做\\\\(w ^{[1]}\\\\)，得到的结果就是\\\\(w ^{[1]}a ^{[0]}\\\\)，我们再加上一个bias项\\\\(b^{[1]}\\\\)，那么就获得了一个liner output\\\\(w ^{[1]}a ^{[0]}+b^{[1]}\\\\)，我们再使用一个non-liner function例如ReLU，如此获得一个4×4×2的output。如此就是CNN的一个layer.\n\n如此我们可以看到，CNN和之前的DNN实质上都存在一种liner function到non-liner function的转化，通过non-liner function去classify线性不可分的data，另外，在CNN中，每一个filter就可以获得一个不同的feature，多个filter可以让我们从多个角度去classify data.\n\n另外，相比较于fully connected 的DNN，CNN所需要的parameters也少了很多，这一点值得我们注意。\n\n### Pooling\nPooling原理还是很简单的，我们来看一张图：\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/13/13-4.png)\n首先我们来看看max pooling，如图，我们取filter尺寸\\\\(f=2\\\\)，stride大小\\\\(s=2\\\\)，对于一个filter中的元素，我们取max作为输出；相对应的，如果我们取average，那么就成了average pooling，pooling中的hyperparameter只有filter尺寸\\\\(f\\\\)和stride大小\\\\(s\\\\)，值得注意的是，pooling过程中不存在学习过程，no parameters to learn!\n\n### Fully Connected layer\nFully connected layer在CNN其实很简单，我们只需要将input展开，按照DNN的方法进行fully connected就可以了。\n\n一般情况下，我们认为有prameter变化的才算一层，因此我们不认为pooling是一个layer，我们举个一个最简单的CNN做例子：CONV-POOL-CONV-POOL-FC-FC-Softmax，我们这就是一个简单的5层的CNN，在下周的课程中我们可以看到一些经典的CNN框架，这里就不再复述。\n\n## Why Convolutions\n关于这个问题，Ng给出了两个意见，我们一起看看：\n> **Parameter sharing**: A feature detector (such as a vertical edge detetor) that's useful in one part of image is probably useful in another part of the image.\n**Sparsity of connections**: In  each layer, each output value depends only on a small number of inputs.\n\n## Reference\n* [Deep learning-Coursera Andrew Ng](hhttps://www.coursera.org/learn/convolutional-neural-networks)\n* [Deep learning-网易云课堂 Andrew Ng](https://mooc.study.163.com/course/2001281004#/info)\n","slug":"course-deep-learning-course4-week1","published":1,"updated":"2020-05-10T06:50:12.528Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cka0wnkw50009qxotd0ezka0w","content":"<p>Hi, all. 最近开始休假了，可以有空继续自己的学习，一方面补一补前面的作业，一方面继续自己的学习，今天我们来到了course4，也就是convolutional neural networks 的内容。我们一起来看看！<br><a id=\"more\"></a></p>\n<h2 id=\"Convolution\"><a href=\"#Convolution\" class=\"headerlink\" title=\"Convolution\"></a>Convolution</h2><p>在课程中，Ng从edge detection的角度来给大家讲了讲convolution，因为本人是image processing出身，所以认为Ng在这里讲的还是很浅显易懂的，我就不再专门的markdown。主要来看看convolution中的一些技巧。</p>\n<h3 id=\"Padding\"><a href=\"#Padding\" class=\"headerlink\" title=\"Padding\"></a>Padding</h3><p>我们都知道，在最纯粹的convolution中，我们假设原image尺寸是\\(n *n\\)，convolution filter尺寸是\\(f *f\\)，那么最终的结果image尺寸应该是\\( (n-f+1) *(n-f+1)\\)，也就是说，结果的尺寸变小了。如果想让输出image的尺寸不发生改变，那么我们就要使用大名鼎鼎的padding了。</p>\n<p>Padding其实就是表示，在原始image中，向外扩大多少尺寸，一般我们会使用简单复制相邻元素值的方法进行扩充。假设对于一个\\(6 *6\\)的原始image，采用\\(3 *3\\)的filter，加上\\(p=1\\)的padding，那么原始图像尺寸变成了\\(8 * 8\\)，结果变成了\\(6 *6\\)，原始image和结果image一模一样！于是加入了padding的convolution公式就成了\\( (n+2p-f+1) *(n+2p-f+1)\\).</p>\n<p>在这里Ng引入了两个概念，valid和same convolutions，所谓valid convolution，就是没有padding 的convolution；所谓same convolution，就是输入输出的尺寸完全一样。</p>\n<h3 id=\"Stride\"><a href=\"#Stride\" class=\"headerlink\" title=\"Stride\"></a>Stride</h3><p>继padding之后，还有一个很重要的参数，就是步长stride，步长stride决定了filter做convolution时候的步长，如果stride=1，那么filter就会挨着计算，如果stride=2，那么就会跳跃这进行计算。</p>\n<p>总结一下，假设原image尺寸是\\(n *n\\)，convolution filter尺寸是\\(f *f\\)，padding值是\\(p\\)，stride值是\\(s\\)那么最终的结果image尺寸应该是\\( ( \\frac {n+2p-f}{s}+1) *( \\frac {n+2p-f}{s}+1)\\)，如果除不尽的话，我们选择向下取整，也就是不足以做convolution的区域，我们选择放弃。</p>\n<p>##Convolution over Volume<br>对于一般的图像处理，我们使用的都是RGB图像，我们都知道，RGB图像有三个channel，这种情况下，convolution应该如何做，我们来看下面的图：<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/13/13-1.png\" alt=\"\"><br>假设我们的图像是6×6×3，也就是hight×width×channel(depth)，因此对应的filter也要有3的channel(depth)，最后可以得到一个4×4的结果。</p>\n<p>当然，我们可以采用不止一个filter，如图<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/13/13-2.png\" alt=\"\"><br>我们加入了两个不同的filter，他们的大小都是3×3×3，于是最终的结果就是4×4×2，请注意：结果的channel数目取决于filter的个数，而和输入的channel没有任何关系。</p>\n<h2 id=\"CNN\"><a href=\"#CNN\" class=\"headerlink\" title=\"CNN\"></a>CNN</h2><h3 id=\"Convolution-Layer\"><a href=\"#Convolution-Layer\" class=\"headerlink\" title=\"Convolution Layer\"></a>Convolution Layer</h3><p>下面我们来看看CNN网络中的一个layer的工作原理是什么，首先来看截图：<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/13/13-3.png\" alt=\"\"><br>这张图十分复杂，我们一起仔细看看这张图，对于一个6×6×3的RGB图像，我们用了两个3×3×3的filter，我们可以把输入image看做\\(x\\)，也就是\\(a ^{[0]}\\)，filter看做\\(w ^{[1]}\\)，得到的结果就是\\(w ^{[1]}a ^{[0]}\\)，我们再加上一个bias项\\(b^{[1]}\\)，那么就获得了一个liner output\\(w ^{[1]}a ^{[0]}+b^{[1]}\\)，我们再使用一个non-liner function例如ReLU，如此获得一个4×4×2的output。如此就是CNN的一个layer.</p>\n<p>如此我们可以看到，CNN和之前的DNN实质上都存在一种liner function到non-liner function的转化，通过non-liner function去classify线性不可分的data，另外，在CNN中，每一个filter就可以获得一个不同的feature，多个filter可以让我们从多个角度去classify data.</p>\n<p>另外，相比较于fully connected 的DNN，CNN所需要的parameters也少了很多，这一点值得我们注意。</p>\n<h3 id=\"Pooling\"><a href=\"#Pooling\" class=\"headerlink\" title=\"Pooling\"></a>Pooling</h3><p>Pooling原理还是很简单的，我们来看一张图：<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/13/13-4.png\" alt=\"\"><br>首先我们来看看max pooling，如图，我们取filter尺寸\\(f=2\\)，stride大小\\(s=2\\)，对于一个filter中的元素，我们取max作为输出；相对应的，如果我们取average，那么就成了average pooling，pooling中的hyperparameter只有filter尺寸\\(f\\)和stride大小\\(s\\)，值得注意的是，pooling过程中不存在学习过程，no parameters to learn!</p>\n<h3 id=\"Fully-Connected-layer\"><a href=\"#Fully-Connected-layer\" class=\"headerlink\" title=\"Fully Connected layer\"></a>Fully Connected layer</h3><p>Fully connected layer在CNN其实很简单，我们只需要将input展开，按照DNN的方法进行fully connected就可以了。</p>\n<p>一般情况下，我们认为有prameter变化的才算一层，因此我们不认为pooling是一个layer，我们举个一个最简单的CNN做例子：CONV-POOL-CONV-POOL-FC-FC-Softmax，我们这就是一个简单的5层的CNN，在下周的课程中我们可以看到一些经典的CNN框架，这里就不再复述。</p>\n<h2 id=\"Why-Convolutions\"><a href=\"#Why-Convolutions\" class=\"headerlink\" title=\"Why Convolutions\"></a>Why Convolutions</h2><p>关于这个问题，Ng给出了两个意见，我们一起看看：</p>\n<blockquote>\n<p><strong>Parameter sharing</strong>: A feature detector (such as a vertical edge detetor) that’s useful in one part of image is probably useful in another part of the image.<br><strong>Sparsity of connections</strong>: In  each layer, each output value depends only on a small number of inputs.</p>\n</blockquote>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ul>\n<li><a href=\"hhttps://www.coursera.org/learn/convolutional-neural-networks\" target=\"_blank\" rel=\"noopener\">Deep learning-Coursera Andrew Ng</a></li>\n<li><a href=\"https://mooc.study.163.com/course/2001281004#/info\" target=\"_blank\" rel=\"noopener\">Deep learning-网易云课堂 Andrew Ng</a></li>\n</ul>\n","site":{"data":{}},"_categories":[{"name":"learning notes","path":"categories/learning-notes/"}],"_tags":[{"name":"CNN","path":"tags/CNN/"}],"excerpt":"<p>Hi, all. 最近开始休假了，可以有空继续自己的学习，一方面补一补前面的作业，一方面继续自己的学习，今天我们来到了course4，也就是convolutional neural networks 的内容。我们一起来看看！<br></p>","more":"</p>\n<h2 id=\"Convolution\"><a href=\"#Convolution\" class=\"headerlink\" title=\"Convolution\"></a>Convolution</h2><p>在课程中，Ng从edge detection的角度来给大家讲了讲convolution，因为本人是image processing出身，所以认为Ng在这里讲的还是很浅显易懂的，我就不再专门的markdown。主要来看看convolution中的一些技巧。</p>\n<h3 id=\"Padding\"><a href=\"#Padding\" class=\"headerlink\" title=\"Padding\"></a>Padding</h3><p>我们都知道，在最纯粹的convolution中，我们假设原image尺寸是\\(n *n\\)，convolution filter尺寸是\\(f *f\\)，那么最终的结果image尺寸应该是\\( (n-f+1) *(n-f+1)\\)，也就是说，结果的尺寸变小了。如果想让输出image的尺寸不发生改变，那么我们就要使用大名鼎鼎的padding了。</p>\n<p>Padding其实就是表示，在原始image中，向外扩大多少尺寸，一般我们会使用简单复制相邻元素值的方法进行扩充。假设对于一个\\(6 *6\\)的原始image，采用\\(3 *3\\)的filter，加上\\(p=1\\)的padding，那么原始图像尺寸变成了\\(8 * 8\\)，结果变成了\\(6 *6\\)，原始image和结果image一模一样！于是加入了padding的convolution公式就成了\\( (n+2p-f+1) *(n+2p-f+1)\\).</p>\n<p>在这里Ng引入了两个概念，valid和same convolutions，所谓valid convolution，就是没有padding 的convolution；所谓same convolution，就是输入输出的尺寸完全一样。</p>\n<h3 id=\"Stride\"><a href=\"#Stride\" class=\"headerlink\" title=\"Stride\"></a>Stride</h3><p>继padding之后，还有一个很重要的参数，就是步长stride，步长stride决定了filter做convolution时候的步长，如果stride=1，那么filter就会挨着计算，如果stride=2，那么就会跳跃这进行计算。</p>\n<p>总结一下，假设原image尺寸是\\(n *n\\)，convolution filter尺寸是\\(f *f\\)，padding值是\\(p\\)，stride值是\\(s\\)那么最终的结果image尺寸应该是\\( ( \\frac {n+2p-f}{s}+1) *( \\frac {n+2p-f}{s}+1)\\)，如果除不尽的话，我们选择向下取整，也就是不足以做convolution的区域，我们选择放弃。</p>\n<p>##Convolution over Volume<br>对于一般的图像处理，我们使用的都是RGB图像，我们都知道，RGB图像有三个channel，这种情况下，convolution应该如何做，我们来看下面的图：<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/13/13-1.png\" alt=\"\"><br>假设我们的图像是6×6×3，也就是hight×width×channel(depth)，因此对应的filter也要有3的channel(depth)，最后可以得到一个4×4的结果。</p>\n<p>当然，我们可以采用不止一个filter，如图<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/13/13-2.png\" alt=\"\"><br>我们加入了两个不同的filter，他们的大小都是3×3×3，于是最终的结果就是4×4×2，请注意：结果的channel数目取决于filter的个数，而和输入的channel没有任何关系。</p>\n<h2 id=\"CNN\"><a href=\"#CNN\" class=\"headerlink\" title=\"CNN\"></a>CNN</h2><h3 id=\"Convolution-Layer\"><a href=\"#Convolution-Layer\" class=\"headerlink\" title=\"Convolution Layer\"></a>Convolution Layer</h3><p>下面我们来看看CNN网络中的一个layer的工作原理是什么，首先来看截图：<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/13/13-3.png\" alt=\"\"><br>这张图十分复杂，我们一起仔细看看这张图，对于一个6×6×3的RGB图像，我们用了两个3×3×3的filter，我们可以把输入image看做\\(x\\)，也就是\\(a ^{[0]}\\)，filter看做\\(w ^{[1]}\\)，得到的结果就是\\(w ^{[1]}a ^{[0]}\\)，我们再加上一个bias项\\(b^{[1]}\\)，那么就获得了一个liner output\\(w ^{[1]}a ^{[0]}+b^{[1]}\\)，我们再使用一个non-liner function例如ReLU，如此获得一个4×4×2的output。如此就是CNN的一个layer.</p>\n<p>如此我们可以看到，CNN和之前的DNN实质上都存在一种liner function到non-liner function的转化，通过non-liner function去classify线性不可分的data，另外，在CNN中，每一个filter就可以获得一个不同的feature，多个filter可以让我们从多个角度去classify data.</p>\n<p>另外，相比较于fully connected 的DNN，CNN所需要的parameters也少了很多，这一点值得我们注意。</p>\n<h3 id=\"Pooling\"><a href=\"#Pooling\" class=\"headerlink\" title=\"Pooling\"></a>Pooling</h3><p>Pooling原理还是很简单的，我们来看一张图：<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/13/13-4.png\" alt=\"\"><br>首先我们来看看max pooling，如图，我们取filter尺寸\\(f=2\\)，stride大小\\(s=2\\)，对于一个filter中的元素，我们取max作为输出；相对应的，如果我们取average，那么就成了average pooling，pooling中的hyperparameter只有filter尺寸\\(f\\)和stride大小\\(s\\)，值得注意的是，pooling过程中不存在学习过程，no parameters to learn!</p>\n<h3 id=\"Fully-Connected-layer\"><a href=\"#Fully-Connected-layer\" class=\"headerlink\" title=\"Fully Connected layer\"></a>Fully Connected layer</h3><p>Fully connected layer在CNN其实很简单，我们只需要将input展开，按照DNN的方法进行fully connected就可以了。</p>\n<p>一般情况下，我们认为有prameter变化的才算一层，因此我们不认为pooling是一个layer，我们举个一个最简单的CNN做例子：CONV-POOL-CONV-POOL-FC-FC-Softmax，我们这就是一个简单的5层的CNN，在下周的课程中我们可以看到一些经典的CNN框架，这里就不再复述。</p>\n<h2 id=\"Why-Convolutions\"><a href=\"#Why-Convolutions\" class=\"headerlink\" title=\"Why Convolutions\"></a>Why Convolutions</h2><p>关于这个问题，Ng给出了两个意见，我们一起看看：</p>\n<blockquote>\n<p><strong>Parameter sharing</strong>: A feature detector (such as a vertical edge detetor) that’s useful in one part of image is probably useful in another part of the image.<br><strong>Sparsity of connections</strong>: In  each layer, each output value depends only on a small number of inputs.</p>\n</blockquote>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ul>\n<li><a href=\"hhttps://www.coursera.org/learn/convolutional-neural-networks\" target=\"_blank\" rel=\"noopener\">Deep learning-Coursera Andrew Ng</a></li>\n<li><a href=\"https://mooc.study.163.com/course/2001281004#/info\" target=\"_blank\" rel=\"noopener\">Deep learning-网易云课堂 Andrew Ng</a></li>\n</ul>"},{"title":"Learning Notes-Deep Learning, course4, week2","date":"2017-11-29T08:26:12.000Z","_content":"我们继续来看看course4的week2，CNN的知识还是蛮丰富的，本周主要讲了一些经典的CNN结构以及一些computer vision的技巧和知识，一起recap一下。\n<!--more-->\n## Classic Networks\nNg一共给我们带来了3个最为经典的CNN网络，这里我会给出网络的截图和paper原文，抽空我也会看看原文，希望大家和我一起来看看。\n### LeNet-5\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/14/14-1.png)\n[Lécun Y, Bottou L, Bengio Y, et al. Gradient-based learning applied to document recognition[J]. Proceedings of the IEEE, 1998, 86(11):2278-2324.](http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf)\n### AlexNet\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/14/14-2.png)\n[Krizhevsky A, Sutskever I, Hinton G E. ImageNet classification with deep convolutional neural networks[J]. Communications of the Acm, 2012, 60(2):2012.](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf)\n### VGG-16\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/14/14-3.png)\n[Simonyan K, Zisserman A. Very Deep Convolutional Networks for Large-Scale Image Recognition[J]. Computer Science, 2014.](https://arxiv.org/pdf/1409.1556.pdf)\n\n以上可以说是最为经典的三个cnn网络了，大家可以通过阅读paper获得一些详细的知识，都是经典之作，推荐阅读。\n\n## Residual Networks(ResNets)\n对于residual networks，我们在这里具体看一下，它的具体原理可以通过下图的residual block来看看：\n其实，residual block是把\\\\(a^{[l]}\\\\)直接作为\\\\(a^{[l+2]}\\\\)输入，也就是说：\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/14/14-4.png)\n$$a^{[l+2]}=g(z^{[l+2]}+a^{[l]})$$\n其中\\\\(g\\\\)是activation function，如ReLU等。这种思想也被称为short circuit或者skip connection。把上面的residual block串联起来，就变成了我们的residual networks，如下图：\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/14/14-5.png)\nResidual networks最大的特点就是，普通networks随着layer增大，training error理论上是会变小，但是实际上会在某个最小点后增大，但是residual networks则会严格的随着layer增多而减小training error，下面是原文：\n[He K, Zhang X, Ren S, et al. Deep Residual Learning for Image Recognition[J]. 2015:770-778.](https://arxiv.org/pdf/1512.03385.pdf)\n\n## Network in Network and 1×1 Convolutions\n通常我们使用的filter，都是奇数的kernel matrix，在某些情况下，1×1的filter也会被我们使用，它到底有什么作用呢？我们来看一张图：\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/14/14-6.png)\n从这张图中可以看出，1×1的filter可以压缩input的channel(depth)，因此1×1filter还是有一些意思的。下面是原文：\n[Lin M, Chen Q, Yan S. Network In Network[J]. Computer Science, 2013.](https://arxiv.org/pdf/1312.4400.pdf)\n\n## Inception Network\n关于inception network，我们先来看一张图：\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/14/14-7.png)\n对于同一个input，我们分别采用不同的filter，甚至max pooling，在保证输出的hight和width一样的前提下，将结果堆叠起来，作为我们的输出，这样做的好处是，我们不需要自己挑选filter，我们将所有的可能都交给network，让它来决定去选择什么样子的结构。原文是：\n[Szegedy C, Liu W, Jia Y, et al. Going deeper with convolutions[C].  Computer Vision and Pattern Recognition. IEEE, 2015:1-9.](https://arxiv.org/pdf/1409.4842.pdf)\n同时，Ng在课程上说明，inception network 中大量使用了1×1filter来降低计算量，这一点值得我们注意。\n我们来看看Inception 单元的图解：\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/14/14-8.png)\n这一周的课程感觉量很大，介绍了很多的网络，我准备下面慢慢的看看这些paper，站在巨人的肩上去看世界，一定会有别样的风景！\n\n## Reference\n* [Deep learning-Coursera Andrew Ng](hhttps://www.coursera.org/learn/convolutional-neural-networks)\n* [Deep learning-网易云课堂 Andrew Ng](https://mooc.study.163.com/course/2001281004#/info)\n","source":"_posts/course-deep-learning-course4-week2.md","raw":"---\ntitle: Learning Notes-Deep Learning, course4, week2\ndate: 2017-11-29 16:26:12\ntags: CNN\ncategories: learning notes\n---\n我们继续来看看course4的week2，CNN的知识还是蛮丰富的，本周主要讲了一些经典的CNN结构以及一些computer vision的技巧和知识，一起recap一下。\n<!--more-->\n## Classic Networks\nNg一共给我们带来了3个最为经典的CNN网络，这里我会给出网络的截图和paper原文，抽空我也会看看原文，希望大家和我一起来看看。\n### LeNet-5\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/14/14-1.png)\n[Lécun Y, Bottou L, Bengio Y, et al. Gradient-based learning applied to document recognition[J]. Proceedings of the IEEE, 1998, 86(11):2278-2324.](http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf)\n### AlexNet\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/14/14-2.png)\n[Krizhevsky A, Sutskever I, Hinton G E. ImageNet classification with deep convolutional neural networks[J]. Communications of the Acm, 2012, 60(2):2012.](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf)\n### VGG-16\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/14/14-3.png)\n[Simonyan K, Zisserman A. Very Deep Convolutional Networks for Large-Scale Image Recognition[J]. Computer Science, 2014.](https://arxiv.org/pdf/1409.1556.pdf)\n\n以上可以说是最为经典的三个cnn网络了，大家可以通过阅读paper获得一些详细的知识，都是经典之作，推荐阅读。\n\n## Residual Networks(ResNets)\n对于residual networks，我们在这里具体看一下，它的具体原理可以通过下图的residual block来看看：\n其实，residual block是把\\\\(a^{[l]}\\\\)直接作为\\\\(a^{[l+2]}\\\\)输入，也就是说：\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/14/14-4.png)\n$$a^{[l+2]}=g(z^{[l+2]}+a^{[l]})$$\n其中\\\\(g\\\\)是activation function，如ReLU等。这种思想也被称为short circuit或者skip connection。把上面的residual block串联起来，就变成了我们的residual networks，如下图：\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/14/14-5.png)\nResidual networks最大的特点就是，普通networks随着layer增大，training error理论上是会变小，但是实际上会在某个最小点后增大，但是residual networks则会严格的随着layer增多而减小training error，下面是原文：\n[He K, Zhang X, Ren S, et al. Deep Residual Learning for Image Recognition[J]. 2015:770-778.](https://arxiv.org/pdf/1512.03385.pdf)\n\n## Network in Network and 1×1 Convolutions\n通常我们使用的filter，都是奇数的kernel matrix，在某些情况下，1×1的filter也会被我们使用，它到底有什么作用呢？我们来看一张图：\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/14/14-6.png)\n从这张图中可以看出，1×1的filter可以压缩input的channel(depth)，因此1×1filter还是有一些意思的。下面是原文：\n[Lin M, Chen Q, Yan S. Network In Network[J]. Computer Science, 2013.](https://arxiv.org/pdf/1312.4400.pdf)\n\n## Inception Network\n关于inception network，我们先来看一张图：\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/14/14-7.png)\n对于同一个input，我们分别采用不同的filter，甚至max pooling，在保证输出的hight和width一样的前提下，将结果堆叠起来，作为我们的输出，这样做的好处是，我们不需要自己挑选filter，我们将所有的可能都交给network，让它来决定去选择什么样子的结构。原文是：\n[Szegedy C, Liu W, Jia Y, et al. Going deeper with convolutions[C].  Computer Vision and Pattern Recognition. IEEE, 2015:1-9.](https://arxiv.org/pdf/1409.4842.pdf)\n同时，Ng在课程上说明，inception network 中大量使用了1×1filter来降低计算量，这一点值得我们注意。\n我们来看看Inception 单元的图解：\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/14/14-8.png)\n这一周的课程感觉量很大，介绍了很多的网络，我准备下面慢慢的看看这些paper，站在巨人的肩上去看世界，一定会有别样的风景！\n\n## Reference\n* [Deep learning-Coursera Andrew Ng](hhttps://www.coursera.org/learn/convolutional-neural-networks)\n* [Deep learning-网易云课堂 Andrew Ng](https://mooc.study.163.com/course/2001281004#/info)\n","slug":"course-deep-learning-course4-week2","published":1,"updated":"2020-05-10T06:50:12.529Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cka0wnkw6000aqxotaaxbjq87","content":"<p>我们继续来看看course4的week2，CNN的知识还是蛮丰富的，本周主要讲了一些经典的CNN结构以及一些computer vision的技巧和知识，一起recap一下。<br><a id=\"more\"></a></p>\n<h2 id=\"Classic-Networks\"><a href=\"#Classic-Networks\" class=\"headerlink\" title=\"Classic Networks\"></a>Classic Networks</h2><p>Ng一共给我们带来了3个最为经典的CNN网络，这里我会给出网络的截图和paper原文，抽空我也会看看原文，希望大家和我一起来看看。</p>\n<h3 id=\"LeNet-5\"><a href=\"#LeNet-5\" class=\"headerlink\" title=\"LeNet-5\"></a>LeNet-5</h3><p><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/14/14-1.png\" alt=\"\"><br><a href=\"http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf\" target=\"_blank\" rel=\"noopener\">Lécun Y, Bottou L, Bengio Y, et al. Gradient-based learning applied to document recognition[J]. Proceedings of the IEEE, 1998, 86(11):2278-2324.</a></p>\n<h3 id=\"AlexNet\"><a href=\"#AlexNet\" class=\"headerlink\" title=\"AlexNet\"></a>AlexNet</h3><p><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/14/14-2.png\" alt=\"\"><br><a href=\"https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf\" target=\"_blank\" rel=\"noopener\">Krizhevsky A, Sutskever I, Hinton G E. ImageNet classification with deep convolutional neural networks[J]. Communications of the Acm, 2012, 60(2):2012.</a></p>\n<h3 id=\"VGG-16\"><a href=\"#VGG-16\" class=\"headerlink\" title=\"VGG-16\"></a>VGG-16</h3><p><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/14/14-3.png\" alt=\"\"><br><a href=\"https://arxiv.org/pdf/1409.1556.pdf\" target=\"_blank\" rel=\"noopener\">Simonyan K, Zisserman A. Very Deep Convolutional Networks for Large-Scale Image Recognition[J]. Computer Science, 2014.</a></p>\n<p>以上可以说是最为经典的三个cnn网络了，大家可以通过阅读paper获得一些详细的知识，都是经典之作，推荐阅读。</p>\n<h2 id=\"Residual-Networks-ResNets\"><a href=\"#Residual-Networks-ResNets\" class=\"headerlink\" title=\"Residual Networks(ResNets)\"></a>Residual Networks(ResNets)</h2><p>对于residual networks，我们在这里具体看一下，它的具体原理可以通过下图的residual block来看看：<br>其实，residual block是把\\(a^{[l]}\\)直接作为\\(a^{[l+2]}\\)输入，也就是说：<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/14/14-4.png\" alt=\"\"><br>$$a^{[l+2]}=g(z^{[l+2]}+a^{[l]})$$<br>其中\\(g\\)是activation function，如ReLU等。这种思想也被称为short circuit或者skip connection。把上面的residual block串联起来，就变成了我们的residual networks，如下图：<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/14/14-5.png\" alt=\"\"><br>Residual networks最大的特点就是，普通networks随着layer增大，training error理论上是会变小，但是实际上会在某个最小点后增大，但是residual networks则会严格的随着layer增多而减小training error，下面是原文：<br><a href=\"https://arxiv.org/pdf/1512.03385.pdf\" target=\"_blank\" rel=\"noopener\">He K, Zhang X, Ren S, et al. Deep Residual Learning for Image Recognition[J]. 2015:770-778.</a></p>\n<h2 id=\"Network-in-Network-and-1×1-Convolutions\"><a href=\"#Network-in-Network-and-1×1-Convolutions\" class=\"headerlink\" title=\"Network in Network and 1×1 Convolutions\"></a>Network in Network and 1×1 Convolutions</h2><p>通常我们使用的filter，都是奇数的kernel matrix，在某些情况下，1×1的filter也会被我们使用，它到底有什么作用呢？我们来看一张图：<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/14/14-6.png\" alt=\"\"><br>从这张图中可以看出，1×1的filter可以压缩input的channel(depth)，因此1×1filter还是有一些意思的。下面是原文：<br><a href=\"https://arxiv.org/pdf/1312.4400.pdf\" target=\"_blank\" rel=\"noopener\">Lin M, Chen Q, Yan S. Network In Network[J]. Computer Science, 2013.</a></p>\n<h2 id=\"Inception-Network\"><a href=\"#Inception-Network\" class=\"headerlink\" title=\"Inception Network\"></a>Inception Network</h2><p>关于inception network，我们先来看一张图：<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/14/14-7.png\" alt=\"\"><br>对于同一个input，我们分别采用不同的filter，甚至max pooling，在保证输出的hight和width一样的前提下，将结果堆叠起来，作为我们的输出，这样做的好处是，我们不需要自己挑选filter，我们将所有的可能都交给network，让它来决定去选择什么样子的结构。原文是：<br><a href=\"https://arxiv.org/pdf/1409.4842.pdf\" target=\"_blank\" rel=\"noopener\">Szegedy C, Liu W, Jia Y, et al. Going deeper with convolutions[C].  Computer Vision and Pattern Recognition. IEEE, 2015:1-9.</a><br>同时，Ng在课程上说明，inception network 中大量使用了1×1filter来降低计算量，这一点值得我们注意。<br>我们来看看Inception 单元的图解：<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/14/14-8.png\" alt=\"\"><br>这一周的课程感觉量很大，介绍了很多的网络，我准备下面慢慢的看看这些paper，站在巨人的肩上去看世界，一定会有别样的风景！</p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ul>\n<li><a href=\"hhttps://www.coursera.org/learn/convolutional-neural-networks\" target=\"_blank\" rel=\"noopener\">Deep learning-Coursera Andrew Ng</a></li>\n<li><a href=\"https://mooc.study.163.com/course/2001281004#/info\" target=\"_blank\" rel=\"noopener\">Deep learning-网易云课堂 Andrew Ng</a></li>\n</ul>\n","site":{"data":{}},"_categories":[{"name":"learning notes","path":"categories/learning-notes/"}],"_tags":[{"name":"CNN","path":"tags/CNN/"}],"excerpt":"<p>我们继续来看看course4的week2，CNN的知识还是蛮丰富的，本周主要讲了一些经典的CNN结构以及一些computer vision的技巧和知识，一起recap一下。<br></p>","more":"</p>\n<h2 id=\"Classic-Networks\"><a href=\"#Classic-Networks\" class=\"headerlink\" title=\"Classic Networks\"></a>Classic Networks</h2><p>Ng一共给我们带来了3个最为经典的CNN网络，这里我会给出网络的截图和paper原文，抽空我也会看看原文，希望大家和我一起来看看。</p>\n<h3 id=\"LeNet-5\"><a href=\"#LeNet-5\" class=\"headerlink\" title=\"LeNet-5\"></a>LeNet-5</h3><p><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/14/14-1.png\" alt=\"\"><br><a href=\"http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf\" target=\"_blank\" rel=\"noopener\">Lécun Y, Bottou L, Bengio Y, et al. Gradient-based learning applied to document recognition[J]. Proceedings of the IEEE, 1998, 86(11):2278-2324.</a></p>\n<h3 id=\"AlexNet\"><a href=\"#AlexNet\" class=\"headerlink\" title=\"AlexNet\"></a>AlexNet</h3><p><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/14/14-2.png\" alt=\"\"><br><a href=\"https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf\" target=\"_blank\" rel=\"noopener\">Krizhevsky A, Sutskever I, Hinton G E. ImageNet classification with deep convolutional neural networks[J]. Communications of the Acm, 2012, 60(2):2012.</a></p>\n<h3 id=\"VGG-16\"><a href=\"#VGG-16\" class=\"headerlink\" title=\"VGG-16\"></a>VGG-16</h3><p><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/14/14-3.png\" alt=\"\"><br><a href=\"https://arxiv.org/pdf/1409.1556.pdf\" target=\"_blank\" rel=\"noopener\">Simonyan K, Zisserman A. Very Deep Convolutional Networks for Large-Scale Image Recognition[J]. Computer Science, 2014.</a></p>\n<p>以上可以说是最为经典的三个cnn网络了，大家可以通过阅读paper获得一些详细的知识，都是经典之作，推荐阅读。</p>\n<h2 id=\"Residual-Networks-ResNets\"><a href=\"#Residual-Networks-ResNets\" class=\"headerlink\" title=\"Residual Networks(ResNets)\"></a>Residual Networks(ResNets)</h2><p>对于residual networks，我们在这里具体看一下，它的具体原理可以通过下图的residual block来看看：<br>其实，residual block是把\\(a^{[l]}\\)直接作为\\(a^{[l+2]}\\)输入，也就是说：<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/14/14-4.png\" alt=\"\"><br>$$a^{[l+2]}=g(z^{[l+2]}+a^{[l]})$$<br>其中\\(g\\)是activation function，如ReLU等。这种思想也被称为short circuit或者skip connection。把上面的residual block串联起来，就变成了我们的residual networks，如下图：<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/14/14-5.png\" alt=\"\"><br>Residual networks最大的特点就是，普通networks随着layer增大，training error理论上是会变小，但是实际上会在某个最小点后增大，但是residual networks则会严格的随着layer增多而减小training error，下面是原文：<br><a href=\"https://arxiv.org/pdf/1512.03385.pdf\" target=\"_blank\" rel=\"noopener\">He K, Zhang X, Ren S, et al. Deep Residual Learning for Image Recognition[J]. 2015:770-778.</a></p>\n<h2 id=\"Network-in-Network-and-1×1-Convolutions\"><a href=\"#Network-in-Network-and-1×1-Convolutions\" class=\"headerlink\" title=\"Network in Network and 1×1 Convolutions\"></a>Network in Network and 1×1 Convolutions</h2><p>通常我们使用的filter，都是奇数的kernel matrix，在某些情况下，1×1的filter也会被我们使用，它到底有什么作用呢？我们来看一张图：<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/14/14-6.png\" alt=\"\"><br>从这张图中可以看出，1×1的filter可以压缩input的channel(depth)，因此1×1filter还是有一些意思的。下面是原文：<br><a href=\"https://arxiv.org/pdf/1312.4400.pdf\" target=\"_blank\" rel=\"noopener\">Lin M, Chen Q, Yan S. Network In Network[J]. Computer Science, 2013.</a></p>\n<h2 id=\"Inception-Network\"><a href=\"#Inception-Network\" class=\"headerlink\" title=\"Inception Network\"></a>Inception Network</h2><p>关于inception network，我们先来看一张图：<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/14/14-7.png\" alt=\"\"><br>对于同一个input，我们分别采用不同的filter，甚至max pooling，在保证输出的hight和width一样的前提下，将结果堆叠起来，作为我们的输出，这样做的好处是，我们不需要自己挑选filter，我们将所有的可能都交给network，让它来决定去选择什么样子的结构。原文是：<br><a href=\"https://arxiv.org/pdf/1409.4842.pdf\" target=\"_blank\" rel=\"noopener\">Szegedy C, Liu W, Jia Y, et al. Going deeper with convolutions[C].  Computer Vision and Pattern Recognition. IEEE, 2015:1-9.</a><br>同时，Ng在课程上说明，inception network 中大量使用了1×1filter来降低计算量，这一点值得我们注意。<br>我们来看看Inception 单元的图解：<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/14/14-8.png\" alt=\"\"><br>这一周的课程感觉量很大，介绍了很多的网络，我准备下面慢慢的看看这些paper，站在巨人的肩上去看世界，一定会有别样的风景！</p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ul>\n<li><a href=\"hhttps://www.coursera.org/learn/convolutional-neural-networks\" target=\"_blank\" rel=\"noopener\">Deep learning-Coursera Andrew Ng</a></li>\n<li><a href=\"https://mooc.study.163.com/course/2001281004#/info\" target=\"_blank\" rel=\"noopener\">Deep learning-网易云课堂 Andrew Ng</a></li>\n</ul>"},{"title":"Arvo, Parquet and ORC","date":"2019-05-05T08:34:37.000Z","toc":true,"_content":"![](https://github.com/JoeAsir/blog-image/raw/master/blog/background/architecture-body-of-water-buildings-2255985.jpg)\nDealing with HIVE is one of my daily work with which I read data from and write back to the HDFS. There are many storage formats in HIVE, such as textFile, Avro,  and so on. Today we will talk about three popular formats that are widely use in HIVE world, also in Spark and even the entire distributed file system world. Not talking about some classical formats like textFile , SequenceFile, RCFile, doesn't mean that they are not important or good enough, so you'd better have some look at them to help you make sense of the storage formats in HIVE.\n<!--more-->\n## Formats Categories\n### Row Formats & Columnar Formats\nRow formats and Columnar formats are two methods of seriallizing and storing a table in Database. And the following figure presents the difference of data stored in Row formats and in Columnar fromats.\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/22/22-1.png)\n\n### Text formats & Binary formats\nText and Binary Formats are two methods of storing data in another dimension. Text formats are human-readable, easy to generate and easy to parse, however, they occupy a lot of disk space because of the readablity and redundancy. The most popular Text formats includs CSV, TSV, JSON and so on. And, Binary formats, occupy much less storage space than Test formats while they are machine-readable rather than human-readable. However, saving in  Binary formats can make all the storage in HDFS more effcient. Also, the formats we are going to talk about, Avro, Parquet and ORC formats, are all Binary formats.\n\n## Avro\nAvor is one of **row-based** formats, and boldly speaking, Avro is a binary alternative to JSON. Avor provides rich data strucures with the **schema**, and the schema file is separately stored from the data file. Also, Avro can generate the serialization and deserialization code form the schema. The figure below presents the sturcture of Avro.\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/22/22-2.png)\n\nAs presented, Avor firstly splits data into multiple **blocks** rather than stores data record by record. The schema infomation is stored in the header and the data information are in blocks followed by a **sync marker** for each block. The sync markers make the Avor format splittable and the schema make it extensibility.  \n## Parquet\nParquet is a **Columnar format**, which is based on the Google Dremel paper, and it's one of the most popular Columnar formats in Hadoop ecosystem and it's well integrated with Apache Spark. What's more, Parquet can easily deal with the nested schema. The following figure shows the structure of Parquet. \n![](https://github.com/JoeAsir/blog-image/raw/master/blog/22/22-3.png)\n\nFirst we need to learn some terms about Parquet:\n* Blocks: The block is actually the block in HDFS, which is unchanged for Parquet;\n* File: The HDFS file with metadata;\n* Row group: A logical horizontal partitioning of the data into rows. A row group consists of a column chunk for each column in the dataset;\n* Column chunk: A chunk of data for a specific column in one Row group. A column chunk consists of one or more than one page;\n* Page: Column chunks are devided into pages. And it is conceptually indivisible unit.\n\nAll the metadata for file, row groups, and column chunks are all stored in a footer, which is the tail of the whole file. The figure presents the structure of Parquet clearly and if you want to learn the detail, you can take a look at the [offcial doc](https://parquet.apache.org/documentation/latest/)\n\n## ORC\nORC(Optimized Row Columnar) is also a **Columnar format** and it is optimized from RCFile format, which is also a Columar format. ORC is now widely used in Hadoop ecosystem especially for HIVE. Let's start from the ORC structure.\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/22/22-4.png)\n\nFirst we have to make some terms clearly:\n* Stripe: Simialr to the row groups in Parquet, stripe in ORC is a group of row data. Each stripe holds index data, row data and a stripe footer;\n* Index data: Include the min and max values and row positions within each column in one stripe;\n* Row data: The real data value;\n* Stripe footer: Contains the directoty of stream location.\n\nNow let's talk about an awesome feature in ORC, which is based on the index.\n### Indexes\nThere are three level of indexes in ORC, which are file, stride and row level index. They contain some statistics like the max/min value and the position information of each vale for specific dimesion. \n> * file level - statistics about the values in each column across the entire file\n> * stripe level - statistics about the values in each column for each stripe\n> * row level - statistics about the values in each column for each set of 10,000 rows within a stripe(the value can be modified by *orc.row.index.stride*)\n\nFile and stride index are in the file footer at the tail of the whole file, and as soon as the file is read, the information can be loaded directly and the useless part of data would be ignored immediately. The row level index is stored in the Index data in each stride and it contains the count of the values and whether there are null value present, and also the min/max values. Besides, the row index can include bloom filter, which will be talked in the next sub-section.\nTo make leverage of ORC index, we'd better insert the data into ORC table with sorting first, usually *distributed by* and *sort by*, to make the data are sorted well in each reducer result. Let's have a simple example. Suppose we have a query with *WHERE id > 10*, the entired file would be filter by the file level index, stride index and row level index. If the file is inserted sorted by *id*, the min/max value in each index would not be overlapping.\n\n### Bloom Filter\nSince Hive 1.2, the bloom filter is supported in row index. If you've never heared about bloom filter, don't worry about it and click [here](https://en.wikipedia.org/wiki/Bloom_filter) to have a quick look.\nAs the last sub-section, the indexes are in good use to filter data and make the query more efficient. But the statistics value like min/max are more efficient for those continuously numerical type columns. When you have some operation like *=* or *in*, bloom filter would be a better choice. \nWith the configuration *orc.bloom.filter.columns* and *orc.bloom.filter.fpp*, we can figure out the colunms(splitted by comma) with bloom filter and specify the positive probability for the bloom filter. \n\n## References\n* [Parquet Documentation](https://parquet.apache.org/documentation/latest/)\n* [ORC Documentation](https://cwiki.apache.org/confluence/display/Hive/LanguageManual+ORC#LanguageManualORC-orc-spec)\n* [Apache Parquet: Parquet File Internals and Inspecting Parquet File Structure](https://www.youtube.com/watch?v=rVC9F1y38oU)\n* [ORC Specification v1](https://orc.apache.org/specification/ORCv1/)\n* [ORC Indexes](https://orc.apache.org/docs/indexes.html)\n* [HIVE Optimizations with Indexes, BLOOM-FILTERS and Statistics](https://snippetessay.wordpress.com/2015/07/25/hive-optimizations-with-indexes-bloom-filters-and-statistics/)\n","source":"_posts/hdfs-arvo-parquet-orc.md","raw":"---\ntitle: Arvo, Parquet and ORC\ndate: 2019-05-05 16:34:37\ntags: \n    - hdfs\n    - hive\ncategories: hadoop\ntoc: true\n---\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/background/architecture-body-of-water-buildings-2255985.jpg)\nDealing with HIVE is one of my daily work with which I read data from and write back to the HDFS. There are many storage formats in HIVE, such as textFile, Avro,  and so on. Today we will talk about three popular formats that are widely use in HIVE world, also in Spark and even the entire distributed file system world. Not talking about some classical formats like textFile , SequenceFile, RCFile, doesn't mean that they are not important or good enough, so you'd better have some look at them to help you make sense of the storage formats in HIVE.\n<!--more-->\n## Formats Categories\n### Row Formats & Columnar Formats\nRow formats and Columnar formats are two methods of seriallizing and storing a table in Database. And the following figure presents the difference of data stored in Row formats and in Columnar fromats.\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/22/22-1.png)\n\n### Text formats & Binary formats\nText and Binary Formats are two methods of storing data in another dimension. Text formats are human-readable, easy to generate and easy to parse, however, they occupy a lot of disk space because of the readablity and redundancy. The most popular Text formats includs CSV, TSV, JSON and so on. And, Binary formats, occupy much less storage space than Test formats while they are machine-readable rather than human-readable. However, saving in  Binary formats can make all the storage in HDFS more effcient. Also, the formats we are going to talk about, Avro, Parquet and ORC formats, are all Binary formats.\n\n## Avro\nAvor is one of **row-based** formats, and boldly speaking, Avro is a binary alternative to JSON. Avor provides rich data strucures with the **schema**, and the schema file is separately stored from the data file. Also, Avro can generate the serialization and deserialization code form the schema. The figure below presents the sturcture of Avro.\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/22/22-2.png)\n\nAs presented, Avor firstly splits data into multiple **blocks** rather than stores data record by record. The schema infomation is stored in the header and the data information are in blocks followed by a **sync marker** for each block. The sync markers make the Avor format splittable and the schema make it extensibility.  \n## Parquet\nParquet is a **Columnar format**, which is based on the Google Dremel paper, and it's one of the most popular Columnar formats in Hadoop ecosystem and it's well integrated with Apache Spark. What's more, Parquet can easily deal with the nested schema. The following figure shows the structure of Parquet. \n![](https://github.com/JoeAsir/blog-image/raw/master/blog/22/22-3.png)\n\nFirst we need to learn some terms about Parquet:\n* Blocks: The block is actually the block in HDFS, which is unchanged for Parquet;\n* File: The HDFS file with metadata;\n* Row group: A logical horizontal partitioning of the data into rows. A row group consists of a column chunk for each column in the dataset;\n* Column chunk: A chunk of data for a specific column in one Row group. A column chunk consists of one or more than one page;\n* Page: Column chunks are devided into pages. And it is conceptually indivisible unit.\n\nAll the metadata for file, row groups, and column chunks are all stored in a footer, which is the tail of the whole file. The figure presents the structure of Parquet clearly and if you want to learn the detail, you can take a look at the [offcial doc](https://parquet.apache.org/documentation/latest/)\n\n## ORC\nORC(Optimized Row Columnar) is also a **Columnar format** and it is optimized from RCFile format, which is also a Columar format. ORC is now widely used in Hadoop ecosystem especially for HIVE. Let's start from the ORC structure.\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/22/22-4.png)\n\nFirst we have to make some terms clearly:\n* Stripe: Simialr to the row groups in Parquet, stripe in ORC is a group of row data. Each stripe holds index data, row data and a stripe footer;\n* Index data: Include the min and max values and row positions within each column in one stripe;\n* Row data: The real data value;\n* Stripe footer: Contains the directoty of stream location.\n\nNow let's talk about an awesome feature in ORC, which is based on the index.\n### Indexes\nThere are three level of indexes in ORC, which are file, stride and row level index. They contain some statistics like the max/min value and the position information of each vale for specific dimesion. \n> * file level - statistics about the values in each column across the entire file\n> * stripe level - statistics about the values in each column for each stripe\n> * row level - statistics about the values in each column for each set of 10,000 rows within a stripe(the value can be modified by *orc.row.index.stride*)\n\nFile and stride index are in the file footer at the tail of the whole file, and as soon as the file is read, the information can be loaded directly and the useless part of data would be ignored immediately. The row level index is stored in the Index data in each stride and it contains the count of the values and whether there are null value present, and also the min/max values. Besides, the row index can include bloom filter, which will be talked in the next sub-section.\nTo make leverage of ORC index, we'd better insert the data into ORC table with sorting first, usually *distributed by* and *sort by*, to make the data are sorted well in each reducer result. Let's have a simple example. Suppose we have a query with *WHERE id > 10*, the entired file would be filter by the file level index, stride index and row level index. If the file is inserted sorted by *id*, the min/max value in each index would not be overlapping.\n\n### Bloom Filter\nSince Hive 1.2, the bloom filter is supported in row index. If you've never heared about bloom filter, don't worry about it and click [here](https://en.wikipedia.org/wiki/Bloom_filter) to have a quick look.\nAs the last sub-section, the indexes are in good use to filter data and make the query more efficient. But the statistics value like min/max are more efficient for those continuously numerical type columns. When you have some operation like *=* or *in*, bloom filter would be a better choice. \nWith the configuration *orc.bloom.filter.columns* and *orc.bloom.filter.fpp*, we can figure out the colunms(splitted by comma) with bloom filter and specify the positive probability for the bloom filter. \n\n## References\n* [Parquet Documentation](https://parquet.apache.org/documentation/latest/)\n* [ORC Documentation](https://cwiki.apache.org/confluence/display/Hive/LanguageManual+ORC#LanguageManualORC-orc-spec)\n* [Apache Parquet: Parquet File Internals and Inspecting Parquet File Structure](https://www.youtube.com/watch?v=rVC9F1y38oU)\n* [ORC Specification v1](https://orc.apache.org/specification/ORCv1/)\n* [ORC Indexes](https://orc.apache.org/docs/indexes.html)\n* [HIVE Optimizations with Indexes, BLOOM-FILTERS and Statistics](https://snippetessay.wordpress.com/2015/07/25/hive-optimizations-with-indexes-bloom-filters-and-statistics/)\n","slug":"hdfs-arvo-parquet-orc","published":1,"updated":"2020-05-10T06:50:12.529Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cka0wnkw8000eqxotbcf0arls","content":"<p><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/background/architecture-body-of-water-buildings-2255985.jpg\" alt=\"\"><br>Dealing with HIVE is one of my daily work with which I read data from and write back to the HDFS. There are many storage formats in HIVE, such as textFile, Avro,  and so on. Today we will talk about three popular formats that are widely use in HIVE world, also in Spark and even the entire distributed file system world. Not talking about some classical formats like textFile , SequenceFile, RCFile, doesn’t mean that they are not important or good enough, so you’d better have some look at them to help you make sense of the storage formats in HIVE.<br><a id=\"more\"></a></p>\n<h2 id=\"Formats-Categories\"><a href=\"#Formats-Categories\" class=\"headerlink\" title=\"Formats Categories\"></a>Formats Categories</h2><h3 id=\"Row-Formats-amp-Columnar-Formats\"><a href=\"#Row-Formats-amp-Columnar-Formats\" class=\"headerlink\" title=\"Row Formats &amp; Columnar Formats\"></a>Row Formats &amp; Columnar Formats</h3><p>Row formats and Columnar formats are two methods of seriallizing and storing a table in Database. And the following figure presents the difference of data stored in Row formats and in Columnar fromats.<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/22/22-1.png\" alt=\"\"></p>\n<h3 id=\"Text-formats-amp-Binary-formats\"><a href=\"#Text-formats-amp-Binary-formats\" class=\"headerlink\" title=\"Text formats &amp; Binary formats\"></a>Text formats &amp; Binary formats</h3><p>Text and Binary Formats are two methods of storing data in another dimension. Text formats are human-readable, easy to generate and easy to parse, however, they occupy a lot of disk space because of the readablity and redundancy. The most popular Text formats includs CSV, TSV, JSON and so on. And, Binary formats, occupy much less storage space than Test formats while they are machine-readable rather than human-readable. However, saving in  Binary formats can make all the storage in HDFS more effcient. Also, the formats we are going to talk about, Avro, Parquet and ORC formats, are all Binary formats.</p>\n<h2 id=\"Avro\"><a href=\"#Avro\" class=\"headerlink\" title=\"Avro\"></a>Avro</h2><p>Avor is one of <strong>row-based</strong> formats, and boldly speaking, Avro is a binary alternative to JSON. Avor provides rich data strucures with the <strong>schema</strong>, and the schema file is separately stored from the data file. Also, Avro can generate the serialization and deserialization code form the schema. The figure below presents the sturcture of Avro.<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/22/22-2.png\" alt=\"\"></p>\n<p>As presented, Avor firstly splits data into multiple <strong>blocks</strong> rather than stores data record by record. The schema infomation is stored in the header and the data information are in blocks followed by a <strong>sync marker</strong> for each block. The sync markers make the Avor format splittable and the schema make it extensibility.  </p>\n<h2 id=\"Parquet\"><a href=\"#Parquet\" class=\"headerlink\" title=\"Parquet\"></a>Parquet</h2><p>Parquet is a <strong>Columnar format</strong>, which is based on the Google Dremel paper, and it’s one of the most popular Columnar formats in Hadoop ecosystem and it’s well integrated with Apache Spark. What’s more, Parquet can easily deal with the nested schema. The following figure shows the structure of Parquet.<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/22/22-3.png\" alt=\"\"></p>\n<p>First we need to learn some terms about Parquet:</p>\n<ul>\n<li>Blocks: The block is actually the block in HDFS, which is unchanged for Parquet;</li>\n<li>File: The HDFS file with metadata;</li>\n<li>Row group: A logical horizontal partitioning of the data into rows. A row group consists of a column chunk for each column in the dataset;</li>\n<li>Column chunk: A chunk of data for a specific column in one Row group. A column chunk consists of one or more than one page;</li>\n<li>Page: Column chunks are devided into pages. And it is conceptually indivisible unit.</li>\n</ul>\n<p>All the metadata for file, row groups, and column chunks are all stored in a footer, which is the tail of the whole file. The figure presents the structure of Parquet clearly and if you want to learn the detail, you can take a look at the <a href=\"https://parquet.apache.org/documentation/latest/\" target=\"_blank\" rel=\"noopener\">offcial doc</a></p>\n<h2 id=\"ORC\"><a href=\"#ORC\" class=\"headerlink\" title=\"ORC\"></a>ORC</h2><p>ORC(Optimized Row Columnar) is also a <strong>Columnar format</strong> and it is optimized from RCFile format, which is also a Columar format. ORC is now widely used in Hadoop ecosystem especially for HIVE. Let’s start from the ORC structure.<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/22/22-4.png\" alt=\"\"></p>\n<p>First we have to make some terms clearly:</p>\n<ul>\n<li>Stripe: Simialr to the row groups in Parquet, stripe in ORC is a group of row data. Each stripe holds index data, row data and a stripe footer;</li>\n<li>Index data: Include the min and max values and row positions within each column in one stripe;</li>\n<li>Row data: The real data value;</li>\n<li>Stripe footer: Contains the directoty of stream location.</li>\n</ul>\n<p>Now let’s talk about an awesome feature in ORC, which is based on the index.</p>\n<h3 id=\"Indexes\"><a href=\"#Indexes\" class=\"headerlink\" title=\"Indexes\"></a>Indexes</h3><p>There are three level of indexes in ORC, which are file, stride and row level index. They contain some statistics like the max/min value and the position information of each vale for specific dimesion. </p>\n<blockquote>\n<ul>\n<li>file level - statistics about the values in each column across the entire file</li>\n<li>stripe level - statistics about the values in each column for each stripe</li>\n<li>row level - statistics about the values in each column for each set of 10,000 rows within a stripe(the value can be modified by <em>orc.row.index.stride</em>)</li>\n</ul>\n</blockquote>\n<p>File and stride index are in the file footer at the tail of the whole file, and as soon as the file is read, the information can be loaded directly and the useless part of data would be ignored immediately. The row level index is stored in the Index data in each stride and it contains the count of the values and whether there are null value present, and also the min/max values. Besides, the row index can include bloom filter, which will be talked in the next sub-section.<br>To make leverage of ORC index, we’d better insert the data into ORC table with sorting first, usually <em>distributed by</em> and <em>sort by</em>, to make the data are sorted well in each reducer result. Let’s have a simple example. Suppose we have a query with <em>WHERE id &gt; 10</em>, the entired file would be filter by the file level index, stride index and row level index. If the file is inserted sorted by <em>id</em>, the min/max value in each index would not be overlapping.</p>\n<h3 id=\"Bloom-Filter\"><a href=\"#Bloom-Filter\" class=\"headerlink\" title=\"Bloom Filter\"></a>Bloom Filter</h3><p>Since Hive 1.2, the bloom filter is supported in row index. If you’ve never heared about bloom filter, don’t worry about it and click <a href=\"https://en.wikipedia.org/wiki/Bloom_filter\" target=\"_blank\" rel=\"noopener\">here</a> to have a quick look.<br>As the last sub-section, the indexes are in good use to filter data and make the query more efficient. But the statistics value like min/max are more efficient for those continuously numerical type columns. When you have some operation like <em>=</em> or <em>in</em>, bloom filter would be a better choice.<br>With the configuration <em>orc.bloom.filter.columns</em> and <em>orc.bloom.filter.fpp</em>, we can figure out the colunms(splitted by comma) with bloom filter and specify the positive probability for the bloom filter. </p>\n<h2 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h2><ul>\n<li><a href=\"https://parquet.apache.org/documentation/latest/\" target=\"_blank\" rel=\"noopener\">Parquet Documentation</a></li>\n<li><a href=\"https://cwiki.apache.org/confluence/display/Hive/LanguageManual+ORC#LanguageManualORC-orc-spec\" target=\"_blank\" rel=\"noopener\">ORC Documentation</a></li>\n<li><a href=\"https://www.youtube.com/watch?v=rVC9F1y38oU\" target=\"_blank\" rel=\"noopener\">Apache Parquet: Parquet File Internals and Inspecting Parquet File Structure</a></li>\n<li><a href=\"https://orc.apache.org/specification/ORCv1/\" target=\"_blank\" rel=\"noopener\">ORC Specification v1</a></li>\n<li><a href=\"https://orc.apache.org/docs/indexes.html\" target=\"_blank\" rel=\"noopener\">ORC Indexes</a></li>\n<li><a href=\"https://snippetessay.wordpress.com/2015/07/25/hive-optimizations-with-indexes-bloom-filters-and-statistics/\" target=\"_blank\" rel=\"noopener\">HIVE Optimizations with Indexes, BLOOM-FILTERS and Statistics</a></li>\n</ul>\n","site":{"data":{}},"_categories":[{"name":"hadoop","path":"categories/hadoop/"}],"_tags":[{"name":"hdfs","path":"tags/hdfs/"},{"name":"hive","path":"tags/hive/"}],"excerpt":"<p><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/background/architecture-body-of-water-buildings-2255985.jpg\" alt=\"\"><br>Dealing with HIVE is one of my daily work with which I read data from and write back to the HDFS. There are many storage formats in HIVE, such as textFile, Avro,  and so on. Today we will talk about three popular formats that are widely use in HIVE world, also in Spark and even the entire distributed file system world. Not talking about some classical formats like textFile , SequenceFile, RCFile, doesn’t mean that they are not important or good enough, so you’d better have some look at them to help you make sense of the storage formats in HIVE.<br></p>","more":"</p>\n<h2 id=\"Formats-Categories\"><a href=\"#Formats-Categories\" class=\"headerlink\" title=\"Formats Categories\"></a>Formats Categories</h2><h3 id=\"Row-Formats-amp-Columnar-Formats\"><a href=\"#Row-Formats-amp-Columnar-Formats\" class=\"headerlink\" title=\"Row Formats &amp; Columnar Formats\"></a>Row Formats &amp; Columnar Formats</h3><p>Row formats and Columnar formats are two methods of seriallizing and storing a table in Database. And the following figure presents the difference of data stored in Row formats and in Columnar fromats.<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/22/22-1.png\" alt=\"\"></p>\n<h3 id=\"Text-formats-amp-Binary-formats\"><a href=\"#Text-formats-amp-Binary-formats\" class=\"headerlink\" title=\"Text formats &amp; Binary formats\"></a>Text formats &amp; Binary formats</h3><p>Text and Binary Formats are two methods of storing data in another dimension. Text formats are human-readable, easy to generate and easy to parse, however, they occupy a lot of disk space because of the readablity and redundancy. The most popular Text formats includs CSV, TSV, JSON and so on. And, Binary formats, occupy much less storage space than Test formats while they are machine-readable rather than human-readable. However, saving in  Binary formats can make all the storage in HDFS more effcient. Also, the formats we are going to talk about, Avro, Parquet and ORC formats, are all Binary formats.</p>\n<h2 id=\"Avro\"><a href=\"#Avro\" class=\"headerlink\" title=\"Avro\"></a>Avro</h2><p>Avor is one of <strong>row-based</strong> formats, and boldly speaking, Avro is a binary alternative to JSON. Avor provides rich data strucures with the <strong>schema</strong>, and the schema file is separately stored from the data file. Also, Avro can generate the serialization and deserialization code form the schema. The figure below presents the sturcture of Avro.<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/22/22-2.png\" alt=\"\"></p>\n<p>As presented, Avor firstly splits data into multiple <strong>blocks</strong> rather than stores data record by record. The schema infomation is stored in the header and the data information are in blocks followed by a <strong>sync marker</strong> for each block. The sync markers make the Avor format splittable and the schema make it extensibility.  </p>\n<h2 id=\"Parquet\"><a href=\"#Parquet\" class=\"headerlink\" title=\"Parquet\"></a>Parquet</h2><p>Parquet is a <strong>Columnar format</strong>, which is based on the Google Dremel paper, and it’s one of the most popular Columnar formats in Hadoop ecosystem and it’s well integrated with Apache Spark. What’s more, Parquet can easily deal with the nested schema. The following figure shows the structure of Parquet.<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/22/22-3.png\" alt=\"\"></p>\n<p>First we need to learn some terms about Parquet:</p>\n<ul>\n<li>Blocks: The block is actually the block in HDFS, which is unchanged for Parquet;</li>\n<li>File: The HDFS file with metadata;</li>\n<li>Row group: A logical horizontal partitioning of the data into rows. A row group consists of a column chunk for each column in the dataset;</li>\n<li>Column chunk: A chunk of data for a specific column in one Row group. A column chunk consists of one or more than one page;</li>\n<li>Page: Column chunks are devided into pages. And it is conceptually indivisible unit.</li>\n</ul>\n<p>All the metadata for file, row groups, and column chunks are all stored in a footer, which is the tail of the whole file. The figure presents the structure of Parquet clearly and if you want to learn the detail, you can take a look at the <a href=\"https://parquet.apache.org/documentation/latest/\" target=\"_blank\" rel=\"noopener\">offcial doc</a></p>\n<h2 id=\"ORC\"><a href=\"#ORC\" class=\"headerlink\" title=\"ORC\"></a>ORC</h2><p>ORC(Optimized Row Columnar) is also a <strong>Columnar format</strong> and it is optimized from RCFile format, which is also a Columar format. ORC is now widely used in Hadoop ecosystem especially for HIVE. Let’s start from the ORC structure.<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/22/22-4.png\" alt=\"\"></p>\n<p>First we have to make some terms clearly:</p>\n<ul>\n<li>Stripe: Simialr to the row groups in Parquet, stripe in ORC is a group of row data. Each stripe holds index data, row data and a stripe footer;</li>\n<li>Index data: Include the min and max values and row positions within each column in one stripe;</li>\n<li>Row data: The real data value;</li>\n<li>Stripe footer: Contains the directoty of stream location.</li>\n</ul>\n<p>Now let’s talk about an awesome feature in ORC, which is based on the index.</p>\n<h3 id=\"Indexes\"><a href=\"#Indexes\" class=\"headerlink\" title=\"Indexes\"></a>Indexes</h3><p>There are three level of indexes in ORC, which are file, stride and row level index. They contain some statistics like the max/min value and the position information of each vale for specific dimesion. </p>\n<blockquote>\n<ul>\n<li>file level - statistics about the values in each column across the entire file</li>\n<li>stripe level - statistics about the values in each column for each stripe</li>\n<li>row level - statistics about the values in each column for each set of 10,000 rows within a stripe(the value can be modified by <em>orc.row.index.stride</em>)</li>\n</ul>\n</blockquote>\n<p>File and stride index are in the file footer at the tail of the whole file, and as soon as the file is read, the information can be loaded directly and the useless part of data would be ignored immediately. The row level index is stored in the Index data in each stride and it contains the count of the values and whether there are null value present, and also the min/max values. Besides, the row index can include bloom filter, which will be talked in the next sub-section.<br>To make leverage of ORC index, we’d better insert the data into ORC table with sorting first, usually <em>distributed by</em> and <em>sort by</em>, to make the data are sorted well in each reducer result. Let’s have a simple example. Suppose we have a query with <em>WHERE id &gt; 10</em>, the entired file would be filter by the file level index, stride index and row level index. If the file is inserted sorted by <em>id</em>, the min/max value in each index would not be overlapping.</p>\n<h3 id=\"Bloom-Filter\"><a href=\"#Bloom-Filter\" class=\"headerlink\" title=\"Bloom Filter\"></a>Bloom Filter</h3><p>Since Hive 1.2, the bloom filter is supported in row index. If you’ve never heared about bloom filter, don’t worry about it and click <a href=\"https://en.wikipedia.org/wiki/Bloom_filter\" target=\"_blank\" rel=\"noopener\">here</a> to have a quick look.<br>As the last sub-section, the indexes are in good use to filter data and make the query more efficient. But the statistics value like min/max are more efficient for those continuously numerical type columns. When you have some operation like <em>=</em> or <em>in</em>, bloom filter would be a better choice.<br>With the configuration <em>orc.bloom.filter.columns</em> and <em>orc.bloom.filter.fpp</em>, we can figure out the colunms(splitted by comma) with bloom filter and specify the positive probability for the bloom filter. </p>\n<h2 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h2><ul>\n<li><a href=\"https://parquet.apache.org/documentation/latest/\" target=\"_blank\" rel=\"noopener\">Parquet Documentation</a></li>\n<li><a href=\"https://cwiki.apache.org/confluence/display/Hive/LanguageManual+ORC#LanguageManualORC-orc-spec\" target=\"_blank\" rel=\"noopener\">ORC Documentation</a></li>\n<li><a href=\"https://www.youtube.com/watch?v=rVC9F1y38oU\" target=\"_blank\" rel=\"noopener\">Apache Parquet: Parquet File Internals and Inspecting Parquet File Structure</a></li>\n<li><a href=\"https://orc.apache.org/specification/ORCv1/\" target=\"_blank\" rel=\"noopener\">ORC Specification v1</a></li>\n<li><a href=\"https://orc.apache.org/docs/indexes.html\" target=\"_blank\" rel=\"noopener\">ORC Indexes</a></li>\n<li><a href=\"https://snippetessay.wordpress.com/2015/07/25/hive-optimizations-with-indexes-bloom-filters-and-statistics/\" target=\"_blank\" rel=\"noopener\">HIVE Optimizations with Indexes, BLOOM-FILTERS and Statistics</a></li>\n</ul>"},{"title":"YARN Architecture","date":"2019-06-11T12:18:54.000Z","toc":true,"_content":"![](https://github.com/JoeAsir/blog-image/raw/master/blog/background/aerial-shot-bird-s-eye-view-forest-2415927.jpg)\nYARN, the abbreviation of *Yet Another Resource Negotiator*, is introduced in Hadoop 2.0. Compared with MRV1(MapReduce Version 1), YARN takes over the responsibility of **resource management and job scheduling** in MRV1, and make **non-MapReduce** jobs run on the Hadoop, Apache Spark for example. Although there are some rising technology that been treated as alternative of YARN by more and more developers, Kubernetes for instance, YARN is still widely used. Let's talk about YARN today. \n<!--more-->\n## Components\nYARN consists of one Resource Manager as master deamon, several slave daemon called Node Manager for each slave node and Application Master for each application.\n### Resource Master(RM)\nResouce Manager(RM) is replacement of **JobTracker** in MapReduce Version 1 and it is the master deamon in YARN and manages the global resources among all the applications. Resource Master has two main components, Scheduler and Application Manager\n#### Scheduler\nScheduler is mainly desgined to negotiate resource and it is responsible for allocating the resource for the running applications based on the abstract notion of a resource Container, such as memory, CPU etc. Notice that Container is the basic unit of resource in YARN. Scheduler allocate the resource based on the resource requirment of the application and the resource availabality. Scheduler doesn't care too much about the application monitoring or tracking, even the restarting failure no matter application or hardware failure.\n#### Applicaiton Manager\nApplication Manager is responsible for the maintaining a collection of submitted applications. Application Manager accept the submission of one application from the client and allocate one Container for the application to load its Application Master. What's more, Application Manager provides the monitoring and tracking service for the running applications and also restarting them if any failure come out.\n### Node Manager(NM)\nNode Manager, the update version of **TaskTrack** in MRV1, is the slave deamon in YARN. Node Manager is mainly responsible for the individual Containers management and monitoring. Node Manager tracks the usage of resource among all the Containers in the node and replay the detailed information to Resource Manager and Node Manager also monitors the heath of the running Containers(Node Manager only manage the resource and running status rather than application context information).\nNode Manager always keep in touch with Resouce Manager with heartbeats, \n### Application Master(AM)\nOne Application Mater runs for one application in YARN, which means each application has a unique Applicaiton Master. Application Master is responsible to send request to Resouce Manager and acquire resource from the Scheduler. What's more, Application Master is the process that coordinates the application's execution and manage the applicaiton faults along with the Node Manager. Also, Application Master sends heartbeats to the Resource Manager periodly to update the health condiction and resource demands.\nNotice that there is a main difference between the NM and AM, NM only monitors and tracks the status of the dividul Containers while AM the application itself. When an application is running, both NM and AM contributes to the monitoring and tracking, but they focus on the different part.\n\n## Applicaiton Submission on YARN\nAs we've learned each component of the YARN, let's take look at how these components work together to submit an application. The workflow is presented below.\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/23/23-1.png)\n\n* Client submits an application to the Resource Manager and request for building an Application Master;\n* Rsource Manager, Application Manager actually, allocates a free Container to load the Application Master for the submitted application and then the Application Master then will start;\n* Application Master registers to the Resource Manager, after which the client can communicate with the Application Master directly;\n* Application Master send request to acquire resource Container for the application to Resouce Manager;\n* Resource Manager(Scheduler) notify the Node Manager to launch Containers, and the launched Containers can communicate with Application Master;\n* Application code will executed in the Container, and the status can be updated to the Application Master by the *application-specific*;\n* While the application is running, client contacts to the Resource Manager(Application Manager) to get the application's running status, health condiction and so on;\n* As soon as the application finishes, Application Master unregisters to the Resource Manager and shut down the connection, and all the Containers will also be free.\n\n## Scheduling in YARN\nScheduling is one of the most important points in YARN. As we talked about in the Resouce Manager, the Scheduler in RM is responsible for scheduling, and actully there are mainly three kinds of scheduler in YARN, which are **FIFO**, **capacity** and **fair** scheduler. The figure below presents the differences among them.\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/23/23-2.png)\n\n### FIFO Scheduler\nAs name presents, FIFO, abbr of *firt in first out*, is the most basic scheduler, the submitted applications are all in a queue and executed one by one in order. The drawback is pretty significant, as shown in the figure, the whloe queue could be easily blocked by the **BIG** applications. Big application always need more time to be executed and the whole queue have to be waiting for the big application execution.\n\n### Capacity Scheduler\nCapacity schdeuler can solve the blocked queue problem in FIFO in some terms. Capacity Scheduler devide the source into multiple queues and some of them are for big applications specifically. Big applications in Capacity take less resource than in the FIFO, which makes the big application take longer time to execute in Capacity Scheduler.\n\n### Fair Scheduler\nHence neither FIFO nor Capacity are good enough for Scheduling, the Fair Scheduler are proposed and it's the most popular scheduler in YARN at present. Similar to the Capacity Scheduler, Fari also divide the cluster into multiple queues. However, the capacity, or the resource, of each queues is dynamical. Also the fraction of the queue source deviding can be modified in configuration.\nSuppose we have two queues, which are A and B, and the YARN is configured to Fari Scheduler. A start the application when B is free, then A can take over all the resource of queue B, but when B start an application while A is free, B can also take over all the resource. The key point of Fair Scheduler is that the resouce is fairly shared for all queues. \n\n## References\n* [Hadoop YARN Tutorial – Learn the Fundamentals of YARN Architecture](https://www.edureka.co/blog/hadoop-yarn-tutorial/)\n* [Hadoop YARN Resource Manager – A Yarn Tutorial](https://data-flair.training/blogs/hadoop-yarn-resource-manager/)\n* [Hadoop Yarn Concept with Fifo, Capacity and Fair Scheduling](http://timepasstechies.com/hadoop-yarn-concept-fifocapacity-fair-scheduling/)\n","source":"_posts/hdfs-yarn-architecture.md","raw":"---\ntitle: YARN Architecture\ndate: 2019-06-11 20:18:54\ntags:\n    - hdfs\n    - yarn\ncategories: hadoop\ntoc: true\n---\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/background/aerial-shot-bird-s-eye-view-forest-2415927.jpg)\nYARN, the abbreviation of *Yet Another Resource Negotiator*, is introduced in Hadoop 2.0. Compared with MRV1(MapReduce Version 1), YARN takes over the responsibility of **resource management and job scheduling** in MRV1, and make **non-MapReduce** jobs run on the Hadoop, Apache Spark for example. Although there are some rising technology that been treated as alternative of YARN by more and more developers, Kubernetes for instance, YARN is still widely used. Let's talk about YARN today. \n<!--more-->\n## Components\nYARN consists of one Resource Manager as master deamon, several slave daemon called Node Manager for each slave node and Application Master for each application.\n### Resource Master(RM)\nResouce Manager(RM) is replacement of **JobTracker** in MapReduce Version 1 and it is the master deamon in YARN and manages the global resources among all the applications. Resource Master has two main components, Scheduler and Application Manager\n#### Scheduler\nScheduler is mainly desgined to negotiate resource and it is responsible for allocating the resource for the running applications based on the abstract notion of a resource Container, such as memory, CPU etc. Notice that Container is the basic unit of resource in YARN. Scheduler allocate the resource based on the resource requirment of the application and the resource availabality. Scheduler doesn't care too much about the application monitoring or tracking, even the restarting failure no matter application or hardware failure.\n#### Applicaiton Manager\nApplication Manager is responsible for the maintaining a collection of submitted applications. Application Manager accept the submission of one application from the client and allocate one Container for the application to load its Application Master. What's more, Application Manager provides the monitoring and tracking service for the running applications and also restarting them if any failure come out.\n### Node Manager(NM)\nNode Manager, the update version of **TaskTrack** in MRV1, is the slave deamon in YARN. Node Manager is mainly responsible for the individual Containers management and monitoring. Node Manager tracks the usage of resource among all the Containers in the node and replay the detailed information to Resource Manager and Node Manager also monitors the heath of the running Containers(Node Manager only manage the resource and running status rather than application context information).\nNode Manager always keep in touch with Resouce Manager with heartbeats, \n### Application Master(AM)\nOne Application Mater runs for one application in YARN, which means each application has a unique Applicaiton Master. Application Master is responsible to send request to Resouce Manager and acquire resource from the Scheduler. What's more, Application Master is the process that coordinates the application's execution and manage the applicaiton faults along with the Node Manager. Also, Application Master sends heartbeats to the Resource Manager periodly to update the health condiction and resource demands.\nNotice that there is a main difference between the NM and AM, NM only monitors and tracks the status of the dividul Containers while AM the application itself. When an application is running, both NM and AM contributes to the monitoring and tracking, but they focus on the different part.\n\n## Applicaiton Submission on YARN\nAs we've learned each component of the YARN, let's take look at how these components work together to submit an application. The workflow is presented below.\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/23/23-1.png)\n\n* Client submits an application to the Resource Manager and request for building an Application Master;\n* Rsource Manager, Application Manager actually, allocates a free Container to load the Application Master for the submitted application and then the Application Master then will start;\n* Application Master registers to the Resource Manager, after which the client can communicate with the Application Master directly;\n* Application Master send request to acquire resource Container for the application to Resouce Manager;\n* Resource Manager(Scheduler) notify the Node Manager to launch Containers, and the launched Containers can communicate with Application Master;\n* Application code will executed in the Container, and the status can be updated to the Application Master by the *application-specific*;\n* While the application is running, client contacts to the Resource Manager(Application Manager) to get the application's running status, health condiction and so on;\n* As soon as the application finishes, Application Master unregisters to the Resource Manager and shut down the connection, and all the Containers will also be free.\n\n## Scheduling in YARN\nScheduling is one of the most important points in YARN. As we talked about in the Resouce Manager, the Scheduler in RM is responsible for scheduling, and actully there are mainly three kinds of scheduler in YARN, which are **FIFO**, **capacity** and **fair** scheduler. The figure below presents the differences among them.\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/23/23-2.png)\n\n### FIFO Scheduler\nAs name presents, FIFO, abbr of *firt in first out*, is the most basic scheduler, the submitted applications are all in a queue and executed one by one in order. The drawback is pretty significant, as shown in the figure, the whloe queue could be easily blocked by the **BIG** applications. Big application always need more time to be executed and the whole queue have to be waiting for the big application execution.\n\n### Capacity Scheduler\nCapacity schdeuler can solve the blocked queue problem in FIFO in some terms. Capacity Scheduler devide the source into multiple queues and some of them are for big applications specifically. Big applications in Capacity take less resource than in the FIFO, which makes the big application take longer time to execute in Capacity Scheduler.\n\n### Fair Scheduler\nHence neither FIFO nor Capacity are good enough for Scheduling, the Fair Scheduler are proposed and it's the most popular scheduler in YARN at present. Similar to the Capacity Scheduler, Fari also divide the cluster into multiple queues. However, the capacity, or the resource, of each queues is dynamical. Also the fraction of the queue source deviding can be modified in configuration.\nSuppose we have two queues, which are A and B, and the YARN is configured to Fari Scheduler. A start the application when B is free, then A can take over all the resource of queue B, but when B start an application while A is free, B can also take over all the resource. The key point of Fair Scheduler is that the resouce is fairly shared for all queues. \n\n## References\n* [Hadoop YARN Tutorial – Learn the Fundamentals of YARN Architecture](https://www.edureka.co/blog/hadoop-yarn-tutorial/)\n* [Hadoop YARN Resource Manager – A Yarn Tutorial](https://data-flair.training/blogs/hadoop-yarn-resource-manager/)\n* [Hadoop Yarn Concept with Fifo, Capacity and Fair Scheduling](http://timepasstechies.com/hadoop-yarn-concept-fifocapacity-fair-scheduling/)\n","slug":"hdfs-yarn-architecture","published":1,"updated":"2020-05-10T06:50:12.529Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cka0wnkwa000hqxotetgj0nh6","content":"<p><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/background/aerial-shot-bird-s-eye-view-forest-2415927.jpg\" alt=\"\"><br>YARN, the abbreviation of <em>Yet Another Resource Negotiator</em>, is introduced in Hadoop 2.0. Compared with MRV1(MapReduce Version 1), YARN takes over the responsibility of <strong>resource management and job scheduling</strong> in MRV1, and make <strong>non-MapReduce</strong> jobs run on the Hadoop, Apache Spark for example. Although there are some rising technology that been treated as alternative of YARN by more and more developers, Kubernetes for instance, YARN is still widely used. Let’s talk about YARN today.<br><a id=\"more\"></a></p>\n<h2 id=\"Components\"><a href=\"#Components\" class=\"headerlink\" title=\"Components\"></a>Components</h2><p>YARN consists of one Resource Manager as master deamon, several slave daemon called Node Manager for each slave node and Application Master for each application.</p>\n<h3 id=\"Resource-Master-RM\"><a href=\"#Resource-Master-RM\" class=\"headerlink\" title=\"Resource Master(RM)\"></a>Resource Master(RM)</h3><p>Resouce Manager(RM) is replacement of <strong>JobTracker</strong> in MapReduce Version 1 and it is the master deamon in YARN and manages the global resources among all the applications. Resource Master has two main components, Scheduler and Application Manager</p>\n<h4 id=\"Scheduler\"><a href=\"#Scheduler\" class=\"headerlink\" title=\"Scheduler\"></a>Scheduler</h4><p>Scheduler is mainly desgined to negotiate resource and it is responsible for allocating the resource for the running applications based on the abstract notion of a resource Container, such as memory, CPU etc. Notice that Container is the basic unit of resource in YARN. Scheduler allocate the resource based on the resource requirment of the application and the resource availabality. Scheduler doesn’t care too much about the application monitoring or tracking, even the restarting failure no matter application or hardware failure.</p>\n<h4 id=\"Applicaiton-Manager\"><a href=\"#Applicaiton-Manager\" class=\"headerlink\" title=\"Applicaiton Manager\"></a>Applicaiton Manager</h4><p>Application Manager is responsible for the maintaining a collection of submitted applications. Application Manager accept the submission of one application from the client and allocate one Container for the application to load its Application Master. What’s more, Application Manager provides the monitoring and tracking service for the running applications and also restarting them if any failure come out.</p>\n<h3 id=\"Node-Manager-NM\"><a href=\"#Node-Manager-NM\" class=\"headerlink\" title=\"Node Manager(NM)\"></a>Node Manager(NM)</h3><p>Node Manager, the update version of <strong>TaskTrack</strong> in MRV1, is the slave deamon in YARN. Node Manager is mainly responsible for the individual Containers management and monitoring. Node Manager tracks the usage of resource among all the Containers in the node and replay the detailed information to Resource Manager and Node Manager also monitors the heath of the running Containers(Node Manager only manage the resource and running status rather than application context information).<br>Node Manager always keep in touch with Resouce Manager with heartbeats, </p>\n<h3 id=\"Application-Master-AM\"><a href=\"#Application-Master-AM\" class=\"headerlink\" title=\"Application Master(AM)\"></a>Application Master(AM)</h3><p>One Application Mater runs for one application in YARN, which means each application has a unique Applicaiton Master. Application Master is responsible to send request to Resouce Manager and acquire resource from the Scheduler. What’s more, Application Master is the process that coordinates the application’s execution and manage the applicaiton faults along with the Node Manager. Also, Application Master sends heartbeats to the Resource Manager periodly to update the health condiction and resource demands.<br>Notice that there is a main difference between the NM and AM, NM only monitors and tracks the status of the dividul Containers while AM the application itself. When an application is running, both NM and AM contributes to the monitoring and tracking, but they focus on the different part.</p>\n<h2 id=\"Applicaiton-Submission-on-YARN\"><a href=\"#Applicaiton-Submission-on-YARN\" class=\"headerlink\" title=\"Applicaiton Submission on YARN\"></a>Applicaiton Submission on YARN</h2><p>As we’ve learned each component of the YARN, let’s take look at how these components work together to submit an application. The workflow is presented below.<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/23/23-1.png\" alt=\"\"></p>\n<ul>\n<li>Client submits an application to the Resource Manager and request for building an Application Master;</li>\n<li>Rsource Manager, Application Manager actually, allocates a free Container to load the Application Master for the submitted application and then the Application Master then will start;</li>\n<li>Application Master registers to the Resource Manager, after which the client can communicate with the Application Master directly;</li>\n<li>Application Master send request to acquire resource Container for the application to Resouce Manager;</li>\n<li>Resource Manager(Scheduler) notify the Node Manager to launch Containers, and the launched Containers can communicate with Application Master;</li>\n<li>Application code will executed in the Container, and the status can be updated to the Application Master by the <em>application-specific</em>;</li>\n<li>While the application is running, client contacts to the Resource Manager(Application Manager) to get the application’s running status, health condiction and so on;</li>\n<li>As soon as the application finishes, Application Master unregisters to the Resource Manager and shut down the connection, and all the Containers will also be free.</li>\n</ul>\n<h2 id=\"Scheduling-in-YARN\"><a href=\"#Scheduling-in-YARN\" class=\"headerlink\" title=\"Scheduling in YARN\"></a>Scheduling in YARN</h2><p>Scheduling is one of the most important points in YARN. As we talked about in the Resouce Manager, the Scheduler in RM is responsible for scheduling, and actully there are mainly three kinds of scheduler in YARN, which are <strong>FIFO</strong>, <strong>capacity</strong> and <strong>fair</strong> scheduler. The figure below presents the differences among them.<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/23/23-2.png\" alt=\"\"></p>\n<h3 id=\"FIFO-Scheduler\"><a href=\"#FIFO-Scheduler\" class=\"headerlink\" title=\"FIFO Scheduler\"></a>FIFO Scheduler</h3><p>As name presents, FIFO, abbr of <em>firt in first out</em>, is the most basic scheduler, the submitted applications are all in a queue and executed one by one in order. The drawback is pretty significant, as shown in the figure, the whloe queue could be easily blocked by the <strong>BIG</strong> applications. Big application always need more time to be executed and the whole queue have to be waiting for the big application execution.</p>\n<h3 id=\"Capacity-Scheduler\"><a href=\"#Capacity-Scheduler\" class=\"headerlink\" title=\"Capacity Scheduler\"></a>Capacity Scheduler</h3><p>Capacity schdeuler can solve the blocked queue problem in FIFO in some terms. Capacity Scheduler devide the source into multiple queues and some of them are for big applications specifically. Big applications in Capacity take less resource than in the FIFO, which makes the big application take longer time to execute in Capacity Scheduler.</p>\n<h3 id=\"Fair-Scheduler\"><a href=\"#Fair-Scheduler\" class=\"headerlink\" title=\"Fair Scheduler\"></a>Fair Scheduler</h3><p>Hence neither FIFO nor Capacity are good enough for Scheduling, the Fair Scheduler are proposed and it’s the most popular scheduler in YARN at present. Similar to the Capacity Scheduler, Fari also divide the cluster into multiple queues. However, the capacity, or the resource, of each queues is dynamical. Also the fraction of the queue source deviding can be modified in configuration.<br>Suppose we have two queues, which are A and B, and the YARN is configured to Fari Scheduler. A start the application when B is free, then A can take over all the resource of queue B, but when B start an application while A is free, B can also take over all the resource. The key point of Fair Scheduler is that the resouce is fairly shared for all queues. </p>\n<h2 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h2><ul>\n<li><a href=\"https://www.edureka.co/blog/hadoop-yarn-tutorial/\" target=\"_blank\" rel=\"noopener\">Hadoop YARN Tutorial – Learn the Fundamentals of YARN Architecture</a></li>\n<li><a href=\"https://data-flair.training/blogs/hadoop-yarn-resource-manager/\" target=\"_blank\" rel=\"noopener\">Hadoop YARN Resource Manager – A Yarn Tutorial</a></li>\n<li><a href=\"http://timepasstechies.com/hadoop-yarn-concept-fifocapacity-fair-scheduling/\" target=\"_blank\" rel=\"noopener\">Hadoop Yarn Concept with Fifo, Capacity and Fair Scheduling</a></li>\n</ul>\n","site":{"data":{}},"_categories":[{"name":"hadoop","path":"categories/hadoop/"}],"_tags":[{"name":"hdfs","path":"tags/hdfs/"},{"name":"yarn","path":"tags/yarn/"}],"excerpt":"<p><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/background/aerial-shot-bird-s-eye-view-forest-2415927.jpg\" alt=\"\"><br>YARN, the abbreviation of <em>Yet Another Resource Negotiator</em>, is introduced in Hadoop 2.0. Compared with MRV1(MapReduce Version 1), YARN takes over the responsibility of <strong>resource management and job scheduling</strong> in MRV1, and make <strong>non-MapReduce</strong> jobs run on the Hadoop, Apache Spark for example. Although there are some rising technology that been treated as alternative of YARN by more and more developers, Kubernetes for instance, YARN is still widely used. Let’s talk about YARN today.<br></p>","more":"</p>\n<h2 id=\"Components\"><a href=\"#Components\" class=\"headerlink\" title=\"Components\"></a>Components</h2><p>YARN consists of one Resource Manager as master deamon, several slave daemon called Node Manager for each slave node and Application Master for each application.</p>\n<h3 id=\"Resource-Master-RM\"><a href=\"#Resource-Master-RM\" class=\"headerlink\" title=\"Resource Master(RM)\"></a>Resource Master(RM)</h3><p>Resouce Manager(RM) is replacement of <strong>JobTracker</strong> in MapReduce Version 1 and it is the master deamon in YARN and manages the global resources among all the applications. Resource Master has two main components, Scheduler and Application Manager</p>\n<h4 id=\"Scheduler\"><a href=\"#Scheduler\" class=\"headerlink\" title=\"Scheduler\"></a>Scheduler</h4><p>Scheduler is mainly desgined to negotiate resource and it is responsible for allocating the resource for the running applications based on the abstract notion of a resource Container, such as memory, CPU etc. Notice that Container is the basic unit of resource in YARN. Scheduler allocate the resource based on the resource requirment of the application and the resource availabality. Scheduler doesn’t care too much about the application monitoring or tracking, even the restarting failure no matter application or hardware failure.</p>\n<h4 id=\"Applicaiton-Manager\"><a href=\"#Applicaiton-Manager\" class=\"headerlink\" title=\"Applicaiton Manager\"></a>Applicaiton Manager</h4><p>Application Manager is responsible for the maintaining a collection of submitted applications. Application Manager accept the submission of one application from the client and allocate one Container for the application to load its Application Master. What’s more, Application Manager provides the monitoring and tracking service for the running applications and also restarting them if any failure come out.</p>\n<h3 id=\"Node-Manager-NM\"><a href=\"#Node-Manager-NM\" class=\"headerlink\" title=\"Node Manager(NM)\"></a>Node Manager(NM)</h3><p>Node Manager, the update version of <strong>TaskTrack</strong> in MRV1, is the slave deamon in YARN. Node Manager is mainly responsible for the individual Containers management and monitoring. Node Manager tracks the usage of resource among all the Containers in the node and replay the detailed information to Resource Manager and Node Manager also monitors the heath of the running Containers(Node Manager only manage the resource and running status rather than application context information).<br>Node Manager always keep in touch with Resouce Manager with heartbeats, </p>\n<h3 id=\"Application-Master-AM\"><a href=\"#Application-Master-AM\" class=\"headerlink\" title=\"Application Master(AM)\"></a>Application Master(AM)</h3><p>One Application Mater runs for one application in YARN, which means each application has a unique Applicaiton Master. Application Master is responsible to send request to Resouce Manager and acquire resource from the Scheduler. What’s more, Application Master is the process that coordinates the application’s execution and manage the applicaiton faults along with the Node Manager. Also, Application Master sends heartbeats to the Resource Manager periodly to update the health condiction and resource demands.<br>Notice that there is a main difference between the NM and AM, NM only monitors and tracks the status of the dividul Containers while AM the application itself. When an application is running, both NM and AM contributes to the monitoring and tracking, but they focus on the different part.</p>\n<h2 id=\"Applicaiton-Submission-on-YARN\"><a href=\"#Applicaiton-Submission-on-YARN\" class=\"headerlink\" title=\"Applicaiton Submission on YARN\"></a>Applicaiton Submission on YARN</h2><p>As we’ve learned each component of the YARN, let’s take look at how these components work together to submit an application. The workflow is presented below.<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/23/23-1.png\" alt=\"\"></p>\n<ul>\n<li>Client submits an application to the Resource Manager and request for building an Application Master;</li>\n<li>Rsource Manager, Application Manager actually, allocates a free Container to load the Application Master for the submitted application and then the Application Master then will start;</li>\n<li>Application Master registers to the Resource Manager, after which the client can communicate with the Application Master directly;</li>\n<li>Application Master send request to acquire resource Container for the application to Resouce Manager;</li>\n<li>Resource Manager(Scheduler) notify the Node Manager to launch Containers, and the launched Containers can communicate with Application Master;</li>\n<li>Application code will executed in the Container, and the status can be updated to the Application Master by the <em>application-specific</em>;</li>\n<li>While the application is running, client contacts to the Resource Manager(Application Manager) to get the application’s running status, health condiction and so on;</li>\n<li>As soon as the application finishes, Application Master unregisters to the Resource Manager and shut down the connection, and all the Containers will also be free.</li>\n</ul>\n<h2 id=\"Scheduling-in-YARN\"><a href=\"#Scheduling-in-YARN\" class=\"headerlink\" title=\"Scheduling in YARN\"></a>Scheduling in YARN</h2><p>Scheduling is one of the most important points in YARN. As we talked about in the Resouce Manager, the Scheduler in RM is responsible for scheduling, and actully there are mainly three kinds of scheduler in YARN, which are <strong>FIFO</strong>, <strong>capacity</strong> and <strong>fair</strong> scheduler. The figure below presents the differences among them.<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/23/23-2.png\" alt=\"\"></p>\n<h3 id=\"FIFO-Scheduler\"><a href=\"#FIFO-Scheduler\" class=\"headerlink\" title=\"FIFO Scheduler\"></a>FIFO Scheduler</h3><p>As name presents, FIFO, abbr of <em>firt in first out</em>, is the most basic scheduler, the submitted applications are all in a queue and executed one by one in order. The drawback is pretty significant, as shown in the figure, the whloe queue could be easily blocked by the <strong>BIG</strong> applications. Big application always need more time to be executed and the whole queue have to be waiting for the big application execution.</p>\n<h3 id=\"Capacity-Scheduler\"><a href=\"#Capacity-Scheduler\" class=\"headerlink\" title=\"Capacity Scheduler\"></a>Capacity Scheduler</h3><p>Capacity schdeuler can solve the blocked queue problem in FIFO in some terms. Capacity Scheduler devide the source into multiple queues and some of them are for big applications specifically. Big applications in Capacity take less resource than in the FIFO, which makes the big application take longer time to execute in Capacity Scheduler.</p>\n<h3 id=\"Fair-Scheduler\"><a href=\"#Fair-Scheduler\" class=\"headerlink\" title=\"Fair Scheduler\"></a>Fair Scheduler</h3><p>Hence neither FIFO nor Capacity are good enough for Scheduling, the Fair Scheduler are proposed and it’s the most popular scheduler in YARN at present. Similar to the Capacity Scheduler, Fari also divide the cluster into multiple queues. However, the capacity, or the resource, of each queues is dynamical. Also the fraction of the queue source deviding can be modified in configuration.<br>Suppose we have two queues, which are A and B, and the YARN is configured to Fari Scheduler. A start the application when B is free, then A can take over all the resource of queue B, but when B start an application while A is free, B can also take over all the resource. The key point of Fair Scheduler is that the resouce is fairly shared for all queues. </p>\n<h2 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h2><ul>\n<li><a href=\"https://www.edureka.co/blog/hadoop-yarn-tutorial/\" target=\"_blank\" rel=\"noopener\">Hadoop YARN Tutorial – Learn the Fundamentals of YARN Architecture</a></li>\n<li><a href=\"https://data-flair.training/blogs/hadoop-yarn-resource-manager/\" target=\"_blank\" rel=\"noopener\">Hadoop YARN Resource Manager – A Yarn Tutorial</a></li>\n<li><a href=\"http://timepasstechies.com/hadoop-yarn-concept-fifocapacity-fair-scheduling/\" target=\"_blank\" rel=\"noopener\">Hadoop Yarn Concept with Fifo, Capacity and Fair Scheduling</a></li>\n</ul>"},{"title":"ORC Stored HIVE Tips","date":"2019-06-22T10:05:12.000Z","toc":"ture","_content":"![](https://github.com/JoeAsir/blog-image/raw/master/blog/background/balloon-bright-celebration-2388650.jpg)\nORC stored HIVE table has been very common in my daily work. Having a deep dive in ORC format recently, I realize that there are many awesome features in ORC format, and many of them have never been used before or somehow in the wrong way by me. So I take some time with some little tests and present two of the most exciting features of ORC stored HVE table in this blog. Let's start this! \nBTW, if you are not familiar with ORC, you can take a quick view from my [previous blog](https://joeasir.github.io/2019/05/05/hdfs-arvo-parquet-orc/#ORC).\n<!--more-->\n## Index Filter\nORC is a kind of columnar format, the basic structure and some terms are introduced in my [previous blog](https://joeasir.github.io/2019/05/05/hdfs-arvo-parquet-orc/#ORC). There are three kinds of the index in ORC file, file level, stripe level, and row level, all of which stores some statistics values such as max/min, sum value, and so on. If you want to learn the specific content of these statistics, you can use `hive --orcfiledump {path-of-your-orc-file}` to lean everything, such as schema, compression, stripe statistics, file statistics, you name it, about your ORC file.\nWhen the ORC file is been queried with a filter execution *WHERE Col1 > 10*, based on the statistics information like max and min value of *Col1*, the stripes whose max value of *Col1* could be skipped and as the result, the input records and mappers could be reduced.\nHowever, there is a little problem, if you want to take advantage of the index filter, you have to set *hive.optimize.index.filter=true*, but the good news is that this configuration is ignored in the HIVE 3.0.\nWhat's more, suppose the min/max value of the filtered column in each stripe is not overlapping, the index filter could be much more efficient, isn't it? So there is a tip for you, if the column is the major filter, it could be better to sort it as soon as it's inserted in the ORC table. I made a simple test and the status of non-sorted and sorted table present different performance under the same following query HIVEQL. \n```sql\nSELECT\n    SUM(Col1),\n    COUNT(1)\nFROM table\nWHERE Col > 1000\n```\nThere are significantly fewer input records and mappers when querying the sorted table, which is pretty awesome! \n![](https://github.com/JoeAsir/blog-image/raw/master/blog/24/24-1.png)\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/24/24-2.png)\n\nLast but not the least, *ORDER BY* in HIVEQL is such an expensive execution that all the records are put into a single reducer if you have a huge data, it's also a good idea to use *DISTRIBUTE BY* to instead.\n    \n## Vectorization\nVectorization, aka as Vectorized Query Execution, is an awesome feature for ORC stored HIVE table. It reduces the CPU usage by processing a block of 1024 rows instead of row by row. You can find detail information [here](https://cwiki.apache.org/confluence/display/Hive/Vectorized+Query+Execution)\nTo make leverage of Vectorization, we must set *hive.vectorized.execution.enabled=true*. I also make a comparison between the vectorized and standard execution. As the figures below, processing 1014 rows each time, the amount of input records is 1000 times less than the standard execution. However, I found hardly ever difference about the CPU time spent, maybe the execution of my SQL is too simple to distinguish the CPU status between vectorized and standard execution.\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/24/24-2.png)\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/24/24-3.png)\n\nThe vectorization in ORC is of good efficiency to reduce the input records in HIVE, and this feature has now been supported in Apache Spark since 2.3, see more [here](https://spark.apache.org/docs/latest/sql-data-sources-orc.html).\n    \n## References\n* [Vectorized Query Execution](https://cwiki.apache.org/confluence/display/Hive/Vectorized+Query+Execution)\n* [5 Ways to Make Your Hive Queries Run Faster](https://hortonworks.com/blog/5-ways-make-hive-queries-run-faster)\n","source":"_posts/hdfs-orc-tips.md","raw":"---\ntitle: ORC Stored HIVE Tips \ndate: 2019-06-22 18:05:12\ntags:\n    - hdfs\n    - hive\ncategories: hadoop\ntoc: ture\n---\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/background/balloon-bright-celebration-2388650.jpg)\nORC stored HIVE table has been very common in my daily work. Having a deep dive in ORC format recently, I realize that there are many awesome features in ORC format, and many of them have never been used before or somehow in the wrong way by me. So I take some time with some little tests and present two of the most exciting features of ORC stored HVE table in this blog. Let's start this! \nBTW, if you are not familiar with ORC, you can take a quick view from my [previous blog](https://joeasir.github.io/2019/05/05/hdfs-arvo-parquet-orc/#ORC).\n<!--more-->\n## Index Filter\nORC is a kind of columnar format, the basic structure and some terms are introduced in my [previous blog](https://joeasir.github.io/2019/05/05/hdfs-arvo-parquet-orc/#ORC). There are three kinds of the index in ORC file, file level, stripe level, and row level, all of which stores some statistics values such as max/min, sum value, and so on. If you want to learn the specific content of these statistics, you can use `hive --orcfiledump {path-of-your-orc-file}` to lean everything, such as schema, compression, stripe statistics, file statistics, you name it, about your ORC file.\nWhen the ORC file is been queried with a filter execution *WHERE Col1 > 10*, based on the statistics information like max and min value of *Col1*, the stripes whose max value of *Col1* could be skipped and as the result, the input records and mappers could be reduced.\nHowever, there is a little problem, if you want to take advantage of the index filter, you have to set *hive.optimize.index.filter=true*, but the good news is that this configuration is ignored in the HIVE 3.0.\nWhat's more, suppose the min/max value of the filtered column in each stripe is not overlapping, the index filter could be much more efficient, isn't it? So there is a tip for you, if the column is the major filter, it could be better to sort it as soon as it's inserted in the ORC table. I made a simple test and the status of non-sorted and sorted table present different performance under the same following query HIVEQL. \n```sql\nSELECT\n    SUM(Col1),\n    COUNT(1)\nFROM table\nWHERE Col > 1000\n```\nThere are significantly fewer input records and mappers when querying the sorted table, which is pretty awesome! \n![](https://github.com/JoeAsir/blog-image/raw/master/blog/24/24-1.png)\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/24/24-2.png)\n\nLast but not the least, *ORDER BY* in HIVEQL is such an expensive execution that all the records are put into a single reducer if you have a huge data, it's also a good idea to use *DISTRIBUTE BY* to instead.\n    \n## Vectorization\nVectorization, aka as Vectorized Query Execution, is an awesome feature for ORC stored HIVE table. It reduces the CPU usage by processing a block of 1024 rows instead of row by row. You can find detail information [here](https://cwiki.apache.org/confluence/display/Hive/Vectorized+Query+Execution)\nTo make leverage of Vectorization, we must set *hive.vectorized.execution.enabled=true*. I also make a comparison between the vectorized and standard execution. As the figures below, processing 1014 rows each time, the amount of input records is 1000 times less than the standard execution. However, I found hardly ever difference about the CPU time spent, maybe the execution of my SQL is too simple to distinguish the CPU status between vectorized and standard execution.\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/24/24-2.png)\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/24/24-3.png)\n\nThe vectorization in ORC is of good efficiency to reduce the input records in HIVE, and this feature has now been supported in Apache Spark since 2.3, see more [here](https://spark.apache.org/docs/latest/sql-data-sources-orc.html).\n    \n## References\n* [Vectorized Query Execution](https://cwiki.apache.org/confluence/display/Hive/Vectorized+Query+Execution)\n* [5 Ways to Make Your Hive Queries Run Faster](https://hortonworks.com/blog/5-ways-make-hive-queries-run-faster)\n","slug":"hdfs-orc-tips","published":1,"updated":"2020-05-10T06:50:12.529Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cka0wnkwc000mqxotj2kijogo","content":"<p><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/background/balloon-bright-celebration-2388650.jpg\" alt=\"\"><br>ORC stored HIVE table has been very common in my daily work. Having a deep dive in ORC format recently, I realize that there are many awesome features in ORC format, and many of them have never been used before or somehow in the wrong way by me. So I take some time with some little tests and present two of the most exciting features of ORC stored HVE table in this blog. Let’s start this!<br>BTW, if you are not familiar with ORC, you can take a quick view from my <a href=\"https://joeasir.github.io/2019/05/05/hdfs-arvo-parquet-orc/#ORC\" target=\"_blank\" rel=\"noopener\">previous blog</a>.<br><a id=\"more\"></a></p>\n<h2 id=\"Index-Filter\"><a href=\"#Index-Filter\" class=\"headerlink\" title=\"Index Filter\"></a>Index Filter</h2><p>ORC is a kind of columnar format, the basic structure and some terms are introduced in my <a href=\"https://joeasir.github.io/2019/05/05/hdfs-arvo-parquet-orc/#ORC\" target=\"_blank\" rel=\"noopener\">previous blog</a>. There are three kinds of the index in ORC file, file level, stripe level, and row level, all of which stores some statistics values such as max/min, sum value, and so on. If you want to learn the specific content of these statistics, you can use <code>hive --orcfiledump {path-of-your-orc-file}</code> to lean everything, such as schema, compression, stripe statistics, file statistics, you name it, about your ORC file.<br>When the ORC file is been queried with a filter execution <em>WHERE Col1 &gt; 10</em>, based on the statistics information like max and min value of <em>Col1</em>, the stripes whose max value of <em>Col1</em> could be skipped and as the result, the input records and mappers could be reduced.<br>However, there is a little problem, if you want to take advantage of the index filter, you have to set <em>hive.optimize.index.filter=true</em>, but the good news is that this configuration is ignored in the HIVE 3.0.<br>What’s more, suppose the min/max value of the filtered column in each stripe is not overlapping, the index filter could be much more efficient, isn’t it? So there is a tip for you, if the column is the major filter, it could be better to sort it as soon as it’s inserted in the ORC table. I made a simple test and the status of non-sorted and sorted table present different performance under the same following query HIVEQL.<br><figure class=\"highlight sql hljs\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"hljs-keyword\">SELECT</span></span><br><span class=\"line\">    <span class=\"hljs-keyword\">SUM</span>(Col1),</span><br><span class=\"line\">    <span class=\"hljs-keyword\">COUNT</span>(<span class=\"hljs-number\">1</span>)</span><br><span class=\"line\"><span class=\"hljs-keyword\">FROM</span> <span class=\"hljs-keyword\">table</span></span><br><span class=\"line\"><span class=\"hljs-keyword\">WHERE</span> <span class=\"hljs-keyword\">Col</span> &gt; <span class=\"hljs-number\">1000</span></span><br></pre></td></tr></table></figure></p>\n<p>There are significantly fewer input records and mappers when querying the sorted table, which is pretty awesome!<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/24/24-1.png\" alt=\"\"><br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/24/24-2.png\" alt=\"\"></p>\n<p>Last but not the least, <em>ORDER BY</em> in HIVEQL is such an expensive execution that all the records are put into a single reducer if you have a huge data, it’s also a good idea to use <em>DISTRIBUTE BY</em> to instead.</p>\n<h2 id=\"Vectorization\"><a href=\"#Vectorization\" class=\"headerlink\" title=\"Vectorization\"></a>Vectorization</h2><p>Vectorization, aka as Vectorized Query Execution, is an awesome feature for ORC stored HIVE table. It reduces the CPU usage by processing a block of 1024 rows instead of row by row. You can find detail information <a href=\"https://cwiki.apache.org/confluence/display/Hive/Vectorized+Query+Execution\" target=\"_blank\" rel=\"noopener\">here</a><br>To make leverage of Vectorization, we must set <em>hive.vectorized.execution.enabled=true</em>. I also make a comparison between the vectorized and standard execution. As the figures below, processing 1014 rows each time, the amount of input records is 1000 times less than the standard execution. However, I found hardly ever difference about the CPU time spent, maybe the execution of my SQL is too simple to distinguish the CPU status between vectorized and standard execution.<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/24/24-2.png\" alt=\"\"><br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/24/24-3.png\" alt=\"\"></p>\n<p>The vectorization in ORC is of good efficiency to reduce the input records in HIVE, and this feature has now been supported in Apache Spark since 2.3, see more <a href=\"https://spark.apache.org/docs/latest/sql-data-sources-orc.html\" target=\"_blank\" rel=\"noopener\">here</a>.</p>\n<h2 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h2><ul>\n<li><a href=\"https://cwiki.apache.org/confluence/display/Hive/Vectorized+Query+Execution\" target=\"_blank\" rel=\"noopener\">Vectorized Query Execution</a></li>\n<li><a href=\"https://hortonworks.com/blog/5-ways-make-hive-queries-run-faster\" target=\"_blank\" rel=\"noopener\">5 Ways to Make Your Hive Queries Run Faster</a></li>\n</ul>\n","site":{"data":{}},"_categories":[{"name":"hadoop","path":"categories/hadoop/"}],"_tags":[{"name":"hdfs","path":"tags/hdfs/"},{"name":"hive","path":"tags/hive/"}],"excerpt":"<p><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/background/balloon-bright-celebration-2388650.jpg\" alt=\"\"><br>ORC stored HIVE table has been very common in my daily work. Having a deep dive in ORC format recently, I realize that there are many awesome features in ORC format, and many of them have never been used before or somehow in the wrong way by me. So I take some time with some little tests and present two of the most exciting features of ORC stored HVE table in this blog. Let’s start this!<br>BTW, if you are not familiar with ORC, you can take a quick view from my <a href=\"https://joeasir.github.io/2019/05/05/hdfs-arvo-parquet-orc/#ORC\" target=\"_blank\" rel=\"noopener\">previous blog</a>.<br></p>","more":"</p>\n<h2 id=\"Index-Filter\"><a href=\"#Index-Filter\" class=\"headerlink\" title=\"Index Filter\"></a>Index Filter</h2><p>ORC is a kind of columnar format, the basic structure and some terms are introduced in my <a href=\"https://joeasir.github.io/2019/05/05/hdfs-arvo-parquet-orc/#ORC\" target=\"_blank\" rel=\"noopener\">previous blog</a>. There are three kinds of the index in ORC file, file level, stripe level, and row level, all of which stores some statistics values such as max/min, sum value, and so on. If you want to learn the specific content of these statistics, you can use <code>hive --orcfiledump {path-of-your-orc-file}</code> to lean everything, such as schema, compression, stripe statistics, file statistics, you name it, about your ORC file.<br>When the ORC file is been queried with a filter execution <em>WHERE Col1 &gt; 10</em>, based on the statistics information like max and min value of <em>Col1</em>, the stripes whose max value of <em>Col1</em> could be skipped and as the result, the input records and mappers could be reduced.<br>However, there is a little problem, if you want to take advantage of the index filter, you have to set <em>hive.optimize.index.filter=true</em>, but the good news is that this configuration is ignored in the HIVE 3.0.<br>What’s more, suppose the min/max value of the filtered column in each stripe is not overlapping, the index filter could be much more efficient, isn’t it? So there is a tip for you, if the column is the major filter, it could be better to sort it as soon as it’s inserted in the ORC table. I made a simple test and the status of non-sorted and sorted table present different performance under the same following query HIVEQL.<br><figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">SELECT</span></span><br><span class=\"line\">    <span class=\"keyword\">SUM</span>(Col1),</span><br><span class=\"line\">    <span class=\"keyword\">COUNT</span>(<span class=\"number\">1</span>)</span><br><span class=\"line\"><span class=\"keyword\">FROM</span> <span class=\"keyword\">table</span></span><br><span class=\"line\"><span class=\"keyword\">WHERE</span> <span class=\"keyword\">Col</span> &gt; <span class=\"number\">1000</span></span><br></pre></td></tr></table></figure></p>\n<p>There are significantly fewer input records and mappers when querying the sorted table, which is pretty awesome!<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/24/24-1.png\" alt=\"\"><br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/24/24-2.png\" alt=\"\"></p>\n<p>Last but not the least, <em>ORDER BY</em> in HIVEQL is such an expensive execution that all the records are put into a single reducer if you have a huge data, it’s also a good idea to use <em>DISTRIBUTE BY</em> to instead.</p>\n<h2 id=\"Vectorization\"><a href=\"#Vectorization\" class=\"headerlink\" title=\"Vectorization\"></a>Vectorization</h2><p>Vectorization, aka as Vectorized Query Execution, is an awesome feature for ORC stored HIVE table. It reduces the CPU usage by processing a block of 1024 rows instead of row by row. You can find detail information <a href=\"https://cwiki.apache.org/confluence/display/Hive/Vectorized+Query+Execution\" target=\"_blank\" rel=\"noopener\">here</a><br>To make leverage of Vectorization, we must set <em>hive.vectorized.execution.enabled=true</em>. I also make a comparison between the vectorized and standard execution. As the figures below, processing 1014 rows each time, the amount of input records is 1000 times less than the standard execution. However, I found hardly ever difference about the CPU time spent, maybe the execution of my SQL is too simple to distinguish the CPU status between vectorized and standard execution.<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/24/24-2.png\" alt=\"\"><br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/24/24-3.png\" alt=\"\"></p>\n<p>The vectorization in ORC is of good efficiency to reduce the input records in HIVE, and this feature has now been supported in Apache Spark since 2.3, see more <a href=\"https://spark.apache.org/docs/latest/sql-data-sources-orc.html\" target=\"_blank\" rel=\"noopener\">here</a>.</p>\n<h2 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h2><ul>\n<li><a href=\"https://cwiki.apache.org/confluence/display/Hive/Vectorized+Query+Execution\" target=\"_blank\" rel=\"noopener\">Vectorized Query Execution</a></li>\n<li><a href=\"https://hortonworks.com/blog/5-ways-make-hive-queries-run-faster\" target=\"_blank\" rel=\"noopener\">5 Ways to Make Your Hive Queries Run Faster</a></li>\n</ul>"},{"title":"HDFS Architecture","date":"2019-04-28T10:25:30.000Z","toc":true,"_content":"![](https://github.com/JoeAsir/blog-image/raw/master/blog/background/clouds-dark-dark-clouds-2308671.jpg)\nI've been using HDFS as storage for almost 3 years reading data from and writing data to it by HIVE and Spark, but I've never learned the detail. Finally I have some time to watch the [Big Data Essentials](https://www.coursera.org/learn/big-data-essentials/) on [Coursera](https://www.coursera.org/), which inspired me to have a deep dive in HDFS architecture. This blog contains so much about HDFS that I spent 3 days to sum up and mark them down. If anything is worng, it's very nice of you to tell me and I'll figure it out! Let's take a look.\n<!--more-->\n## Namenode\nNamenode is the **master node** of the HDFS, it contains the metadata of the filesystem, such as the number and location of the block, replica and so on. Notice the metadata is stored in-memory for the fast retrival of data.\n> Task of Namenode\n* Manage file system namespace.\n* Regulates client’s access to files.\n* It also executes file system execution such as naming, closing, opening files/directories.\n* All DataNodes sends a Heartbeat and block report to the NameNode in the Hadoop cluster. It ensures that the DataNodes are alive. A block report contains a list of all blocks on a datanode.\n* NameNode is also responsible for taking care of the Replication Factor of all the blocks.\n\n### FsImage & EditLogs\nThe HDFS Namenade, as the Master Node, manages the whole architecture of the filesystem by the metadata. Data in the metadata are present as FsImage and EditLogs. Let's the detail about them.\n* FsImage: FsImage contains the image of the entire filesystem, including serialized form of all the **directories** and file **inodes**. The FsImage is actually stored as a **local file** in the filesystem in Namenode and actually, you can treat FsImage as a snapshot of the present filesystem architecture.\n  > Each inode is an internal representation of file or directory’s metadata.  \n  \n  * EditLogs: Editlogs contains the modification made to the entire filesystem on the most recently FsImage, such as creating, moving, updating, deleting and so on. Also, EditLogs is stored on the Namenode as a local file, similar to the FsImage.\n  \nAs you may get it, FsImage and EditLogs, one for storing the present situation and one for the modification. With the help of these two files, Namenode could can recover the matadata in case of something unexpected. Let's go ahead to the specific strategy.\n  \n### Secondary Namenode(Checkpoint Node) & Backup Node\nFsImage and EditLogs can help us recover the entire HDFS, but is there any problem? Suppose the Namenode has been running for a month, and once it restarted, the Namenode would read the FsImage and EditLogs to rebuild the state of the HDFS. However, since the Namenode has been running for such a long time that the EditLogs could be so large that it would spend a long time to load and parse, even for few hours, which is unacceptable for us. To solve this problem, let's have a look at the Secondary Namenode and Backup Node.\n* Secondary Namenode(Checkpoint Node): Secondary Namenode runs on another machine apart from the Namenode, it **fetches the FsImage and EditLogs periodically** from the Namenode and merge them to a start-of-art FsImage, aka a checkpoint, and push it back to the Namenode(it may be little confusing that so-called Secondary Namenode would not upload the checkpoint automaticlly while the checkpoint Node would, I'm not quite sure about it actually). \n* Backup Node: Take care that Backup Node is a different term from Secondary Namenode or Checkpoint Node. Backup Node doesn't fetch the FsImage and EditLogs periodically because it receive a filesystem edit stream from the Namenode. As a result, the state image is always stored **in-memory** on the Backup Node.\n    > The Backup Node provides the same functionality as the Checkpoint Node, but is synchronized with the NameNode. \n    \n## Datanode\nDatanode is the **slaver node** of HDFS, which is actually where the data stored.\n> Tasks of Datanode \n* Block replica creation, deletion, and replication according to the instruction of Namenode.\n* DataNode manages data storage of the system.\n* DataNodes send heartbeat to the NameNode to report the health of HDFS. By default, this frequency is set to 3 seconds.\n\n## Block & Replica \n### Block\nBlock is unit of data stored on HDFS, which cannot be controlled by us and the value is often 128M by default. Why we need data block? Suppose we have two files stored on the HDFS, one is bigger, 1G for instance, and another is 129M. As these two files been read synchronously, it could be an imbalanced progress. But what if we split all the files to a same unit size and these splitted units would be read in balance. That's why we need data block in HDFS. \nSince we've learnt why we need data block, you may ask, why is data block 128M? When files splitted by data block size, instead of one huge single file, few small chunks are stored on HDFS, and the main information of these chunks, aka metadata, are stored in-memory on the Namenode, including block size, block location and so on. So if the block size is too small that the chunks would be too many, then the Namenode in-memory stroage would be under great pressure. And that's also why the **small files problem** damage to your HDFS. However, on the other hand, too large block size would make the reading data process on datanode slow, which is not a good situation for HDFS. It's a trade-off, and that's why we choose 128M as an eclectic solution for HDFS. \n### Replication Management via Rack Awareness\nAs written to the HDFS, a single file would be divided into many blocks and these blocks would be stored across the cluster, at the same time, the replica of each block is created and there are 3 replicas by default which is can be modified in setting. The replica is actually the backup data from the blocks in case of the potentially unfavorable conditions of the Datanode, aka HDFS fault tolerance. Let's take a look at how HDFS manage the replication under the rack awareness.\nEvery Datanode in a cluster is actually a single machine, and several Datanodes are put on one rack for better management and they share the network. Several racks are set in one data center and one cluster may be built across several data centers, which could be in different areas even counties, as results, the network distance between each nodes are different. See the figure below.\nWhen the cluster start to write data to the HDFS, Namenode chooses the Datanode which is closer to the same rack or nearby rack to the write request. This distance is calculated by the rules below. Rack Awareness will choose the Datanode which is closer to get rid of too much network commuication cost.\n* Distance is 0 when data in the same node;\n* Distance is 2 when data in two different nodes but the same rack;\n* Distance is 4 when data in two different racks but the same data center;\n* Distance is 6 when data in two different data centers.\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/21/21-1.png)\n\nVia Rack Awareness, Namenode will not only choose the namenode to store the data, but also the replicas. Let's make a sample with one data block, once the data block has been already stored on the Datanode, the first replica will be stored in the local Datanode, then the second replica will be cast to another Datanode in the different rack and the third replica will be stored at the different Datanode on the local rack of the second replica. \n![](https://github.com/JoeAsir/blog-image/raw/master/blog/21/21-2.png)\n> A simple but nonoptimal policy is to place replicas on the different racks. This prevents losing data when an entire rack fails and allows us to use bandwidth from multiple racks while reading the data. This policy evenly distributes the data among replicas in the cluster which makes it easy to balance load in case of component failure. But the biggest drawback of this policy is that it will increase the cost of write operation because a writer needs to transfer blocks to multiple racks and communication between the two nodes in different racks has to go through switches.\n\n## Read & Write \n### Read Operation \n1. Client opens the file by the *DistributedFileSystem* object;\n2. *DistributedFileSystem* calls the Namenode via RPC and get the blocks and replicas location according to the distance between datanode and client, and a *FSDataInputStream* is also returned;\n3. With the address of Datanotes, *FSDataInputStream* open the I/O stream and bring data from Datanodes back to the client;\n4. Once the reading is finished, client will call *close()* to end up the stream.\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/21/21-3.png)\n\n> If the *DFSInputStream* encounters an error while communicating with a datanode, it will try the next closest one for that block. It will also remember datanodes that have failed so that it doesn’t needlessly retry them for later blocks. The *DFSInputStream* also verifies checksums for the data transferred to it from the datanode. If it finds a corrupt block, it reports this to the namenode before the *DFSInputStream* attempts to read a replica of the block from another datanode.\n\n### Write Operation\n1. Client sends a **create** request on the *DistributedFileSystem*, and *DistributedFileSystem* makes a RPC call to the Namenode to create a new file in the filesystem's namespace, and Namenode would check for the file names, permission and so on. And a *FSDataOutputStream* containing the Datanode location is returned by the Namenode if everything is OK;\n2. *FSDataOutputStream* would split the data into packets and make them a queue, aka data queue, consumed by the *DataStreamer*, which would allocate new blocks by picking a list of suitable Datanodes to store the replica from the Namenode;\n3. Assume that the replication factor is set to 3, the list of Datanodes form a pipeline containing 3 Datanodes(the first replica is stored in the local Datanode, so there are 3 Datanodes instead of 4);\n4. *DataStreamer* streams the packet to the first Datanode in the pipeline and then forwards it to the second one, then the third one.\n5. *FSDataOutputStream* also maintains an interal queue of packets waiting for the acknowledge by Datanodes. Once the acknowledge is send from Datanode in the pipeline, which is sent when the block is stored and the replicas are created, the packet is removed from the packet queue.\n6. All the blocks are stored and replicated on the different datanodes, the data blocks are copied in parallel.\n7. Client calls *close()* when writing operation finished.\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/21/21-4.png)\n \n ## References\n * [Hadoop - NameNode, Checkpoint Node and Backup Node](https://morrisjobke.de/2013/12/11/Hadoop-NameNode-and-siblings/)\n * [Hadoop HDFS Architecture Explanation and Assumptions](https://data-flair.training/blogs/hadoop-hdfs-architecture/)\n * [Big Data Essentials: HDFS, MapReduce and Spark RDD](https://www.coursera.org/learn/big-data-essentials/)\n","source":"_posts/hdfs-architecture.md","raw":"---\ntitle: HDFS Architecture\ndate: 2019-04-28 18:25:30\ntags: \n    - hdfs\n    - hadoop\ncategories: hadoop\ntoc: true\n---\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/background/clouds-dark-dark-clouds-2308671.jpg)\nI've been using HDFS as storage for almost 3 years reading data from and writing data to it by HIVE and Spark, but I've never learned the detail. Finally I have some time to watch the [Big Data Essentials](https://www.coursera.org/learn/big-data-essentials/) on [Coursera](https://www.coursera.org/), which inspired me to have a deep dive in HDFS architecture. This blog contains so much about HDFS that I spent 3 days to sum up and mark them down. If anything is worng, it's very nice of you to tell me and I'll figure it out! Let's take a look.\n<!--more-->\n## Namenode\nNamenode is the **master node** of the HDFS, it contains the metadata of the filesystem, such as the number and location of the block, replica and so on. Notice the metadata is stored in-memory for the fast retrival of data.\n> Task of Namenode\n* Manage file system namespace.\n* Regulates client’s access to files.\n* It also executes file system execution such as naming, closing, opening files/directories.\n* All DataNodes sends a Heartbeat and block report to the NameNode in the Hadoop cluster. It ensures that the DataNodes are alive. A block report contains a list of all blocks on a datanode.\n* NameNode is also responsible for taking care of the Replication Factor of all the blocks.\n\n### FsImage & EditLogs\nThe HDFS Namenade, as the Master Node, manages the whole architecture of the filesystem by the metadata. Data in the metadata are present as FsImage and EditLogs. Let's the detail about them.\n* FsImage: FsImage contains the image of the entire filesystem, including serialized form of all the **directories** and file **inodes**. The FsImage is actually stored as a **local file** in the filesystem in Namenode and actually, you can treat FsImage as a snapshot of the present filesystem architecture.\n  > Each inode is an internal representation of file or directory’s metadata.  \n  \n  * EditLogs: Editlogs contains the modification made to the entire filesystem on the most recently FsImage, such as creating, moving, updating, deleting and so on. Also, EditLogs is stored on the Namenode as a local file, similar to the FsImage.\n  \nAs you may get it, FsImage and EditLogs, one for storing the present situation and one for the modification. With the help of these two files, Namenode could can recover the matadata in case of something unexpected. Let's go ahead to the specific strategy.\n  \n### Secondary Namenode(Checkpoint Node) & Backup Node\nFsImage and EditLogs can help us recover the entire HDFS, but is there any problem? Suppose the Namenode has been running for a month, and once it restarted, the Namenode would read the FsImage and EditLogs to rebuild the state of the HDFS. However, since the Namenode has been running for such a long time that the EditLogs could be so large that it would spend a long time to load and parse, even for few hours, which is unacceptable for us. To solve this problem, let's have a look at the Secondary Namenode and Backup Node.\n* Secondary Namenode(Checkpoint Node): Secondary Namenode runs on another machine apart from the Namenode, it **fetches the FsImage and EditLogs periodically** from the Namenode and merge them to a start-of-art FsImage, aka a checkpoint, and push it back to the Namenode(it may be little confusing that so-called Secondary Namenode would not upload the checkpoint automaticlly while the checkpoint Node would, I'm not quite sure about it actually). \n* Backup Node: Take care that Backup Node is a different term from Secondary Namenode or Checkpoint Node. Backup Node doesn't fetch the FsImage and EditLogs periodically because it receive a filesystem edit stream from the Namenode. As a result, the state image is always stored **in-memory** on the Backup Node.\n    > The Backup Node provides the same functionality as the Checkpoint Node, but is synchronized with the NameNode. \n    \n## Datanode\nDatanode is the **slaver node** of HDFS, which is actually where the data stored.\n> Tasks of Datanode \n* Block replica creation, deletion, and replication according to the instruction of Namenode.\n* DataNode manages data storage of the system.\n* DataNodes send heartbeat to the NameNode to report the health of HDFS. By default, this frequency is set to 3 seconds.\n\n## Block & Replica \n### Block\nBlock is unit of data stored on HDFS, which cannot be controlled by us and the value is often 128M by default. Why we need data block? Suppose we have two files stored on the HDFS, one is bigger, 1G for instance, and another is 129M. As these two files been read synchronously, it could be an imbalanced progress. But what if we split all the files to a same unit size and these splitted units would be read in balance. That's why we need data block in HDFS. \nSince we've learnt why we need data block, you may ask, why is data block 128M? When files splitted by data block size, instead of one huge single file, few small chunks are stored on HDFS, and the main information of these chunks, aka metadata, are stored in-memory on the Namenode, including block size, block location and so on. So if the block size is too small that the chunks would be too many, then the Namenode in-memory stroage would be under great pressure. And that's also why the **small files problem** damage to your HDFS. However, on the other hand, too large block size would make the reading data process on datanode slow, which is not a good situation for HDFS. It's a trade-off, and that's why we choose 128M as an eclectic solution for HDFS. \n### Replication Management via Rack Awareness\nAs written to the HDFS, a single file would be divided into many blocks and these blocks would be stored across the cluster, at the same time, the replica of each block is created and there are 3 replicas by default which is can be modified in setting. The replica is actually the backup data from the blocks in case of the potentially unfavorable conditions of the Datanode, aka HDFS fault tolerance. Let's take a look at how HDFS manage the replication under the rack awareness.\nEvery Datanode in a cluster is actually a single machine, and several Datanodes are put on one rack for better management and they share the network. Several racks are set in one data center and one cluster may be built across several data centers, which could be in different areas even counties, as results, the network distance between each nodes are different. See the figure below.\nWhen the cluster start to write data to the HDFS, Namenode chooses the Datanode which is closer to the same rack or nearby rack to the write request. This distance is calculated by the rules below. Rack Awareness will choose the Datanode which is closer to get rid of too much network commuication cost.\n* Distance is 0 when data in the same node;\n* Distance is 2 when data in two different nodes but the same rack;\n* Distance is 4 when data in two different racks but the same data center;\n* Distance is 6 when data in two different data centers.\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/21/21-1.png)\n\nVia Rack Awareness, Namenode will not only choose the namenode to store the data, but also the replicas. Let's make a sample with one data block, once the data block has been already stored on the Datanode, the first replica will be stored in the local Datanode, then the second replica will be cast to another Datanode in the different rack and the third replica will be stored at the different Datanode on the local rack of the second replica. \n![](https://github.com/JoeAsir/blog-image/raw/master/blog/21/21-2.png)\n> A simple but nonoptimal policy is to place replicas on the different racks. This prevents losing data when an entire rack fails and allows us to use bandwidth from multiple racks while reading the data. This policy evenly distributes the data among replicas in the cluster which makes it easy to balance load in case of component failure. But the biggest drawback of this policy is that it will increase the cost of write operation because a writer needs to transfer blocks to multiple racks and communication between the two nodes in different racks has to go through switches.\n\n## Read & Write \n### Read Operation \n1. Client opens the file by the *DistributedFileSystem* object;\n2. *DistributedFileSystem* calls the Namenode via RPC and get the blocks and replicas location according to the distance between datanode and client, and a *FSDataInputStream* is also returned;\n3. With the address of Datanotes, *FSDataInputStream* open the I/O stream and bring data from Datanodes back to the client;\n4. Once the reading is finished, client will call *close()* to end up the stream.\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/21/21-3.png)\n\n> If the *DFSInputStream* encounters an error while communicating with a datanode, it will try the next closest one for that block. It will also remember datanodes that have failed so that it doesn’t needlessly retry them for later blocks. The *DFSInputStream* also verifies checksums for the data transferred to it from the datanode. If it finds a corrupt block, it reports this to the namenode before the *DFSInputStream* attempts to read a replica of the block from another datanode.\n\n### Write Operation\n1. Client sends a **create** request on the *DistributedFileSystem*, and *DistributedFileSystem* makes a RPC call to the Namenode to create a new file in the filesystem's namespace, and Namenode would check for the file names, permission and so on. And a *FSDataOutputStream* containing the Datanode location is returned by the Namenode if everything is OK;\n2. *FSDataOutputStream* would split the data into packets and make them a queue, aka data queue, consumed by the *DataStreamer*, which would allocate new blocks by picking a list of suitable Datanodes to store the replica from the Namenode;\n3. Assume that the replication factor is set to 3, the list of Datanodes form a pipeline containing 3 Datanodes(the first replica is stored in the local Datanode, so there are 3 Datanodes instead of 4);\n4. *DataStreamer* streams the packet to the first Datanode in the pipeline and then forwards it to the second one, then the third one.\n5. *FSDataOutputStream* also maintains an interal queue of packets waiting for the acknowledge by Datanodes. Once the acknowledge is send from Datanode in the pipeline, which is sent when the block is stored and the replicas are created, the packet is removed from the packet queue.\n6. All the blocks are stored and replicated on the different datanodes, the data blocks are copied in parallel.\n7. Client calls *close()* when writing operation finished.\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/21/21-4.png)\n \n ## References\n * [Hadoop - NameNode, Checkpoint Node and Backup Node](https://morrisjobke.de/2013/12/11/Hadoop-NameNode-and-siblings/)\n * [Hadoop HDFS Architecture Explanation and Assumptions](https://data-flair.training/blogs/hadoop-hdfs-architecture/)\n * [Big Data Essentials: HDFS, MapReduce and Spark RDD](https://www.coursera.org/learn/big-data-essentials/)\n","slug":"hdfs-architecture","published":1,"updated":"2020-05-10T06:50:12.529Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cka0wnkwd000oqxotjlkpcakg","content":"<p><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/background/clouds-dark-dark-clouds-2308671.jpg\" alt=\"\"><br>I’ve been using HDFS as storage for almost 3 years reading data from and writing data to it by HIVE and Spark, but I’ve never learned the detail. Finally I have some time to watch the <a href=\"https://www.coursera.org/learn/big-data-essentials/\" target=\"_blank\" rel=\"noopener\">Big Data Essentials</a> on <a href=\"https://www.coursera.org/\" target=\"_blank\" rel=\"noopener\">Coursera</a>, which inspired me to have a deep dive in HDFS architecture. This blog contains so much about HDFS that I spent 3 days to sum up and mark them down. If anything is worng, it’s very nice of you to tell me and I’ll figure it out! Let’s take a look.<br><a id=\"more\"></a></p>\n<h2 id=\"Namenode\"><a href=\"#Namenode\" class=\"headerlink\" title=\"Namenode\"></a>Namenode</h2><p>Namenode is the <strong>master node</strong> of the HDFS, it contains the metadata of the filesystem, such as the number and location of the block, replica and so on. Notice the metadata is stored in-memory for the fast retrival of data.</p>\n<blockquote>\n<p>Task of Namenode</p>\n<ul>\n<li>Manage file system namespace.</li>\n<li>Regulates client’s access to files.</li>\n<li>It also executes file system execution such as naming, closing, opening files/directories.</li>\n<li>All DataNodes sends a Heartbeat and block report to the NameNode in the Hadoop cluster. It ensures that the DataNodes are alive. A block report contains a list of all blocks on a datanode.</li>\n<li>NameNode is also responsible for taking care of the Replication Factor of all the blocks.</li>\n</ul>\n</blockquote>\n<h3 id=\"FsImage-amp-EditLogs\"><a href=\"#FsImage-amp-EditLogs\" class=\"headerlink\" title=\"FsImage &amp; EditLogs\"></a>FsImage &amp; EditLogs</h3><p>The HDFS Namenade, as the Master Node, manages the whole architecture of the filesystem by the metadata. Data in the metadata are present as FsImage and EditLogs. Let’s the detail about them.</p>\n<ul>\n<li><p>FsImage: FsImage contains the image of the entire filesystem, including serialized form of all the <strong>directories</strong> and file <strong>inodes</strong>. The FsImage is actually stored as a <strong>local file</strong> in the filesystem in Namenode and actually, you can treat FsImage as a snapshot of the present filesystem architecture.</p>\n<blockquote>\n<p>Each inode is an internal representation of file or directory’s metadata.  </p>\n</blockquote>\n<ul>\n<li>EditLogs: Editlogs contains the modification made to the entire filesystem on the most recently FsImage, such as creating, moving, updating, deleting and so on. Also, EditLogs is stored on the Namenode as a local file, similar to the FsImage.</li>\n</ul>\n</li>\n</ul>\n<p>As you may get it, FsImage and EditLogs, one for storing the present situation and one for the modification. With the help of these two files, Namenode could can recover the matadata in case of something unexpected. Let’s go ahead to the specific strategy.</p>\n<h3 id=\"Secondary-Namenode-Checkpoint-Node-amp-Backup-Node\"><a href=\"#Secondary-Namenode-Checkpoint-Node-amp-Backup-Node\" class=\"headerlink\" title=\"Secondary Namenode(Checkpoint Node) &amp; Backup Node\"></a>Secondary Namenode(Checkpoint Node) &amp; Backup Node</h3><p>FsImage and EditLogs can help us recover the entire HDFS, but is there any problem? Suppose the Namenode has been running for a month, and once it restarted, the Namenode would read the FsImage and EditLogs to rebuild the state of the HDFS. However, since the Namenode has been running for such a long time that the EditLogs could be so large that it would spend a long time to load and parse, even for few hours, which is unacceptable for us. To solve this problem, let’s have a look at the Secondary Namenode and Backup Node.</p>\n<ul>\n<li>Secondary Namenode(Checkpoint Node): Secondary Namenode runs on another machine apart from the Namenode, it <strong>fetches the FsImage and EditLogs periodically</strong> from the Namenode and merge them to a start-of-art FsImage, aka a checkpoint, and push it back to the Namenode(it may be little confusing that so-called Secondary Namenode would not upload the checkpoint automaticlly while the checkpoint Node would, I’m not quite sure about it actually). </li>\n<li>Backup Node: Take care that Backup Node is a different term from Secondary Namenode or Checkpoint Node. Backup Node doesn’t fetch the FsImage and EditLogs periodically because it receive a filesystem edit stream from the Namenode. As a result, the state image is always stored <strong>in-memory</strong> on the Backup Node.<blockquote>\n<p>The Backup Node provides the same functionality as the Checkpoint Node, but is synchronized with the NameNode. </p>\n</blockquote>\n</li>\n</ul>\n<h2 id=\"Datanode\"><a href=\"#Datanode\" class=\"headerlink\" title=\"Datanode\"></a>Datanode</h2><p>Datanode is the <strong>slaver node</strong> of HDFS, which is actually where the data stored.</p>\n<blockquote>\n<p>Tasks of Datanode </p>\n<ul>\n<li>Block replica creation, deletion, and replication according to the instruction of Namenode.</li>\n<li>DataNode manages data storage of the system.</li>\n<li>DataNodes send heartbeat to the NameNode to report the health of HDFS. By default, this frequency is set to 3 seconds.</li>\n</ul>\n</blockquote>\n<h2 id=\"Block-amp-Replica\"><a href=\"#Block-amp-Replica\" class=\"headerlink\" title=\"Block &amp; Replica\"></a>Block &amp; Replica</h2><h3 id=\"Block\"><a href=\"#Block\" class=\"headerlink\" title=\"Block\"></a>Block</h3><p>Block is unit of data stored on HDFS, which cannot be controlled by us and the value is often 128M by default. Why we need data block? Suppose we have two files stored on the HDFS, one is bigger, 1G for instance, and another is 129M. As these two files been read synchronously, it could be an imbalanced progress. But what if we split all the files to a same unit size and these splitted units would be read in balance. That’s why we need data block in HDFS.<br>Since we’ve learnt why we need data block, you may ask, why is data block 128M? When files splitted by data block size, instead of one huge single file, few small chunks are stored on HDFS, and the main information of these chunks, aka metadata, are stored in-memory on the Namenode, including block size, block location and so on. So if the block size is too small that the chunks would be too many, then the Namenode in-memory stroage would be under great pressure. And that’s also why the <strong>small files problem</strong> damage to your HDFS. However, on the other hand, too large block size would make the reading data process on datanode slow, which is not a good situation for HDFS. It’s a trade-off, and that’s why we choose 128M as an eclectic solution for HDFS. </p>\n<h3 id=\"Replication-Management-via-Rack-Awareness\"><a href=\"#Replication-Management-via-Rack-Awareness\" class=\"headerlink\" title=\"Replication Management via Rack Awareness\"></a>Replication Management via Rack Awareness</h3><p>As written to the HDFS, a single file would be divided into many blocks and these blocks would be stored across the cluster, at the same time, the replica of each block is created and there are 3 replicas by default which is can be modified in setting. The replica is actually the backup data from the blocks in case of the potentially unfavorable conditions of the Datanode, aka HDFS fault tolerance. Let’s take a look at how HDFS manage the replication under the rack awareness.<br>Every Datanode in a cluster is actually a single machine, and several Datanodes are put on one rack for better management and they share the network. Several racks are set in one data center and one cluster may be built across several data centers, which could be in different areas even counties, as results, the network distance between each nodes are different. See the figure below.<br>When the cluster start to write data to the HDFS, Namenode chooses the Datanode which is closer to the same rack or nearby rack to the write request. This distance is calculated by the rules below. Rack Awareness will choose the Datanode which is closer to get rid of too much network commuication cost.</p>\n<ul>\n<li>Distance is 0 when data in the same node;</li>\n<li>Distance is 2 when data in two different nodes but the same rack;</li>\n<li>Distance is 4 when data in two different racks but the same data center;</li>\n<li>Distance is 6 when data in two different data centers.<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/21/21-1.png\" alt=\"\"></li>\n</ul>\n<p>Via Rack Awareness, Namenode will not only choose the namenode to store the data, but also the replicas. Let’s make a sample with one data block, once the data block has been already stored on the Datanode, the first replica will be stored in the local Datanode, then the second replica will be cast to another Datanode in the different rack and the third replica will be stored at the different Datanode on the local rack of the second replica.<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/21/21-2.png\" alt=\"\"></p>\n<blockquote>\n<p>A simple but nonoptimal policy is to place replicas on the different racks. This prevents losing data when an entire rack fails and allows us to use bandwidth from multiple racks while reading the data. This policy evenly distributes the data among replicas in the cluster which makes it easy to balance load in case of component failure. But the biggest drawback of this policy is that it will increase the cost of write operation because a writer needs to transfer blocks to multiple racks and communication between the two nodes in different racks has to go through switches.</p>\n</blockquote>\n<h2 id=\"Read-amp-Write\"><a href=\"#Read-amp-Write\" class=\"headerlink\" title=\"Read &amp; Write\"></a>Read &amp; Write</h2><h3 id=\"Read-Operation\"><a href=\"#Read-Operation\" class=\"headerlink\" title=\"Read Operation\"></a>Read Operation</h3><ol>\n<li>Client opens the file by the <em>DistributedFileSystem</em> object;</li>\n<li><em>DistributedFileSystem</em> calls the Namenode via RPC and get the blocks and replicas location according to the distance between datanode and client, and a <em>FSDataInputStream</em> is also returned;</li>\n<li>With the address of Datanotes, <em>FSDataInputStream</em> open the I/O stream and bring data from Datanodes back to the client;</li>\n<li>Once the reading is finished, client will call <em>close()</em> to end up the stream.<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/21/21-3.png\" alt=\"\"></li>\n</ol>\n<blockquote>\n<p>If the <em>DFSInputStream</em> encounters an error while communicating with a datanode, it will try the next closest one for that block. It will also remember datanodes that have failed so that it doesn’t needlessly retry them for later blocks. The <em>DFSInputStream</em> also verifies checksums for the data transferred to it from the datanode. If it finds a corrupt block, it reports this to the namenode before the <em>DFSInputStream</em> attempts to read a replica of the block from another datanode.</p>\n</blockquote>\n<h3 id=\"Write-Operation\"><a href=\"#Write-Operation\" class=\"headerlink\" title=\"Write Operation\"></a>Write Operation</h3><ol>\n<li>Client sends a <strong>create</strong> request on the <em>DistributedFileSystem</em>, and <em>DistributedFileSystem</em> makes a RPC call to the Namenode to create a new file in the filesystem’s namespace, and Namenode would check for the file names, permission and so on. And a <em>FSDataOutputStream</em> containing the Datanode location is returned by the Namenode if everything is OK;</li>\n<li><em>FSDataOutputStream</em> would split the data into packets and make them a queue, aka data queue, consumed by the <em>DataStreamer</em>, which would allocate new blocks by picking a list of suitable Datanodes to store the replica from the Namenode;</li>\n<li>Assume that the replication factor is set to 3, the list of Datanodes form a pipeline containing 3 Datanodes(the first replica is stored in the local Datanode, so there are 3 Datanodes instead of 4);</li>\n<li><em>DataStreamer</em> streams the packet to the first Datanode in the pipeline and then forwards it to the second one, then the third one.</li>\n<li><em>FSDataOutputStream</em> also maintains an interal queue of packets waiting for the acknowledge by Datanodes. Once the acknowledge is send from Datanode in the pipeline, which is sent when the block is stored and the replicas are created, the packet is removed from the packet queue.</li>\n<li>All the blocks are stored and replicated on the different datanodes, the data blocks are copied in parallel.</li>\n<li><p>Client calls <em>close()</em> when writing operation finished.<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/21/21-4.png\" alt=\"\"></p>\n<h2 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h2><ul>\n<li><a href=\"https://morrisjobke.de/2013/12/11/Hadoop-NameNode-and-siblings/\" target=\"_blank\" rel=\"noopener\">Hadoop - NameNode, Checkpoint Node and Backup Node</a></li>\n<li><a href=\"https://data-flair.training/blogs/hadoop-hdfs-architecture/\" target=\"_blank\" rel=\"noopener\">Hadoop HDFS Architecture Explanation and Assumptions</a></li>\n<li><a href=\"https://www.coursera.org/learn/big-data-essentials/\" target=\"_blank\" rel=\"noopener\">Big Data Essentials: HDFS, MapReduce and Spark RDD</a></li>\n</ul>\n</li>\n</ol>\n","site":{"data":{}},"_categories":[{"name":"hadoop","path":"categories/hadoop/"}],"_tags":[{"name":"hdfs","path":"tags/hdfs/"},{"name":"hadoop","path":"tags/hadoop/"}],"excerpt":"<p><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/background/clouds-dark-dark-clouds-2308671.jpg\" alt=\"\"><br>I’ve been using HDFS as storage for almost 3 years reading data from and writing data to it by HIVE and Spark, but I’ve never learned the detail. Finally I have some time to watch the <a href=\"https://www.coursera.org/learn/big-data-essentials/\" target=\"_blank\" rel=\"noopener\">Big Data Essentials</a> on <a href=\"https://www.coursera.org/\" target=\"_blank\" rel=\"noopener\">Coursera</a>, which inspired me to have a deep dive in HDFS architecture. This blog contains so much about HDFS that I spent 3 days to sum up and mark them down. If anything is worng, it’s very nice of you to tell me and I’ll figure it out! Let’s take a look.<br></p>","more":"</p>\n<h2 id=\"Namenode\"><a href=\"#Namenode\" class=\"headerlink\" title=\"Namenode\"></a>Namenode</h2><p>Namenode is the <strong>master node</strong> of the HDFS, it contains the metadata of the filesystem, such as the number and location of the block, replica and so on. Notice the metadata is stored in-memory for the fast retrival of data.</p>\n<blockquote>\n<p>Task of Namenode</p>\n<ul>\n<li>Manage file system namespace.</li>\n<li>Regulates client’s access to files.</li>\n<li>It also executes file system execution such as naming, closing, opening files/directories.</li>\n<li>All DataNodes sends a Heartbeat and block report to the NameNode in the Hadoop cluster. It ensures that the DataNodes are alive. A block report contains a list of all blocks on a datanode.</li>\n<li>NameNode is also responsible for taking care of the Replication Factor of all the blocks.</li>\n</ul>\n</blockquote>\n<h3 id=\"FsImage-amp-EditLogs\"><a href=\"#FsImage-amp-EditLogs\" class=\"headerlink\" title=\"FsImage &amp; EditLogs\"></a>FsImage &amp; EditLogs</h3><p>The HDFS Namenade, as the Master Node, manages the whole architecture of the filesystem by the metadata. Data in the metadata are present as FsImage and EditLogs. Let’s the detail about them.</p>\n<ul>\n<li><p>FsImage: FsImage contains the image of the entire filesystem, including serialized form of all the <strong>directories</strong> and file <strong>inodes</strong>. The FsImage is actually stored as a <strong>local file</strong> in the filesystem in Namenode and actually, you can treat FsImage as a snapshot of the present filesystem architecture.</p>\n<blockquote>\n<p>Each inode is an internal representation of file or directory’s metadata.  </p>\n</blockquote>\n<ul>\n<li>EditLogs: Editlogs contains the modification made to the entire filesystem on the most recently FsImage, such as creating, moving, updating, deleting and so on. Also, EditLogs is stored on the Namenode as a local file, similar to the FsImage.</li>\n</ul>\n</li>\n</ul>\n<p>As you may get it, FsImage and EditLogs, one for storing the present situation and one for the modification. With the help of these two files, Namenode could can recover the matadata in case of something unexpected. Let’s go ahead to the specific strategy.</p>\n<h3 id=\"Secondary-Namenode-Checkpoint-Node-amp-Backup-Node\"><a href=\"#Secondary-Namenode-Checkpoint-Node-amp-Backup-Node\" class=\"headerlink\" title=\"Secondary Namenode(Checkpoint Node) &amp; Backup Node\"></a>Secondary Namenode(Checkpoint Node) &amp; Backup Node</h3><p>FsImage and EditLogs can help us recover the entire HDFS, but is there any problem? Suppose the Namenode has been running for a month, and once it restarted, the Namenode would read the FsImage and EditLogs to rebuild the state of the HDFS. However, since the Namenode has been running for such a long time that the EditLogs could be so large that it would spend a long time to load and parse, even for few hours, which is unacceptable for us. To solve this problem, let’s have a look at the Secondary Namenode and Backup Node.</p>\n<ul>\n<li>Secondary Namenode(Checkpoint Node): Secondary Namenode runs on another machine apart from the Namenode, it <strong>fetches the FsImage and EditLogs periodically</strong> from the Namenode and merge them to a start-of-art FsImage, aka a checkpoint, and push it back to the Namenode(it may be little confusing that so-called Secondary Namenode would not upload the checkpoint automaticlly while the checkpoint Node would, I’m not quite sure about it actually). </li>\n<li>Backup Node: Take care that Backup Node is a different term from Secondary Namenode or Checkpoint Node. Backup Node doesn’t fetch the FsImage and EditLogs periodically because it receive a filesystem edit stream from the Namenode. As a result, the state image is always stored <strong>in-memory</strong> on the Backup Node.<blockquote>\n<p>The Backup Node provides the same functionality as the Checkpoint Node, but is synchronized with the NameNode. </p>\n</blockquote>\n</li>\n</ul>\n<h2 id=\"Datanode\"><a href=\"#Datanode\" class=\"headerlink\" title=\"Datanode\"></a>Datanode</h2><p>Datanode is the <strong>slaver node</strong> of HDFS, which is actually where the data stored.</p>\n<blockquote>\n<p>Tasks of Datanode </p>\n<ul>\n<li>Block replica creation, deletion, and replication according to the instruction of Namenode.</li>\n<li>DataNode manages data storage of the system.</li>\n<li>DataNodes send heartbeat to the NameNode to report the health of HDFS. By default, this frequency is set to 3 seconds.</li>\n</ul>\n</blockquote>\n<h2 id=\"Block-amp-Replica\"><a href=\"#Block-amp-Replica\" class=\"headerlink\" title=\"Block &amp; Replica\"></a>Block &amp; Replica</h2><h3 id=\"Block\"><a href=\"#Block\" class=\"headerlink\" title=\"Block\"></a>Block</h3><p>Block is unit of data stored on HDFS, which cannot be controlled by us and the value is often 128M by default. Why we need data block? Suppose we have two files stored on the HDFS, one is bigger, 1G for instance, and another is 129M. As these two files been read synchronously, it could be an imbalanced progress. But what if we split all the files to a same unit size and these splitted units would be read in balance. That’s why we need data block in HDFS.<br>Since we’ve learnt why we need data block, you may ask, why is data block 128M? When files splitted by data block size, instead of one huge single file, few small chunks are stored on HDFS, and the main information of these chunks, aka metadata, are stored in-memory on the Namenode, including block size, block location and so on. So if the block size is too small that the chunks would be too many, then the Namenode in-memory stroage would be under great pressure. And that’s also why the <strong>small files problem</strong> damage to your HDFS. However, on the other hand, too large block size would make the reading data process on datanode slow, which is not a good situation for HDFS. It’s a trade-off, and that’s why we choose 128M as an eclectic solution for HDFS. </p>\n<h3 id=\"Replication-Management-via-Rack-Awareness\"><a href=\"#Replication-Management-via-Rack-Awareness\" class=\"headerlink\" title=\"Replication Management via Rack Awareness\"></a>Replication Management via Rack Awareness</h3><p>As written to the HDFS, a single file would be divided into many blocks and these blocks would be stored across the cluster, at the same time, the replica of each block is created and there are 3 replicas by default which is can be modified in setting. The replica is actually the backup data from the blocks in case of the potentially unfavorable conditions of the Datanode, aka HDFS fault tolerance. Let’s take a look at how HDFS manage the replication under the rack awareness.<br>Every Datanode in a cluster is actually a single machine, and several Datanodes are put on one rack for better management and they share the network. Several racks are set in one data center and one cluster may be built across several data centers, which could be in different areas even counties, as results, the network distance between each nodes are different. See the figure below.<br>When the cluster start to write data to the HDFS, Namenode chooses the Datanode which is closer to the same rack or nearby rack to the write request. This distance is calculated by the rules below. Rack Awareness will choose the Datanode which is closer to get rid of too much network commuication cost.</p>\n<ul>\n<li>Distance is 0 when data in the same node;</li>\n<li>Distance is 2 when data in two different nodes but the same rack;</li>\n<li>Distance is 4 when data in two different racks but the same data center;</li>\n<li>Distance is 6 when data in two different data centers.<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/21/21-1.png\" alt=\"\"></li>\n</ul>\n<p>Via Rack Awareness, Namenode will not only choose the namenode to store the data, but also the replicas. Let’s make a sample with one data block, once the data block has been already stored on the Datanode, the first replica will be stored in the local Datanode, then the second replica will be cast to another Datanode in the different rack and the third replica will be stored at the different Datanode on the local rack of the second replica.<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/21/21-2.png\" alt=\"\"></p>\n<blockquote>\n<p>A simple but nonoptimal policy is to place replicas on the different racks. This prevents losing data when an entire rack fails and allows us to use bandwidth from multiple racks while reading the data. This policy evenly distributes the data among replicas in the cluster which makes it easy to balance load in case of component failure. But the biggest drawback of this policy is that it will increase the cost of write operation because a writer needs to transfer blocks to multiple racks and communication between the two nodes in different racks has to go through switches.</p>\n</blockquote>\n<h2 id=\"Read-amp-Write\"><a href=\"#Read-amp-Write\" class=\"headerlink\" title=\"Read &amp; Write\"></a>Read &amp; Write</h2><h3 id=\"Read-Operation\"><a href=\"#Read-Operation\" class=\"headerlink\" title=\"Read Operation\"></a>Read Operation</h3><ol>\n<li>Client opens the file by the <em>DistributedFileSystem</em> object;</li>\n<li><em>DistributedFileSystem</em> calls the Namenode via RPC and get the blocks and replicas location according to the distance between datanode and client, and a <em>FSDataInputStream</em> is also returned;</li>\n<li>With the address of Datanotes, <em>FSDataInputStream</em> open the I/O stream and bring data from Datanodes back to the client;</li>\n<li>Once the reading is finished, client will call <em>close()</em> to end up the stream.<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/21/21-3.png\" alt=\"\"></li>\n</ol>\n<blockquote>\n<p>If the <em>DFSInputStream</em> encounters an error while communicating with a datanode, it will try the next closest one for that block. It will also remember datanodes that have failed so that it doesn’t needlessly retry them for later blocks. The <em>DFSInputStream</em> also verifies checksums for the data transferred to it from the datanode. If it finds a corrupt block, it reports this to the namenode before the <em>DFSInputStream</em> attempts to read a replica of the block from another datanode.</p>\n</blockquote>\n<h3 id=\"Write-Operation\"><a href=\"#Write-Operation\" class=\"headerlink\" title=\"Write Operation\"></a>Write Operation</h3><ol>\n<li>Client sends a <strong>create</strong> request on the <em>DistributedFileSystem</em>, and <em>DistributedFileSystem</em> makes a RPC call to the Namenode to create a new file in the filesystem’s namespace, and Namenode would check for the file names, permission and so on. And a <em>FSDataOutputStream</em> containing the Datanode location is returned by the Namenode if everything is OK;</li>\n<li><em>FSDataOutputStream</em> would split the data into packets and make them a queue, aka data queue, consumed by the <em>DataStreamer</em>, which would allocate new blocks by picking a list of suitable Datanodes to store the replica from the Namenode;</li>\n<li>Assume that the replication factor is set to 3, the list of Datanodes form a pipeline containing 3 Datanodes(the first replica is stored in the local Datanode, so there are 3 Datanodes instead of 4);</li>\n<li><em>DataStreamer</em> streams the packet to the first Datanode in the pipeline and then forwards it to the second one, then the third one.</li>\n<li><em>FSDataOutputStream</em> also maintains an interal queue of packets waiting for the acknowledge by Datanodes. Once the acknowledge is send from Datanode in the pipeline, which is sent when the block is stored and the replicas are created, the packet is removed from the packet queue.</li>\n<li>All the blocks are stored and replicated on the different datanodes, the data blocks are copied in parallel.</li>\n<li><p>Client calls <em>close()</em> when writing operation finished.<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/21/21-4.png\" alt=\"\"></p>\n<h2 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h2><ul>\n<li><a href=\"https://morrisjobke.de/2013/12/11/Hadoop-NameNode-and-siblings/\" target=\"_blank\" rel=\"noopener\">Hadoop - NameNode, Checkpoint Node and Backup Node</a></li>\n<li><a href=\"https://data-flair.training/blogs/hadoop-hdfs-architecture/\" target=\"_blank\" rel=\"noopener\">Hadoop HDFS Architecture Explanation and Assumptions</a></li>\n<li><a href=\"https://www.coursera.org/learn/big-data-essentials/\" target=\"_blank\" rel=\"noopener\">Big Data Essentials: HDFS, MapReduce and Spark RDD</a></li>\n</ul>\n</li>\n</ol>"},{"title":"Java Garbage Collection Overview","date":"2019-02-26T08:35:08.000Z","toc":true,"_content":"![](https://github.com/JoeAsir/blog-image/raw/master/blog/background/beach-boats-clouds.jpg)\nJava Garbage Collection has confused me for such a long time when I try to tune my Spark Application, but unfortunately I'm not a good Java developer. I really feel terrible when staring at the red blocks representing high GC time in my SparkUI while having no idea how to fix it up. So I spent some time digging in GC and finally got to learn about what GC is and how to analysis the GC logs. So today I'm sharing you something I learnt and let's move on.\n<!--more-->\nJust as name presenting, Java Garbage Collection is used to collect the garbage, which is actually the unused objects, in JVM Heap memory. If you don't know what does Heap memory represents, you'd better make sense of JVM memory management first.\n## GC overview\nLet's firstly take a quick look at on how GC track and remove the so-called grabage. The whole porcess, consisting of two steps, marking and deletion, might be much simpler than you can imagine.\n* Marking - It's easy to understand what **Marking** does if you know what GC does. Yep! **Marking** is purposed to distingrish and mark down which objects are still referenced and which are to be collected. And it's a time consuming process for all the objects all scanned.\n* Deletion - **Deleteing** removes the unreferenced objects by moving the referenced objects together, it compact the memory to make benefis to the further memory allocation.\n\n## GC Process\n### JVM Generations\nThe Heap memory in JVM is devided into manly three parts, which are Eden, Survivor and Tenured Space(and Permanet space in formal JDK version). Eden and Survivor spaces are called Yong Generation while the Tenured space Old Generation. I have to say the names of JVM Generations are really vivid, the Young Generation is where all the new objects allocated, and Old Generation is used to store long-living objects.\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/20/20-1.png)\n### Young Generation\nThere are two parts of Heap spaces in Young Generation, which are Eden and Survivor spaces as we talked above. And the Survivor space is actually made up with two parts, Survivor 0 and Survivor 1, aka \"from\" Survivor and \"to\" Survivor, or s0 and s1. The key point is that, those two Survivors space are not immutable as Eden space, actually they are relative just as the name \"from\" and \"to\" shows. Once an object is first allocated, it wil be stored in the Eden space. Time goes by, the Eden space is gradually filled up with so many new-born objects, then the **Minor Garbage Collection(Minor GC)** is triggered. Hey, you see? the **Minor GC** finally comes out here and it's actually the GC process in the Young Generation. \nIn the **Minor GC** process, all the unreferenced or unused objects are removed, and all the referenced ones are moved to one of the Survivor spaces, which is actually the \"from\" Survivor space. As new objects are continuously born(or allocated), with the Eden space gradually filled up with those objects again, the **Minor GC** is triggered another time, all the unreferenced objects in the Eden space and \"from\" Survivor space are all removed while the referenced ones are moved to the \"to\" Survivor space. That's why the two Survivor spaces are called \"from\" and \"to\", really vivid, isn't it? Hahaha. The survived objects(stored in Survivor space) are moved between these two Survivors space gathering with new-born ones moved from the Eden It's interesting to find out that the \"from\" Survivor space will be called as \"to\" Survivor space in the next data moving time, which always keep the survived objects are all stored in one Survivor, whil another empty.\nOnce the referenced objects are moved, they are also aged once. As soon as the age of the objects reach a certain threshold(15 as the default value, and you can custom it as you wish), they would be moved to a new world, that's the Old Generation(this moving stage is aka so-called promotion), which I'm about to talk below.\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/20/20-2.png)\n### Old Generation\nAs we talked above, when referenced objects are \"old\" enough(couldn't stop laughing), or one of the Survivor space is full-filled, they will be then promoted to the the Tenured space, which means the long-time survived objects will be stored in the Old Generation. As **Minor GC** keeps invoked once again and again, survived objects will be continuous promoted to the Old Generation. And the **Major Garbage Collection(Major GC)** would be triggered eventually on the Old Generation to clean up and compact that space.\nWhat's more, what if the Old Generation is full? Wow, that's really not a good news for your JVM application because the **Full Garbage Collection(Full GC)** is triggered, which clean up all the objects on **both** the Young Generation and Old Generation.\nI have to say that there is no brief definitions for **Major GC** and **Full GC**, which really confuses me, see more details in [this blog](https://plumbr.io/blog/garbage-collection/minor-gc-vs-major-gc-vs-full-gc).\n### Stop-the-wordld(STW)\nNow we almostly get the generatinal garbage collection process of JVM GC, but why we foucs a lot on it, and how does it ruins our application? The answer is quite simple, that is **Stop-the-world** aka STW. Once a GC process triggered, the JVM has to move and update those references object before the application manipulate them, otherwise there could be something wrong. [See this answers on Stackoverflow to learn more](https://stackoverflow.com/questions/40182392/does-java-garbage-collect-always-has-to-stop-the-world). \nActually, all the GC process could bring in STW problem, and especially for **Full GC** process, which could be a nightmare your application proformance. \n\n## Java Garbage Collector\n* Serial Garbage Collector\nAs name presents, Serial Garbage Collector is basicaly works with a single thread. Serial Garbage Collector would stop all the other application threads because of the memory compaction, also known as the **STW** we talked about.  As a result, Serial Garbage often be used in the applications without low pause time requirements and run on client-style machines.\n\n* Parallel Garbage Collector\nInstead of the single thread in Serial Garbage Collector, Parallel Garbage Collector uses multiple threads to preform GC. And Parallel Collector could also bring the **STW** as the Serial Garbage Collector does. \n\n* CMS Garbage Collector\nThe Concurrent Mark Sweep Garbage Collector, aka CMS, uses multiple garbage collector threads to reduce the pauses time in application when collecting Old Generation. Normally the CMS Garbage Collector dosen't compact the memory and move the referenced objects together after collection, thus that's the mainly disadvtange of CMS.\n\n* G1 Garbage Collector\nGarbage First or G1 Collector is purposed to be the alternative of CMS Garbage Collector. G1 Collector seems to be a perfect choice as it's a parallel, concurrent and compacting low-pause collector.\n\n> Unlike other collectors, G1 collector partitions the heap into a set of equal-sized heap regions, each a contiguous range of virtual memory. When performing garbage collections, G1 shows a concurrent global marking phase (i.e. phase 1 known as Marking) to determine the liveness of objects throughout the heap.\nAfter the mark phase is completed, G1 knows which regions are mostly empty. It collects in these areas first, which usually yields a significant amount of free space (i.e. phase 2 known as Sweeping). It is why this method of garbage collection is called Garbage-First.\n\n## Conclusion\nThis blog mainly talks about the overview of the JVM Garbage Collection from the generation based process to garbage collector. Honestly speaking, GC is much more complex than presented in the blog, thus there is still long way to go to make a deep dive in GC, even JVM. \n\n## References\n* [Basics of Java Garbage Collection](https://codeahoy.com/2017/08/06/basics-of-java-garbage-collection/)\n* [JVM Garbage Collectors](https://www.baeldung.com/jvm-garbage-collectors)\n* [Java Garbage Collection Basics](https://www.oracle.com/webfolder/technetwork/tutorials/obe/java/gc01/index.html)\n* [Minor GC vs Major GC vs Full GC](https://plumbr.io/blog/garbage-collection/minor-gc-vs-major-gc-vs-full-gc)\n","source":"_posts/jvm-java-garbage-collection-overview.md","raw":"---\ntitle: Java Garbage Collection Overview\ndate: 2019-02-26 16:35:08\ntags: jvm\ncategories: jvm\ntoc: true\n---\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/background/beach-boats-clouds.jpg)\nJava Garbage Collection has confused me for such a long time when I try to tune my Spark Application, but unfortunately I'm not a good Java developer. I really feel terrible when staring at the red blocks representing high GC time in my SparkUI while having no idea how to fix it up. So I spent some time digging in GC and finally got to learn about what GC is and how to analysis the GC logs. So today I'm sharing you something I learnt and let's move on.\n<!--more-->\nJust as name presenting, Java Garbage Collection is used to collect the garbage, which is actually the unused objects, in JVM Heap memory. If you don't know what does Heap memory represents, you'd better make sense of JVM memory management first.\n## GC overview\nLet's firstly take a quick look at on how GC track and remove the so-called grabage. The whole porcess, consisting of two steps, marking and deletion, might be much simpler than you can imagine.\n* Marking - It's easy to understand what **Marking** does if you know what GC does. Yep! **Marking** is purposed to distingrish and mark down which objects are still referenced and which are to be collected. And it's a time consuming process for all the objects all scanned.\n* Deletion - **Deleteing** removes the unreferenced objects by moving the referenced objects together, it compact the memory to make benefis to the further memory allocation.\n\n## GC Process\n### JVM Generations\nThe Heap memory in JVM is devided into manly three parts, which are Eden, Survivor and Tenured Space(and Permanet space in formal JDK version). Eden and Survivor spaces are called Yong Generation while the Tenured space Old Generation. I have to say the names of JVM Generations are really vivid, the Young Generation is where all the new objects allocated, and Old Generation is used to store long-living objects.\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/20/20-1.png)\n### Young Generation\nThere are two parts of Heap spaces in Young Generation, which are Eden and Survivor spaces as we talked above. And the Survivor space is actually made up with two parts, Survivor 0 and Survivor 1, aka \"from\" Survivor and \"to\" Survivor, or s0 and s1. The key point is that, those two Survivors space are not immutable as Eden space, actually they are relative just as the name \"from\" and \"to\" shows. Once an object is first allocated, it wil be stored in the Eden space. Time goes by, the Eden space is gradually filled up with so many new-born objects, then the **Minor Garbage Collection(Minor GC)** is triggered. Hey, you see? the **Minor GC** finally comes out here and it's actually the GC process in the Young Generation. \nIn the **Minor GC** process, all the unreferenced or unused objects are removed, and all the referenced ones are moved to one of the Survivor spaces, which is actually the \"from\" Survivor space. As new objects are continuously born(or allocated), with the Eden space gradually filled up with those objects again, the **Minor GC** is triggered another time, all the unreferenced objects in the Eden space and \"from\" Survivor space are all removed while the referenced ones are moved to the \"to\" Survivor space. That's why the two Survivor spaces are called \"from\" and \"to\", really vivid, isn't it? Hahaha. The survived objects(stored in Survivor space) are moved between these two Survivors space gathering with new-born ones moved from the Eden It's interesting to find out that the \"from\" Survivor space will be called as \"to\" Survivor space in the next data moving time, which always keep the survived objects are all stored in one Survivor, whil another empty.\nOnce the referenced objects are moved, they are also aged once. As soon as the age of the objects reach a certain threshold(15 as the default value, and you can custom it as you wish), they would be moved to a new world, that's the Old Generation(this moving stage is aka so-called promotion), which I'm about to talk below.\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/20/20-2.png)\n### Old Generation\nAs we talked above, when referenced objects are \"old\" enough(couldn't stop laughing), or one of the Survivor space is full-filled, they will be then promoted to the the Tenured space, which means the long-time survived objects will be stored in the Old Generation. As **Minor GC** keeps invoked once again and again, survived objects will be continuous promoted to the Old Generation. And the **Major Garbage Collection(Major GC)** would be triggered eventually on the Old Generation to clean up and compact that space.\nWhat's more, what if the Old Generation is full? Wow, that's really not a good news for your JVM application because the **Full Garbage Collection(Full GC)** is triggered, which clean up all the objects on **both** the Young Generation and Old Generation.\nI have to say that there is no brief definitions for **Major GC** and **Full GC**, which really confuses me, see more details in [this blog](https://plumbr.io/blog/garbage-collection/minor-gc-vs-major-gc-vs-full-gc).\n### Stop-the-wordld(STW)\nNow we almostly get the generatinal garbage collection process of JVM GC, but why we foucs a lot on it, and how does it ruins our application? The answer is quite simple, that is **Stop-the-world** aka STW. Once a GC process triggered, the JVM has to move and update those references object before the application manipulate them, otherwise there could be something wrong. [See this answers on Stackoverflow to learn more](https://stackoverflow.com/questions/40182392/does-java-garbage-collect-always-has-to-stop-the-world). \nActually, all the GC process could bring in STW problem, and especially for **Full GC** process, which could be a nightmare your application proformance. \n\n## Java Garbage Collector\n* Serial Garbage Collector\nAs name presents, Serial Garbage Collector is basicaly works with a single thread. Serial Garbage Collector would stop all the other application threads because of the memory compaction, also known as the **STW** we talked about.  As a result, Serial Garbage often be used in the applications without low pause time requirements and run on client-style machines.\n\n* Parallel Garbage Collector\nInstead of the single thread in Serial Garbage Collector, Parallel Garbage Collector uses multiple threads to preform GC. And Parallel Collector could also bring the **STW** as the Serial Garbage Collector does. \n\n* CMS Garbage Collector\nThe Concurrent Mark Sweep Garbage Collector, aka CMS, uses multiple garbage collector threads to reduce the pauses time in application when collecting Old Generation. Normally the CMS Garbage Collector dosen't compact the memory and move the referenced objects together after collection, thus that's the mainly disadvtange of CMS.\n\n* G1 Garbage Collector\nGarbage First or G1 Collector is purposed to be the alternative of CMS Garbage Collector. G1 Collector seems to be a perfect choice as it's a parallel, concurrent and compacting low-pause collector.\n\n> Unlike other collectors, G1 collector partitions the heap into a set of equal-sized heap regions, each a contiguous range of virtual memory. When performing garbage collections, G1 shows a concurrent global marking phase (i.e. phase 1 known as Marking) to determine the liveness of objects throughout the heap.\nAfter the mark phase is completed, G1 knows which regions are mostly empty. It collects in these areas first, which usually yields a significant amount of free space (i.e. phase 2 known as Sweeping). It is why this method of garbage collection is called Garbage-First.\n\n## Conclusion\nThis blog mainly talks about the overview of the JVM Garbage Collection from the generation based process to garbage collector. Honestly speaking, GC is much more complex than presented in the blog, thus there is still long way to go to make a deep dive in GC, even JVM. \n\n## References\n* [Basics of Java Garbage Collection](https://codeahoy.com/2017/08/06/basics-of-java-garbage-collection/)\n* [JVM Garbage Collectors](https://www.baeldung.com/jvm-garbage-collectors)\n* [Java Garbage Collection Basics](https://www.oracle.com/webfolder/technetwork/tutorials/obe/java/gc01/index.html)\n* [Minor GC vs Major GC vs Full GC](https://plumbr.io/blog/garbage-collection/minor-gc-vs-major-gc-vs-full-gc)\n","slug":"jvm-java-garbage-collection-overview","published":1,"updated":"2020-05-10T06:50:12.529Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cka0wnkwe000rqxotr6psfc75","content":"<p><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/background/beach-boats-clouds.jpg\" alt=\"\"><br>Java Garbage Collection has confused me for such a long time when I try to tune my Spark Application, but unfortunately I’m not a good Java developer. I really feel terrible when staring at the red blocks representing high GC time in my SparkUI while having no idea how to fix it up. So I spent some time digging in GC and finally got to learn about what GC is and how to analysis the GC logs. So today I’m sharing you something I learnt and let’s move on.<br><a id=\"more\"></a><br>Just as name presenting, Java Garbage Collection is used to collect the garbage, which is actually the unused objects, in JVM Heap memory. If you don’t know what does Heap memory represents, you’d better make sense of JVM memory management first.</p>\n<h2 id=\"GC-overview\"><a href=\"#GC-overview\" class=\"headerlink\" title=\"GC overview\"></a>GC overview</h2><p>Let’s firstly take a quick look at on how GC track and remove the so-called grabage. The whole porcess, consisting of two steps, marking and deletion, might be much simpler than you can imagine.</p>\n<ul>\n<li>Marking - It’s easy to understand what <strong>Marking</strong> does if you know what GC does. Yep! <strong>Marking</strong> is purposed to distingrish and mark down which objects are still referenced and which are to be collected. And it’s a time consuming process for all the objects all scanned.</li>\n<li>Deletion - <strong>Deleteing</strong> removes the unreferenced objects by moving the referenced objects together, it compact the memory to make benefis to the further memory allocation.</li>\n</ul>\n<h2 id=\"GC-Process\"><a href=\"#GC-Process\" class=\"headerlink\" title=\"GC Process\"></a>GC Process</h2><h3 id=\"JVM-Generations\"><a href=\"#JVM-Generations\" class=\"headerlink\" title=\"JVM Generations\"></a>JVM Generations</h3><p>The Heap memory in JVM is devided into manly three parts, which are Eden, Survivor and Tenured Space(and Permanet space in formal JDK version). Eden and Survivor spaces are called Yong Generation while the Tenured space Old Generation. I have to say the names of JVM Generations are really vivid, the Young Generation is where all the new objects allocated, and Old Generation is used to store long-living objects.<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/20/20-1.png\" alt=\"\"></p>\n<h3 id=\"Young-Generation\"><a href=\"#Young-Generation\" class=\"headerlink\" title=\"Young Generation\"></a>Young Generation</h3><p>There are two parts of Heap spaces in Young Generation, which are Eden and Survivor spaces as we talked above. And the Survivor space is actually made up with two parts, Survivor 0 and Survivor 1, aka “from” Survivor and “to” Survivor, or s0 and s1. The key point is that, those two Survivors space are not immutable as Eden space, actually they are relative just as the name “from” and “to” shows. Once an object is first allocated, it wil be stored in the Eden space. Time goes by, the Eden space is gradually filled up with so many new-born objects, then the <strong>Minor Garbage Collection(Minor GC)</strong> is triggered. Hey, you see? the <strong>Minor GC</strong> finally comes out here and it’s actually the GC process in the Young Generation.<br>In the <strong>Minor GC</strong> process, all the unreferenced or unused objects are removed, and all the referenced ones are moved to one of the Survivor spaces, which is actually the “from” Survivor space. As new objects are continuously born(or allocated), with the Eden space gradually filled up with those objects again, the <strong>Minor GC</strong> is triggered another time, all the unreferenced objects in the Eden space and “from” Survivor space are all removed while the referenced ones are moved to the “to” Survivor space. That’s why the two Survivor spaces are called “from” and “to”, really vivid, isn’t it? Hahaha. The survived objects(stored in Survivor space) are moved between these two Survivors space gathering with new-born ones moved from the Eden It’s interesting to find out that the “from” Survivor space will be called as “to” Survivor space in the next data moving time, which always keep the survived objects are all stored in one Survivor, whil another empty.<br>Once the referenced objects are moved, they are also aged once. As soon as the age of the objects reach a certain threshold(15 as the default value, and you can custom it as you wish), they would be moved to a new world, that’s the Old Generation(this moving stage is aka so-called promotion), which I’m about to talk below.<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/20/20-2.png\" alt=\"\"></p>\n<h3 id=\"Old-Generation\"><a href=\"#Old-Generation\" class=\"headerlink\" title=\"Old Generation\"></a>Old Generation</h3><p>As we talked above, when referenced objects are “old” enough(couldn’t stop laughing), or one of the Survivor space is full-filled, they will be then promoted to the the Tenured space, which means the long-time survived objects will be stored in the Old Generation. As <strong>Minor GC</strong> keeps invoked once again and again, survived objects will be continuous promoted to the Old Generation. And the <strong>Major Garbage Collection(Major GC)</strong> would be triggered eventually on the Old Generation to clean up and compact that space.<br>What’s more, what if the Old Generation is full? Wow, that’s really not a good news for your JVM application because the <strong>Full Garbage Collection(Full GC)</strong> is triggered, which clean up all the objects on <strong>both</strong> the Young Generation and Old Generation.<br>I have to say that there is no brief definitions for <strong>Major GC</strong> and <strong>Full GC</strong>, which really confuses me, see more details in <a href=\"https://plumbr.io/blog/garbage-collection/minor-gc-vs-major-gc-vs-full-gc\" target=\"_blank\" rel=\"noopener\">this blog</a>.</p>\n<h3 id=\"Stop-the-wordld-STW\"><a href=\"#Stop-the-wordld-STW\" class=\"headerlink\" title=\"Stop-the-wordld(STW)\"></a>Stop-the-wordld(STW)</h3><p>Now we almostly get the generatinal garbage collection process of JVM GC, but why we foucs a lot on it, and how does it ruins our application? The answer is quite simple, that is <strong>Stop-the-world</strong> aka STW. Once a GC process triggered, the JVM has to move and update those references object before the application manipulate them, otherwise there could be something wrong. <a href=\"https://stackoverflow.com/questions/40182392/does-java-garbage-collect-always-has-to-stop-the-world\" target=\"_blank\" rel=\"noopener\">See this answers on Stackoverflow to learn more</a>.<br>Actually, all the GC process could bring in STW problem, and especially for <strong>Full GC</strong> process, which could be a nightmare your application proformance. </p>\n<h2 id=\"Java-Garbage-Collector\"><a href=\"#Java-Garbage-Collector\" class=\"headerlink\" title=\"Java Garbage Collector\"></a>Java Garbage Collector</h2><ul>\n<li><p>Serial Garbage Collector<br>As name presents, Serial Garbage Collector is basicaly works with a single thread. Serial Garbage Collector would stop all the other application threads because of the memory compaction, also known as the <strong>STW</strong> we talked about.  As a result, Serial Garbage often be used in the applications without low pause time requirements and run on client-style machines.</p>\n</li>\n<li><p>Parallel Garbage Collector<br>Instead of the single thread in Serial Garbage Collector, Parallel Garbage Collector uses multiple threads to preform GC. And Parallel Collector could also bring the <strong>STW</strong> as the Serial Garbage Collector does. </p>\n</li>\n<li><p>CMS Garbage Collector<br>The Concurrent Mark Sweep Garbage Collector, aka CMS, uses multiple garbage collector threads to reduce the pauses time in application when collecting Old Generation. Normally the CMS Garbage Collector dosen’t compact the memory and move the referenced objects together after collection, thus that’s the mainly disadvtange of CMS.</p>\n</li>\n<li><p>G1 Garbage Collector<br>Garbage First or G1 Collector is purposed to be the alternative of CMS Garbage Collector. G1 Collector seems to be a perfect choice as it’s a parallel, concurrent and compacting low-pause collector.</p>\n</li>\n</ul>\n<blockquote>\n<p>Unlike other collectors, G1 collector partitions the heap into a set of equal-sized heap regions, each a contiguous range of virtual memory. When performing garbage collections, G1 shows a concurrent global marking phase (i.e. phase 1 known as Marking) to determine the liveness of objects throughout the heap.<br>After the mark phase is completed, G1 knows which regions are mostly empty. It collects in these areas first, which usually yields a significant amount of free space (i.e. phase 2 known as Sweeping). It is why this method of garbage collection is called Garbage-First.</p>\n</blockquote>\n<h2 id=\"Conclusion\"><a href=\"#Conclusion\" class=\"headerlink\" title=\"Conclusion\"></a>Conclusion</h2><p>This blog mainly talks about the overview of the JVM Garbage Collection from the generation based process to garbage collector. Honestly speaking, GC is much more complex than presented in the blog, thus there is still long way to go to make a deep dive in GC, even JVM. </p>\n<h2 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h2><ul>\n<li><a href=\"https://codeahoy.com/2017/08/06/basics-of-java-garbage-collection/\" target=\"_blank\" rel=\"noopener\">Basics of Java Garbage Collection</a></li>\n<li><a href=\"https://www.baeldung.com/jvm-garbage-collectors\" target=\"_blank\" rel=\"noopener\">JVM Garbage Collectors</a></li>\n<li><a href=\"https://www.oracle.com/webfolder/technetwork/tutorials/obe/java/gc01/index.html\" target=\"_blank\" rel=\"noopener\">Java Garbage Collection Basics</a></li>\n<li><a href=\"https://plumbr.io/blog/garbage-collection/minor-gc-vs-major-gc-vs-full-gc\" target=\"_blank\" rel=\"noopener\">Minor GC vs Major GC vs Full GC</a></li>\n</ul>\n","site":{"data":{}},"_categories":[{"name":"jvm","path":"categories/jvm/"}],"_tags":[{"name":"jvm","path":"tags/jvm/"}],"excerpt":"<p><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/background/beach-boats-clouds.jpg\" alt=\"\"><br>Java Garbage Collection has confused me for such a long time when I try to tune my Spark Application, but unfortunately I’m not a good Java developer. I really feel terrible when staring at the red blocks representing high GC time in my SparkUI while having no idea how to fix it up. So I spent some time digging in GC and finally got to learn about what GC is and how to analysis the GC logs. So today I’m sharing you something I learnt and let’s move on.<br></p>","more":"<br>Just as name presenting, Java Garbage Collection is used to collect the garbage, which is actually the unused objects, in JVM Heap memory. If you don’t know what does Heap memory represents, you’d better make sense of JVM memory management first.</p>\n<h2 id=\"GC-overview\"><a href=\"#GC-overview\" class=\"headerlink\" title=\"GC overview\"></a>GC overview</h2><p>Let’s firstly take a quick look at on how GC track and remove the so-called grabage. The whole porcess, consisting of two steps, marking and deletion, might be much simpler than you can imagine.</p>\n<ul>\n<li>Marking - It’s easy to understand what <strong>Marking</strong> does if you know what GC does. Yep! <strong>Marking</strong> is purposed to distingrish and mark down which objects are still referenced and which are to be collected. And it’s a time consuming process for all the objects all scanned.</li>\n<li>Deletion - <strong>Deleteing</strong> removes the unreferenced objects by moving the referenced objects together, it compact the memory to make benefis to the further memory allocation.</li>\n</ul>\n<h2 id=\"GC-Process\"><a href=\"#GC-Process\" class=\"headerlink\" title=\"GC Process\"></a>GC Process</h2><h3 id=\"JVM-Generations\"><a href=\"#JVM-Generations\" class=\"headerlink\" title=\"JVM Generations\"></a>JVM Generations</h3><p>The Heap memory in JVM is devided into manly three parts, which are Eden, Survivor and Tenured Space(and Permanet space in formal JDK version). Eden and Survivor spaces are called Yong Generation while the Tenured space Old Generation. I have to say the names of JVM Generations are really vivid, the Young Generation is where all the new objects allocated, and Old Generation is used to store long-living objects.<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/20/20-1.png\" alt=\"\"></p>\n<h3 id=\"Young-Generation\"><a href=\"#Young-Generation\" class=\"headerlink\" title=\"Young Generation\"></a>Young Generation</h3><p>There are two parts of Heap spaces in Young Generation, which are Eden and Survivor spaces as we talked above. And the Survivor space is actually made up with two parts, Survivor 0 and Survivor 1, aka “from” Survivor and “to” Survivor, or s0 and s1. The key point is that, those two Survivors space are not immutable as Eden space, actually they are relative just as the name “from” and “to” shows. Once an object is first allocated, it wil be stored in the Eden space. Time goes by, the Eden space is gradually filled up with so many new-born objects, then the <strong>Minor Garbage Collection(Minor GC)</strong> is triggered. Hey, you see? the <strong>Minor GC</strong> finally comes out here and it’s actually the GC process in the Young Generation.<br>In the <strong>Minor GC</strong> process, all the unreferenced or unused objects are removed, and all the referenced ones are moved to one of the Survivor spaces, which is actually the “from” Survivor space. As new objects are continuously born(or allocated), with the Eden space gradually filled up with those objects again, the <strong>Minor GC</strong> is triggered another time, all the unreferenced objects in the Eden space and “from” Survivor space are all removed while the referenced ones are moved to the “to” Survivor space. That’s why the two Survivor spaces are called “from” and “to”, really vivid, isn’t it? Hahaha. The survived objects(stored in Survivor space) are moved between these two Survivors space gathering with new-born ones moved from the Eden It’s interesting to find out that the “from” Survivor space will be called as “to” Survivor space in the next data moving time, which always keep the survived objects are all stored in one Survivor, whil another empty.<br>Once the referenced objects are moved, they are also aged once. As soon as the age of the objects reach a certain threshold(15 as the default value, and you can custom it as you wish), they would be moved to a new world, that’s the Old Generation(this moving stage is aka so-called promotion), which I’m about to talk below.<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/20/20-2.png\" alt=\"\"></p>\n<h3 id=\"Old-Generation\"><a href=\"#Old-Generation\" class=\"headerlink\" title=\"Old Generation\"></a>Old Generation</h3><p>As we talked above, when referenced objects are “old” enough(couldn’t stop laughing), or one of the Survivor space is full-filled, they will be then promoted to the the Tenured space, which means the long-time survived objects will be stored in the Old Generation. As <strong>Minor GC</strong> keeps invoked once again and again, survived objects will be continuous promoted to the Old Generation. And the <strong>Major Garbage Collection(Major GC)</strong> would be triggered eventually on the Old Generation to clean up and compact that space.<br>What’s more, what if the Old Generation is full? Wow, that’s really not a good news for your JVM application because the <strong>Full Garbage Collection(Full GC)</strong> is triggered, which clean up all the objects on <strong>both</strong> the Young Generation and Old Generation.<br>I have to say that there is no brief definitions for <strong>Major GC</strong> and <strong>Full GC</strong>, which really confuses me, see more details in <a href=\"https://plumbr.io/blog/garbage-collection/minor-gc-vs-major-gc-vs-full-gc\" target=\"_blank\" rel=\"noopener\">this blog</a>.</p>\n<h3 id=\"Stop-the-wordld-STW\"><a href=\"#Stop-the-wordld-STW\" class=\"headerlink\" title=\"Stop-the-wordld(STW)\"></a>Stop-the-wordld(STW)</h3><p>Now we almostly get the generatinal garbage collection process of JVM GC, but why we foucs a lot on it, and how does it ruins our application? The answer is quite simple, that is <strong>Stop-the-world</strong> aka STW. Once a GC process triggered, the JVM has to move and update those references object before the application manipulate them, otherwise there could be something wrong. <a href=\"https://stackoverflow.com/questions/40182392/does-java-garbage-collect-always-has-to-stop-the-world\" target=\"_blank\" rel=\"noopener\">See this answers on Stackoverflow to learn more</a>.<br>Actually, all the GC process could bring in STW problem, and especially for <strong>Full GC</strong> process, which could be a nightmare your application proformance. </p>\n<h2 id=\"Java-Garbage-Collector\"><a href=\"#Java-Garbage-Collector\" class=\"headerlink\" title=\"Java Garbage Collector\"></a>Java Garbage Collector</h2><ul>\n<li><p>Serial Garbage Collector<br>As name presents, Serial Garbage Collector is basicaly works with a single thread. Serial Garbage Collector would stop all the other application threads because of the memory compaction, also known as the <strong>STW</strong> we talked about.  As a result, Serial Garbage often be used in the applications without low pause time requirements and run on client-style machines.</p>\n</li>\n<li><p>Parallel Garbage Collector<br>Instead of the single thread in Serial Garbage Collector, Parallel Garbage Collector uses multiple threads to preform GC. And Parallel Collector could also bring the <strong>STW</strong> as the Serial Garbage Collector does. </p>\n</li>\n<li><p>CMS Garbage Collector<br>The Concurrent Mark Sweep Garbage Collector, aka CMS, uses multiple garbage collector threads to reduce the pauses time in application when collecting Old Generation. Normally the CMS Garbage Collector dosen’t compact the memory and move the referenced objects together after collection, thus that’s the mainly disadvtange of CMS.</p>\n</li>\n<li><p>G1 Garbage Collector<br>Garbage First or G1 Collector is purposed to be the alternative of CMS Garbage Collector. G1 Collector seems to be a perfect choice as it’s a parallel, concurrent and compacting low-pause collector.</p>\n</li>\n</ul>\n<blockquote>\n<p>Unlike other collectors, G1 collector partitions the heap into a set of equal-sized heap regions, each a contiguous range of virtual memory. When performing garbage collections, G1 shows a concurrent global marking phase (i.e. phase 1 known as Marking) to determine the liveness of objects throughout the heap.<br>After the mark phase is completed, G1 knows which regions are mostly empty. It collects in these areas first, which usually yields a significant amount of free space (i.e. phase 2 known as Sweeping). It is why this method of garbage collection is called Garbage-First.</p>\n</blockquote>\n<h2 id=\"Conclusion\"><a href=\"#Conclusion\" class=\"headerlink\" title=\"Conclusion\"></a>Conclusion</h2><p>This blog mainly talks about the overview of the JVM Garbage Collection from the generation based process to garbage collector. Honestly speaking, GC is much more complex than presented in the blog, thus there is still long way to go to make a deep dive in GC, even JVM. </p>\n<h2 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h2><ul>\n<li><a href=\"https://codeahoy.com/2017/08/06/basics-of-java-garbage-collection/\" target=\"_blank\" rel=\"noopener\">Basics of Java Garbage Collection</a></li>\n<li><a href=\"https://www.baeldung.com/jvm-garbage-collectors\" target=\"_blank\" rel=\"noopener\">JVM Garbage Collectors</a></li>\n<li><a href=\"https://www.oracle.com/webfolder/technetwork/tutorials/obe/java/gc01/index.html\" target=\"_blank\" rel=\"noopener\">Java Garbage Collection Basics</a></li>\n<li><a href=\"https://plumbr.io/blog/garbage-collection/minor-gc-vs-major-gc-vs-full-gc\" target=\"_blank\" rel=\"noopener\">Minor GC vs Major GC vs Full GC</a></li>\n</ul>"},{"title":"再深入聊聊梯度下降和牛顿法","date":"2017-08-11T09:26:56.000Z","_content":"上次我们一起聊到了gradient descent和newton's method，而且我们已经知道了gradient descent和newton's method都是convex optimization的好方法，这次我们就跳出convex optimization，从更大的unconstrained optimization角度来探讨下这两种方法之间的关联和区别。\n<!--more-->\n\n假设我们现有一个的optimization task，要求objective function \\\\(f(x)\\\\)的最小值，我们一般有两种方案：\n* 考虑到\\\\(f(x)\\\\)的最小值很有可能是全局最小值，那么我们可以通过寻找\\\\( \\nabla f(x)=0\\\\)的点来确定最小值，这就是**newton's method**的思想\n* 既然我们要寻找最小值，那我们可以顺着一条\\\\(f(x)\\\\)逐渐减小的路径，顺着这条路径一直走下去，直到不再变小，这就是**gradient descent**的思想\n\nOK，简单的叙述之后，我们开始正题！\n\n## 泰勒级数(Taylor series)\n首先我们需要回忆一下高等数学中重要的Taylor series，如果\\\\( f(x)\\\\)在点\\\\( x_0\\\\)的领域内具有\\\\(n+1\\\\)阶导数，那么，在该领域内，\\\\( f(x)\\\\)可展开成\\\\(n\\\\)阶Taylor series，忽略无限大次项的形式就是\n$$f(x)=f(x_0)+ \\nabla f(x_0)(x-x_0) + \\frac{ \\nabla ^2 f(x_0)}{2!}(x-x_0)^2 +...+\\frac{ \\nabla ^n f(x_0)}{n!}(x-x_0)^n $$\n其实在高等数学中学到Taylor series的时候，我本人是十分无感的，我并不知道这个东西到底有什么用处，相信很多人和我有相似的经历。\n\n> In mathematics, a Taylor series is a representation of a function as an infinite sum of terms that are calculated from the values of the function's derivatives at a single point.\n\n事实上，Taylor series所表现的是，对于\\\\( f(x)\\\\)在点\\\\( x_0\\\\)附近的一个估计，也可以理解为，根据\\\\( x_0\\\\)点处的各阶derivatives之和构成一个新的function，这个function就是对\\\\(f(x)\\\\)的逼近和拟合，而且这种逼近和拟合，随着Taylor series阶数增加而更接近于真实的\\\\(f(x)\\\\)。如果我们使用0阶Taylor series来逼近的话，那我们就粗暴的认为，\\\\( f(x)\\\\)在点\\\\( x_0\\\\)附近的值就都是\\\\(x_0\\\\)，这当然太粗暴直接了，哈哈。\n\n既然这太粗暴了，那么我们就用1st order Taylor series来做一个逼近和估计，这就是gradient descent的思想；如果我们用2nd order Taylor series来估计呢，那就成了newton's method了\n\nOK，我们继续娓娓道来。\n\n## 1st order Taylor series & gradient descent\n假设\\\\(x_k\\\\)是第k次gradient descent迭代后的\\\\(x\\\\)取值，那我们在此处的1st order Taylor series 就是\n$$f(x)=f(x_k)+ \\nabla f(x_k)(x-x_k)$$\n其中\\\\(x\\\\)是迭代的下一个方向，gradient descent的目标就是让\\\\(f(x)\\\\)达到局部甚至全局最小值，那么每一次迭代，也需要尽可能的减小更多以达到这个目的，那么\n$$f(x_k)-f(x)=- \\nabla f(x_k)(x-x_k)$$\n显然，上式应该尽可能的大，即**\\\\(- \\nabla f(x_k)(x-x_k)\\\\)越大越好**，我们现在把\\\\((x-x_k)\\\\)做一个替换，用单位向量\\\\(\\vec g\\\\)和标量\\\\( \\alpha\\\\)分别代表方向和大小，现在的任务就变成了\n$$ \\min \\nabla f(x_k)(x-x_k) = \\min( \\alpha \\vec{\\nabla f(x_k)}⋅ \\vec g)$$\n我们都知道，**对于两个向量来说，当他们方向相反时，他们的内积是最小的**。\n\n>梯度方向的定义是该点梯度在标量场增长最快的方向\n\n因此当\\\\(\\vec g\\\\)的方向是\\\\( \\vec{\\nabla f(x_k)}\\\\)的反方向时，上式可以取到最小值，于是就有\n$$x-x_k=- \\alpha \\nabla f(x_k)$$\n$$x:=x_k- \\alpha \\nabla f(x_k)$$\n到这一步，是不是看到了熟悉的gradient descent呢，yeah mate！We make it!\n## 2nd order Taylor series & newton's method\n和上面的gradient descent相似，假设\\\\(x_k\\\\)是第\\\\(k\\\\)次newton's method迭代后的\\\\(x\\\\)取值，那我们在此处的2nd order Taylor series 是\n$$f(x)=f(x_k)+ \\nabla f(x_k)(x-x_k) + \\frac{1}{2}(x-x_k)^T \\nabla^2 f(x_k)(x-x_k) $$\n我们对等号两边同时对\\\\(x\\\\)求导，并令其为零\n$$ \\nabla f(x) = \\nabla f(x_k) + (x-x_k) \\nabla^2 f(x_k)=0$$\n由于newton's method的原理就是通过\\\\(\\nabla f(x)=0\\\\)来寻找最小值，**故上式为零的解\\\\(x\\\\)其实就是newton's method在\\\\(k+1\\\\)次迭代后的新的\\\\(x\\\\)值**。其中\\\\(\\nabla f(x_k)\\\\)是\\\\(x_k\\\\)处的一阶导数，\\\\( \\nabla^2 f(x_k)\\\\)是\\\\(x_k\\\\)处的二阶导数Hessian矩阵元素\n\n我们令\\\\(\\nabla f(x_k)=g\\\\)，\\\\(\\nabla^2 f(x_k)=H\\\\)，则上式变成\n$$g+H(x-x_k)=0$$\n进一步的\n$$x=x_k-H^{-1}g$$\n由于\\\\(-g H^{-1} \\\\) 是优化的前进方向，在寻找最小值的过程中，这个方向一定是和梯度方向\\\\(g\\\\)相反才可以更快的下降，那么就有\\\\( g^T H^{-1} g > 0\\\\)，这不就是positive definite的定义吗？也就是说，**Hessian矩阵是positive definite的**。\n\n想象一下，如果Hessian是negative definite的话，参数更新的方向就成了和\\\\(g\\\\)相同的方向，newton's method将会发散，这一点，也是newton's method的缺点。在objective function是non-convex function的情况下，如果第\\\\(k\\\\)次迭代获得的\\\\(x_k\\\\)处的Hessian matrix negative definite，那么newton's method将会发散，从而导致不收敛。当然，为了解决这种问题，后续有改进的BFGS等方法，我们在这里暂时不详细讨论。\n## Sum up\n下面我们再来总结性质的对比一下两种方法，来看一张图\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/2/2-1.png)\n事实上，这两种方法都采用了一种逼近和拟合的思想。假设现在处于迭代\\\\(k\\\\)次之后的\\\\(x_k\\\\)点，对于objective function，我们用\\\\(x_k\\\\)点的Taylor series \\\\(f(x)\\\\)来逼近和拟合，当然了，上图我们看到，gradient descent是用一次function而newton's method采用的是二次function，这是二者之间最显著的区别。\n\n对于new's method，在拟合之后，我们通过\\\\( \\nabla f(x)=0\\\\)求得的\\\\(x \\_{k+1}\\\\)点作为此次迭代的结果，下次迭代时候，又在\\\\(x \\_{k+1}\\\\)处次进行二次function的拟合，并如此迭代下去。\n\nNewton's method采用二次function来拟合，我们可以感性的理解为，newton's method在寻找下降的方向时候，关注的不仅仅是此处objective function value是不是减小(一阶value)，还关注此处value下降的趋势如何(二阶value)，而gradient descent只关心此处function value是不是减小，因此newton's method可以迭代更少次数获得最优解。对于标准二次型的objective function，newton's method甚至可以一次迭代就找到全局最小值。\n\n但是值得注意的是，上面所说的标准二次型function，实质上是convex function，在一般的unconstrained optimization中，更多的情况则是non-convex optimization，对于一般的non-convex optimization，newton's method是相对不稳定的，因为我们很难保证Hessian matrix的positive definite。鉴于此，我们会加入步长\\\\(\\lambda\\\\)限制，防止其一次迭代过大而带来迭代后Hessian matrix negative definite的情况，即\n$$x:=x- \\lambda H^{-1} g$$\n对于这种思想，我个人认为，是在整体non-convex function中寻找一个局部的convex function，通过步长将newton's method限制在这个局部中，最后收敛到局部最优中。由此可见，newton's mtehod在non-convex中受限制比较大。\n\n相比之下，由于gradient descent采用的一次function做拟合，只需要考虑沿着梯度反方向寻找最小值，因此gradient descent适用于各种场景，甚至是non-convex optimization，虽然不能保证是全局最优，但至少gradient descent是可以值得一试的方法。\n\n下面来总结一下：\n* Gradient descent 和 newton's method都是利用Taylor series对objective function进行拟合来实现迭代的；\n* Gradient descent 采用一次型function拟合而 newton's method采用的是二次型function，因此newton's method迭代更迅速；\n* Newton's method每次迭代都会计算Hessian matrix的逆，在高维feature情况下，这使得每次迭代会比较慢；\n* Newton's method在non-convex optimization中很受限制，而gradient descent则不受影响。\n\n好了，先写这么多，这其中的知识量还是很深奥的，也不知道自己有没有叙述明白，欢迎大家一起来讨论！\n\n**最后感谢优男的宝贵意见！**\n## Reference\n* [UCLA courseware](http://www.math.ucla.edu/~biskup/164.2.14f/PDFs/recursions.pdf)\n* [CCU courseware](https://www.cs.ccu.edu.tw/~wtchu/courses/2014s_OPT/Lectures/Chapter%209%20Newton%27s%20Method.pdf)\n* [Taylor series](https://en.wikipedia.org/wiki/Taylor_series)\n","source":"_posts/ml-gd-and-nm.md","raw":"---\ntitle: 再深入聊聊梯度下降和牛顿法\ndate: 2017-08-11 17:26:56\ntags: \n\t- unconstrained optimization\n\t- gradient descent\n\t- newton's method\ncategories: machine learning\n---\n上次我们一起聊到了gradient descent和newton's method，而且我们已经知道了gradient descent和newton's method都是convex optimization的好方法，这次我们就跳出convex optimization，从更大的unconstrained optimization角度来探讨下这两种方法之间的关联和区别。\n<!--more-->\n\n假设我们现有一个的optimization task，要求objective function \\\\(f(x)\\\\)的最小值，我们一般有两种方案：\n* 考虑到\\\\(f(x)\\\\)的最小值很有可能是全局最小值，那么我们可以通过寻找\\\\( \\nabla f(x)=0\\\\)的点来确定最小值，这就是**newton's method**的思想\n* 既然我们要寻找最小值，那我们可以顺着一条\\\\(f(x)\\\\)逐渐减小的路径，顺着这条路径一直走下去，直到不再变小，这就是**gradient descent**的思想\n\nOK，简单的叙述之后，我们开始正题！\n\n## 泰勒级数(Taylor series)\n首先我们需要回忆一下高等数学中重要的Taylor series，如果\\\\( f(x)\\\\)在点\\\\( x_0\\\\)的领域内具有\\\\(n+1\\\\)阶导数，那么，在该领域内，\\\\( f(x)\\\\)可展开成\\\\(n\\\\)阶Taylor series，忽略无限大次项的形式就是\n$$f(x)=f(x_0)+ \\nabla f(x_0)(x-x_0) + \\frac{ \\nabla ^2 f(x_0)}{2!}(x-x_0)^2 +...+\\frac{ \\nabla ^n f(x_0)}{n!}(x-x_0)^n $$\n其实在高等数学中学到Taylor series的时候，我本人是十分无感的，我并不知道这个东西到底有什么用处，相信很多人和我有相似的经历。\n\n> In mathematics, a Taylor series is a representation of a function as an infinite sum of terms that are calculated from the values of the function's derivatives at a single point.\n\n事实上，Taylor series所表现的是，对于\\\\( f(x)\\\\)在点\\\\( x_0\\\\)附近的一个估计，也可以理解为，根据\\\\( x_0\\\\)点处的各阶derivatives之和构成一个新的function，这个function就是对\\\\(f(x)\\\\)的逼近和拟合，而且这种逼近和拟合，随着Taylor series阶数增加而更接近于真实的\\\\(f(x)\\\\)。如果我们使用0阶Taylor series来逼近的话，那我们就粗暴的认为，\\\\( f(x)\\\\)在点\\\\( x_0\\\\)附近的值就都是\\\\(x_0\\\\)，这当然太粗暴直接了，哈哈。\n\n既然这太粗暴了，那么我们就用1st order Taylor series来做一个逼近和估计，这就是gradient descent的思想；如果我们用2nd order Taylor series来估计呢，那就成了newton's method了\n\nOK，我们继续娓娓道来。\n\n## 1st order Taylor series & gradient descent\n假设\\\\(x_k\\\\)是第k次gradient descent迭代后的\\\\(x\\\\)取值，那我们在此处的1st order Taylor series 就是\n$$f(x)=f(x_k)+ \\nabla f(x_k)(x-x_k)$$\n其中\\\\(x\\\\)是迭代的下一个方向，gradient descent的目标就是让\\\\(f(x)\\\\)达到局部甚至全局最小值，那么每一次迭代，也需要尽可能的减小更多以达到这个目的，那么\n$$f(x_k)-f(x)=- \\nabla f(x_k)(x-x_k)$$\n显然，上式应该尽可能的大，即**\\\\(- \\nabla f(x_k)(x-x_k)\\\\)越大越好**，我们现在把\\\\((x-x_k)\\\\)做一个替换，用单位向量\\\\(\\vec g\\\\)和标量\\\\( \\alpha\\\\)分别代表方向和大小，现在的任务就变成了\n$$ \\min \\nabla f(x_k)(x-x_k) = \\min( \\alpha \\vec{\\nabla f(x_k)}⋅ \\vec g)$$\n我们都知道，**对于两个向量来说，当他们方向相反时，他们的内积是最小的**。\n\n>梯度方向的定义是该点梯度在标量场增长最快的方向\n\n因此当\\\\(\\vec g\\\\)的方向是\\\\( \\vec{\\nabla f(x_k)}\\\\)的反方向时，上式可以取到最小值，于是就有\n$$x-x_k=- \\alpha \\nabla f(x_k)$$\n$$x:=x_k- \\alpha \\nabla f(x_k)$$\n到这一步，是不是看到了熟悉的gradient descent呢，yeah mate！We make it!\n## 2nd order Taylor series & newton's method\n和上面的gradient descent相似，假设\\\\(x_k\\\\)是第\\\\(k\\\\)次newton's method迭代后的\\\\(x\\\\)取值，那我们在此处的2nd order Taylor series 是\n$$f(x)=f(x_k)+ \\nabla f(x_k)(x-x_k) + \\frac{1}{2}(x-x_k)^T \\nabla^2 f(x_k)(x-x_k) $$\n我们对等号两边同时对\\\\(x\\\\)求导，并令其为零\n$$ \\nabla f(x) = \\nabla f(x_k) + (x-x_k) \\nabla^2 f(x_k)=0$$\n由于newton's method的原理就是通过\\\\(\\nabla f(x)=0\\\\)来寻找最小值，**故上式为零的解\\\\(x\\\\)其实就是newton's method在\\\\(k+1\\\\)次迭代后的新的\\\\(x\\\\)值**。其中\\\\(\\nabla f(x_k)\\\\)是\\\\(x_k\\\\)处的一阶导数，\\\\( \\nabla^2 f(x_k)\\\\)是\\\\(x_k\\\\)处的二阶导数Hessian矩阵元素\n\n我们令\\\\(\\nabla f(x_k)=g\\\\)，\\\\(\\nabla^2 f(x_k)=H\\\\)，则上式变成\n$$g+H(x-x_k)=0$$\n进一步的\n$$x=x_k-H^{-1}g$$\n由于\\\\(-g H^{-1} \\\\) 是优化的前进方向，在寻找最小值的过程中，这个方向一定是和梯度方向\\\\(g\\\\)相反才可以更快的下降，那么就有\\\\( g^T H^{-1} g > 0\\\\)，这不就是positive definite的定义吗？也就是说，**Hessian矩阵是positive definite的**。\n\n想象一下，如果Hessian是negative definite的话，参数更新的方向就成了和\\\\(g\\\\)相同的方向，newton's method将会发散，这一点，也是newton's method的缺点。在objective function是non-convex function的情况下，如果第\\\\(k\\\\)次迭代获得的\\\\(x_k\\\\)处的Hessian matrix negative definite，那么newton's method将会发散，从而导致不收敛。当然，为了解决这种问题，后续有改进的BFGS等方法，我们在这里暂时不详细讨论。\n## Sum up\n下面我们再来总结性质的对比一下两种方法，来看一张图\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/2/2-1.png)\n事实上，这两种方法都采用了一种逼近和拟合的思想。假设现在处于迭代\\\\(k\\\\)次之后的\\\\(x_k\\\\)点，对于objective function，我们用\\\\(x_k\\\\)点的Taylor series \\\\(f(x)\\\\)来逼近和拟合，当然了，上图我们看到，gradient descent是用一次function而newton's method采用的是二次function，这是二者之间最显著的区别。\n\n对于new's method，在拟合之后，我们通过\\\\( \\nabla f(x)=0\\\\)求得的\\\\(x \\_{k+1}\\\\)点作为此次迭代的结果，下次迭代时候，又在\\\\(x \\_{k+1}\\\\)处次进行二次function的拟合，并如此迭代下去。\n\nNewton's method采用二次function来拟合，我们可以感性的理解为，newton's method在寻找下降的方向时候，关注的不仅仅是此处objective function value是不是减小(一阶value)，还关注此处value下降的趋势如何(二阶value)，而gradient descent只关心此处function value是不是减小，因此newton's method可以迭代更少次数获得最优解。对于标准二次型的objective function，newton's method甚至可以一次迭代就找到全局最小值。\n\n但是值得注意的是，上面所说的标准二次型function，实质上是convex function，在一般的unconstrained optimization中，更多的情况则是non-convex optimization，对于一般的non-convex optimization，newton's method是相对不稳定的，因为我们很难保证Hessian matrix的positive definite。鉴于此，我们会加入步长\\\\(\\lambda\\\\)限制，防止其一次迭代过大而带来迭代后Hessian matrix negative definite的情况，即\n$$x:=x- \\lambda H^{-1} g$$\n对于这种思想，我个人认为，是在整体non-convex function中寻找一个局部的convex function，通过步长将newton's method限制在这个局部中，最后收敛到局部最优中。由此可见，newton's mtehod在non-convex中受限制比较大。\n\n相比之下，由于gradient descent采用的一次function做拟合，只需要考虑沿着梯度反方向寻找最小值，因此gradient descent适用于各种场景，甚至是non-convex optimization，虽然不能保证是全局最优，但至少gradient descent是可以值得一试的方法。\n\n下面来总结一下：\n* Gradient descent 和 newton's method都是利用Taylor series对objective function进行拟合来实现迭代的；\n* Gradient descent 采用一次型function拟合而 newton's method采用的是二次型function，因此newton's method迭代更迅速；\n* Newton's method每次迭代都会计算Hessian matrix的逆，在高维feature情况下，这使得每次迭代会比较慢；\n* Newton's method在non-convex optimization中很受限制，而gradient descent则不受影响。\n\n好了，先写这么多，这其中的知识量还是很深奥的，也不知道自己有没有叙述明白，欢迎大家一起来讨论！\n\n**最后感谢优男的宝贵意见！**\n## Reference\n* [UCLA courseware](http://www.math.ucla.edu/~biskup/164.2.14f/PDFs/recursions.pdf)\n* [CCU courseware](https://www.cs.ccu.edu.tw/~wtchu/courses/2014s_OPT/Lectures/Chapter%209%20Newton%27s%20Method.pdf)\n* [Taylor series](https://en.wikipedia.org/wiki/Taylor_series)\n","slug":"ml-gd-and-nm","published":1,"updated":"2020-05-10T06:50:12.530Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cka0wnkwg000uqxotxdkjaqia","content":"<p>上次我们一起聊到了gradient descent和newton’s method，而且我们已经知道了gradient descent和newton’s method都是convex optimization的好方法，这次我们就跳出convex optimization，从更大的unconstrained optimization角度来探讨下这两种方法之间的关联和区别。<br><a id=\"more\"></a></p>\n<p>假设我们现有一个的optimization task，要求objective function \\(f(x)\\)的最小值，我们一般有两种方案：</p>\n<ul>\n<li>考虑到\\(f(x)\\)的最小值很有可能是全局最小值，那么我们可以通过寻找\\( \\nabla f(x)=0\\)的点来确定最小值，这就是<strong>newton’s method</strong>的思想</li>\n<li>既然我们要寻找最小值，那我们可以顺着一条\\(f(x)\\)逐渐减小的路径，顺着这条路径一直走下去，直到不再变小，这就是<strong>gradient descent</strong>的思想</li>\n</ul>\n<p>OK，简单的叙述之后，我们开始正题！</p>\n<h2 id=\"泰勒级数-Taylor-series\"><a href=\"#泰勒级数-Taylor-series\" class=\"headerlink\" title=\"泰勒级数(Taylor series)\"></a>泰勒级数(Taylor series)</h2><p>首先我们需要回忆一下高等数学中重要的Taylor series，如果\\( f(x)\\)在点\\( x_0\\)的领域内具有\\(n+1\\)阶导数，那么，在该领域内，\\( f(x)\\)可展开成\\(n\\)阶Taylor series，忽略无限大次项的形式就是<br>$$f(x)=f(x_0)+ \\nabla f(x_0)(x-x_0) + \\frac{ \\nabla ^2 f(x_0)}{2!}(x-x_0)^2 +…+\\frac{ \\nabla ^n f(x_0)}{n!}(x-x_0)^n $$<br>其实在高等数学中学到Taylor series的时候，我本人是十分无感的，我并不知道这个东西到底有什么用处，相信很多人和我有相似的经历。</p>\n<blockquote>\n<p>In mathematics, a Taylor series is a representation of a function as an infinite sum of terms that are calculated from the values of the function’s derivatives at a single point.</p>\n</blockquote>\n<p>事实上，Taylor series所表现的是，对于\\( f(x)\\)在点\\( x_0\\)附近的一个估计，也可以理解为，根据\\( x_0\\)点处的各阶derivatives之和构成一个新的function，这个function就是对\\(f(x)\\)的逼近和拟合，而且这种逼近和拟合，随着Taylor series阶数增加而更接近于真实的\\(f(x)\\)。如果我们使用0阶Taylor series来逼近的话，那我们就粗暴的认为，\\( f(x)\\)在点\\( x_0\\)附近的值就都是\\(x_0\\)，这当然太粗暴直接了，哈哈。</p>\n<p>既然这太粗暴了，那么我们就用1st order Taylor series来做一个逼近和估计，这就是gradient descent的思想；如果我们用2nd order Taylor series来估计呢，那就成了newton’s method了</p>\n<p>OK，我们继续娓娓道来。</p>\n<h2 id=\"1st-order-Taylor-series-amp-gradient-descent\"><a href=\"#1st-order-Taylor-series-amp-gradient-descent\" class=\"headerlink\" title=\"1st order Taylor series &amp; gradient descent\"></a>1st order Taylor series &amp; gradient descent</h2><p>假设\\(x_k\\)是第k次gradient descent迭代后的\\(x\\)取值，那我们在此处的1st order Taylor series 就是<br>$$f(x)=f(x_k)+ \\nabla f(x_k)(x-x_k)$$<br>其中\\(x\\)是迭代的下一个方向，gradient descent的目标就是让\\(f(x)\\)达到局部甚至全局最小值，那么每一次迭代，也需要尽可能的减小更多以达到这个目的，那么<br>$$f(x_k)-f(x)=- \\nabla f(x_k)(x-x_k)$$<br>显然，上式应该尽可能的大，即<strong>\\(- \\nabla f(x_k)(x-x_k)\\)越大越好</strong>，我们现在把\\((x-x_k)\\)做一个替换，用单位向量\\(\\vec g\\)和标量\\( \\alpha\\)分别代表方向和大小，现在的任务就变成了<br>$$ \\min \\nabla f(x_k)(x-x_k) = \\min( \\alpha \\vec{\\nabla f(x_k)}⋅ \\vec g)$$<br>我们都知道，<strong>对于两个向量来说，当他们方向相反时，他们的内积是最小的</strong>。</p>\n<blockquote>\n<p>梯度方向的定义是该点梯度在标量场增长最快的方向</p>\n</blockquote>\n<p>因此当\\(\\vec g\\)的方向是\\( \\vec{\\nabla f(x_k)}\\)的反方向时，上式可以取到最小值，于是就有<br>$$x-x_k=- \\alpha \\nabla f(x_k)$$<br>$$x:=x_k- \\alpha \\nabla f(x_k)$$<br>到这一步，是不是看到了熟悉的gradient descent呢，yeah mate！We make it!</p>\n<h2 id=\"2nd-order-Taylor-series-amp-newton’s-method\"><a href=\"#2nd-order-Taylor-series-amp-newton’s-method\" class=\"headerlink\" title=\"2nd order Taylor series &amp; newton’s method\"></a>2nd order Taylor series &amp; newton’s method</h2><p>和上面的gradient descent相似，假设\\(x_k\\)是第\\(k\\)次newton’s method迭代后的\\(x\\)取值，那我们在此处的2nd order Taylor series 是<br>$$f(x)=f(x_k)+ \\nabla f(x_k)(x-x_k) + \\frac{1}{2}(x-x_k)^T \\nabla^2 f(x_k)(x-x_k) $$<br>我们对等号两边同时对\\(x\\)求导，并令其为零<br>$$ \\nabla f(x) = \\nabla f(x_k) + (x-x_k) \\nabla^2 f(x_k)=0$$<br>由于newton’s method的原理就是通过\\(\\nabla f(x)=0\\)来寻找最小值，<strong>故上式为零的解\\(x\\)其实就是newton’s method在\\(k+1\\)次迭代后的新的\\(x\\)值</strong>。其中\\(\\nabla f(x_k)\\)是\\(x_k\\)处的一阶导数，\\( \\nabla^2 f(x_k)\\)是\\(x_k\\)处的二阶导数Hessian矩阵元素</p>\n<p>我们令\\(\\nabla f(x_k)=g\\)，\\(\\nabla^2 f(x_k)=H\\)，则上式变成<br>$$g+H(x-x_k)=0$$<br>进一步的<br>$$x=x_k-H^{-1}g$$<br>由于\\(-g H^{-1} \\) 是优化的前进方向，在寻找最小值的过程中，这个方向一定是和梯度方向\\(g\\)相反才可以更快的下降，那么就有\\( g^T H^{-1} g &gt; 0\\)，这不就是positive definite的定义吗？也就是说，<strong>Hessian矩阵是positive definite的</strong>。</p>\n<p>想象一下，如果Hessian是negative definite的话，参数更新的方向就成了和\\(g\\)相同的方向，newton’s method将会发散，这一点，也是newton’s method的缺点。在objective function是non-convex function的情况下，如果第\\(k\\)次迭代获得的\\(x_k\\)处的Hessian matrix negative definite，那么newton’s method将会发散，从而导致不收敛。当然，为了解决这种问题，后续有改进的BFGS等方法，我们在这里暂时不详细讨论。</p>\n<h2 id=\"Sum-up\"><a href=\"#Sum-up\" class=\"headerlink\" title=\"Sum up\"></a>Sum up</h2><p>下面我们再来总结性质的对比一下两种方法，来看一张图<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/2/2-1.png\" alt=\"\"><br>事实上，这两种方法都采用了一种逼近和拟合的思想。假设现在处于迭代\\(k\\)次之后的\\(x_k\\)点，对于objective function，我们用\\(x_k\\)点的Taylor series \\(f(x)\\)来逼近和拟合，当然了，上图我们看到，gradient descent是用一次function而newton’s method采用的是二次function，这是二者之间最显著的区别。</p>\n<p>对于new’s method，在拟合之后，我们通过\\( \\nabla f(x)=0\\)求得的\\(x _{k+1}\\)点作为此次迭代的结果，下次迭代时候，又在\\(x _{k+1}\\)处次进行二次function的拟合，并如此迭代下去。</p>\n<p>Newton’s method采用二次function来拟合，我们可以感性的理解为，newton’s method在寻找下降的方向时候，关注的不仅仅是此处objective function value是不是减小(一阶value)，还关注此处value下降的趋势如何(二阶value)，而gradient descent只关心此处function value是不是减小，因此newton’s method可以迭代更少次数获得最优解。对于标准二次型的objective function，newton’s method甚至可以一次迭代就找到全局最小值。</p>\n<p>但是值得注意的是，上面所说的标准二次型function，实质上是convex function，在一般的unconstrained optimization中，更多的情况则是non-convex optimization，对于一般的non-convex optimization，newton’s method是相对不稳定的，因为我们很难保证Hessian matrix的positive definite。鉴于此，我们会加入步长\\(\\lambda\\)限制，防止其一次迭代过大而带来迭代后Hessian matrix negative definite的情况，即<br>$$x:=x- \\lambda H^{-1} g$$<br>对于这种思想，我个人认为，是在整体non-convex function中寻找一个局部的convex function，通过步长将newton’s method限制在这个局部中，最后收敛到局部最优中。由此可见，newton’s mtehod在non-convex中受限制比较大。</p>\n<p>相比之下，由于gradient descent采用的一次function做拟合，只需要考虑沿着梯度反方向寻找最小值，因此gradient descent适用于各种场景，甚至是non-convex optimization，虽然不能保证是全局最优，但至少gradient descent是可以值得一试的方法。</p>\n<p>下面来总结一下：</p>\n<ul>\n<li>Gradient descent 和 newton’s method都是利用Taylor series对objective function进行拟合来实现迭代的；</li>\n<li>Gradient descent 采用一次型function拟合而 newton’s method采用的是二次型function，因此newton’s method迭代更迅速；</li>\n<li>Newton’s method每次迭代都会计算Hessian matrix的逆，在高维feature情况下，这使得每次迭代会比较慢；</li>\n<li>Newton’s method在non-convex optimization中很受限制，而gradient descent则不受影响。</li>\n</ul>\n<p>好了，先写这么多，这其中的知识量还是很深奥的，也不知道自己有没有叙述明白，欢迎大家一起来讨论！</p>\n<p><strong>最后感谢优男的宝贵意见！</strong></p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ul>\n<li><a href=\"http://www.math.ucla.edu/~biskup/164.2.14f/PDFs/recursions.pdf\" target=\"_blank\" rel=\"noopener\">UCLA courseware</a></li>\n<li><a href=\"https://www.cs.ccu.edu.tw/~wtchu/courses/2014s_OPT/Lectures/Chapter%209%20Newton%27s%20Method.pdf\" target=\"_blank\" rel=\"noopener\">CCU courseware</a></li>\n<li><a href=\"https://en.wikipedia.org/wiki/Taylor_series\" target=\"_blank\" rel=\"noopener\">Taylor series</a></li>\n</ul>\n","site":{"data":{}},"_categories":[{"name":"machine learning","path":"categories/machine-learning/"}],"_tags":[{"name":"gradient descent","path":"tags/gradient-descent/"},{"name":"unconstrained optimization","path":"tags/unconstrained-optimization/"},{"name":"newton's method","path":"tags/newton-s-method/"}],"excerpt":"<p>上次我们一起聊到了gradient descent和newton’s method，而且我们已经知道了gradient descent和newton’s method都是convex optimization的好方法，这次我们就跳出convex optimization，从更大的unconstrained optimization角度来探讨下这两种方法之间的关联和区别。<br></p>","more":"</p>\n<p>假设我们现有一个的optimization task，要求objective function \\(f(x)\\)的最小值，我们一般有两种方案：</p>\n<ul>\n<li>考虑到\\(f(x)\\)的最小值很有可能是全局最小值，那么我们可以通过寻找\\( \\nabla f(x)=0\\)的点来确定最小值，这就是<strong>newton’s method</strong>的思想</li>\n<li>既然我们要寻找最小值，那我们可以顺着一条\\(f(x)\\)逐渐减小的路径，顺着这条路径一直走下去，直到不再变小，这就是<strong>gradient descent</strong>的思想</li>\n</ul>\n<p>OK，简单的叙述之后，我们开始正题！</p>\n<h2 id=\"泰勒级数-Taylor-series\"><a href=\"#泰勒级数-Taylor-series\" class=\"headerlink\" title=\"泰勒级数(Taylor series)\"></a>泰勒级数(Taylor series)</h2><p>首先我们需要回忆一下高等数学中重要的Taylor series，如果\\( f(x)\\)在点\\( x_0\\)的领域内具有\\(n+1\\)阶导数，那么，在该领域内，\\( f(x)\\)可展开成\\(n\\)阶Taylor series，忽略无限大次项的形式就是<br>$$f(x)=f(x_0)+ \\nabla f(x_0)(x-x_0) + \\frac{ \\nabla ^2 f(x_0)}{2!}(x-x_0)^2 +…+\\frac{ \\nabla ^n f(x_0)}{n!}(x-x_0)^n $$<br>其实在高等数学中学到Taylor series的时候，我本人是十分无感的，我并不知道这个东西到底有什么用处，相信很多人和我有相似的经历。</p>\n<blockquote>\n<p>In mathematics, a Taylor series is a representation of a function as an infinite sum of terms that are calculated from the values of the function’s derivatives at a single point.</p>\n</blockquote>\n<p>事实上，Taylor series所表现的是，对于\\( f(x)\\)在点\\( x_0\\)附近的一个估计，也可以理解为，根据\\( x_0\\)点处的各阶derivatives之和构成一个新的function，这个function就是对\\(f(x)\\)的逼近和拟合，而且这种逼近和拟合，随着Taylor series阶数增加而更接近于真实的\\(f(x)\\)。如果我们使用0阶Taylor series来逼近的话，那我们就粗暴的认为，\\( f(x)\\)在点\\( x_0\\)附近的值就都是\\(x_0\\)，这当然太粗暴直接了，哈哈。</p>\n<p>既然这太粗暴了，那么我们就用1st order Taylor series来做一个逼近和估计，这就是gradient descent的思想；如果我们用2nd order Taylor series来估计呢，那就成了newton’s method了</p>\n<p>OK，我们继续娓娓道来。</p>\n<h2 id=\"1st-order-Taylor-series-amp-gradient-descent\"><a href=\"#1st-order-Taylor-series-amp-gradient-descent\" class=\"headerlink\" title=\"1st order Taylor series &amp; gradient descent\"></a>1st order Taylor series &amp; gradient descent</h2><p>假设\\(x_k\\)是第k次gradient descent迭代后的\\(x\\)取值，那我们在此处的1st order Taylor series 就是<br>$$f(x)=f(x_k)+ \\nabla f(x_k)(x-x_k)$$<br>其中\\(x\\)是迭代的下一个方向，gradient descent的目标就是让\\(f(x)\\)达到局部甚至全局最小值，那么每一次迭代，也需要尽可能的减小更多以达到这个目的，那么<br>$$f(x_k)-f(x)=- \\nabla f(x_k)(x-x_k)$$<br>显然，上式应该尽可能的大，即<strong>\\(- \\nabla f(x_k)(x-x_k)\\)越大越好</strong>，我们现在把\\((x-x_k)\\)做一个替换，用单位向量\\(\\vec g\\)和标量\\( \\alpha\\)分别代表方向和大小，现在的任务就变成了<br>$$ \\min \\nabla f(x_k)(x-x_k) = \\min( \\alpha \\vec{\\nabla f(x_k)}⋅ \\vec g)$$<br>我们都知道，<strong>对于两个向量来说，当他们方向相反时，他们的内积是最小的</strong>。</p>\n<blockquote>\n<p>梯度方向的定义是该点梯度在标量场增长最快的方向</p>\n</blockquote>\n<p>因此当\\(\\vec g\\)的方向是\\( \\vec{\\nabla f(x_k)}\\)的反方向时，上式可以取到最小值，于是就有<br>$$x-x_k=- \\alpha \\nabla f(x_k)$$<br>$$x:=x_k- \\alpha \\nabla f(x_k)$$<br>到这一步，是不是看到了熟悉的gradient descent呢，yeah mate！We make it!</p>\n<h2 id=\"2nd-order-Taylor-series-amp-newton’s-method\"><a href=\"#2nd-order-Taylor-series-amp-newton’s-method\" class=\"headerlink\" title=\"2nd order Taylor series &amp; newton’s method\"></a>2nd order Taylor series &amp; newton’s method</h2><p>和上面的gradient descent相似，假设\\(x_k\\)是第\\(k\\)次newton’s method迭代后的\\(x\\)取值，那我们在此处的2nd order Taylor series 是<br>$$f(x)=f(x_k)+ \\nabla f(x_k)(x-x_k) + \\frac{1}{2}(x-x_k)^T \\nabla^2 f(x_k)(x-x_k) $$<br>我们对等号两边同时对\\(x\\)求导，并令其为零<br>$$ \\nabla f(x) = \\nabla f(x_k) + (x-x_k) \\nabla^2 f(x_k)=0$$<br>由于newton’s method的原理就是通过\\(\\nabla f(x)=0\\)来寻找最小值，<strong>故上式为零的解\\(x\\)其实就是newton’s method在\\(k+1\\)次迭代后的新的\\(x\\)值</strong>。其中\\(\\nabla f(x_k)\\)是\\(x_k\\)处的一阶导数，\\( \\nabla^2 f(x_k)\\)是\\(x_k\\)处的二阶导数Hessian矩阵元素</p>\n<p>我们令\\(\\nabla f(x_k)=g\\)，\\(\\nabla^2 f(x_k)=H\\)，则上式变成<br>$$g+H(x-x_k)=0$$<br>进一步的<br>$$x=x_k-H^{-1}g$$<br>由于\\(-g H^{-1} \\) 是优化的前进方向，在寻找最小值的过程中，这个方向一定是和梯度方向\\(g\\)相反才可以更快的下降，那么就有\\( g^T H^{-1} g &gt; 0\\)，这不就是positive definite的定义吗？也就是说，<strong>Hessian矩阵是positive definite的</strong>。</p>\n<p>想象一下，如果Hessian是negative definite的话，参数更新的方向就成了和\\(g\\)相同的方向，newton’s method将会发散，这一点，也是newton’s method的缺点。在objective function是non-convex function的情况下，如果第\\(k\\)次迭代获得的\\(x_k\\)处的Hessian matrix negative definite，那么newton’s method将会发散，从而导致不收敛。当然，为了解决这种问题，后续有改进的BFGS等方法，我们在这里暂时不详细讨论。</p>\n<h2 id=\"Sum-up\"><a href=\"#Sum-up\" class=\"headerlink\" title=\"Sum up\"></a>Sum up</h2><p>下面我们再来总结性质的对比一下两种方法，来看一张图<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/2/2-1.png\" alt=\"\"><br>事实上，这两种方法都采用了一种逼近和拟合的思想。假设现在处于迭代\\(k\\)次之后的\\(x_k\\)点，对于objective function，我们用\\(x_k\\)点的Taylor series \\(f(x)\\)来逼近和拟合，当然了，上图我们看到，gradient descent是用一次function而newton’s method采用的是二次function，这是二者之间最显著的区别。</p>\n<p>对于new’s method，在拟合之后，我们通过\\( \\nabla f(x)=0\\)求得的\\(x _{k+1}\\)点作为此次迭代的结果，下次迭代时候，又在\\(x _{k+1}\\)处次进行二次function的拟合，并如此迭代下去。</p>\n<p>Newton’s method采用二次function来拟合，我们可以感性的理解为，newton’s method在寻找下降的方向时候，关注的不仅仅是此处objective function value是不是减小(一阶value)，还关注此处value下降的趋势如何(二阶value)，而gradient descent只关心此处function value是不是减小，因此newton’s method可以迭代更少次数获得最优解。对于标准二次型的objective function，newton’s method甚至可以一次迭代就找到全局最小值。</p>\n<p>但是值得注意的是，上面所说的标准二次型function，实质上是convex function，在一般的unconstrained optimization中，更多的情况则是non-convex optimization，对于一般的non-convex optimization，newton’s method是相对不稳定的，因为我们很难保证Hessian matrix的positive definite。鉴于此，我们会加入步长\\(\\lambda\\)限制，防止其一次迭代过大而带来迭代后Hessian matrix negative definite的情况，即<br>$$x:=x- \\lambda H^{-1} g$$<br>对于这种思想，我个人认为，是在整体non-convex function中寻找一个局部的convex function，通过步长将newton’s method限制在这个局部中，最后收敛到局部最优中。由此可见，newton’s mtehod在non-convex中受限制比较大。</p>\n<p>相比之下，由于gradient descent采用的一次function做拟合，只需要考虑沿着梯度反方向寻找最小值，因此gradient descent适用于各种场景，甚至是non-convex optimization，虽然不能保证是全局最优，但至少gradient descent是可以值得一试的方法。</p>\n<p>下面来总结一下：</p>\n<ul>\n<li>Gradient descent 和 newton’s method都是利用Taylor series对objective function进行拟合来实现迭代的；</li>\n<li>Gradient descent 采用一次型function拟合而 newton’s method采用的是二次型function，因此newton’s method迭代更迅速；</li>\n<li>Newton’s method每次迭代都会计算Hessian matrix的逆，在高维feature情况下，这使得每次迭代会比较慢；</li>\n<li>Newton’s method在non-convex optimization中很受限制，而gradient descent则不受影响。</li>\n</ul>\n<p>好了，先写这么多，这其中的知识量还是很深奥的，也不知道自己有没有叙述明白，欢迎大家一起来讨论！</p>\n<p><strong>最后感谢优男的宝贵意见！</strong></p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ul>\n<li><a href=\"http://www.math.ucla.edu/~biskup/164.2.14f/PDFs/recursions.pdf\" target=\"_blank\" rel=\"noopener\">UCLA courseware</a></li>\n<li><a href=\"https://www.cs.ccu.edu.tw/~wtchu/courses/2014s_OPT/Lectures/Chapter%209%20Newton%27s%20Method.pdf\" target=\"_blank\" rel=\"noopener\">CCU courseware</a></li>\n<li><a href=\"https://en.wikipedia.org/wiki/Taylor_series\" target=\"_blank\" rel=\"noopener\">Taylor series</a></li>\n</ul>"},{"title":"从凸函数到梯度下降和牛顿法","date":"2017-08-02T07:53:55.000Z","_content":"记得我在和优男一起研究logistic regression的时候，他问了我几个非常尖锐的问题，让我顿时哑口无言\n* 怎么保证logistic regression通过gradient descent找到的是最优解；\n* 为什么logistic regression可以用newton's method呢？\n* Newton's method中Hessian matrix必须positive definite有什么意义呢，log cost function能保证吗？\n\n这些细节问题，说实话我也没有认真的想过。在夸奖他之余，我们也一起开始了研究，希望从中学习到一些更深层的东西，趁着现在有个blog分享给大家\n<!--more-->\n\n## 凸函数(Convex function)\n在开始之前，我有一个关于术语的倡议。中文里的“凸函数”，看上去是凹下去的，对应的，中文里的“凹函数”看上去凸起来的，amazing吧？这是有一定历史原因的，感兴趣的朋友可以去查阅下资料，这里我们不再复述。所以为了避免让大家产生误解，我鼓励大家使用英文，**convex function**和**concave function**.这样会避免很多不必要的麻烦。\n\nOK，我们来一起看看，convex function\n\n对于一维函数 \\\\(f(x)\\\\)来说，在定义域内的任意值 \\\\(a\\\\)和\\\\(b\\\\)，对于任意的 \\\\( 0 \\leq \\theta \\leq 1\\\\)，如果满足以下条件，则称为convex function\n$$f(\\theta a+(1-\\theta) b) \\leq \\theta f(a) + (1- \\theta)f(b)$$\n我们再用图片直观的感受一下\n\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/1/1-1.png)\n显而易见的是，当公式中等号去掉的时候，函数就是**strictly convex function**.\n\nConvex function具有一定的性质，我们简单的描述一下。\n\n### First order condition\n对于 function \\\\(f\\\\)，在定义域内**一阶可导**，且导数为\n$$ \t\\nabla f=( \\frac{\\partial f(x)}{x_1}, \\frac{\\partial f(x)}{x_2},...,  \\frac{\\partial f(x)}{x_n})$$\n那么 \\\\(f\\\\)是convex function的**充要条件**是：对于定义域内任意 \\\\(x\\\\) 和 \\\\(y\\\\)\n$$ f(y) \\geq f(x) + \\nabla f(x)^T (y - x)$$\nOK，再来张图片直观感受一下：\n\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/1/1-2.png)\n\n其实简单的来讲，就是对于convex function \\\\(f\\\\)，它的函数值永远大于等于切线上的值！\n\n### Second order condition\n对于 function \\\\(f\\\\)，在定义域内**二阶可导**，且 \\\\(n\\\\) 维方阵Hessian matrix的元素为\n$$ \\nabla ^2 f(x) = \\frac{ \\partial ^2 f(x)}{ \\partial x_i \\partial x_j}, \\quad i,j = 1,2,...,n$$\n当且仅当Hessian matrix positive semi-defnite的时候，\\\\(f\\\\) 是convex function。以上互为**充要条件**。这里的证明我不想展开讲，在后面我会给出reference链接。\n\n---\n下面给出一些 \\\\(\\Bbb R\\\\) 空间下常见的convex function：\n* 线性函数：\\\\(f(x) = ax+b\\\\)\n* 指数函数：\\\\(f(x)=e^ {ax}\\\\)\n* 负熵函数： \\\\(f(x)=xlogx \\quad on \\quad \\Bbb R_{++}\\\\)\n\n对应的，一些常见的concave function：\n* 线性函数：\\\\(f(x) = ax+b\\\\)\n* 对数函数： \\\\(f(x)=logx  \\quad on \\quad \\Bbb R_{++}\\\\)\n\n其中大家可以看到，线性函数既是convex也是concave函数，比较特殊，这和它本身的first order condition为常数有关。\n\n以上就是convex function的一个简单介绍，你也许会问，为什么花这么多力气来介绍convex function. 其实，在machine learning中，convex function的优化是非常重要的，很多算法说到底，都是要optimize一个convex function，我们会用liner regression和logistic regression为例子，进一步从convex function的简介过渡到gradient descent和newton's method.\n\n## 梯度下降法(Gradient descent)\n关于gradient descent，我们使用liner regression作为例子来讨论。Liner regression算法的实质是least square method，他的cost function是\n$$J( \\theta)= \\frac{1}{2} \\sum_{i=1} ^m (h_ \\theta (x ^{(i)})-y^{(i)})^2, \\quad h_ \\theta (x)= \\theta ^Tx $$\n对于liner regression来说，算法的实质就是去求出\\\\( J( \\theta) \\\\) **以\\\\( \\theta\\\\)为参数**的minimum，gradient descent算法的作用就是去实现了这个过程，gradient descent的基础知识详见reference. \n\n那么针对cost function，gradient descent是如何保证收敛的呢，我们一起来看看\n\n对于\\\\(J( \\theta)\\\\)，我们将其带入convex function的定义公式中，注意这里我们的自变量是\\\\( \\theta\\\\)，我们可以通过推导证明该式成立，也就是说，least square cost function是convex function.\n\n既然有这个结论了，那么我们可以想象一下，least square cost function作为convex function，是存在全局最小值的，也就是说，gradient descent不会出现陷入局部最优无法自拔的现象，只要gradient descent保证参数足够好的情况下，理论上，是完全可以很好的逼近全局最优的解的。\n\n> Gradien descent算法本身并不能保证获得全局最小值，只有在objective function是convex function的时候才可以保证\n\n下图可以看出，右边的object function是non-convex function，因而很容易陷入到局部最小值无法自拔，而左边的objective function是一个标准的convex function，在gradient descent参数合理的前提下，可以逼近全局最优。\n\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/1/1-3.png)\n\n当然，gradien descent的一些改进方法，例如stochastic gradient descent在解决non-convex optimization上有一些帮助，但是我们在这里不做讨论，后面有时间我会专门再写。\n\n由此，我们可以得出，gradient descent不仅仅是minimize liner regression的一个很好的方法，也是convex optimization的一种理想方法\n\n## 牛顿法(Newton's method)\nNewton's method 这块内容，我们将会用logistic regression作为例子。同样，我们先来关注下log cost function，这里，**我们取label为-1和+1**，因为这样得到的cost function比0,1下的计算更加简单\n$$J( \\omega)= - \\frac{1}{m} \\sum_{i=1} ^{m} log(1+e^{-y^{(i)} \\omega^T x^{(i)}})$$\n这里我们采用了-1和+1作为标签值，和大多数教材中不一样，大家可以下来自己推导一下\\\\(J( \\omega)\\\\)，这种写法广泛的应用在了比较logistic regression和SVM两大分类器的文献中，希望大家熟知。\n\n此处我们对原始的likehood function加上了 \\\\(- \\frac{1}{m}\\\\)的系数，同样，当我们把 \\\\(J( \\omega)\\\\)带入到convex function的定义中，可以验证上式为convex function，值得注意的是，\\\\(J( \\omega)\\\\)是\\\\( \\omega\\\\)的函数。\n\n其实，我们也可以将log cost function展开后，利用最基本的函数convex和concave性质来获得上式是convex function的结论，碍于公式实在太难打，就留给大家去证明吧。\n\nOK，既然log cost function是convex function，我们一定是可以用gradient descent去求解的。问题是，如果我们用newton's method呢？\n\nNewton's method的基本原理详见reference，这里我们可以发现，既然log cost function是convex function，那么根据second order condition可以知道，它的Hessian matrix一定是positive semi-definite的。如果我们加上了L2 regularizer，**由于L2 regularizer本身就是一个strict convex function**，那么log cost function就一定是strict convex function了，也就是：\n$$J( \\omega)= - \\frac{1}{m} \\sum_{i=1} ^{m} log(1+e^{-y^{(i)} \\omega^T x^{(i)}})+ \\frac{1}{2}|| \\omega||^2$$\n因此，在log cost function中，**Hessian matrix是positive definite的**，完全满足newton's method 的要求。同样，类似于上一部分，newton's method也可以找到log cost function的全局最优。\n## Sum up\nOK，我们说到这里也确实讲了不少，这篇blog有些冗长，希望朋友们不要焦虑。总体来说，我想表达的是以下几个观点：\n* Machine learning中我们寻求的其实就是objective function一个全局最优值，这些问题是通过gradient descent等方法解决的；\n* Gradient descent和newton's method都是convex optimization的好方法，他们都可以对于convex function获得全局最优；\n* 对于non-convex optimization问题，stochastic gradient descent也很有效果，我们后续再慢慢学习。\n\n好了，核心思想就这三点，今天先说这么多！\n\n## Reference\n* [EE364, Convex Optimization Stanford University](https://see.stanford.edu/materials/lsocoee364a/03ConvexFunctions.pdf)\n* [Regularized Logistic Regression is Strictly Convex](http://qwone.com/~jason/writing/convexLR.pdf)\n* [XinyiLI大神的blog](https://www.yangzhou301.com/2016/03/14/826442654/)\n* [Liner regression](https://en.wikipedia.org/wiki/Linear_regression)\n* [Logsitc regression](https://en.wikipedia.org/wiki/Logistic_regression)\n* [Gradient descent](https://en.wikipedia.org/wiki/Gradient_descent)\n* [Newton's method](https://en.wikipedia.org/wiki/Newton%27s_method)\n\n","source":"_posts/ml-convex-opt.md","raw":"---\ntitle: 从凸函数到梯度下降和牛顿法\ndate: 2017-08-02 15:53:55\ntags: \n    - convex optimization\n    - gradient descent\n    - newton's method\ncategories: machine learning\n---\n记得我在和优男一起研究logistic regression的时候，他问了我几个非常尖锐的问题，让我顿时哑口无言\n* 怎么保证logistic regression通过gradient descent找到的是最优解；\n* 为什么logistic regression可以用newton's method呢？\n* Newton's method中Hessian matrix必须positive definite有什么意义呢，log cost function能保证吗？\n\n这些细节问题，说实话我也没有认真的想过。在夸奖他之余，我们也一起开始了研究，希望从中学习到一些更深层的东西，趁着现在有个blog分享给大家\n<!--more-->\n\n## 凸函数(Convex function)\n在开始之前，我有一个关于术语的倡议。中文里的“凸函数”，看上去是凹下去的，对应的，中文里的“凹函数”看上去凸起来的，amazing吧？这是有一定历史原因的，感兴趣的朋友可以去查阅下资料，这里我们不再复述。所以为了避免让大家产生误解，我鼓励大家使用英文，**convex function**和**concave function**.这样会避免很多不必要的麻烦。\n\nOK，我们来一起看看，convex function\n\n对于一维函数 \\\\(f(x)\\\\)来说，在定义域内的任意值 \\\\(a\\\\)和\\\\(b\\\\)，对于任意的 \\\\( 0 \\leq \\theta \\leq 1\\\\)，如果满足以下条件，则称为convex function\n$$f(\\theta a+(1-\\theta) b) \\leq \\theta f(a) + (1- \\theta)f(b)$$\n我们再用图片直观的感受一下\n\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/1/1-1.png)\n显而易见的是，当公式中等号去掉的时候，函数就是**strictly convex function**.\n\nConvex function具有一定的性质，我们简单的描述一下。\n\n### First order condition\n对于 function \\\\(f\\\\)，在定义域内**一阶可导**，且导数为\n$$ \t\\nabla f=( \\frac{\\partial f(x)}{x_1}, \\frac{\\partial f(x)}{x_2},...,  \\frac{\\partial f(x)}{x_n})$$\n那么 \\\\(f\\\\)是convex function的**充要条件**是：对于定义域内任意 \\\\(x\\\\) 和 \\\\(y\\\\)\n$$ f(y) \\geq f(x) + \\nabla f(x)^T (y - x)$$\nOK，再来张图片直观感受一下：\n\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/1/1-2.png)\n\n其实简单的来讲，就是对于convex function \\\\(f\\\\)，它的函数值永远大于等于切线上的值！\n\n### Second order condition\n对于 function \\\\(f\\\\)，在定义域内**二阶可导**，且 \\\\(n\\\\) 维方阵Hessian matrix的元素为\n$$ \\nabla ^2 f(x) = \\frac{ \\partial ^2 f(x)}{ \\partial x_i \\partial x_j}, \\quad i,j = 1,2,...,n$$\n当且仅当Hessian matrix positive semi-defnite的时候，\\\\(f\\\\) 是convex function。以上互为**充要条件**。这里的证明我不想展开讲，在后面我会给出reference链接。\n\n---\n下面给出一些 \\\\(\\Bbb R\\\\) 空间下常见的convex function：\n* 线性函数：\\\\(f(x) = ax+b\\\\)\n* 指数函数：\\\\(f(x)=e^ {ax}\\\\)\n* 负熵函数： \\\\(f(x)=xlogx \\quad on \\quad \\Bbb R_{++}\\\\)\n\n对应的，一些常见的concave function：\n* 线性函数：\\\\(f(x) = ax+b\\\\)\n* 对数函数： \\\\(f(x)=logx  \\quad on \\quad \\Bbb R_{++}\\\\)\n\n其中大家可以看到，线性函数既是convex也是concave函数，比较特殊，这和它本身的first order condition为常数有关。\n\n以上就是convex function的一个简单介绍，你也许会问，为什么花这么多力气来介绍convex function. 其实，在machine learning中，convex function的优化是非常重要的，很多算法说到底，都是要optimize一个convex function，我们会用liner regression和logistic regression为例子，进一步从convex function的简介过渡到gradient descent和newton's method.\n\n## 梯度下降法(Gradient descent)\n关于gradient descent，我们使用liner regression作为例子来讨论。Liner regression算法的实质是least square method，他的cost function是\n$$J( \\theta)= \\frac{1}{2} \\sum_{i=1} ^m (h_ \\theta (x ^{(i)})-y^{(i)})^2, \\quad h_ \\theta (x)= \\theta ^Tx $$\n对于liner regression来说，算法的实质就是去求出\\\\( J( \\theta) \\\\) **以\\\\( \\theta\\\\)为参数**的minimum，gradient descent算法的作用就是去实现了这个过程，gradient descent的基础知识详见reference. \n\n那么针对cost function，gradient descent是如何保证收敛的呢，我们一起来看看\n\n对于\\\\(J( \\theta)\\\\)，我们将其带入convex function的定义公式中，注意这里我们的自变量是\\\\( \\theta\\\\)，我们可以通过推导证明该式成立，也就是说，least square cost function是convex function.\n\n既然有这个结论了，那么我们可以想象一下，least square cost function作为convex function，是存在全局最小值的，也就是说，gradient descent不会出现陷入局部最优无法自拔的现象，只要gradient descent保证参数足够好的情况下，理论上，是完全可以很好的逼近全局最优的解的。\n\n> Gradien descent算法本身并不能保证获得全局最小值，只有在objective function是convex function的时候才可以保证\n\n下图可以看出，右边的object function是non-convex function，因而很容易陷入到局部最小值无法自拔，而左边的objective function是一个标准的convex function，在gradient descent参数合理的前提下，可以逼近全局最优。\n\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/1/1-3.png)\n\n当然，gradien descent的一些改进方法，例如stochastic gradient descent在解决non-convex optimization上有一些帮助，但是我们在这里不做讨论，后面有时间我会专门再写。\n\n由此，我们可以得出，gradient descent不仅仅是minimize liner regression的一个很好的方法，也是convex optimization的一种理想方法\n\n## 牛顿法(Newton's method)\nNewton's method 这块内容，我们将会用logistic regression作为例子。同样，我们先来关注下log cost function，这里，**我们取label为-1和+1**，因为这样得到的cost function比0,1下的计算更加简单\n$$J( \\omega)= - \\frac{1}{m} \\sum_{i=1} ^{m} log(1+e^{-y^{(i)} \\omega^T x^{(i)}})$$\n这里我们采用了-1和+1作为标签值，和大多数教材中不一样，大家可以下来自己推导一下\\\\(J( \\omega)\\\\)，这种写法广泛的应用在了比较logistic regression和SVM两大分类器的文献中，希望大家熟知。\n\n此处我们对原始的likehood function加上了 \\\\(- \\frac{1}{m}\\\\)的系数，同样，当我们把 \\\\(J( \\omega)\\\\)带入到convex function的定义中，可以验证上式为convex function，值得注意的是，\\\\(J( \\omega)\\\\)是\\\\( \\omega\\\\)的函数。\n\n其实，我们也可以将log cost function展开后，利用最基本的函数convex和concave性质来获得上式是convex function的结论，碍于公式实在太难打，就留给大家去证明吧。\n\nOK，既然log cost function是convex function，我们一定是可以用gradient descent去求解的。问题是，如果我们用newton's method呢？\n\nNewton's method的基本原理详见reference，这里我们可以发现，既然log cost function是convex function，那么根据second order condition可以知道，它的Hessian matrix一定是positive semi-definite的。如果我们加上了L2 regularizer，**由于L2 regularizer本身就是一个strict convex function**，那么log cost function就一定是strict convex function了，也就是：\n$$J( \\omega)= - \\frac{1}{m} \\sum_{i=1} ^{m} log(1+e^{-y^{(i)} \\omega^T x^{(i)}})+ \\frac{1}{2}|| \\omega||^2$$\n因此，在log cost function中，**Hessian matrix是positive definite的**，完全满足newton's method 的要求。同样，类似于上一部分，newton's method也可以找到log cost function的全局最优。\n## Sum up\nOK，我们说到这里也确实讲了不少，这篇blog有些冗长，希望朋友们不要焦虑。总体来说，我想表达的是以下几个观点：\n* Machine learning中我们寻求的其实就是objective function一个全局最优值，这些问题是通过gradient descent等方法解决的；\n* Gradient descent和newton's method都是convex optimization的好方法，他们都可以对于convex function获得全局最优；\n* 对于non-convex optimization问题，stochastic gradient descent也很有效果，我们后续再慢慢学习。\n\n好了，核心思想就这三点，今天先说这么多！\n\n## Reference\n* [EE364, Convex Optimization Stanford University](https://see.stanford.edu/materials/lsocoee364a/03ConvexFunctions.pdf)\n* [Regularized Logistic Regression is Strictly Convex](http://qwone.com/~jason/writing/convexLR.pdf)\n* [XinyiLI大神的blog](https://www.yangzhou301.com/2016/03/14/826442654/)\n* [Liner regression](https://en.wikipedia.org/wiki/Linear_regression)\n* [Logsitc regression](https://en.wikipedia.org/wiki/Logistic_regression)\n* [Gradient descent](https://en.wikipedia.org/wiki/Gradient_descent)\n* [Newton's method](https://en.wikipedia.org/wiki/Newton%27s_method)\n\n","slug":"ml-convex-opt","published":1,"updated":"2020-05-10T06:50:12.530Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cka0wnkwh000wqxotw5ylpaju","content":"<p>记得我在和优男一起研究logistic regression的时候，他问了我几个非常尖锐的问题，让我顿时哑口无言</p>\n<ul>\n<li>怎么保证logistic regression通过gradient descent找到的是最优解；</li>\n<li>为什么logistic regression可以用newton’s method呢？</li>\n<li>Newton’s method中Hessian matrix必须positive definite有什么意义呢，log cost function能保证吗？</li>\n</ul>\n<p>这些细节问题，说实话我也没有认真的想过。在夸奖他之余，我们也一起开始了研究，希望从中学习到一些更深层的东西，趁着现在有个blog分享给大家<br><a id=\"more\"></a></p>\n<h2 id=\"凸函数-Convex-function\"><a href=\"#凸函数-Convex-function\" class=\"headerlink\" title=\"凸函数(Convex function)\"></a>凸函数(Convex function)</h2><p>在开始之前，我有一个关于术语的倡议。中文里的“凸函数”，看上去是凹下去的，对应的，中文里的“凹函数”看上去凸起来的，amazing吧？这是有一定历史原因的，感兴趣的朋友可以去查阅下资料，这里我们不再复述。所以为了避免让大家产生误解，我鼓励大家使用英文，<strong>convex function</strong>和<strong>concave function</strong>.这样会避免很多不必要的麻烦。</p>\n<p>OK，我们来一起看看，convex function</p>\n<p>对于一维函数 \\(f(x)\\)来说，在定义域内的任意值 \\(a\\)和\\(b\\)，对于任意的 \\( 0 \\leq \\theta \\leq 1\\)，如果满足以下条件，则称为convex function<br>$$f(\\theta a+(1-\\theta) b) \\leq \\theta f(a) + (1- \\theta)f(b)$$<br>我们再用图片直观的感受一下</p>\n<p><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/1/1-1.png\" alt=\"\"><br>显而易见的是，当公式中等号去掉的时候，函数就是<strong>strictly convex function</strong>.</p>\n<p>Convex function具有一定的性质，我们简单的描述一下。</p>\n<h3 id=\"First-order-condition\"><a href=\"#First-order-condition\" class=\"headerlink\" title=\"First order condition\"></a>First order condition</h3><p>对于 function \\(f\\)，在定义域内<strong>一阶可导</strong>，且导数为<br>$$     \\nabla f=( \\frac{\\partial f(x)}{x_1}, \\frac{\\partial f(x)}{x_2},…,  \\frac{\\partial f(x)}{x_n})$$<br>那么 \\(f\\)是convex function的<strong>充要条件</strong>是：对于定义域内任意 \\(x\\) 和 \\(y\\)<br>$$ f(y) \\geq f(x) + \\nabla f(x)^T (y - x)$$<br>OK，再来张图片直观感受一下：</p>\n<p><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/1/1-2.png\" alt=\"\"></p>\n<p>其实简单的来讲，就是对于convex function \\(f\\)，它的函数值永远大于等于切线上的值！</p>\n<h3 id=\"Second-order-condition\"><a href=\"#Second-order-condition\" class=\"headerlink\" title=\"Second order condition\"></a>Second order condition</h3><p>对于 function \\(f\\)，在定义域内<strong>二阶可导</strong>，且 \\(n\\) 维方阵Hessian matrix的元素为<br>$$ \\nabla ^2 f(x) = \\frac{ \\partial ^2 f(x)}{ \\partial x_i \\partial x_j}, \\quad i,j = 1,2,…,n$$<br>当且仅当Hessian matrix positive semi-defnite的时候，\\(f\\) 是convex function。以上互为<strong>充要条件</strong>。这里的证明我不想展开讲，在后面我会给出reference链接。</p>\n<hr>\n<p>下面给出一些 \\(\\Bbb R\\) 空间下常见的convex function：</p>\n<ul>\n<li>线性函数：\\(f(x) = ax+b\\)</li>\n<li>指数函数：\\(f(x)=e^ {ax}\\)</li>\n<li>负熵函数： \\(f(x)=xlogx \\quad on \\quad \\Bbb R_{++}\\)</li>\n</ul>\n<p>对应的，一些常见的concave function：</p>\n<ul>\n<li>线性函数：\\(f(x) = ax+b\\)</li>\n<li>对数函数： \\(f(x)=logx  \\quad on \\quad \\Bbb R_{++}\\)</li>\n</ul>\n<p>其中大家可以看到，线性函数既是convex也是concave函数，比较特殊，这和它本身的first order condition为常数有关。</p>\n<p>以上就是convex function的一个简单介绍，你也许会问，为什么花这么多力气来介绍convex function. 其实，在machine learning中，convex function的优化是非常重要的，很多算法说到底，都是要optimize一个convex function，我们会用liner regression和logistic regression为例子，进一步从convex function的简介过渡到gradient descent和newton’s method.</p>\n<h2 id=\"梯度下降法-Gradient-descent\"><a href=\"#梯度下降法-Gradient-descent\" class=\"headerlink\" title=\"梯度下降法(Gradient descent)\"></a>梯度下降法(Gradient descent)</h2><p>关于gradient descent，我们使用liner regression作为例子来讨论。Liner regression算法的实质是least square method，他的cost function是<br>$$J( \\theta)= \\frac{1}{2} \\sum_{i=1} ^m (h_ \\theta (x ^{(i)})-y^{(i)})^2, \\quad h_ \\theta (x)= \\theta ^Tx $$<br>对于liner regression来说，算法的实质就是去求出\\( J( \\theta) \\) <strong>以\\( \\theta\\)为参数</strong>的minimum，gradient descent算法的作用就是去实现了这个过程，gradient descent的基础知识详见reference. </p>\n<p>那么针对cost function，gradient descent是如何保证收敛的呢，我们一起来看看</p>\n<p>对于\\(J( \\theta)\\)，我们将其带入convex function的定义公式中，注意这里我们的自变量是\\( \\theta\\)，我们可以通过推导证明该式成立，也就是说，least square cost function是convex function.</p>\n<p>既然有这个结论了，那么我们可以想象一下，least square cost function作为convex function，是存在全局最小值的，也就是说，gradient descent不会出现陷入局部最优无法自拔的现象，只要gradient descent保证参数足够好的情况下，理论上，是完全可以很好的逼近全局最优的解的。</p>\n<blockquote>\n<p>Gradien descent算法本身并不能保证获得全局最小值，只有在objective function是convex function的时候才可以保证</p>\n</blockquote>\n<p>下图可以看出，右边的object function是non-convex function，因而很容易陷入到局部最小值无法自拔，而左边的objective function是一个标准的convex function，在gradient descent参数合理的前提下，可以逼近全局最优。</p>\n<p><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/1/1-3.png\" alt=\"\"></p>\n<p>当然，gradien descent的一些改进方法，例如stochastic gradient descent在解决non-convex optimization上有一些帮助，但是我们在这里不做讨论，后面有时间我会专门再写。</p>\n<p>由此，我们可以得出，gradient descent不仅仅是minimize liner regression的一个很好的方法，也是convex optimization的一种理想方法</p>\n<h2 id=\"牛顿法-Newton’s-method\"><a href=\"#牛顿法-Newton’s-method\" class=\"headerlink\" title=\"牛顿法(Newton’s method)\"></a>牛顿法(Newton’s method)</h2><p>Newton’s method 这块内容，我们将会用logistic regression作为例子。同样，我们先来关注下log cost function，这里，<strong>我们取label为-1和+1</strong>，因为这样得到的cost function比0,1下的计算更加简单<br>$$J( \\omega)= - \\frac{1}{m} \\sum_{i=1} ^{m} log(1+e^{-y^{(i)} \\omega^T x^{(i)}})$$<br>这里我们采用了-1和+1作为标签值，和大多数教材中不一样，大家可以下来自己推导一下\\(J( \\omega)\\)，这种写法广泛的应用在了比较logistic regression和SVM两大分类器的文献中，希望大家熟知。</p>\n<p>此处我们对原始的likehood function加上了 \\(- \\frac{1}{m}\\)的系数，同样，当我们把 \\(J( \\omega)\\)带入到convex function的定义中，可以验证上式为convex function，值得注意的是，\\(J( \\omega)\\)是\\( \\omega\\)的函数。</p>\n<p>其实，我们也可以将log cost function展开后，利用最基本的函数convex和concave性质来获得上式是convex function的结论，碍于公式实在太难打，就留给大家去证明吧。</p>\n<p>OK，既然log cost function是convex function，我们一定是可以用gradient descent去求解的。问题是，如果我们用newton’s method呢？</p>\n<p>Newton’s method的基本原理详见reference，这里我们可以发现，既然log cost function是convex function，那么根据second order condition可以知道，它的Hessian matrix一定是positive semi-definite的。如果我们加上了L2 regularizer，<strong>由于L2 regularizer本身就是一个strict convex function</strong>，那么log cost function就一定是strict convex function了，也就是：<br>$$J( \\omega)= - \\frac{1}{m} \\sum_{i=1} ^{m} log(1+e^{-y^{(i)} \\omega^T x^{(i)}})+ \\frac{1}{2}|| \\omega||^2$$<br>因此，在log cost function中，<strong>Hessian matrix是positive definite的</strong>，完全满足newton’s method 的要求。同样，类似于上一部分，newton’s method也可以找到log cost function的全局最优。</p>\n<h2 id=\"Sum-up\"><a href=\"#Sum-up\" class=\"headerlink\" title=\"Sum up\"></a>Sum up</h2><p>OK，我们说到这里也确实讲了不少，这篇blog有些冗长，希望朋友们不要焦虑。总体来说，我想表达的是以下几个观点：</p>\n<ul>\n<li>Machine learning中我们寻求的其实就是objective function一个全局最优值，这些问题是通过gradient descent等方法解决的；</li>\n<li>Gradient descent和newton’s method都是convex optimization的好方法，他们都可以对于convex function获得全局最优；</li>\n<li>对于non-convex optimization问题，stochastic gradient descent也很有效果，我们后续再慢慢学习。</li>\n</ul>\n<p>好了，核心思想就这三点，今天先说这么多！</p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ul>\n<li><a href=\"https://see.stanford.edu/materials/lsocoee364a/03ConvexFunctions.pdf\" target=\"_blank\" rel=\"noopener\">EE364, Convex Optimization Stanford University</a></li>\n<li><a href=\"http://qwone.com/~jason/writing/convexLR.pdf\" target=\"_blank\" rel=\"noopener\">Regularized Logistic Regression is Strictly Convex</a></li>\n<li><a href=\"https://www.yangzhou301.com/2016/03/14/826442654/\" target=\"_blank\" rel=\"noopener\">XinyiLI大神的blog</a></li>\n<li><a href=\"https://en.wikipedia.org/wiki/Linear_regression\" target=\"_blank\" rel=\"noopener\">Liner regression</a></li>\n<li><a href=\"https://en.wikipedia.org/wiki/Logistic_regression\" target=\"_blank\" rel=\"noopener\">Logsitc regression</a></li>\n<li><a href=\"https://en.wikipedia.org/wiki/Gradient_descent\" target=\"_blank\" rel=\"noopener\">Gradient descent</a></li>\n<li><a href=\"https://en.wikipedia.org/wiki/Newton%27s_method\" target=\"_blank\" rel=\"noopener\">Newton’s method</a></li>\n</ul>\n","site":{"data":{}},"_categories":[{"name":"machine learning","path":"categories/machine-learning/"}],"_tags":[{"name":"gradient descent","path":"tags/gradient-descent/"},{"name":"newton's method","path":"tags/newton-s-method/"},{"name":"convex optimization","path":"tags/convex-optimization/"}],"excerpt":"<p>记得我在和优男一起研究logistic regression的时候，他问了我几个非常尖锐的问题，让我顿时哑口无言</p>\n<ul>\n<li>怎么保证logistic regression通过gradient descent找到的是最优解；</li>\n<li>为什么logistic regression可以用newton’s method呢？</li>\n<li>Newton’s method中Hessian matrix必须positive definite有什么意义呢，log cost function能保证吗？</li>\n</ul>\n<p>这些细节问题，说实话我也没有认真的想过。在夸奖他之余，我们也一起开始了研究，希望从中学习到一些更深层的东西，趁着现在有个blog分享给大家<br></p>","more":"</p>\n<h2 id=\"凸函数-Convex-function\"><a href=\"#凸函数-Convex-function\" class=\"headerlink\" title=\"凸函数(Convex function)\"></a>凸函数(Convex function)</h2><p>在开始之前，我有一个关于术语的倡议。中文里的“凸函数”，看上去是凹下去的，对应的，中文里的“凹函数”看上去凸起来的，amazing吧？这是有一定历史原因的，感兴趣的朋友可以去查阅下资料，这里我们不再复述。所以为了避免让大家产生误解，我鼓励大家使用英文，<strong>convex function</strong>和<strong>concave function</strong>.这样会避免很多不必要的麻烦。</p>\n<p>OK，我们来一起看看，convex function</p>\n<p>对于一维函数 \\(f(x)\\)来说，在定义域内的任意值 \\(a\\)和\\(b\\)，对于任意的 \\( 0 \\leq \\theta \\leq 1\\)，如果满足以下条件，则称为convex function<br>$$f(\\theta a+(1-\\theta) b) \\leq \\theta f(a) + (1- \\theta)f(b)$$<br>我们再用图片直观的感受一下</p>\n<p><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/1/1-1.png\" alt=\"\"><br>显而易见的是，当公式中等号去掉的时候，函数就是<strong>strictly convex function</strong>.</p>\n<p>Convex function具有一定的性质，我们简单的描述一下。</p>\n<h3 id=\"First-order-condition\"><a href=\"#First-order-condition\" class=\"headerlink\" title=\"First order condition\"></a>First order condition</h3><p>对于 function \\(f\\)，在定义域内<strong>一阶可导</strong>，且导数为<br>$$     \\nabla f=( \\frac{\\partial f(x)}{x_1}, \\frac{\\partial f(x)}{x_2},…,  \\frac{\\partial f(x)}{x_n})$$<br>那么 \\(f\\)是convex function的<strong>充要条件</strong>是：对于定义域内任意 \\(x\\) 和 \\(y\\)<br>$$ f(y) \\geq f(x) + \\nabla f(x)^T (y - x)$$<br>OK，再来张图片直观感受一下：</p>\n<p><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/1/1-2.png\" alt=\"\"></p>\n<p>其实简单的来讲，就是对于convex function \\(f\\)，它的函数值永远大于等于切线上的值！</p>\n<h3 id=\"Second-order-condition\"><a href=\"#Second-order-condition\" class=\"headerlink\" title=\"Second order condition\"></a>Second order condition</h3><p>对于 function \\(f\\)，在定义域内<strong>二阶可导</strong>，且 \\(n\\) 维方阵Hessian matrix的元素为<br>$$ \\nabla ^2 f(x) = \\frac{ \\partial ^2 f(x)}{ \\partial x_i \\partial x_j}, \\quad i,j = 1,2,…,n$$<br>当且仅当Hessian matrix positive semi-defnite的时候，\\(f\\) 是convex function。以上互为<strong>充要条件</strong>。这里的证明我不想展开讲，在后面我会给出reference链接。</p>\n<hr>\n<p>下面给出一些 \\(\\Bbb R\\) 空间下常见的convex function：</p>\n<ul>\n<li>线性函数：\\(f(x) = ax+b\\)</li>\n<li>指数函数：\\(f(x)=e^ {ax}\\)</li>\n<li>负熵函数： \\(f(x)=xlogx \\quad on \\quad \\Bbb R_{++}\\)</li>\n</ul>\n<p>对应的，一些常见的concave function：</p>\n<ul>\n<li>线性函数：\\(f(x) = ax+b\\)</li>\n<li>对数函数： \\(f(x)=logx  \\quad on \\quad \\Bbb R_{++}\\)</li>\n</ul>\n<p>其中大家可以看到，线性函数既是convex也是concave函数，比较特殊，这和它本身的first order condition为常数有关。</p>\n<p>以上就是convex function的一个简单介绍，你也许会问，为什么花这么多力气来介绍convex function. 其实，在machine learning中，convex function的优化是非常重要的，很多算法说到底，都是要optimize一个convex function，我们会用liner regression和logistic regression为例子，进一步从convex function的简介过渡到gradient descent和newton’s method.</p>\n<h2 id=\"梯度下降法-Gradient-descent\"><a href=\"#梯度下降法-Gradient-descent\" class=\"headerlink\" title=\"梯度下降法(Gradient descent)\"></a>梯度下降法(Gradient descent)</h2><p>关于gradient descent，我们使用liner regression作为例子来讨论。Liner regression算法的实质是least square method，他的cost function是<br>$$J( \\theta)= \\frac{1}{2} \\sum_{i=1} ^m (h_ \\theta (x ^{(i)})-y^{(i)})^2, \\quad h_ \\theta (x)= \\theta ^Tx $$<br>对于liner regression来说，算法的实质就是去求出\\( J( \\theta) \\) <strong>以\\( \\theta\\)为参数</strong>的minimum，gradient descent算法的作用就是去实现了这个过程，gradient descent的基础知识详见reference. </p>\n<p>那么针对cost function，gradient descent是如何保证收敛的呢，我们一起来看看</p>\n<p>对于\\(J( \\theta)\\)，我们将其带入convex function的定义公式中，注意这里我们的自变量是\\( \\theta\\)，我们可以通过推导证明该式成立，也就是说，least square cost function是convex function.</p>\n<p>既然有这个结论了，那么我们可以想象一下，least square cost function作为convex function，是存在全局最小值的，也就是说，gradient descent不会出现陷入局部最优无法自拔的现象，只要gradient descent保证参数足够好的情况下，理论上，是完全可以很好的逼近全局最优的解的。</p>\n<blockquote>\n<p>Gradien descent算法本身并不能保证获得全局最小值，只有在objective function是convex function的时候才可以保证</p>\n</blockquote>\n<p>下图可以看出，右边的object function是non-convex function，因而很容易陷入到局部最小值无法自拔，而左边的objective function是一个标准的convex function，在gradient descent参数合理的前提下，可以逼近全局最优。</p>\n<p><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/1/1-3.png\" alt=\"\"></p>\n<p>当然，gradien descent的一些改进方法，例如stochastic gradient descent在解决non-convex optimization上有一些帮助，但是我们在这里不做讨论，后面有时间我会专门再写。</p>\n<p>由此，我们可以得出，gradient descent不仅仅是minimize liner regression的一个很好的方法，也是convex optimization的一种理想方法</p>\n<h2 id=\"牛顿法-Newton’s-method\"><a href=\"#牛顿法-Newton’s-method\" class=\"headerlink\" title=\"牛顿法(Newton’s method)\"></a>牛顿法(Newton’s method)</h2><p>Newton’s method 这块内容，我们将会用logistic regression作为例子。同样，我们先来关注下log cost function，这里，<strong>我们取label为-1和+1</strong>，因为这样得到的cost function比0,1下的计算更加简单<br>$$J( \\omega)= - \\frac{1}{m} \\sum_{i=1} ^{m} log(1+e^{-y^{(i)} \\omega^T x^{(i)}})$$<br>这里我们采用了-1和+1作为标签值，和大多数教材中不一样，大家可以下来自己推导一下\\(J( \\omega)\\)，这种写法广泛的应用在了比较logistic regression和SVM两大分类器的文献中，希望大家熟知。</p>\n<p>此处我们对原始的likehood function加上了 \\(- \\frac{1}{m}\\)的系数，同样，当我们把 \\(J( \\omega)\\)带入到convex function的定义中，可以验证上式为convex function，值得注意的是，\\(J( \\omega)\\)是\\( \\omega\\)的函数。</p>\n<p>其实，我们也可以将log cost function展开后，利用最基本的函数convex和concave性质来获得上式是convex function的结论，碍于公式实在太难打，就留给大家去证明吧。</p>\n<p>OK，既然log cost function是convex function，我们一定是可以用gradient descent去求解的。问题是，如果我们用newton’s method呢？</p>\n<p>Newton’s method的基本原理详见reference，这里我们可以发现，既然log cost function是convex function，那么根据second order condition可以知道，它的Hessian matrix一定是positive semi-definite的。如果我们加上了L2 regularizer，<strong>由于L2 regularizer本身就是一个strict convex function</strong>，那么log cost function就一定是strict convex function了，也就是：<br>$$J( \\omega)= - \\frac{1}{m} \\sum_{i=1} ^{m} log(1+e^{-y^{(i)} \\omega^T x^{(i)}})+ \\frac{1}{2}|| \\omega||^2$$<br>因此，在log cost function中，<strong>Hessian matrix是positive definite的</strong>，完全满足newton’s method 的要求。同样，类似于上一部分，newton’s method也可以找到log cost function的全局最优。</p>\n<h2 id=\"Sum-up\"><a href=\"#Sum-up\" class=\"headerlink\" title=\"Sum up\"></a>Sum up</h2><p>OK，我们说到这里也确实讲了不少，这篇blog有些冗长，希望朋友们不要焦虑。总体来说，我想表达的是以下几个观点：</p>\n<ul>\n<li>Machine learning中我们寻求的其实就是objective function一个全局最优值，这些问题是通过gradient descent等方法解决的；</li>\n<li>Gradient descent和newton’s method都是convex optimization的好方法，他们都可以对于convex function获得全局最优；</li>\n<li>对于non-convex optimization问题，stochastic gradient descent也很有效果，我们后续再慢慢学习。</li>\n</ul>\n<p>好了，核心思想就这三点，今天先说这么多！</p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ul>\n<li><a href=\"https://see.stanford.edu/materials/lsocoee364a/03ConvexFunctions.pdf\" target=\"_blank\" rel=\"noopener\">EE364, Convex Optimization Stanford University</a></li>\n<li><a href=\"http://qwone.com/~jason/writing/convexLR.pdf\" target=\"_blank\" rel=\"noopener\">Regularized Logistic Regression is Strictly Convex</a></li>\n<li><a href=\"https://www.yangzhou301.com/2016/03/14/826442654/\" target=\"_blank\" rel=\"noopener\">XinyiLI大神的blog</a></li>\n<li><a href=\"https://en.wikipedia.org/wiki/Linear_regression\" target=\"_blank\" rel=\"noopener\">Liner regression</a></li>\n<li><a href=\"https://en.wikipedia.org/wiki/Logistic_regression\" target=\"_blank\" rel=\"noopener\">Logsitc regression</a></li>\n<li><a href=\"https://en.wikipedia.org/wiki/Gradient_descent\" target=\"_blank\" rel=\"noopener\">Gradient descent</a></li>\n<li><a href=\"https://en.wikipedia.org/wiki/Newton%27s_method\" target=\"_blank\" rel=\"noopener\">Newton’s method</a></li>\n</ul>"},{"title":"深入聊聊正则化","date":"2017-08-26T17:55:07.000Z","_content":"最近和优男一起聊到了L1和L2 regularization，期间遇到了很多没有想明白的问题，加上最近工作有些忙，空余时间用来倒腾新到货的小米路由器，只能趁周末自己研究研究，下面和大家分享一下regularization中一些深入的问题。不讨论基础知识，直接上干货。\n<!--more-->\n## MAP and regularization\n我们都知道，当cost function在没有加regularization的时候，我们对参数使用的是**MLE**(Maximum likelihood estimation)，对应频率学派所认为的参数本无分布规律的观点；在Andrew Ng经典的CS229中，这位AI大师曾经提到，regularization其实是对参数的**MAP**(Maximum a posteriori estimation)，是基于贝叶斯学派认为的参数本有**priori distribution**，同时吸纳了MLE的一种中间观点。\n\n这里的priori distribution，就是根据经验，认为参数应该大致符合某个distribution，这样，最终获得的参数估计结果也会和这个被认为的distribution有一些相近\n\n而我们所熟知的L1 regularization，其实就是认为参数的priori distribution是**Laplacian distribution**，而L2 regularization，则认为参数的priorit distribution是**Gaussian distribution**，相信大家对Gaussian distribution是很熟悉的，而对于Laplacian distribution，它的分布是\n$$p(x;a)= \\frac{a}{2} e^{-a|x|}$$\n下图就是两者的一个比较：\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/4/4-1.png)\n在后文中，我们会看到，两种distribution的特点决定了两种regularization的性质。\n### Lasso regression\n在liner regression中，我们假设参数\\\\( \\theta\\\\)服从Laplacian distribution，cost function就成了\n$$J( \\theta) = \\frac{1}{2} \\sum^{m}_{i=1} (y^{(i)}- \\theta^T x^{(i)})+ \\lambda \\sum^{d}_{j=1}| \\theta^{(i)}|$$\n上式就是Lasso regression\n### Ridge regression\n在liner regression中，我们假设参数\\\\( \\theta\\\\)服从Gaussian distribution，cost function就成了\n$$J( \\theta) = \\frac{1}{2} \\sum^{m}_{i=1} (y^{(i)}- \\theta^T x^{(i)})+ \\lambda \\sum^{d}_{j=1} ( \\theta^{(i)})^2$$\n上式就是Ridge regression或shrinkage\n## geometry of error surfaces\n在不考虑参数priori distribution的时候，cost function的形式是\n$$J( \\theta) = \\frac{1}{2} \\sum^{m}_{i=1} (y^{(i)}- \\theta^T x^{(i)})^2$$\n用二维截面图展示就是\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/4/4-2.png)\n图中只有objective function，横纵轴是参数\\\\( \\theta\\\\)，截取过来的图，所以上面的参数是\\\\(w\\\\)，\\\\(l\\\\)是loss值，箭头指向的点就是cost function的极小点。在不考虑参数priori distribution的时候，这个点就是我们的optimization target.\n\n下面大家来我一起做一个头脑风暴，所谓参数的priori distribution，其实就是用来限制最后optimization结果的一个限定，那么我们其实就是在做一个受限制的的convex optimization，即：\n$$ \\theta=argmin \\frac{1}{2} \\sum^{m}_{i=1} (y^{(i)}- \\theta^T x^{(i)})^2$$\n$$ s.t. \\sum^{d}_{j=1}| \\theta^{(i)}|^p \\geq \\beta$$\n其中，\\\\( \\beta\\\\)是ridge或者lasso的最小值。\n那么此时的图就变成了：\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/4/4-3.png)\n我们从图中可以看到，在加入了限制后，最终的optimization不是落在极小值点，而是落在图中所示的位置。从另一个角度来想，regularization item的加入，使得整个cost function在寻找最小值的时候，要均衡的考虑objective function和regularization item的大小.\n\n在这个地方，我和优男讨论的时候有一个地方没有想通，例如使用gradient descent进行optimzation的时候，怎么保证优化可以落到图中的点呢，我是这么考虑的：当加入regularization后，cost function本身就有了变化，随之而来的是gradient也发生了变化，在gradient descent迭代过程中就已经把regularization的影响带了进去，因此在每一次迭代的时候，实际上应该都是按照上式的限制进行优化的。\n\n当然，上图也可以用来就是为什么lasso可以获得稀疏特征，那就是因为lasso更可能在坐标轴上和objective function产生交点，进而使得一些特征变成0.\n\n## Reference\n* [CS 195-5: Machine Learning](https://pdfs.semanticscholar.org/91a9/5626d24c8393e3b784e44f62de201d20dede.pdf)\n* [STAT 897D](https://onlinecourses.science.psu.edu/stat857/node/155)\n","source":"_posts/ml-ridge-lasso.md","raw":"---\ntitle: 深入聊聊正则化\ndate: 2017-08-27 01:55:07\ntags: \n\t- regularization\n\t- MAP\n\t- ridge regression\n\t- lasso regression\ncategories: machine learning\n---\n最近和优男一起聊到了L1和L2 regularization，期间遇到了很多没有想明白的问题，加上最近工作有些忙，空余时间用来倒腾新到货的小米路由器，只能趁周末自己研究研究，下面和大家分享一下regularization中一些深入的问题。不讨论基础知识，直接上干货。\n<!--more-->\n## MAP and regularization\n我们都知道，当cost function在没有加regularization的时候，我们对参数使用的是**MLE**(Maximum likelihood estimation)，对应频率学派所认为的参数本无分布规律的观点；在Andrew Ng经典的CS229中，这位AI大师曾经提到，regularization其实是对参数的**MAP**(Maximum a posteriori estimation)，是基于贝叶斯学派认为的参数本有**priori distribution**，同时吸纳了MLE的一种中间观点。\n\n这里的priori distribution，就是根据经验，认为参数应该大致符合某个distribution，这样，最终获得的参数估计结果也会和这个被认为的distribution有一些相近\n\n而我们所熟知的L1 regularization，其实就是认为参数的priori distribution是**Laplacian distribution**，而L2 regularization，则认为参数的priorit distribution是**Gaussian distribution**，相信大家对Gaussian distribution是很熟悉的，而对于Laplacian distribution，它的分布是\n$$p(x;a)= \\frac{a}{2} e^{-a|x|}$$\n下图就是两者的一个比较：\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/4/4-1.png)\n在后文中，我们会看到，两种distribution的特点决定了两种regularization的性质。\n### Lasso regression\n在liner regression中，我们假设参数\\\\( \\theta\\\\)服从Laplacian distribution，cost function就成了\n$$J( \\theta) = \\frac{1}{2} \\sum^{m}_{i=1} (y^{(i)}- \\theta^T x^{(i)})+ \\lambda \\sum^{d}_{j=1}| \\theta^{(i)}|$$\n上式就是Lasso regression\n### Ridge regression\n在liner regression中，我们假设参数\\\\( \\theta\\\\)服从Gaussian distribution，cost function就成了\n$$J( \\theta) = \\frac{1}{2} \\sum^{m}_{i=1} (y^{(i)}- \\theta^T x^{(i)})+ \\lambda \\sum^{d}_{j=1} ( \\theta^{(i)})^2$$\n上式就是Ridge regression或shrinkage\n## geometry of error surfaces\n在不考虑参数priori distribution的时候，cost function的形式是\n$$J( \\theta) = \\frac{1}{2} \\sum^{m}_{i=1} (y^{(i)}- \\theta^T x^{(i)})^2$$\n用二维截面图展示就是\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/4/4-2.png)\n图中只有objective function，横纵轴是参数\\\\( \\theta\\\\)，截取过来的图，所以上面的参数是\\\\(w\\\\)，\\\\(l\\\\)是loss值，箭头指向的点就是cost function的极小点。在不考虑参数priori distribution的时候，这个点就是我们的optimization target.\n\n下面大家来我一起做一个头脑风暴，所谓参数的priori distribution，其实就是用来限制最后optimization结果的一个限定，那么我们其实就是在做一个受限制的的convex optimization，即：\n$$ \\theta=argmin \\frac{1}{2} \\sum^{m}_{i=1} (y^{(i)}- \\theta^T x^{(i)})^2$$\n$$ s.t. \\sum^{d}_{j=1}| \\theta^{(i)}|^p \\geq \\beta$$\n其中，\\\\( \\beta\\\\)是ridge或者lasso的最小值。\n那么此时的图就变成了：\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/4/4-3.png)\n我们从图中可以看到，在加入了限制后，最终的optimization不是落在极小值点，而是落在图中所示的位置。从另一个角度来想，regularization item的加入，使得整个cost function在寻找最小值的时候，要均衡的考虑objective function和regularization item的大小.\n\n在这个地方，我和优男讨论的时候有一个地方没有想通，例如使用gradient descent进行optimzation的时候，怎么保证优化可以落到图中的点呢，我是这么考虑的：当加入regularization后，cost function本身就有了变化，随之而来的是gradient也发生了变化，在gradient descent迭代过程中就已经把regularization的影响带了进去，因此在每一次迭代的时候，实际上应该都是按照上式的限制进行优化的。\n\n当然，上图也可以用来就是为什么lasso可以获得稀疏特征，那就是因为lasso更可能在坐标轴上和objective function产生交点，进而使得一些特征变成0.\n\n## Reference\n* [CS 195-5: Machine Learning](https://pdfs.semanticscholar.org/91a9/5626d24c8393e3b784e44f62de201d20dede.pdf)\n* [STAT 897D](https://onlinecourses.science.psu.edu/stat857/node/155)\n","slug":"ml-ridge-lasso","published":1,"updated":"2020-05-10T06:50:12.530Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cka0wnkwi0011qxothcjkqnb7","content":"<p>最近和优男一起聊到了L1和L2 regularization，期间遇到了很多没有想明白的问题，加上最近工作有些忙，空余时间用来倒腾新到货的小米路由器，只能趁周末自己研究研究，下面和大家分享一下regularization中一些深入的问题。不讨论基础知识，直接上干货。<br><a id=\"more\"></a></p>\n<h2 id=\"MAP-and-regularization\"><a href=\"#MAP-and-regularization\" class=\"headerlink\" title=\"MAP and regularization\"></a>MAP and regularization</h2><p>我们都知道，当cost function在没有加regularization的时候，我们对参数使用的是<strong>MLE</strong>(Maximum likelihood estimation)，对应频率学派所认为的参数本无分布规律的观点；在Andrew Ng经典的CS229中，这位AI大师曾经提到，regularization其实是对参数的<strong>MAP</strong>(Maximum a posteriori estimation)，是基于贝叶斯学派认为的参数本有<strong>priori distribution</strong>，同时吸纳了MLE的一种中间观点。</p>\n<p>这里的priori distribution，就是根据经验，认为参数应该大致符合某个distribution，这样，最终获得的参数估计结果也会和这个被认为的distribution有一些相近</p>\n<p>而我们所熟知的L1 regularization，其实就是认为参数的priori distribution是<strong>Laplacian distribution</strong>，而L2 regularization，则认为参数的priorit distribution是<strong>Gaussian distribution</strong>，相信大家对Gaussian distribution是很熟悉的，而对于Laplacian distribution，它的分布是<br>$$p(x;a)= \\frac{a}{2} e^{-a|x|}$$<br>下图就是两者的一个比较：<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/4/4-1.png\" alt=\"\"><br>在后文中，我们会看到，两种distribution的特点决定了两种regularization的性质。</p>\n<h3 id=\"Lasso-regression\"><a href=\"#Lasso-regression\" class=\"headerlink\" title=\"Lasso regression\"></a>Lasso regression</h3><p>在liner regression中，我们假设参数\\( \\theta\\)服从Laplacian distribution，cost function就成了<br>$$J( \\theta) = \\frac{1}{2} \\sum^{m}<em>{i=1} (y^{(i)}- \\theta^T x^{(i)})+ \\lambda \\sum^{d}</em>{j=1}| \\theta^{(i)}|$$<br>上式就是Lasso regression</p>\n<h3 id=\"Ridge-regression\"><a href=\"#Ridge-regression\" class=\"headerlink\" title=\"Ridge regression\"></a>Ridge regression</h3><p>在liner regression中，我们假设参数\\( \\theta\\)服从Gaussian distribution，cost function就成了<br>$$J( \\theta) = \\frac{1}{2} \\sum^{m}<em>{i=1} (y^{(i)}- \\theta^T x^{(i)})+ \\lambda \\sum^{d}</em>{j=1} ( \\theta^{(i)})^2$$<br>上式就是Ridge regression或shrinkage</p>\n<h2 id=\"geometry-of-error-surfaces\"><a href=\"#geometry-of-error-surfaces\" class=\"headerlink\" title=\"geometry of error surfaces\"></a>geometry of error surfaces</h2><p>在不考虑参数priori distribution的时候，cost function的形式是<br>$$J( \\theta) = \\frac{1}{2} \\sum^{m}_{i=1} (y^{(i)}- \\theta^T x^{(i)})^2$$<br>用二维截面图展示就是<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/4/4-2.png\" alt=\"\"><br>图中只有objective function，横纵轴是参数\\( \\theta\\)，截取过来的图，所以上面的参数是\\(w\\)，\\(l\\)是loss值，箭头指向的点就是cost function的极小点。在不考虑参数priori distribution的时候，这个点就是我们的optimization target.</p>\n<p>下面大家来我一起做一个头脑风暴，所谓参数的priori distribution，其实就是用来限制最后optimization结果的一个限定，那么我们其实就是在做一个受限制的的convex optimization，即：<br>$$ \\theta=argmin \\frac{1}{2} \\sum^{m}<em>{i=1} (y^{(i)}- \\theta^T x^{(i)})^2$$<br>$$ s.t. \\sum^{d}</em>{j=1}| \\theta^{(i)}|^p \\geq \\beta$$<br>其中，\\( \\beta\\)是ridge或者lasso的最小值。<br>那么此时的图就变成了：<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/4/4-3.png\" alt=\"\"><br>我们从图中可以看到，在加入了限制后，最终的optimization不是落在极小值点，而是落在图中所示的位置。从另一个角度来想，regularization item的加入，使得整个cost function在寻找最小值的时候，要均衡的考虑objective function和regularization item的大小.</p>\n<p>在这个地方，我和优男讨论的时候有一个地方没有想通，例如使用gradient descent进行optimzation的时候，怎么保证优化可以落到图中的点呢，我是这么考虑的：当加入regularization后，cost function本身就有了变化，随之而来的是gradient也发生了变化，在gradient descent迭代过程中就已经把regularization的影响带了进去，因此在每一次迭代的时候，实际上应该都是按照上式的限制进行优化的。</p>\n<p>当然，上图也可以用来就是为什么lasso可以获得稀疏特征，那就是因为lasso更可能在坐标轴上和objective function产生交点，进而使得一些特征变成0.</p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ul>\n<li><a href=\"https://pdfs.semanticscholar.org/91a9/5626d24c8393e3b784e44f62de201d20dede.pdf\" target=\"_blank\" rel=\"noopener\">CS 195-5: Machine Learning</a></li>\n<li><a href=\"https://onlinecourses.science.psu.edu/stat857/node/155\" target=\"_blank\" rel=\"noopener\">STAT 897D</a></li>\n</ul>\n","site":{"data":{}},"_categories":[{"name":"machine learning","path":"categories/machine-learning/"}],"_tags":[{"name":"regularization","path":"tags/regularization/"},{"name":"MAP","path":"tags/MAP/"},{"name":"ridge regression","path":"tags/ridge-regression/"},{"name":"lasso regression","path":"tags/lasso-regression/"}],"excerpt":"<p>最近和优男一起聊到了L1和L2 regularization，期间遇到了很多没有想明白的问题，加上最近工作有些忙，空余时间用来倒腾新到货的小米路由器，只能趁周末自己研究研究，下面和大家分享一下regularization中一些深入的问题。不讨论基础知识，直接上干货。<br></p>","more":"</p>\n<h2 id=\"MAP-and-regularization\"><a href=\"#MAP-and-regularization\" class=\"headerlink\" title=\"MAP and regularization\"></a>MAP and regularization</h2><p>我们都知道，当cost function在没有加regularization的时候，我们对参数使用的是<strong>MLE</strong>(Maximum likelihood estimation)，对应频率学派所认为的参数本无分布规律的观点；在Andrew Ng经典的CS229中，这位AI大师曾经提到，regularization其实是对参数的<strong>MAP</strong>(Maximum a posteriori estimation)，是基于贝叶斯学派认为的参数本有<strong>priori distribution</strong>，同时吸纳了MLE的一种中间观点。</p>\n<p>这里的priori distribution，就是根据经验，认为参数应该大致符合某个distribution，这样，最终获得的参数估计结果也会和这个被认为的distribution有一些相近</p>\n<p>而我们所熟知的L1 regularization，其实就是认为参数的priori distribution是<strong>Laplacian distribution</strong>，而L2 regularization，则认为参数的priorit distribution是<strong>Gaussian distribution</strong>，相信大家对Gaussian distribution是很熟悉的，而对于Laplacian distribution，它的分布是<br>$$p(x;a)= \\frac{a}{2} e^{-a|x|}$$<br>下图就是两者的一个比较：<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/4/4-1.png\" alt=\"\"><br>在后文中，我们会看到，两种distribution的特点决定了两种regularization的性质。</p>\n<h3 id=\"Lasso-regression\"><a href=\"#Lasso-regression\" class=\"headerlink\" title=\"Lasso regression\"></a>Lasso regression</h3><p>在liner regression中，我们假设参数\\( \\theta\\)服从Laplacian distribution，cost function就成了<br>$$J( \\theta) = \\frac{1}{2} \\sum^{m}<em>{i=1} (y^{(i)}- \\theta^T x^{(i)})+ \\lambda \\sum^{d}</em>{j=1}| \\theta^{(i)}|$$<br>上式就是Lasso regression</p>\n<h3 id=\"Ridge-regression\"><a href=\"#Ridge-regression\" class=\"headerlink\" title=\"Ridge regression\"></a>Ridge regression</h3><p>在liner regression中，我们假设参数\\( \\theta\\)服从Gaussian distribution，cost function就成了<br>$$J( \\theta) = \\frac{1}{2} \\sum^{m}<em>{i=1} (y^{(i)}- \\theta^T x^{(i)})+ \\lambda \\sum^{d}</em>{j=1} ( \\theta^{(i)})^2$$<br>上式就是Ridge regression或shrinkage</p>\n<h2 id=\"geometry-of-error-surfaces\"><a href=\"#geometry-of-error-surfaces\" class=\"headerlink\" title=\"geometry of error surfaces\"></a>geometry of error surfaces</h2><p>在不考虑参数priori distribution的时候，cost function的形式是<br>$$J( \\theta) = \\frac{1}{2} \\sum^{m}_{i=1} (y^{(i)}- \\theta^T x^{(i)})^2$$<br>用二维截面图展示就是<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/4/4-2.png\" alt=\"\"><br>图中只有objective function，横纵轴是参数\\( \\theta\\)，截取过来的图，所以上面的参数是\\(w\\)，\\(l\\)是loss值，箭头指向的点就是cost function的极小点。在不考虑参数priori distribution的时候，这个点就是我们的optimization target.</p>\n<p>下面大家来我一起做一个头脑风暴，所谓参数的priori distribution，其实就是用来限制最后optimization结果的一个限定，那么我们其实就是在做一个受限制的的convex optimization，即：<br>$$ \\theta=argmin \\frac{1}{2} \\sum^{m}<em>{i=1} (y^{(i)}- \\theta^T x^{(i)})^2$$<br>$$ s.t. \\sum^{d}</em>{j=1}| \\theta^{(i)}|^p \\geq \\beta$$<br>其中，\\( \\beta\\)是ridge或者lasso的最小值。<br>那么此时的图就变成了：<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/4/4-3.png\" alt=\"\"><br>我们从图中可以看到，在加入了限制后，最终的optimization不是落在极小值点，而是落在图中所示的位置。从另一个角度来想，regularization item的加入，使得整个cost function在寻找最小值的时候，要均衡的考虑objective function和regularization item的大小.</p>\n<p>在这个地方，我和优男讨论的时候有一个地方没有想通，例如使用gradient descent进行optimzation的时候，怎么保证优化可以落到图中的点呢，我是这么考虑的：当加入regularization后，cost function本身就有了变化，随之而来的是gradient也发生了变化，在gradient descent迭代过程中就已经把regularization的影响带了进去，因此在每一次迭代的时候，实际上应该都是按照上式的限制进行优化的。</p>\n<p>当然，上图也可以用来就是为什么lasso可以获得稀疏特征，那就是因为lasso更可能在坐标轴上和objective function产生交点，进而使得一些特征变成0.</p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ul>\n<li><a href=\"https://pdfs.semanticscholar.org/91a9/5626d24c8393e3b784e44f62de201d20dede.pdf\" target=\"_blank\" rel=\"noopener\">CS 195-5: Machine Learning</a></li>\n<li><a href=\"https://onlinecourses.science.psu.edu/stat857/node/155\" target=\"_blank\" rel=\"noopener\">STAT 897D</a></li>\n</ul>"},{"title":"Imbalanced data 问题总结方法汇总","date":"2017-11-11T15:01:09.000Z","_content":"Hello，大家好，双十一真的很累，一直在加班，忙里偷闲看了[A systematic study of the class imbalance  problem in convolutional neural networks](https://arxiv.org/pdf/1710.05381.pdf)，感觉paper呈现的研究内容感觉很一般，但是，paper中关于imbalanced data的solution方法倒是写的很不错，也勾起了我对于这一块总结的欲望。之前也写过一篇关于imbalanced data的paper notes，但是对于这一块的具体方法总结还不是很足够，于是用这篇paper为主线好好sum up一计。\n\n我们来一起看看。\n<!--more-->\n## Data level methods\n首先我们来看一看data level methods，这类方法有一个共性，那就是通过改变data的数量来完成对imbalanced data problem的解决。\n### Oversampling\nOversampling可以说是最直观的solution之一，它的核心思想是，对于较少一类别的samples，过此重复采样，以此让两种类别的样本接近平衡。**但是，对于一个sample多次重复训练，很有可能带来overfitting**，因此，简单粗暴的重复采样并不可取。因此，很多改进的版本应运而生：\n#### SMOTE\nSMOTE算法是一种经典的oversampling方法，它的主要思想是对较少数目类别的样本，随机抽取\\\\(m\\\\)个样本，对于随机抽取出的样本，每个样本选取距离最近的\\\\(n\\\\)个样本，在他们的连线上随机选取一个点，作为较少类别的补充样本。假设原样本点为\\\\(x\\\\)，被选中的附近的点为\\\\(x'\\\\)，则新的样本点为：\n$$x_{new}= x + rand(0,1) \\cdot |x-x'|$$\n\n通过这种方式，SMOTE可以对较少类别样本进行扩充，进而实现oversampling，平衡数据分布。\n#### Cluster-base oversampling\nCluster-based方法的最大特点莫过于最开始对数据进行一个聚类分析，数据会变成数个cluster，然后对于每一个cluster在进行数据的oversampling，**同时兼顾类别之间的between-class imbalance，还要考虑到类内部各个cluster的within-class imbalance**.\n> Its idea is to consider not only the between class imbalance (the imbalance occurring between the two classes) but also the with in class imbalance (the imbalance occurring between the sub clusters of each class) and to oversample the data set by rectifying these two types of imbalances simultaneously.\n\n原paper大致叙述了整个流程，首先我们对imbalanced data进行k-means(或者其他算法也可以)聚类，聚成多个cluster之后，我们开始进行oversampling，假设majority class有\\\\(m\\\\)个cluster，minority有\\\\(n\\\\)个cluster，我们以cluster最大的data数目\\\\(k\\\\)为标准，我们先对majority class中所有cluster，都进行oversampling，使得他们的数目都达到\\\\(k\\\\)，随后，对于minority中每个cluster进行oversampling，使得每一个cluster数目变成\\\\(m * k /n\\\\)，最终实现between-class balance和within-class balance.\n\n### Undersampling\n与oversampling相对应的则是undersampling，undersampling的核心思想是对于较多类别的samples抽样，使得两个类别数据趋于相近。但是，随机抽样获得会使得类别丧失很多的信息，甚至导致数据分布发生改变。\n#### One-sided selection\none-sided selection的主要思想是，为了保证数据整体的分布，我们优先去除靠近边界的样本，这样可以保证较多分类的数据分布。\n\n## Classifier level methods\n下面我们来看看通过改变classifier level来解决imbalanced data的方法，这类方法侧重于分类器本身的一些性质而并非两类数据的个数。\n### Thresholding\n我在之前的博客中聊到过，imbalanced data的分类平面会倾向于较少数据的分类一侧，所以我们可以通过改变类别预测的probabilty的threshold来修正分类平面。常用的方法就是加入关于类别数目的prior probability：\n$$y_i(x)=p(i|x)= \\frac{p(i) \\cdot p(x|i)}{p(x)}$$\n### Cost sensitive learning\nThresholding方法其实对已经train好的模型的采取的一种方式。相应的，我们在模型训练的时候就来消除imbalanced data的一些影响，如何做到呢？答案就是cost function.\n\n我们可以通过调整learning rate，加强对cost比较大的samples，并且最终的优化目标从标准的cost function变成misclassification cost，如此就可以解决imbalanced data的问题了\n### One-class classification\n该方法可以说是换了一种思维看问题，我们不再将classification作为我们的task，而是变成了对于一种异常检测的问题。我们只是着眼于较多samples的类别，认为另一类别的samples是一种异常值。\n\n当然，这种方法适合那种极端的imbalanced data，对于一般的情况并不一定很适用。\n\n## Recommendation\nProjection:[Imbalanced-learn](https://github.com/scikit-learn-contrib/imbalanced-learn)\n\n## Reference\n* [Buda, Mateusz, Atsuto Maki, and Maciej A. Mazurowski. \"A systematic study of the class imbalance problem in convolutional neural networks.\" arXiv preprint arXiv:1710.05381 (2017).](https://arxiv.org/pdf/1710.05381.pdf)\n* [Chawla, Nitesh V., et al. \"SMOTE: synthetic minority over-sampling technique.\" Journal of artificial intelligence research 16 (2002): 321-357.](https://www.jair.org/media/953/live-953-2037-jair.pdf)\n* [Jo, Taeho, and Nathalie Japkowicz. \"Class imbalances versus small disjuncts.\" ACM Sigkdd Explorations Newsletter 6.1 (2004): 40-49.](http://sci2s.ugr.es/keel/pdf/specific/articulo/jo.pdf)\n* [Richard, Michael D., and Richard P. Lippmann. \"Neural network classifiers estimate Bayesian a posteriori probabilities.\" Neural computation 3.4 (1991): 461-483.](http://www.ee.iisc.ac.in/new/people/faculty/prasantg/downloads/NeuralNetworksPosteriors_Lippmann1991.pdf)\n","source":"_posts/ml-imbalanced-data-solution.md","raw":"---\ntitle: Imbalanced data 问题总结方法汇总\ndate: 2017-11-11 23:01:09\ntags:\n\t- imbalanced data\ncategories: machine learning\n---\nHello，大家好，双十一真的很累，一直在加班，忙里偷闲看了[A systematic study of the class imbalance  problem in convolutional neural networks](https://arxiv.org/pdf/1710.05381.pdf)，感觉paper呈现的研究内容感觉很一般，但是，paper中关于imbalanced data的solution方法倒是写的很不错，也勾起了我对于这一块总结的欲望。之前也写过一篇关于imbalanced data的paper notes，但是对于这一块的具体方法总结还不是很足够，于是用这篇paper为主线好好sum up一计。\n\n我们来一起看看。\n<!--more-->\n## Data level methods\n首先我们来看一看data level methods，这类方法有一个共性，那就是通过改变data的数量来完成对imbalanced data problem的解决。\n### Oversampling\nOversampling可以说是最直观的solution之一，它的核心思想是，对于较少一类别的samples，过此重复采样，以此让两种类别的样本接近平衡。**但是，对于一个sample多次重复训练，很有可能带来overfitting**，因此，简单粗暴的重复采样并不可取。因此，很多改进的版本应运而生：\n#### SMOTE\nSMOTE算法是一种经典的oversampling方法，它的主要思想是对较少数目类别的样本，随机抽取\\\\(m\\\\)个样本，对于随机抽取出的样本，每个样本选取距离最近的\\\\(n\\\\)个样本，在他们的连线上随机选取一个点，作为较少类别的补充样本。假设原样本点为\\\\(x\\\\)，被选中的附近的点为\\\\(x'\\\\)，则新的样本点为：\n$$x_{new}= x + rand(0,1) \\cdot |x-x'|$$\n\n通过这种方式，SMOTE可以对较少类别样本进行扩充，进而实现oversampling，平衡数据分布。\n#### Cluster-base oversampling\nCluster-based方法的最大特点莫过于最开始对数据进行一个聚类分析，数据会变成数个cluster，然后对于每一个cluster在进行数据的oversampling，**同时兼顾类别之间的between-class imbalance，还要考虑到类内部各个cluster的within-class imbalance**.\n> Its idea is to consider not only the between class imbalance (the imbalance occurring between the two classes) but also the with in class imbalance (the imbalance occurring between the sub clusters of each class) and to oversample the data set by rectifying these two types of imbalances simultaneously.\n\n原paper大致叙述了整个流程，首先我们对imbalanced data进行k-means(或者其他算法也可以)聚类，聚成多个cluster之后，我们开始进行oversampling，假设majority class有\\\\(m\\\\)个cluster，minority有\\\\(n\\\\)个cluster，我们以cluster最大的data数目\\\\(k\\\\)为标准，我们先对majority class中所有cluster，都进行oversampling，使得他们的数目都达到\\\\(k\\\\)，随后，对于minority中每个cluster进行oversampling，使得每一个cluster数目变成\\\\(m * k /n\\\\)，最终实现between-class balance和within-class balance.\n\n### Undersampling\n与oversampling相对应的则是undersampling，undersampling的核心思想是对于较多类别的samples抽样，使得两个类别数据趋于相近。但是，随机抽样获得会使得类别丧失很多的信息，甚至导致数据分布发生改变。\n#### One-sided selection\none-sided selection的主要思想是，为了保证数据整体的分布，我们优先去除靠近边界的样本，这样可以保证较多分类的数据分布。\n\n## Classifier level methods\n下面我们来看看通过改变classifier level来解决imbalanced data的方法，这类方法侧重于分类器本身的一些性质而并非两类数据的个数。\n### Thresholding\n我在之前的博客中聊到过，imbalanced data的分类平面会倾向于较少数据的分类一侧，所以我们可以通过改变类别预测的probabilty的threshold来修正分类平面。常用的方法就是加入关于类别数目的prior probability：\n$$y_i(x)=p(i|x)= \\frac{p(i) \\cdot p(x|i)}{p(x)}$$\n### Cost sensitive learning\nThresholding方法其实对已经train好的模型的采取的一种方式。相应的，我们在模型训练的时候就来消除imbalanced data的一些影响，如何做到呢？答案就是cost function.\n\n我们可以通过调整learning rate，加强对cost比较大的samples，并且最终的优化目标从标准的cost function变成misclassification cost，如此就可以解决imbalanced data的问题了\n### One-class classification\n该方法可以说是换了一种思维看问题，我们不再将classification作为我们的task，而是变成了对于一种异常检测的问题。我们只是着眼于较多samples的类别，认为另一类别的samples是一种异常值。\n\n当然，这种方法适合那种极端的imbalanced data，对于一般的情况并不一定很适用。\n\n## Recommendation\nProjection:[Imbalanced-learn](https://github.com/scikit-learn-contrib/imbalanced-learn)\n\n## Reference\n* [Buda, Mateusz, Atsuto Maki, and Maciej A. Mazurowski. \"A systematic study of the class imbalance problem in convolutional neural networks.\" arXiv preprint arXiv:1710.05381 (2017).](https://arxiv.org/pdf/1710.05381.pdf)\n* [Chawla, Nitesh V., et al. \"SMOTE: synthetic minority over-sampling technique.\" Journal of artificial intelligence research 16 (2002): 321-357.](https://www.jair.org/media/953/live-953-2037-jair.pdf)\n* [Jo, Taeho, and Nathalie Japkowicz. \"Class imbalances versus small disjuncts.\" ACM Sigkdd Explorations Newsletter 6.1 (2004): 40-49.](http://sci2s.ugr.es/keel/pdf/specific/articulo/jo.pdf)\n* [Richard, Michael D., and Richard P. Lippmann. \"Neural network classifiers estimate Bayesian a posteriori probabilities.\" Neural computation 3.4 (1991): 461-483.](http://www.ee.iisc.ac.in/new/people/faculty/prasantg/downloads/NeuralNetworksPosteriors_Lippmann1991.pdf)\n","slug":"ml-imbalanced-data-solution","published":1,"updated":"2020-05-10T06:50:12.530Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cka0wnkwj0014qxotqyqyyki0","content":"<p>Hello，大家好，双十一真的很累，一直在加班，忙里偷闲看了<a href=\"https://arxiv.org/pdf/1710.05381.pdf\" target=\"_blank\" rel=\"noopener\">A systematic study of the class imbalance  problem in convolutional neural networks</a>，感觉paper呈现的研究内容感觉很一般，但是，paper中关于imbalanced data的solution方法倒是写的很不错，也勾起了我对于这一块总结的欲望。之前也写过一篇关于imbalanced data的paper notes，但是对于这一块的具体方法总结还不是很足够，于是用这篇paper为主线好好sum up一计。</p>\n<p>我们来一起看看。<br><a id=\"more\"></a></p>\n<h2 id=\"Data-level-methods\"><a href=\"#Data-level-methods\" class=\"headerlink\" title=\"Data level methods\"></a>Data level methods</h2><p>首先我们来看一看data level methods，这类方法有一个共性，那就是通过改变data的数量来完成对imbalanced data problem的解决。</p>\n<h3 id=\"Oversampling\"><a href=\"#Oversampling\" class=\"headerlink\" title=\"Oversampling\"></a>Oversampling</h3><p>Oversampling可以说是最直观的solution之一，它的核心思想是，对于较少一类别的samples，过此重复采样，以此让两种类别的样本接近平衡。<strong>但是，对于一个sample多次重复训练，很有可能带来overfitting</strong>，因此，简单粗暴的重复采样并不可取。因此，很多改进的版本应运而生：</p>\n<h4 id=\"SMOTE\"><a href=\"#SMOTE\" class=\"headerlink\" title=\"SMOTE\"></a>SMOTE</h4><p>SMOTE算法是一种经典的oversampling方法，它的主要思想是对较少数目类别的样本，随机抽取\\(m\\)个样本，对于随机抽取出的样本，每个样本选取距离最近的\\(n\\)个样本，在他们的连线上随机选取一个点，作为较少类别的补充样本。假设原样本点为\\(x\\)，被选中的附近的点为\\(x’\\)，则新的样本点为：<br>$$x_{new}= x + rand(0,1) \\cdot |x-x’|$$</p>\n<p>通过这种方式，SMOTE可以对较少类别样本进行扩充，进而实现oversampling，平衡数据分布。</p>\n<h4 id=\"Cluster-base-oversampling\"><a href=\"#Cluster-base-oversampling\" class=\"headerlink\" title=\"Cluster-base oversampling\"></a>Cluster-base oversampling</h4><p>Cluster-based方法的最大特点莫过于最开始对数据进行一个聚类分析，数据会变成数个cluster，然后对于每一个cluster在进行数据的oversampling，<strong>同时兼顾类别之间的between-class imbalance，还要考虑到类内部各个cluster的within-class imbalance</strong>.</p>\n<blockquote>\n<p>Its idea is to consider not only the between class imbalance (the imbalance occurring between the two classes) but also the with in class imbalance (the imbalance occurring between the sub clusters of each class) and to oversample the data set by rectifying these two types of imbalances simultaneously.</p>\n</blockquote>\n<p>原paper大致叙述了整个流程，首先我们对imbalanced data进行k-means(或者其他算法也可以)聚类，聚成多个cluster之后，我们开始进行oversampling，假设majority class有\\(m\\)个cluster，minority有\\(n\\)个cluster，我们以cluster最大的data数目\\(k\\)为标准，我们先对majority class中所有cluster，都进行oversampling，使得他们的数目都达到\\(k\\)，随后，对于minority中每个cluster进行oversampling，使得每一个cluster数目变成\\(m * k /n\\)，最终实现between-class balance和within-class balance.</p>\n<h3 id=\"Undersampling\"><a href=\"#Undersampling\" class=\"headerlink\" title=\"Undersampling\"></a>Undersampling</h3><p>与oversampling相对应的则是undersampling，undersampling的核心思想是对于较多类别的samples抽样，使得两个类别数据趋于相近。但是，随机抽样获得会使得类别丧失很多的信息，甚至导致数据分布发生改变。</p>\n<h4 id=\"One-sided-selection\"><a href=\"#One-sided-selection\" class=\"headerlink\" title=\"One-sided selection\"></a>One-sided selection</h4><p>one-sided selection的主要思想是，为了保证数据整体的分布，我们优先去除靠近边界的样本，这样可以保证较多分类的数据分布。</p>\n<h2 id=\"Classifier-level-methods\"><a href=\"#Classifier-level-methods\" class=\"headerlink\" title=\"Classifier level methods\"></a>Classifier level methods</h2><p>下面我们来看看通过改变classifier level来解决imbalanced data的方法，这类方法侧重于分类器本身的一些性质而并非两类数据的个数。</p>\n<h3 id=\"Thresholding\"><a href=\"#Thresholding\" class=\"headerlink\" title=\"Thresholding\"></a>Thresholding</h3><p>我在之前的博客中聊到过，imbalanced data的分类平面会倾向于较少数据的分类一侧，所以我们可以通过改变类别预测的probabilty的threshold来修正分类平面。常用的方法就是加入关于类别数目的prior probability：<br>$$y_i(x)=p(i|x)= \\frac{p(i) \\cdot p(x|i)}{p(x)}$$</p>\n<h3 id=\"Cost-sensitive-learning\"><a href=\"#Cost-sensitive-learning\" class=\"headerlink\" title=\"Cost sensitive learning\"></a>Cost sensitive learning</h3><p>Thresholding方法其实对已经train好的模型的采取的一种方式。相应的，我们在模型训练的时候就来消除imbalanced data的一些影响，如何做到呢？答案就是cost function.</p>\n<p>我们可以通过调整learning rate，加强对cost比较大的samples，并且最终的优化目标从标准的cost function变成misclassification cost，如此就可以解决imbalanced data的问题了</p>\n<h3 id=\"One-class-classification\"><a href=\"#One-class-classification\" class=\"headerlink\" title=\"One-class classification\"></a>One-class classification</h3><p>该方法可以说是换了一种思维看问题，我们不再将classification作为我们的task，而是变成了对于一种异常检测的问题。我们只是着眼于较多samples的类别，认为另一类别的samples是一种异常值。</p>\n<p>当然，这种方法适合那种极端的imbalanced data，对于一般的情况并不一定很适用。</p>\n<h2 id=\"Recommendation\"><a href=\"#Recommendation\" class=\"headerlink\" title=\"Recommendation\"></a>Recommendation</h2><p>Projection:<a href=\"https://github.com/scikit-learn-contrib/imbalanced-learn\" target=\"_blank\" rel=\"noopener\">Imbalanced-learn</a></p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ul>\n<li><a href=\"https://arxiv.org/pdf/1710.05381.pdf\" target=\"_blank\" rel=\"noopener\">Buda, Mateusz, Atsuto Maki, and Maciej A. Mazurowski. “A systematic study of the class imbalance problem in convolutional neural networks.” arXiv preprint arXiv:1710.05381 (2017).</a></li>\n<li><a href=\"https://www.jair.org/media/953/live-953-2037-jair.pdf\" target=\"_blank\" rel=\"noopener\">Chawla, Nitesh V., et al. “SMOTE: synthetic minority over-sampling technique.” Journal of artificial intelligence research 16 (2002): 321-357.</a></li>\n<li><a href=\"http://sci2s.ugr.es/keel/pdf/specific/articulo/jo.pdf\" target=\"_blank\" rel=\"noopener\">Jo, Taeho, and Nathalie Japkowicz. “Class imbalances versus small disjuncts.” ACM Sigkdd Explorations Newsletter 6.1 (2004): 40-49.</a></li>\n<li><a href=\"http://www.ee.iisc.ac.in/new/people/faculty/prasantg/downloads/NeuralNetworksPosteriors_Lippmann1991.pdf\" target=\"_blank\" rel=\"noopener\">Richard, Michael D., and Richard P. Lippmann. “Neural network classifiers estimate Bayesian a posteriori probabilities.” Neural computation 3.4 (1991): 461-483.</a></li>\n</ul>\n","site":{"data":{}},"_categories":[{"name":"machine learning","path":"categories/machine-learning/"}],"_tags":[{"name":"imbalanced data","path":"tags/imbalanced-data/"}],"excerpt":"<p>Hello，大家好，双十一真的很累，一直在加班，忙里偷闲看了<a href=\"https://arxiv.org/pdf/1710.05381.pdf\" target=\"_blank\" rel=\"noopener\">A systematic study of the class imbalance  problem in convolutional neural networks</a>，感觉paper呈现的研究内容感觉很一般，但是，paper中关于imbalanced data的solution方法倒是写的很不错，也勾起了我对于这一块总结的欲望。之前也写过一篇关于imbalanced data的paper notes，但是对于这一块的具体方法总结还不是很足够，于是用这篇paper为主线好好sum up一计。</p>\n<p>我们来一起看看。<br></p>","more":"</p>\n<h2 id=\"Data-level-methods\"><a href=\"#Data-level-methods\" class=\"headerlink\" title=\"Data level methods\"></a>Data level methods</h2><p>首先我们来看一看data level methods，这类方法有一个共性，那就是通过改变data的数量来完成对imbalanced data problem的解决。</p>\n<h3 id=\"Oversampling\"><a href=\"#Oversampling\" class=\"headerlink\" title=\"Oversampling\"></a>Oversampling</h3><p>Oversampling可以说是最直观的solution之一，它的核心思想是，对于较少一类别的samples，过此重复采样，以此让两种类别的样本接近平衡。<strong>但是，对于一个sample多次重复训练，很有可能带来overfitting</strong>，因此，简单粗暴的重复采样并不可取。因此，很多改进的版本应运而生：</p>\n<h4 id=\"SMOTE\"><a href=\"#SMOTE\" class=\"headerlink\" title=\"SMOTE\"></a>SMOTE</h4><p>SMOTE算法是一种经典的oversampling方法，它的主要思想是对较少数目类别的样本，随机抽取\\(m\\)个样本，对于随机抽取出的样本，每个样本选取距离最近的\\(n\\)个样本，在他们的连线上随机选取一个点，作为较少类别的补充样本。假设原样本点为\\(x\\)，被选中的附近的点为\\(x’\\)，则新的样本点为：<br>$$x_{new}= x + rand(0,1) \\cdot |x-x’|$$</p>\n<p>通过这种方式，SMOTE可以对较少类别样本进行扩充，进而实现oversampling，平衡数据分布。</p>\n<h4 id=\"Cluster-base-oversampling\"><a href=\"#Cluster-base-oversampling\" class=\"headerlink\" title=\"Cluster-base oversampling\"></a>Cluster-base oversampling</h4><p>Cluster-based方法的最大特点莫过于最开始对数据进行一个聚类分析，数据会变成数个cluster，然后对于每一个cluster在进行数据的oversampling，<strong>同时兼顾类别之间的between-class imbalance，还要考虑到类内部各个cluster的within-class imbalance</strong>.</p>\n<blockquote>\n<p>Its idea is to consider not only the between class imbalance (the imbalance occurring between the two classes) but also the with in class imbalance (the imbalance occurring between the sub clusters of each class) and to oversample the data set by rectifying these two types of imbalances simultaneously.</p>\n</blockquote>\n<p>原paper大致叙述了整个流程，首先我们对imbalanced data进行k-means(或者其他算法也可以)聚类，聚成多个cluster之后，我们开始进行oversampling，假设majority class有\\(m\\)个cluster，minority有\\(n\\)个cluster，我们以cluster最大的data数目\\(k\\)为标准，我们先对majority class中所有cluster，都进行oversampling，使得他们的数目都达到\\(k\\)，随后，对于minority中每个cluster进行oversampling，使得每一个cluster数目变成\\(m * k /n\\)，最终实现between-class balance和within-class balance.</p>\n<h3 id=\"Undersampling\"><a href=\"#Undersampling\" class=\"headerlink\" title=\"Undersampling\"></a>Undersampling</h3><p>与oversampling相对应的则是undersampling，undersampling的核心思想是对于较多类别的samples抽样，使得两个类别数据趋于相近。但是，随机抽样获得会使得类别丧失很多的信息，甚至导致数据分布发生改变。</p>\n<h4 id=\"One-sided-selection\"><a href=\"#One-sided-selection\" class=\"headerlink\" title=\"One-sided selection\"></a>One-sided selection</h4><p>one-sided selection的主要思想是，为了保证数据整体的分布，我们优先去除靠近边界的样本，这样可以保证较多分类的数据分布。</p>\n<h2 id=\"Classifier-level-methods\"><a href=\"#Classifier-level-methods\" class=\"headerlink\" title=\"Classifier level methods\"></a>Classifier level methods</h2><p>下面我们来看看通过改变classifier level来解决imbalanced data的方法，这类方法侧重于分类器本身的一些性质而并非两类数据的个数。</p>\n<h3 id=\"Thresholding\"><a href=\"#Thresholding\" class=\"headerlink\" title=\"Thresholding\"></a>Thresholding</h3><p>我在之前的博客中聊到过，imbalanced data的分类平面会倾向于较少数据的分类一侧，所以我们可以通过改变类别预测的probabilty的threshold来修正分类平面。常用的方法就是加入关于类别数目的prior probability：<br>$$y_i(x)=p(i|x)= \\frac{p(i) \\cdot p(x|i)}{p(x)}$$</p>\n<h3 id=\"Cost-sensitive-learning\"><a href=\"#Cost-sensitive-learning\" class=\"headerlink\" title=\"Cost sensitive learning\"></a>Cost sensitive learning</h3><p>Thresholding方法其实对已经train好的模型的采取的一种方式。相应的，我们在模型训练的时候就来消除imbalanced data的一些影响，如何做到呢？答案就是cost function.</p>\n<p>我们可以通过调整learning rate，加强对cost比较大的samples，并且最终的优化目标从标准的cost function变成misclassification cost，如此就可以解决imbalanced data的问题了</p>\n<h3 id=\"One-class-classification\"><a href=\"#One-class-classification\" class=\"headerlink\" title=\"One-class classification\"></a>One-class classification</h3><p>该方法可以说是换了一种思维看问题，我们不再将classification作为我们的task，而是变成了对于一种异常检测的问题。我们只是着眼于较多samples的类别，认为另一类别的samples是一种异常值。</p>\n<p>当然，这种方法适合那种极端的imbalanced data，对于一般的情况并不一定很适用。</p>\n<h2 id=\"Recommendation\"><a href=\"#Recommendation\" class=\"headerlink\" title=\"Recommendation\"></a>Recommendation</h2><p>Projection:<a href=\"https://github.com/scikit-learn-contrib/imbalanced-learn\" target=\"_blank\" rel=\"noopener\">Imbalanced-learn</a></p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ul>\n<li><a href=\"https://arxiv.org/pdf/1710.05381.pdf\" target=\"_blank\" rel=\"noopener\">Buda, Mateusz, Atsuto Maki, and Maciej A. Mazurowski. “A systematic study of the class imbalance problem in convolutional neural networks.” arXiv preprint arXiv:1710.05381 (2017).</a></li>\n<li><a href=\"https://www.jair.org/media/953/live-953-2037-jair.pdf\" target=\"_blank\" rel=\"noopener\">Chawla, Nitesh V., et al. “SMOTE: synthetic minority over-sampling technique.” Journal of artificial intelligence research 16 (2002): 321-357.</a></li>\n<li><a href=\"http://sci2s.ugr.es/keel/pdf/specific/articulo/jo.pdf\" target=\"_blank\" rel=\"noopener\">Jo, Taeho, and Nathalie Japkowicz. “Class imbalances versus small disjuncts.” ACM Sigkdd Explorations Newsletter 6.1 (2004): 40-49.</a></li>\n<li><a href=\"http://www.ee.iisc.ac.in/new/people/faculty/prasantg/downloads/NeuralNetworksPosteriors_Lippmann1991.pdf\" target=\"_blank\" rel=\"noopener\">Richard, Michael D., and Richard P. Lippmann. “Neural network classifiers estimate Bayesian a posteriori probabilities.” Neural computation 3.4 (1991): 461-483.</a></li>\n</ul>"},{"title":"Hello World","date":"2017-07-26T09:40:40.000Z","_content":"**我终于把blog搭建起来了!**\n\n这是一个属于**Asir** 自己的博客,在这里我会写一些技术分享,记录自己平时学到的东西,也会整点吐槽或者鸡汤.总之,有了一个真正的属于自己的天地,可以随便整,这种感觉非常棒.\n\n其实自己在博客园也尝试过一次,可是体验并不是很理想,在这里我并没有抨击的意思,因为自己搭建起来的成就感那不是一点两点.希望后面可以趁热打铁,开启blog之旅.\n<!--more-->\n***\n在这里感谢下亲铁[**圈羊**](https://www.unbelievable9.info/)为我提供的完美设备和深夜技术支持,非常棒!\n\n最后,作为一个coding man, 在所有事情的最开始,都不应该缺少这句话\n\n**Hello world!!!**\n","source":"_posts/other-hello.md","raw":"---\ntitle: Hello World\ndate: 2017-07-26 17:40:40\ntags: life\ncategories: others\n---\n**我终于把blog搭建起来了!**\n\n这是一个属于**Asir** 自己的博客,在这里我会写一些技术分享,记录自己平时学到的东西,也会整点吐槽或者鸡汤.总之,有了一个真正的属于自己的天地,可以随便整,这种感觉非常棒.\n\n其实自己在博客园也尝试过一次,可是体验并不是很理想,在这里我并没有抨击的意思,因为自己搭建起来的成就感那不是一点两点.希望后面可以趁热打铁,开启blog之旅.\n<!--more-->\n***\n在这里感谢下亲铁[**圈羊**](https://www.unbelievable9.info/)为我提供的完美设备和深夜技术支持,非常棒!\n\n最后,作为一个coding man, 在所有事情的最开始,都不应该缺少这句话\n\n**Hello world!!!**\n","slug":"other-hello","published":1,"updated":"2020-05-10T06:50:12.530Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cka0wnkwk0019qxotrz9wig2d","content":"<p><strong>我终于把blog搭建起来了!</strong></p>\n<p>这是一个属于<strong>Asir</strong> 自己的博客,在这里我会写一些技术分享,记录自己平时学到的东西,也会整点吐槽或者鸡汤.总之,有了一个真正的属于自己的天地,可以随便整,这种感觉非常棒.</p>\n<p>其实自己在博客园也尝试过一次,可是体验并不是很理想,在这里我并没有抨击的意思,因为自己搭建起来的成就感那不是一点两点.希望后面可以趁热打铁,开启blog之旅.<br><a id=\"more\"></a></p>\n<hr>\n<p>在这里感谢下亲铁<a href=\"https://www.unbelievable9.info/\" target=\"_blank\" rel=\"noopener\"><strong>圈羊</strong></a>为我提供的完美设备和深夜技术支持,非常棒!</p>\n<p>最后,作为一个coding man, 在所有事情的最开始,都不应该缺少这句话</p>\n<p><strong>Hello world!!!</strong></p>\n","site":{"data":{}},"_categories":[{"name":"others","path":"categories/others/"}],"_tags":[{"name":"life","path":"tags/life/"}],"excerpt":"<p><strong>我终于把blog搭建起来了!</strong></p>\n<p>这是一个属于<strong>Asir</strong> 自己的博客,在这里我会写一些技术分享,记录自己平时学到的东西,也会整点吐槽或者鸡汤.总之,有了一个真正的属于自己的天地,可以随便整,这种感觉非常棒.</p>\n<p>其实自己在博客园也尝试过一次,可是体验并不是很理想,在这里我并没有抨击的意思,因为自己搭建起来的成就感那不是一点两点.希望后面可以趁热打铁,开启blog之旅.<br></p>","more":"</p>\n<hr>\n<p>在这里感谢下亲铁<a href=\"https://www.unbelievable9.info/\" target=\"_blank\" rel=\"noopener\"><strong>圈羊</strong></a>为我提供的完美设备和深夜技术支持,非常棒!</p>\n<p>最后,作为一个coding man, 在所有事情的最开始,都不应该缺少这句话</p>\n<p><strong>Hello world!!!</strong></p>"},{"title":"Reading Notes-Practical lessons from predicting clicks on ads at facebook","date":"2017-08-23T03:30:43.000Z","_content":"OK，今天我们来review一篇经典的paper，这篇paper是3年前facebook的研究成果，关于gbt和lr的结合，这个搭配对于近几年的CTR预测以及推荐系统的发展都产生了深远的影响。虽然已经很难被称为一篇新paper了，但是还是值得我们去看看。\n\n我们一起简单看看这篇paper的核心point.\n<!--more-->\n## Notes\n传统CTR预测中，logistic regression一直有着很好的效果，lr不仅可以线性分类，同时也可以给出样本属于该类别的posterior probability\n\n但是传统的lr也有着本身的缺憾，lr本身就是liner分类器，对于线性不可分的features效果不是很理想。同时在对于连续feature离散化的时候，效果很大程度依赖于离散分桶的人为经验。\n\n该paper提出了一种依靠gbt进行feature transform的方法，不多说废话，我们直接上图\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/3/3-1.png)\n这就是这篇paper最最最核心的部分了。\n\n> Input features are transformed by means of boosted decision trees.The output of each individual tree is treated as a categorical input feature to a sparse linear classifier. Boosted decision trees prove to be very powerful feature transforms.\n\n从图中我们可以看到，原始feature被gbt进行了transform，样本落入到哪个tree node，则该位置1，其他位置0，随后再进入线性分类器lr中进行最后的分类。\n\n假设有一个sample，在图中所示的模型中，gbt有两棵树，从左到右是tree1和tree2，tree1中sample被分到了第一个tree node，tree2中sample被分到了第二个tree node，那么最终transform得到的new sample就变成了(1,0,0,0,1)\n\n通过gbt的transform后，feature不仅从非线性转换成了线性（类似于SVM的kernel效果），而且feature被完全的离散成了0-1稀疏feature，无论从线性可分还是特征稀疏的角度上，都变得比原始feature更加理想！\n\n因为是一篇相对老一些的paper，所以我叙述的比较简单，大家可以get到gbt+lr这个模型的基本原理就可以了。我自己在私下也用python写了一个简单的demo，感兴趣的朋友[可以看看](https://github.com/JoeAsir/Machine-learning-demo/blob/master/algorithm/gbtWithLogisticRegression/gradient_logistic.py)，欢迎提出意见，欢迎folk！\n\n## Reference\n* [He, Xinran, et al. \"Practical lessons from predicting clicks on ads at facebook.\" Proceedings of the Eighth International Workshop on Data Mining for Online Advertising. ACM, 2014.](http://quinonero.net/Publications/predicting-clicks-facebook.pdf)\n","source":"_posts/paper-facebook.md","raw":"---\ntitle: Reading Notes-Practical lessons from predicting clicks on ads at facebook\ndate: 2017-08-23 11:30:43\ntags: \n\t- gbt\n\t- logistic regression\n\t- gradient descent\ncategories: reading notes\n---\nOK，今天我们来review一篇经典的paper，这篇paper是3年前facebook的研究成果，关于gbt和lr的结合，这个搭配对于近几年的CTR预测以及推荐系统的发展都产生了深远的影响。虽然已经很难被称为一篇新paper了，但是还是值得我们去看看。\n\n我们一起简单看看这篇paper的核心point.\n<!--more-->\n## Notes\n传统CTR预测中，logistic regression一直有着很好的效果，lr不仅可以线性分类，同时也可以给出样本属于该类别的posterior probability\n\n但是传统的lr也有着本身的缺憾，lr本身就是liner分类器，对于线性不可分的features效果不是很理想。同时在对于连续feature离散化的时候，效果很大程度依赖于离散分桶的人为经验。\n\n该paper提出了一种依靠gbt进行feature transform的方法，不多说废话，我们直接上图\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/3/3-1.png)\n这就是这篇paper最最最核心的部分了。\n\n> Input features are transformed by means of boosted decision trees.The output of each individual tree is treated as a categorical input feature to a sparse linear classifier. Boosted decision trees prove to be very powerful feature transforms.\n\n从图中我们可以看到，原始feature被gbt进行了transform，样本落入到哪个tree node，则该位置1，其他位置0，随后再进入线性分类器lr中进行最后的分类。\n\n假设有一个sample，在图中所示的模型中，gbt有两棵树，从左到右是tree1和tree2，tree1中sample被分到了第一个tree node，tree2中sample被分到了第二个tree node，那么最终transform得到的new sample就变成了(1,0,0,0,1)\n\n通过gbt的transform后，feature不仅从非线性转换成了线性（类似于SVM的kernel效果），而且feature被完全的离散成了0-1稀疏feature，无论从线性可分还是特征稀疏的角度上，都变得比原始feature更加理想！\n\n因为是一篇相对老一些的paper，所以我叙述的比较简单，大家可以get到gbt+lr这个模型的基本原理就可以了。我自己在私下也用python写了一个简单的demo，感兴趣的朋友[可以看看](https://github.com/JoeAsir/Machine-learning-demo/blob/master/algorithm/gbtWithLogisticRegression/gradient_logistic.py)，欢迎提出意见，欢迎folk！\n\n## Reference\n* [He, Xinran, et al. \"Practical lessons from predicting clicks on ads at facebook.\" Proceedings of the Eighth International Workshop on Data Mining for Online Advertising. ACM, 2014.](http://quinonero.net/Publications/predicting-clicks-facebook.pdf)\n","slug":"paper-facebook","published":1,"updated":"2020-05-10T06:50:12.530Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cka0wnkwl001aqxotv7blftm0","content":"<p>OK，今天我们来review一篇经典的paper，这篇paper是3年前facebook的研究成果，关于gbt和lr的结合，这个搭配对于近几年的CTR预测以及推荐系统的发展都产生了深远的影响。虽然已经很难被称为一篇新paper了，但是还是值得我们去看看。</p>\n<p>我们一起简单看看这篇paper的核心point.<br><a id=\"more\"></a></p>\n<h2 id=\"Notes\"><a href=\"#Notes\" class=\"headerlink\" title=\"Notes\"></a>Notes</h2><p>传统CTR预测中，logistic regression一直有着很好的效果，lr不仅可以线性分类，同时也可以给出样本属于该类别的posterior probability</p>\n<p>但是传统的lr也有着本身的缺憾，lr本身就是liner分类器，对于线性不可分的features效果不是很理想。同时在对于连续feature离散化的时候，效果很大程度依赖于离散分桶的人为经验。</p>\n<p>该paper提出了一种依靠gbt进行feature transform的方法，不多说废话，我们直接上图<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/3/3-1.png\" alt=\"\"><br>这就是这篇paper最最最核心的部分了。</p>\n<blockquote>\n<p>Input features are transformed by means of boosted decision trees.The output of each individual tree is treated as a categorical input feature to a sparse linear classifier. Boosted decision trees prove to be very powerful feature transforms.</p>\n</blockquote>\n<p>从图中我们可以看到，原始feature被gbt进行了transform，样本落入到哪个tree node，则该位置1，其他位置0，随后再进入线性分类器lr中进行最后的分类。</p>\n<p>假设有一个sample，在图中所示的模型中，gbt有两棵树，从左到右是tree1和tree2，tree1中sample被分到了第一个tree node，tree2中sample被分到了第二个tree node，那么最终transform得到的new sample就变成了(1,0,0,0,1)</p>\n<p>通过gbt的transform后，feature不仅从非线性转换成了线性（类似于SVM的kernel效果），而且feature被完全的离散成了0-1稀疏feature，无论从线性可分还是特征稀疏的角度上，都变得比原始feature更加理想！</p>\n<p>因为是一篇相对老一些的paper，所以我叙述的比较简单，大家可以get到gbt+lr这个模型的基本原理就可以了。我自己在私下也用python写了一个简单的demo，感兴趣的朋友<a href=\"https://github.com/JoeAsir/Machine-learning-demo/blob/master/algorithm/gbtWithLogisticRegression/gradient_logistic.py\" target=\"_blank\" rel=\"noopener\">可以看看</a>，欢迎提出意见，欢迎folk！</p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ul>\n<li><a href=\"http://quinonero.net/Publications/predicting-clicks-facebook.pdf\" target=\"_blank\" rel=\"noopener\">He, Xinran, et al. “Practical lessons from predicting clicks on ads at facebook.” Proceedings of the Eighth International Workshop on Data Mining for Online Advertising. ACM, 2014.</a></li>\n</ul>\n","site":{"data":{}},"_categories":[{"name":"reading notes","path":"categories/reading-notes/"}],"_tags":[{"name":"gradient descent","path":"tags/gradient-descent/"},{"name":"gbt","path":"tags/gbt/"},{"name":"logistic regression","path":"tags/logistic-regression/"}],"excerpt":"<p>OK，今天我们来review一篇经典的paper，这篇paper是3年前facebook的研究成果，关于gbt和lr的结合，这个搭配对于近几年的CTR预测以及推荐系统的发展都产生了深远的影响。虽然已经很难被称为一篇新paper了，但是还是值得我们去看看。</p>\n<p>我们一起简单看看这篇paper的核心point.<br></p>","more":"</p>\n<h2 id=\"Notes\"><a href=\"#Notes\" class=\"headerlink\" title=\"Notes\"></a>Notes</h2><p>传统CTR预测中，logistic regression一直有着很好的效果，lr不仅可以线性分类，同时也可以给出样本属于该类别的posterior probability</p>\n<p>但是传统的lr也有着本身的缺憾，lr本身就是liner分类器，对于线性不可分的features效果不是很理想。同时在对于连续feature离散化的时候，效果很大程度依赖于离散分桶的人为经验。</p>\n<p>该paper提出了一种依靠gbt进行feature transform的方法，不多说废话，我们直接上图<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/3/3-1.png\" alt=\"\"><br>这就是这篇paper最最最核心的部分了。</p>\n<blockquote>\n<p>Input features are transformed by means of boosted decision trees.The output of each individual tree is treated as a categorical input feature to a sparse linear classifier. Boosted decision trees prove to be very powerful feature transforms.</p>\n</blockquote>\n<p>从图中我们可以看到，原始feature被gbt进行了transform，样本落入到哪个tree node，则该位置1，其他位置0，随后再进入线性分类器lr中进行最后的分类。</p>\n<p>假设有一个sample，在图中所示的模型中，gbt有两棵树，从左到右是tree1和tree2，tree1中sample被分到了第一个tree node，tree2中sample被分到了第二个tree node，那么最终transform得到的new sample就变成了(1,0,0,0,1)</p>\n<p>通过gbt的transform后，feature不仅从非线性转换成了线性（类似于SVM的kernel效果），而且feature被完全的离散成了0-1稀疏feature，无论从线性可分还是特征稀疏的角度上，都变得比原始feature更加理想！</p>\n<p>因为是一篇相对老一些的paper，所以我叙述的比较简单，大家可以get到gbt+lr这个模型的基本原理就可以了。我自己在私下也用python写了一个简单的demo，感兴趣的朋友<a href=\"https://github.com/JoeAsir/Machine-learning-demo/blob/master/algorithm/gbtWithLogisticRegression/gradient_logistic.py\" target=\"_blank\" rel=\"noopener\">可以看看</a>，欢迎提出意见，欢迎folk！</p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ul>\n<li><a href=\"http://quinonero.net/Publications/predicting-clicks-facebook.pdf\" target=\"_blank\" rel=\"noopener\">He, Xinran, et al. “Practical lessons from predicting clicks on ads at facebook.” Proceedings of the Eighth International Workshop on Data Mining for Online Advertising. ACM, 2014.</a></li>\n</ul>"},{"title":"Reading Notes-Swish：A Self-gated Activation Function","date":"2017-10-22T08:13:30.000Z","_content":"Hi all，今天和大家分享一篇比较新的paper，是关于一种新的activation function，关于我们知道的activation function，有sigmoid，tanh，ReLU以及ReLU的一些变种，那我们今天来看看这种新提出的activation function到底有什么特色。\n<!--more-->\n## Notes\n首先定义，swish activation function \\\\(f(x)=x \\cdot \\sigma (x)\\\\)，其中\\\\(\\sigma(x)\\\\)是sigmoid function，也就是\\\\( \\sigma(x)=1/(1+ e^{-x})\\\\).Swish functin的图像如图所示：\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/11/11-1.png)\n我们再来看下swish function的1st and 2nd derivatives，\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/11/11-2.png)\n下面我们一起来集中看看swish function的优点都有什么，作者给出了以下几点：函数值没有上限，函数值有下限，函数不单调，函数光滑连续，我们一起看看：\n### Unbounded above\nUnbounded above的实质，是防止activation function在bounded value处发生saturation. bounded above 带来的问题，就是越接近bounded value的时候，function gradient就会越小，逐渐接近0，这就导致gradient descent异常缓慢甚至无法converge。例如sigmoid 和tanh function，他们都是bounded below and above，当我们采用这两种activation function的时候，我们必须谨慎小心的让初始值尽量在function的接近liner的部分来避免上面问题的产生，因此，unbounded above是一个很好的优点，例如ReLU及其变种都采用了这一原则。\n\n### Bounded below & non-monotonicity\nBounded below其实也是一种很好的方法，并且也有activation function已经采用了，采用该方法后，所有负数input都会得到相差无几的activation value，也就是说，-1000和-1的值几乎没有区别，按照author的话来讲，就是我们将\n> make large negative input \"fogotten\"\n\n这其实也是regularzation的一种思想，这种方法在ReLU等方法中也有体现，但是，swish可以通过自身的非单调性质，将比较小的negative input仍然以negative value输出，non-monotonicity提供了更好的gradient flow.\n\n### Smothness\n关于smoothness的优点，我们来看一张图：\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/11/11-3.png)\n \n 总而言之，个人感觉swish应该算是一个不错的activation，本人由于时间原因，还没有来得及自己测试它，但是据我所看到的讨论，swish的实际效果貌似不是十分稳定，所以我们可以持保留意见，进一步观察它的表现。\n \n## Reference\n* [Ramachandran P, Zoph B, Le Q V. Swish：a Self-Gated Activation Function[J]. arXiv preprint arXiv:1710.05941, 2017.](https://arxiv.org/pdf/1710.05941.pdf)\n","source":"_posts/paper-swish.md","raw":"---\ntitle: Reading Notes-Swish：A Self-gated Activation Function\ndate: 2017-10-22 16:13:30\ntags: \n\t- activtion function\ncategories: reading notes\n---\nHi all，今天和大家分享一篇比较新的paper，是关于一种新的activation function，关于我们知道的activation function，有sigmoid，tanh，ReLU以及ReLU的一些变种，那我们今天来看看这种新提出的activation function到底有什么特色。\n<!--more-->\n## Notes\n首先定义，swish activation function \\\\(f(x)=x \\cdot \\sigma (x)\\\\)，其中\\\\(\\sigma(x)\\\\)是sigmoid function，也就是\\\\( \\sigma(x)=1/(1+ e^{-x})\\\\).Swish functin的图像如图所示：\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/11/11-1.png)\n我们再来看下swish function的1st and 2nd derivatives，\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/11/11-2.png)\n下面我们一起来集中看看swish function的优点都有什么，作者给出了以下几点：函数值没有上限，函数值有下限，函数不单调，函数光滑连续，我们一起看看：\n### Unbounded above\nUnbounded above的实质，是防止activation function在bounded value处发生saturation. bounded above 带来的问题，就是越接近bounded value的时候，function gradient就会越小，逐渐接近0，这就导致gradient descent异常缓慢甚至无法converge。例如sigmoid 和tanh function，他们都是bounded below and above，当我们采用这两种activation function的时候，我们必须谨慎小心的让初始值尽量在function的接近liner的部分来避免上面问题的产生，因此，unbounded above是一个很好的优点，例如ReLU及其变种都采用了这一原则。\n\n### Bounded below & non-monotonicity\nBounded below其实也是一种很好的方法，并且也有activation function已经采用了，采用该方法后，所有负数input都会得到相差无几的activation value，也就是说，-1000和-1的值几乎没有区别，按照author的话来讲，就是我们将\n> make large negative input \"fogotten\"\n\n这其实也是regularzation的一种思想，这种方法在ReLU等方法中也有体现，但是，swish可以通过自身的非单调性质，将比较小的negative input仍然以negative value输出，non-monotonicity提供了更好的gradient flow.\n\n### Smothness\n关于smoothness的优点，我们来看一张图：\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/11/11-3.png)\n \n 总而言之，个人感觉swish应该算是一个不错的activation，本人由于时间原因，还没有来得及自己测试它，但是据我所看到的讨论，swish的实际效果貌似不是十分稳定，所以我们可以持保留意见，进一步观察它的表现。\n \n## Reference\n* [Ramachandran P, Zoph B, Le Q V. Swish：a Self-Gated Activation Function[J]. arXiv preprint arXiv:1710.05941, 2017.](https://arxiv.org/pdf/1710.05941.pdf)\n","slug":"paper-swish","published":1,"updated":"2020-05-10T06:50:12.531Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cka0wnkwn001eqxotxssjeddn","content":"<p>Hi all，今天和大家分享一篇比较新的paper，是关于一种新的activation function，关于我们知道的activation function，有sigmoid，tanh，ReLU以及ReLU的一些变种，那我们今天来看看这种新提出的activation function到底有什么特色。<br><a id=\"more\"></a></p>\n<h2 id=\"Notes\"><a href=\"#Notes\" class=\"headerlink\" title=\"Notes\"></a>Notes</h2><p>首先定义，swish activation function \\(f(x)=x \\cdot \\sigma (x)\\)，其中\\(\\sigma(x)\\)是sigmoid function，也就是\\( \\sigma(x)=1/(1+ e^{-x})\\).Swish functin的图像如图所示：<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/11/11-1.png\" alt=\"\"><br>我们再来看下swish function的1st and 2nd derivatives，<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/11/11-2.png\" alt=\"\"><br>下面我们一起来集中看看swish function的优点都有什么，作者给出了以下几点：函数值没有上限，函数值有下限，函数不单调，函数光滑连续，我们一起看看：</p>\n<h3 id=\"Unbounded-above\"><a href=\"#Unbounded-above\" class=\"headerlink\" title=\"Unbounded above\"></a>Unbounded above</h3><p>Unbounded above的实质，是防止activation function在bounded value处发生saturation. bounded above 带来的问题，就是越接近bounded value的时候，function gradient就会越小，逐渐接近0，这就导致gradient descent异常缓慢甚至无法converge。例如sigmoid 和tanh function，他们都是bounded below and above，当我们采用这两种activation function的时候，我们必须谨慎小心的让初始值尽量在function的接近liner的部分来避免上面问题的产生，因此，unbounded above是一个很好的优点，例如ReLU及其变种都采用了这一原则。</p>\n<h3 id=\"Bounded-below-amp-non-monotonicity\"><a href=\"#Bounded-below-amp-non-monotonicity\" class=\"headerlink\" title=\"Bounded below &amp; non-monotonicity\"></a>Bounded below &amp; non-monotonicity</h3><p>Bounded below其实也是一种很好的方法，并且也有activation function已经采用了，采用该方法后，所有负数input都会得到相差无几的activation value，也就是说，-1000和-1的值几乎没有区别，按照author的话来讲，就是我们将</p>\n<blockquote>\n<p>make large negative input “fogotten”</p>\n</blockquote>\n<p>这其实也是regularzation的一种思想，这种方法在ReLU等方法中也有体现，但是，swish可以通过自身的非单调性质，将比较小的negative input仍然以negative value输出，non-monotonicity提供了更好的gradient flow.</p>\n<h3 id=\"Smothness\"><a href=\"#Smothness\" class=\"headerlink\" title=\"Smothness\"></a>Smothness</h3><p>关于smoothness的优点，我们来看一张图：<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/11/11-3.png\" alt=\"\"></p>\n<p> 总而言之，个人感觉swish应该算是一个不错的activation，本人由于时间原因，还没有来得及自己测试它，但是据我所看到的讨论，swish的实际效果貌似不是十分稳定，所以我们可以持保留意见，进一步观察它的表现。</p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ul>\n<li><a href=\"https://arxiv.org/pdf/1710.05941.pdf\" target=\"_blank\" rel=\"noopener\">Ramachandran P, Zoph B, Le Q V. Swish：a Self-Gated Activation Function[J]. arXiv preprint arXiv:1710.05941, 2017.</a></li>\n</ul>\n","site":{"data":{}},"_categories":[{"name":"reading notes","path":"categories/reading-notes/"}],"_tags":[{"name":"activtion function","path":"tags/activtion-function/"}],"excerpt":"<p>Hi all，今天和大家分享一篇比较新的paper，是关于一种新的activation function，关于我们知道的activation function，有sigmoid，tanh，ReLU以及ReLU的一些变种，那我们今天来看看这种新提出的activation function到底有什么特色。<br></p>","more":"</p>\n<h2 id=\"Notes\"><a href=\"#Notes\" class=\"headerlink\" title=\"Notes\"></a>Notes</h2><p>首先定义，swish activation function \\(f(x)=x \\cdot \\sigma (x)\\)，其中\\(\\sigma(x)\\)是sigmoid function，也就是\\( \\sigma(x)=1/(1+ e^{-x})\\).Swish functin的图像如图所示：<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/11/11-1.png\" alt=\"\"><br>我们再来看下swish function的1st and 2nd derivatives，<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/11/11-2.png\" alt=\"\"><br>下面我们一起来集中看看swish function的优点都有什么，作者给出了以下几点：函数值没有上限，函数值有下限，函数不单调，函数光滑连续，我们一起看看：</p>\n<h3 id=\"Unbounded-above\"><a href=\"#Unbounded-above\" class=\"headerlink\" title=\"Unbounded above\"></a>Unbounded above</h3><p>Unbounded above的实质，是防止activation function在bounded value处发生saturation. bounded above 带来的问题，就是越接近bounded value的时候，function gradient就会越小，逐渐接近0，这就导致gradient descent异常缓慢甚至无法converge。例如sigmoid 和tanh function，他们都是bounded below and above，当我们采用这两种activation function的时候，我们必须谨慎小心的让初始值尽量在function的接近liner的部分来避免上面问题的产生，因此，unbounded above是一个很好的优点，例如ReLU及其变种都采用了这一原则。</p>\n<h3 id=\"Bounded-below-amp-non-monotonicity\"><a href=\"#Bounded-below-amp-non-monotonicity\" class=\"headerlink\" title=\"Bounded below &amp; non-monotonicity\"></a>Bounded below &amp; non-monotonicity</h3><p>Bounded below其实也是一种很好的方法，并且也有activation function已经采用了，采用该方法后，所有负数input都会得到相差无几的activation value，也就是说，-1000和-1的值几乎没有区别，按照author的话来讲，就是我们将</p>\n<blockquote>\n<p>make large negative input “fogotten”</p>\n</blockquote>\n<p>这其实也是regularzation的一种思想，这种方法在ReLU等方法中也有体现，但是，swish可以通过自身的非单调性质，将比较小的negative input仍然以negative value输出，non-monotonicity提供了更好的gradient flow.</p>\n<h3 id=\"Smothness\"><a href=\"#Smothness\" class=\"headerlink\" title=\"Smothness\"></a>Smothness</h3><p>关于smoothness的优点，我们来看一张图：<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/11/11-3.png\" alt=\"\"></p>\n<p> 总而言之，个人感觉swish应该算是一个不错的activation，本人由于时间原因，还没有来得及自己测试它，但是据我所看到的讨论，swish的实际效果貌似不是十分稳定，所以我们可以持保留意见，进一步观察它的表现。</p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ul>\n<li><a href=\"https://arxiv.org/pdf/1710.05941.pdf\" target=\"_blank\" rel=\"noopener\">Ramachandran P, Zoph B, Le Q V. Swish：a Self-Gated Activation Function[J]. arXiv preprint arXiv:1710.05941, 2017.</a></li>\n</ul>"},{"title":"Reading Notes-Class Imbalance, Redux","date":"2017-09-10T05:21:56.000Z","_content":"再次感谢优男，向我提出了又一个尖锐的问题，使得我有机会思考和研究，并且最终可以看到这篇paper，并且最后可以分享给大家。\n\n我个人在工作之中遇到过imbalanced data的问题，我只是直观的感受到，imbalanced data的最后效果往往不是很棒，网上也只是给出了oversampling和undersampling的建议，并没有提及这其中的一些缘故，今天我们一起通过这篇paper来学习学习。\n<!--more-->\n## Notes\n我们假设有positive和negative两类sample，其中positive samples符合\\\\(P(x)\\\\)的Guassian分布，negative samples符合\\\\(G(x)\\\\)的Guassian分布，分类平面将空间划分成positive region\\\\(\\cal R^{+} \\_{w}\\\\)和negative region\\\\(\\cal R^{-} \\_{w}\\\\)，如下图所示：\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/5/5-1.png)\n图中\\\\(w^{ \\*}\\\\)是理想的分割平面，\\\\(w^{ \\*}\\\\) 应该是使loss最小的取值，即\n$$w^{*}= \\arg\\underset{w}{\\min} \\cal L^{*}(w)$$\n对于loss值，其实就是分类中被错分的fn(false negative)和fp(false positive)的期望值，显然，通过minimun该loss得到的会是图中的\\\\(w^{*}\\\\)，因为这个分类平面所带来的error明显是最少的。\n$$\\cal L^{*}(w) = \\cal C_{fn} \\int _{\\cal R^{w} _{-}} \\it P(x)dx + \\cal C_{fp} \\int _{\\cal R^{w} _{+}} \\it G(x)dx$$\n对于整个数据集\\\\(\\cal D \\\\)来说，我们假设数据量较少的一类(paper中设定positive类较少)所占比例为\\\\(\\pi\\\\)(小于0.5)，那么对于带有比例\\\\(\\pi\\\\)的数据集\\\\(\\cal D_{\\pi}\\\\)，全局期望是\n$$\\bf E_{\\cal D_{ \\pi}} [\\cal L(w)]=\\pi \\cal C_{fn} \\int _{\\cal R^{w} _{-}} \\it P(x)dx + (1- \\pi) \\cal C_{fp} \\int _{\\cal R^{w} _{+}} \\it G(x)dx$$\n此处，我个人的理解是，在两类数据均衡的情况下，全局情况下的期望其实是和上面的loss等价的，但是imbalanced data带来了不均衡的因子\\\\(\\pi\\\\)，因此，两个公式不再等价。\n\nOK，既然不等价，那么问题就来了，paper上说，通过最小化全局期望获得的\\\\(\\hat w\\\\)，是向着较少数量类别的样本倾斜，也就是第一幅图中，向较少的postive那边skewed，原因是因为\\\\( \\cal R \\_{+} ^{ \\hat w} < \\cal R \\_{+} ^{w^{\\*}}\\\\), 也就是说，\\\\(\\hat w\\\\)分割的positive region面积小于\\\\(w^{\\*}\\\\)分割出的面积，面积的减小势必导致分割平面向positive类别方向偏移。\n\n遗憾的是，关于面积的证明我实在看不明白，也email了一些人，也没有得到一个满意的答案，如果有朋友看明白了的话，**记得留言或者email我！**\n\n到了这里，paper大概介绍了undersampling的裨益，undersampling的核心其实就是消除前面提到的比例\\\\(\\pi\\\\)，让它趋近于0.5后，分类平面\\\\(\\hat w\\\\)就会趋近于理想分类平面\\\\(w^{*}\\\\)。\n\n这里，作者提出了一个bagging方法，就是多次做undersampling，最后最结果做bagging可以获得更好的效果，如下图\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/5/5-2.png)\npaper还对比了其他的方法，比如Weighted Empirical Cost Minimization(如weighted SVM)和SMOTE方法效果不如bagging undersampling，我上一幅图说明下SMOTE的缺点，更多细节，大家可以详细看看paper，如图：\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/5/5-3.png)\nSMOTE方法是随机选择方向生成新的sample，但是如果新的sample产生了图中位置，则效果不会很好。\n\nOK，今天就这么多，记得看明白了中间的推导一起分享啊！\n## Reference\n* [Wallace, Byron C., et al. \"Class imbalance, redux.\" Data Mining (ICDM), 2011 IEEE 11th International Conference on. IEEE, 2011.](https://pdfs.semanticscholar.org/a8ef/5a810099178b70d1490a4e6fc4426b642cde.pdf)\n* [PPT-Class Imbalance, Redux](https://www.google.co.jp/url?sa=t&rct=j&q=&esrc=s&source=web&cd=3&cad=rja&uact=8&ved=0ahUKEwiVmqXlkprWAhVEsJQKHYJiCf4QFgg8MAI&url=https%3A%2F%2Fcourse.ccs.neu.edu%2Fcs6140sp15%2F4_boosting%2Fslides%2Fwallace_imbalance_icdm_11_for_class_2012_final.pptx&usg=AFQjCNG6GpjKeinzCsXrZWWY1edtbBMgog)\n","source":"_posts/paper-imbalance.md","raw":"---\ntitle: Reading Notes-Class Imbalance, Redux\ndate: 2017-09-10 13:21:56\ntags: \n\t- imbalanced data\n\t- undersampling\n\t- bagging\ncategories: reading notes\n---\n再次感谢优男，向我提出了又一个尖锐的问题，使得我有机会思考和研究，并且最终可以看到这篇paper，并且最后可以分享给大家。\n\n我个人在工作之中遇到过imbalanced data的问题，我只是直观的感受到，imbalanced data的最后效果往往不是很棒，网上也只是给出了oversampling和undersampling的建议，并没有提及这其中的一些缘故，今天我们一起通过这篇paper来学习学习。\n<!--more-->\n## Notes\n我们假设有positive和negative两类sample，其中positive samples符合\\\\(P(x)\\\\)的Guassian分布，negative samples符合\\\\(G(x)\\\\)的Guassian分布，分类平面将空间划分成positive region\\\\(\\cal R^{+} \\_{w}\\\\)和negative region\\\\(\\cal R^{-} \\_{w}\\\\)，如下图所示：\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/5/5-1.png)\n图中\\\\(w^{ \\*}\\\\)是理想的分割平面，\\\\(w^{ \\*}\\\\) 应该是使loss最小的取值，即\n$$w^{*}= \\arg\\underset{w}{\\min} \\cal L^{*}(w)$$\n对于loss值，其实就是分类中被错分的fn(false negative)和fp(false positive)的期望值，显然，通过minimun该loss得到的会是图中的\\\\(w^{*}\\\\)，因为这个分类平面所带来的error明显是最少的。\n$$\\cal L^{*}(w) = \\cal C_{fn} \\int _{\\cal R^{w} _{-}} \\it P(x)dx + \\cal C_{fp} \\int _{\\cal R^{w} _{+}} \\it G(x)dx$$\n对于整个数据集\\\\(\\cal D \\\\)来说，我们假设数据量较少的一类(paper中设定positive类较少)所占比例为\\\\(\\pi\\\\)(小于0.5)，那么对于带有比例\\\\(\\pi\\\\)的数据集\\\\(\\cal D_{\\pi}\\\\)，全局期望是\n$$\\bf E_{\\cal D_{ \\pi}} [\\cal L(w)]=\\pi \\cal C_{fn} \\int _{\\cal R^{w} _{-}} \\it P(x)dx + (1- \\pi) \\cal C_{fp} \\int _{\\cal R^{w} _{+}} \\it G(x)dx$$\n此处，我个人的理解是，在两类数据均衡的情况下，全局情况下的期望其实是和上面的loss等价的，但是imbalanced data带来了不均衡的因子\\\\(\\pi\\\\)，因此，两个公式不再等价。\n\nOK，既然不等价，那么问题就来了，paper上说，通过最小化全局期望获得的\\\\(\\hat w\\\\)，是向着较少数量类别的样本倾斜，也就是第一幅图中，向较少的postive那边skewed，原因是因为\\\\( \\cal R \\_{+} ^{ \\hat w} < \\cal R \\_{+} ^{w^{\\*}}\\\\), 也就是说，\\\\(\\hat w\\\\)分割的positive region面积小于\\\\(w^{\\*}\\\\)分割出的面积，面积的减小势必导致分割平面向positive类别方向偏移。\n\n遗憾的是，关于面积的证明我实在看不明白，也email了一些人，也没有得到一个满意的答案，如果有朋友看明白了的话，**记得留言或者email我！**\n\n到了这里，paper大概介绍了undersampling的裨益，undersampling的核心其实就是消除前面提到的比例\\\\(\\pi\\\\)，让它趋近于0.5后，分类平面\\\\(\\hat w\\\\)就会趋近于理想分类平面\\\\(w^{*}\\\\)。\n\n这里，作者提出了一个bagging方法，就是多次做undersampling，最后最结果做bagging可以获得更好的效果，如下图\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/5/5-2.png)\npaper还对比了其他的方法，比如Weighted Empirical Cost Minimization(如weighted SVM)和SMOTE方法效果不如bagging undersampling，我上一幅图说明下SMOTE的缺点，更多细节，大家可以详细看看paper，如图：\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/5/5-3.png)\nSMOTE方法是随机选择方向生成新的sample，但是如果新的sample产生了图中位置，则效果不会很好。\n\nOK，今天就这么多，记得看明白了中间的推导一起分享啊！\n## Reference\n* [Wallace, Byron C., et al. \"Class imbalance, redux.\" Data Mining (ICDM), 2011 IEEE 11th International Conference on. IEEE, 2011.](https://pdfs.semanticscholar.org/a8ef/5a810099178b70d1490a4e6fc4426b642cde.pdf)\n* [PPT-Class Imbalance, Redux](https://www.google.co.jp/url?sa=t&rct=j&q=&esrc=s&source=web&cd=3&cad=rja&uact=8&ved=0ahUKEwiVmqXlkprWAhVEsJQKHYJiCf4QFgg8MAI&url=https%3A%2F%2Fcourse.ccs.neu.edu%2Fcs6140sp15%2F4_boosting%2Fslides%2Fwallace_imbalance_icdm_11_for_class_2012_final.pptx&usg=AFQjCNG6GpjKeinzCsXrZWWY1edtbBMgog)\n","slug":"paper-imbalance","published":1,"updated":"2020-05-10T06:50:12.531Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cka0wnkwn001gqxotznfx9std","content":"<p>再次感谢优男，向我提出了又一个尖锐的问题，使得我有机会思考和研究，并且最终可以看到这篇paper，并且最后可以分享给大家。</p>\n<p>我个人在工作之中遇到过imbalanced data的问题，我只是直观的感受到，imbalanced data的最后效果往往不是很棒，网上也只是给出了oversampling和undersampling的建议，并没有提及这其中的一些缘故，今天我们一起通过这篇paper来学习学习。<br><a id=\"more\"></a></p>\n<h2 id=\"Notes\"><a href=\"#Notes\" class=\"headerlink\" title=\"Notes\"></a>Notes</h2><p>我们假设有positive和negative两类sample，其中positive samples符合\\(P(x)\\)的Guassian分布，negative samples符合\\(G(x)\\)的Guassian分布，分类平面将空间划分成positive region\\(\\cal R^{+} _{w}\\)和negative region\\(\\cal R^{-} _{w}\\)，如下图所示：<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/5/5-1.png\" alt=\"\"><br>图中\\(w^{ *}\\)是理想的分割平面，\\(w^{ *}\\) 应该是使loss最小的取值，即<br>$$w^{<em>}= \\arg\\underset{w}{\\min} \\cal L^{</em>}(w)$$<br>对于loss值，其实就是分类中被错分的fn(false negative)和fp(false positive)的期望值，显然，通过minimun该loss得到的会是图中的\\(w^{<em>}\\)，因为这个分类平面所带来的error明显是最少的。<br>$$\\cal L^{</em>}(w) = \\cal C_{fn} \\int _{\\cal R^{w} <em>{-}} \\it P(x)dx + \\cal C</em>{fp} \\int _{\\cal R^{w} <em>{+}} \\it G(x)dx$$<br>对于整个数据集\\(\\cal D \\)来说，我们假设数据量较少的一类(paper中设定positive类较少)所占比例为\\(\\pi\\)(小于0.5)，那么对于带有比例\\(\\pi\\)的数据集\\(\\cal D</em>{\\pi}\\)，全局期望是<br>$$\\bf E_{\\cal D_{ \\pi}} [\\cal L(w)]=\\pi \\cal C_{fn} \\int _{\\cal R^{w} <em>{-}} \\it P(x)dx + (1- \\pi) \\cal C</em>{fp} \\int _{\\cal R^{w} _{+}} \\it G(x)dx$$<br>此处，我个人的理解是，在两类数据均衡的情况下，全局情况下的期望其实是和上面的loss等价的，但是imbalanced data带来了不均衡的因子\\(\\pi\\)，因此，两个公式不再等价。</p>\n<p>OK，既然不等价，那么问题就来了，paper上说，通过最小化全局期望获得的\\(\\hat w\\)，是向着较少数量类别的样本倾斜，也就是第一幅图中，向较少的postive那边skewed，原因是因为\\( \\cal R _{+} ^{ \\hat w} &lt; \\cal R _{+} ^{w^{*}}\\), 也就是说，\\(\\hat w\\)分割的positive region面积小于\\(w^{*}\\)分割出的面积，面积的减小势必导致分割平面向positive类别方向偏移。</p>\n<p>遗憾的是，关于面积的证明我实在看不明白，也email了一些人，也没有得到一个满意的答案，如果有朋友看明白了的话，<strong>记得留言或者email我！</strong></p>\n<p>到了这里，paper大概介绍了undersampling的裨益，undersampling的核心其实就是消除前面提到的比例\\(\\pi\\)，让它趋近于0.5后，分类平面\\(\\hat w\\)就会趋近于理想分类平面\\(w^{*}\\)。</p>\n<p>这里，作者提出了一个bagging方法，就是多次做undersampling，最后最结果做bagging可以获得更好的效果，如下图<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/5/5-2.png\" alt=\"\"><br>paper还对比了其他的方法，比如Weighted Empirical Cost Minimization(如weighted SVM)和SMOTE方法效果不如bagging undersampling，我上一幅图说明下SMOTE的缺点，更多细节，大家可以详细看看paper，如图：<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/5/5-3.png\" alt=\"\"><br>SMOTE方法是随机选择方向生成新的sample，但是如果新的sample产生了图中位置，则效果不会很好。</p>\n<p>OK，今天就这么多，记得看明白了中间的推导一起分享啊！</p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ul>\n<li><a href=\"https://pdfs.semanticscholar.org/a8ef/5a810099178b70d1490a4e6fc4426b642cde.pdf\" target=\"_blank\" rel=\"noopener\">Wallace, Byron C., et al. “Class imbalance, redux.” Data Mining (ICDM), 2011 IEEE 11th International Conference on. IEEE, 2011.</a></li>\n<li><a href=\"https://www.google.co.jp/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=3&amp;cad=rja&amp;uact=8&amp;ved=0ahUKEwiVmqXlkprWAhVEsJQKHYJiCf4QFgg8MAI&amp;url=https%3A%2F%2Fcourse.ccs.neu.edu%2Fcs6140sp15%2F4_boosting%2Fslides%2Fwallace_imbalance_icdm_11_for_class_2012_final.pptx&amp;usg=AFQjCNG6GpjKeinzCsXrZWWY1edtbBMgog\" target=\"_blank\" rel=\"noopener\">PPT-Class Imbalance, Redux</a></li>\n</ul>\n","site":{"data":{}},"_categories":[{"name":"reading notes","path":"categories/reading-notes/"}],"_tags":[{"name":"imbalanced data","path":"tags/imbalanced-data/"},{"name":"undersampling","path":"tags/undersampling/"},{"name":"bagging","path":"tags/bagging/"}],"excerpt":"<p>再次感谢优男，向我提出了又一个尖锐的问题，使得我有机会思考和研究，并且最终可以看到这篇paper，并且最后可以分享给大家。</p>\n<p>我个人在工作之中遇到过imbalanced data的问题，我只是直观的感受到，imbalanced data的最后效果往往不是很棒，网上也只是给出了oversampling和undersampling的建议，并没有提及这其中的一些缘故，今天我们一起通过这篇paper来学习学习。<br></p>","more":"</p>\n<h2 id=\"Notes\"><a href=\"#Notes\" class=\"headerlink\" title=\"Notes\"></a>Notes</h2><p>我们假设有positive和negative两类sample，其中positive samples符合\\(P(x)\\)的Guassian分布，negative samples符合\\(G(x)\\)的Guassian分布，分类平面将空间划分成positive region\\(\\cal R^{+} _{w}\\)和negative region\\(\\cal R^{-} _{w}\\)，如下图所示：<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/5/5-1.png\" alt=\"\"><br>图中\\(w^{ *}\\)是理想的分割平面，\\(w^{ *}\\) 应该是使loss最小的取值，即<br>$$w^{<em>}= \\arg\\underset{w}{\\min} \\cal L^{</em>}(w)$$<br>对于loss值，其实就是分类中被错分的fn(false negative)和fp(false positive)的期望值，显然，通过minimun该loss得到的会是图中的\\(w^{<em>}\\)，因为这个分类平面所带来的error明显是最少的。<br>$$\\cal L^{</em>}(w) = \\cal C_{fn} \\int _{\\cal R^{w} <em>{-}} \\it P(x)dx + \\cal C</em>{fp} \\int _{\\cal R^{w} <em>{+}} \\it G(x)dx$$<br>对于整个数据集\\(\\cal D \\)来说，我们假设数据量较少的一类(paper中设定positive类较少)所占比例为\\(\\pi\\)(小于0.5)，那么对于带有比例\\(\\pi\\)的数据集\\(\\cal D</em>{\\pi}\\)，全局期望是<br>$$\\bf E_{\\cal D_{ \\pi}} [\\cal L(w)]=\\pi \\cal C_{fn} \\int _{\\cal R^{w} <em>{-}} \\it P(x)dx + (1- \\pi) \\cal C</em>{fp} \\int _{\\cal R^{w} _{+}} \\it G(x)dx$$<br>此处，我个人的理解是，在两类数据均衡的情况下，全局情况下的期望其实是和上面的loss等价的，但是imbalanced data带来了不均衡的因子\\(\\pi\\)，因此，两个公式不再等价。</p>\n<p>OK，既然不等价，那么问题就来了，paper上说，通过最小化全局期望获得的\\(\\hat w\\)，是向着较少数量类别的样本倾斜，也就是第一幅图中，向较少的postive那边skewed，原因是因为\\( \\cal R _{+} ^{ \\hat w} &lt; \\cal R _{+} ^{w^{*}}\\), 也就是说，\\(\\hat w\\)分割的positive region面积小于\\(w^{*}\\)分割出的面积，面积的减小势必导致分割平面向positive类别方向偏移。</p>\n<p>遗憾的是，关于面积的证明我实在看不明白，也email了一些人，也没有得到一个满意的答案，如果有朋友看明白了的话，<strong>记得留言或者email我！</strong></p>\n<p>到了这里，paper大概介绍了undersampling的裨益，undersampling的核心其实就是消除前面提到的比例\\(\\pi\\)，让它趋近于0.5后，分类平面\\(\\hat w\\)就会趋近于理想分类平面\\(w^{*}\\)。</p>\n<p>这里，作者提出了一个bagging方法，就是多次做undersampling，最后最结果做bagging可以获得更好的效果，如下图<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/5/5-2.png\" alt=\"\"><br>paper还对比了其他的方法，比如Weighted Empirical Cost Minimization(如weighted SVM)和SMOTE方法效果不如bagging undersampling，我上一幅图说明下SMOTE的缺点，更多细节，大家可以详细看看paper，如图：<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/5/5-3.png\" alt=\"\"><br>SMOTE方法是随机选择方向生成新的sample，但是如果新的sample产生了图中位置，则效果不会很好。</p>\n<p>OK，今天就这么多，记得看明白了中间的推导一起分享啊！</p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ul>\n<li><a href=\"https://pdfs.semanticscholar.org/a8ef/5a810099178b70d1490a4e6fc4426b642cde.pdf\" target=\"_blank\" rel=\"noopener\">Wallace, Byron C., et al. “Class imbalance, redux.” Data Mining (ICDM), 2011 IEEE 11th International Conference on. IEEE, 2011.</a></li>\n<li><a href=\"https://www.google.co.jp/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=3&amp;cad=rja&amp;uact=8&amp;ved=0ahUKEwiVmqXlkprWAhVEsJQKHYJiCf4QFgg8MAI&amp;url=https%3A%2F%2Fcourse.ccs.neu.edu%2Fcs6140sp15%2F4_boosting%2Fslides%2Fwallace_imbalance_icdm_11_for_class_2012_final.pptx&amp;usg=AFQjCNG6GpjKeinzCsXrZWWY1edtbBMgog\" target=\"_blank\" rel=\"noopener\">PPT-Class Imbalance, Redux</a></li>\n</ul>"},{"title":"Catalyst Optimization in Spark SQL","date":"2018-09-25T11:52:15.000Z","toc":true,"_content":"![](https://github.com/JoeAsir/blog-image/raw/master/blog/background/app-applications-apps.jpg)\nSpark SQL is one of the most important components of Apache Spark and has become the fundation of Structure Streaming, ML Pipeline, GraphFrames and so on since Spark 2.0. Also, Spark SQL provides SQL queries and DataFrame/Dataset API, both of which are optimized by Catalyst. Let's talk about Catalyst today.\n<!--more-->\nCatalyst Optimizer is a great optimization framework to improve Spark SQL performance, especially query capabilities of Spark SQL and it is written by functional programming construct in Scala. RBO(Rule Based Optimization) and CBO(Cost Based Optimization) are the optimization principles of Catalyst. The concept of Catalyst is familiar to many other query optimizers, so understanding Catalyst may help you learn the working pipeline of other query optimizers.\n## Trees And Rules\nWe will have a quick review of trees and rules. You can learn more about them by the references. \n### Trees\nThe tree is the basic datatype in Catalyst, which is consisted of node objects. Each node has a node type, and some children, which could be none. Let's have an example, the tree for expression x+(1+2) could be translated in Scala as:\n\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/18/18-3.png)\n```scala\nAdd(Attribute(x),Add(Literal(1),Literal(2)))\n```\nActually, most of the SQL query optimization, like Catalyst, would transform the SQL to a huge tree structure as the first step of the optimization. Tree structure could be modified, tailored and optimized easily, also it represents the query plan briefly. What's more, the data can be thrown to every node of the tree by the query plan iteratively. That's why tree datatype is used and introduced firstly in Catalyst.\n\n### Rules\nTrees can be manipulated by the rules. The optimization can be treated as some transformations from one tree to another under some rules we provide. With the help of rules, we can run arbitrary code and many other operations on the input tree. And the Catalyst will know which part of the tree that rules could match or apply, and will automatically skip over the tree that does not match. The rules are all applied by Scala case class, in other words, one rule can match multiple patterns. Let's see an example.\n```Scala\ntree.transform {\n  case Add(Literal(c1),Literal(c2)) => Literal(c1+c2)\n  case Add(left, Literal(0)) => left\n  case Add(Literal(0), right) => right\n}\n```\n\n## Catalyst\nFrom this section, we will know how Catalyst optimizes Spark SQL, which might improve your understanding of how Spark SQL works and how to manipulate it better. There are four plan through the Catalyst, which are Parsed(Unresolved) Logical Plan, Analyzed Logical Plan, Optimized Logical Plan and Physical Plan. You can find all the four plans in your Spark UI. The figure below presents the workflow of Spark SQL, and the Analysis, Logical Optimization and Physical Planning are all working under the Catalyst, and we will go over one by one.\n\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/18/18-2.png)\n### Parser\nThe first stage in the figure above is called Parser. Despite not a part of Catalyst, Parser is also very crucial in the whole optimization. Spark SQL transform SQL queries to an [AST](http://ns.inria.fr/ast/sql/index.html)(Abstract Syntax Tree), also called Parsed Logical Plan in Catalyst,  by [ANTLR](https://www.antlr.org/), which is also used in the Hive, presto and so on. However, DataFrame/Dataset object can be transformed to Parsed Logical Plan by the API.\n### Analysis\nReturned by the Parser, Parsed Logical Plan contains many unresolved attribute reference of relation. For example, you have no idea whether the column name provided is correct or type of the column, either. Spark SQL use Catalyst and Catalog object that tracks the data all the time to resolve the attributes. Looking up relations by name from Catalog, mapping all the named attributes to the input, checking the attributes which match to the same value and giving the unique ID and pushing the type information of the attributes, the Parsed Logical Plan is transformed to Analyzed Logical Plan by Catalyst.\n> Catalogs are named collections of schemas in an SQL-environment. An SQL-environment contains zero or more catalogs. A catalog contains one or more schemas, but always contains a schema named INFORMATION\\_SCHEMA that contains the views and domains of the Information Schema.\n### Logcial Optimization\nLogical Optimization is mainly based on the rules, which is also called RBO(Rules Based Optimization). Lots of rules are provided in this step to accomplish RBO, including constant folding, predicate pushdown, project pruning and so on. As results, the Optimized Logical Plan is returned by Logical Optimization from Analyzed Logical Plan. Some figures below describe these ROB mentioned.\n**Predicate pushdown** can reduce the computation of join operation by filtering unnecessary data before join.\n\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/18/18-4.png)\n**Constant folding** avoids calculating the same operation between constants for each record.\n\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/18/18-5.png)\n**Column Pruning** makes Spark SQL only load data which would be used in the table.\n\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/18/18-6.png)\n### Physical Planning\nSince we get the Optimized Logical Plan, Spark still doesn't know how to execute the it. For instance, Spark knows that there is a join operation, while whether sortMerge Join or Hash Shuffle Join should be invoked. So all the operation would be mapped into a real exection through [SparkStrategy](https://github.com/apache/spark/blob/v2.3.2/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala). Transformed from Opktimized Logical Plan by Physical Planning, Physical Plan would guide the Spark on how to handle the data. In Physical Planning, several Physical Plans are generated from Logical Plan, using physical operator matches the Spark execution engine. Using CBO(Cost Based Optimization), Catalyst will select the best performance one as the Physical Plan. However, the CBO is now only used for join and aggregation algorithms selection. Also, CBO is used in Physical Planning to pipelining projections or filters into single Spark *map()* transformation. What's more, all\n\n### Code Generation\nGetting the Physical Plan from the Physical Planning stage, Spark will translate the AST into Java bytecode to run on each machine. Thanks to quasiquotes, a greate feature of Scala, ASTs would be built by Scala language based on the Physical Plan. And the AST will be send to Scala Compile at runtime to generate \nava bytecode. \n> We use Catalyst to transform a tree representing an expression in SQL to an AST for Scala code to evaluate that expression, and then compile and run the generated code.\n\nIn Spark 2.x, the WholeStageCodeGen has been used in code generation stage, you can read more from [here](https://joeasir.github.io/2018/11/14/spark-second-generation-tungsten-in-spark).\n\nAt last, if you want to see the Logical Plan and Physical Plan of your own Spark SQL, you may do as follows:\n```Scala\n// for Logical Plan\nspark.sql(\"your SQL\").queryExecution\n// for Physical Plan\nspark.sql(\"your SQL\").explain\n```\n\n## References\n* [Deep Dive into Spark SQL’s Catalyst Optimizer](https://databricks.com/blog/2015/04/13/deep-dive-into-spark-sqls-catalyst-optimizer.html)\n* [Spark SQL Optimization – Understanding the Catalyst Optimizer](https://data-flair.training/blogs/spark-sql-optimization-catalyst-optimizer/)\n* [Catalyst Source Code](https://github.com/apache/spark/tree/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst)\n* [Quasiquotes Introduction](https://docs.scala-lang.org/overviews/quasiquotes/intro.html)\n","source":"_posts/spark-catalyst-optimization.md","raw":"---\ntitle: Catalyst Optimization in Spark SQL \ndate: 2018-09-25 19:52:15\ntags: spark\ncategories: spark\ntoc: true\n---\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/background/app-applications-apps.jpg)\nSpark SQL is one of the most important components of Apache Spark and has become the fundation of Structure Streaming, ML Pipeline, GraphFrames and so on since Spark 2.0. Also, Spark SQL provides SQL queries and DataFrame/Dataset API, both of which are optimized by Catalyst. Let's talk about Catalyst today.\n<!--more-->\nCatalyst Optimizer is a great optimization framework to improve Spark SQL performance, especially query capabilities of Spark SQL and it is written by functional programming construct in Scala. RBO(Rule Based Optimization) and CBO(Cost Based Optimization) are the optimization principles of Catalyst. The concept of Catalyst is familiar to many other query optimizers, so understanding Catalyst may help you learn the working pipeline of other query optimizers.\n## Trees And Rules\nWe will have a quick review of trees and rules. You can learn more about them by the references. \n### Trees\nThe tree is the basic datatype in Catalyst, which is consisted of node objects. Each node has a node type, and some children, which could be none. Let's have an example, the tree for expression x+(1+2) could be translated in Scala as:\n\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/18/18-3.png)\n```scala\nAdd(Attribute(x),Add(Literal(1),Literal(2)))\n```\nActually, most of the SQL query optimization, like Catalyst, would transform the SQL to a huge tree structure as the first step of the optimization. Tree structure could be modified, tailored and optimized easily, also it represents the query plan briefly. What's more, the data can be thrown to every node of the tree by the query plan iteratively. That's why tree datatype is used and introduced firstly in Catalyst.\n\n### Rules\nTrees can be manipulated by the rules. The optimization can be treated as some transformations from one tree to another under some rules we provide. With the help of rules, we can run arbitrary code and many other operations on the input tree. And the Catalyst will know which part of the tree that rules could match or apply, and will automatically skip over the tree that does not match. The rules are all applied by Scala case class, in other words, one rule can match multiple patterns. Let's see an example.\n```Scala\ntree.transform {\n  case Add(Literal(c1),Literal(c2)) => Literal(c1+c2)\n  case Add(left, Literal(0)) => left\n  case Add(Literal(0), right) => right\n}\n```\n\n## Catalyst\nFrom this section, we will know how Catalyst optimizes Spark SQL, which might improve your understanding of how Spark SQL works and how to manipulate it better. There are four plan through the Catalyst, which are Parsed(Unresolved) Logical Plan, Analyzed Logical Plan, Optimized Logical Plan and Physical Plan. You can find all the four plans in your Spark UI. The figure below presents the workflow of Spark SQL, and the Analysis, Logical Optimization and Physical Planning are all working under the Catalyst, and we will go over one by one.\n\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/18/18-2.png)\n### Parser\nThe first stage in the figure above is called Parser. Despite not a part of Catalyst, Parser is also very crucial in the whole optimization. Spark SQL transform SQL queries to an [AST](http://ns.inria.fr/ast/sql/index.html)(Abstract Syntax Tree), also called Parsed Logical Plan in Catalyst,  by [ANTLR](https://www.antlr.org/), which is also used in the Hive, presto and so on. However, DataFrame/Dataset object can be transformed to Parsed Logical Plan by the API.\n### Analysis\nReturned by the Parser, Parsed Logical Plan contains many unresolved attribute reference of relation. For example, you have no idea whether the column name provided is correct or type of the column, either. Spark SQL use Catalyst and Catalog object that tracks the data all the time to resolve the attributes. Looking up relations by name from Catalog, mapping all the named attributes to the input, checking the attributes which match to the same value and giving the unique ID and pushing the type information of the attributes, the Parsed Logical Plan is transformed to Analyzed Logical Plan by Catalyst.\n> Catalogs are named collections of schemas in an SQL-environment. An SQL-environment contains zero or more catalogs. A catalog contains one or more schemas, but always contains a schema named INFORMATION\\_SCHEMA that contains the views and domains of the Information Schema.\n### Logcial Optimization\nLogical Optimization is mainly based on the rules, which is also called RBO(Rules Based Optimization). Lots of rules are provided in this step to accomplish RBO, including constant folding, predicate pushdown, project pruning and so on. As results, the Optimized Logical Plan is returned by Logical Optimization from Analyzed Logical Plan. Some figures below describe these ROB mentioned.\n**Predicate pushdown** can reduce the computation of join operation by filtering unnecessary data before join.\n\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/18/18-4.png)\n**Constant folding** avoids calculating the same operation between constants for each record.\n\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/18/18-5.png)\n**Column Pruning** makes Spark SQL only load data which would be used in the table.\n\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/18/18-6.png)\n### Physical Planning\nSince we get the Optimized Logical Plan, Spark still doesn't know how to execute the it. For instance, Spark knows that there is a join operation, while whether sortMerge Join or Hash Shuffle Join should be invoked. So all the operation would be mapped into a real exection through [SparkStrategy](https://github.com/apache/spark/blob/v2.3.2/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala). Transformed from Opktimized Logical Plan by Physical Planning, Physical Plan would guide the Spark on how to handle the data. In Physical Planning, several Physical Plans are generated from Logical Plan, using physical operator matches the Spark execution engine. Using CBO(Cost Based Optimization), Catalyst will select the best performance one as the Physical Plan. However, the CBO is now only used for join and aggregation algorithms selection. Also, CBO is used in Physical Planning to pipelining projections or filters into single Spark *map()* transformation. What's more, all\n\n### Code Generation\nGetting the Physical Plan from the Physical Planning stage, Spark will translate the AST into Java bytecode to run on each machine. Thanks to quasiquotes, a greate feature of Scala, ASTs would be built by Scala language based on the Physical Plan. And the AST will be send to Scala Compile at runtime to generate \nava bytecode. \n> We use Catalyst to transform a tree representing an expression in SQL to an AST for Scala code to evaluate that expression, and then compile and run the generated code.\n\nIn Spark 2.x, the WholeStageCodeGen has been used in code generation stage, you can read more from [here](https://joeasir.github.io/2018/11/14/spark-second-generation-tungsten-in-spark).\n\nAt last, if you want to see the Logical Plan and Physical Plan of your own Spark SQL, you may do as follows:\n```Scala\n// for Logical Plan\nspark.sql(\"your SQL\").queryExecution\n// for Physical Plan\nspark.sql(\"your SQL\").explain\n```\n\n## References\n* [Deep Dive into Spark SQL’s Catalyst Optimizer](https://databricks.com/blog/2015/04/13/deep-dive-into-spark-sqls-catalyst-optimizer.html)\n* [Spark SQL Optimization – Understanding the Catalyst Optimizer](https://data-flair.training/blogs/spark-sql-optimization-catalyst-optimizer/)\n* [Catalyst Source Code](https://github.com/apache/spark/tree/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst)\n* [Quasiquotes Introduction](https://docs.scala-lang.org/overviews/quasiquotes/intro.html)\n","slug":"spark-catalyst-optimization","published":1,"updated":"2020-05-10T06:50:12.531Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cka0wnkwr001lqxot5g0nrxsf","content":"<p><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/background/app-applications-apps.jpg\" alt=\"\"><br>Spark SQL is one of the most important components of Apache Spark and has become the fundation of Structure Streaming, ML Pipeline, GraphFrames and so on since Spark 2.0. Also, Spark SQL provides SQL queries and DataFrame/Dataset API, both of which are optimized by Catalyst. Let’s talk about Catalyst today.<br><a id=\"more\"></a><br>Catalyst Optimizer is a great optimization framework to improve Spark SQL performance, especially query capabilities of Spark SQL and it is written by functional programming construct in Scala. RBO(Rule Based Optimization) and CBO(Cost Based Optimization) are the optimization principles of Catalyst. The concept of Catalyst is familiar to many other query optimizers, so understanding Catalyst may help you learn the working pipeline of other query optimizers.</p>\n<h2 id=\"Trees-And-Rules\"><a href=\"#Trees-And-Rules\" class=\"headerlink\" title=\"Trees And Rules\"></a>Trees And Rules</h2><p>We will have a quick review of trees and rules. You can learn more about them by the references. </p>\n<h3 id=\"Trees\"><a href=\"#Trees\" class=\"headerlink\" title=\"Trees\"></a>Trees</h3><p>The tree is the basic datatype in Catalyst, which is consisted of node objects. Each node has a node type, and some children, which could be none. Let’s have an example, the tree for expression x+(1+2) could be translated in Scala as:</p>\n<p><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/18/18-3.png\" alt=\"\"><br><figure class=\"highlight scala hljs\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"hljs-type\">Add</span>(<span class=\"hljs-type\">Attribute</span>(x),<span class=\"hljs-type\">Add</span>(<span class=\"hljs-type\">Literal</span>(<span class=\"hljs-number\">1</span>),<span class=\"hljs-type\">Literal</span>(<span class=\"hljs-number\">2</span>)))</span><br></pre></td></tr></table></figure></p>\n<p>Actually, most of the SQL query optimization, like Catalyst, would transform the SQL to a huge tree structure as the first step of the optimization. Tree structure could be modified, tailored and optimized easily, also it represents the query plan briefly. What’s more, the data can be thrown to every node of the tree by the query plan iteratively. That’s why tree datatype is used and introduced firstly in Catalyst.</p>\n<h3 id=\"Rules\"><a href=\"#Rules\" class=\"headerlink\" title=\"Rules\"></a>Rules</h3><p>Trees can be manipulated by the rules. The optimization can be treated as some transformations from one tree to another under some rules we provide. With the help of rules, we can run arbitrary code and many other operations on the input tree. And the Catalyst will know which part of the tree that rules could match or apply, and will automatically skip over the tree that does not match. The rules are all applied by Scala case class, in other words, one rule can match multiple patterns. Let’s see an example.<br><figure class=\"highlight scala hljs\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">tree.transform &#123;</span><br><span class=\"line\">  <span class=\"hljs-keyword\">case</span> <span class=\"hljs-type\">Add</span>(<span class=\"hljs-type\">Literal</span>(c1),<span class=\"hljs-type\">Literal</span>(c2)) =&gt; <span class=\"hljs-type\">Literal</span>(c1+c2)</span><br><span class=\"line\">  <span class=\"hljs-keyword\">case</span> <span class=\"hljs-type\">Add</span>(left, <span class=\"hljs-type\">Literal</span>(<span class=\"hljs-number\">0</span>)) =&gt; left</span><br><span class=\"line\">  <span class=\"hljs-keyword\">case</span> <span class=\"hljs-type\">Add</span>(<span class=\"hljs-type\">Literal</span>(<span class=\"hljs-number\">0</span>), right) =&gt; right</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"Catalyst\"><a href=\"#Catalyst\" class=\"headerlink\" title=\"Catalyst\"></a>Catalyst</h2><p>From this section, we will know how Catalyst optimizes Spark SQL, which might improve your understanding of how Spark SQL works and how to manipulate it better. There are four plan through the Catalyst, which are Parsed(Unresolved) Logical Plan, Analyzed Logical Plan, Optimized Logical Plan and Physical Plan. You can find all the four plans in your Spark UI. The figure below presents the workflow of Spark SQL, and the Analysis, Logical Optimization and Physical Planning are all working under the Catalyst, and we will go over one by one.</p>\n<p><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/18/18-2.png\" alt=\"\"></p>\n<h3 id=\"Parser\"><a href=\"#Parser\" class=\"headerlink\" title=\"Parser\"></a>Parser</h3><p>The first stage in the figure above is called Parser. Despite not a part of Catalyst, Parser is also very crucial in the whole optimization. Spark SQL transform SQL queries to an <a href=\"http://ns.inria.fr/ast/sql/index.html\" target=\"_blank\" rel=\"noopener\">AST</a>(Abstract Syntax Tree), also called Parsed Logical Plan in Catalyst,  by <a href=\"https://www.antlr.org/\" target=\"_blank\" rel=\"noopener\">ANTLR</a>, which is also used in the Hive, presto and so on. However, DataFrame/Dataset object can be transformed to Parsed Logical Plan by the API.</p>\n<h3 id=\"Analysis\"><a href=\"#Analysis\" class=\"headerlink\" title=\"Analysis\"></a>Analysis</h3><p>Returned by the Parser, Parsed Logical Plan contains many unresolved attribute reference of relation. For example, you have no idea whether the column name provided is correct or type of the column, either. Spark SQL use Catalyst and Catalog object that tracks the data all the time to resolve the attributes. Looking up relations by name from Catalog, mapping all the named attributes to the input, checking the attributes which match to the same value and giving the unique ID and pushing the type information of the attributes, the Parsed Logical Plan is transformed to Analyzed Logical Plan by Catalyst.</p>\n<blockquote>\n<p>Catalogs are named collections of schemas in an SQL-environment. An SQL-environment contains zero or more catalogs. A catalog contains one or more schemas, but always contains a schema named INFORMATION_SCHEMA that contains the views and domains of the Information Schema.</p>\n</blockquote>\n<h3 id=\"Logcial-Optimization\"><a href=\"#Logcial-Optimization\" class=\"headerlink\" title=\"Logcial Optimization\"></a>Logcial Optimization</h3><p>Logical Optimization is mainly based on the rules, which is also called RBO(Rules Based Optimization). Lots of rules are provided in this step to accomplish RBO, including constant folding, predicate pushdown, project pruning and so on. As results, the Optimized Logical Plan is returned by Logical Optimization from Analyzed Logical Plan. Some figures below describe these ROB mentioned.<br><strong>Predicate pushdown</strong> can reduce the computation of join operation by filtering unnecessary data before join.</p>\n<p><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/18/18-4.png\" alt=\"\"><br><strong>Constant folding</strong> avoids calculating the same operation between constants for each record.</p>\n<p><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/18/18-5.png\" alt=\"\"><br><strong>Column Pruning</strong> makes Spark SQL only load data which would be used in the table.</p>\n<p><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/18/18-6.png\" alt=\"\"></p>\n<h3 id=\"Physical-Planning\"><a href=\"#Physical-Planning\" class=\"headerlink\" title=\"Physical Planning\"></a>Physical Planning</h3><p>Since we get the Optimized Logical Plan, Spark still doesn’t know how to execute the it. For instance, Spark knows that there is a join operation, while whether sortMerge Join or Hash Shuffle Join should be invoked. So all the operation would be mapped into a real exection through <a href=\"https://github.com/apache/spark/blob/v2.3.2/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala\" target=\"_blank\" rel=\"noopener\">SparkStrategy</a>. Transformed from Opktimized Logical Plan by Physical Planning, Physical Plan would guide the Spark on how to handle the data. In Physical Planning, several Physical Plans are generated from Logical Plan, using physical operator matches the Spark execution engine. Using CBO(Cost Based Optimization), Catalyst will select the best performance one as the Physical Plan. However, the CBO is now only used for join and aggregation algorithms selection. Also, CBO is used in Physical Planning to pipelining projections or filters into single Spark <em>map()</em> transformation. What’s more, all</p>\n<h3 id=\"Code-Generation\"><a href=\"#Code-Generation\" class=\"headerlink\" title=\"Code Generation\"></a>Code Generation</h3><p>Getting the Physical Plan from the Physical Planning stage, Spark will translate the AST into Java bytecode to run on each machine. Thanks to quasiquotes, a greate feature of Scala, ASTs would be built by Scala language based on the Physical Plan. And the AST will be send to Scala Compile at runtime to generate<br>ava bytecode. </p>\n<blockquote>\n<p>We use Catalyst to transform a tree representing an expression in SQL to an AST for Scala code to evaluate that expression, and then compile and run the generated code.</p>\n</blockquote>\n<p>In Spark 2.x, the WholeStageCodeGen has been used in code generation stage, you can read more from <a href=\"https://joeasir.github.io/2018/11/14/spark-second-generation-tungsten-in-spark\" target=\"_blank\" rel=\"noopener\">here</a>.</p>\n<p>At last, if you want to see the Logical Plan and Physical Plan of your own Spark SQL, you may do as follows:<br><figure class=\"highlight scala hljs\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"hljs-comment\">// for Logical Plan</span></span><br><span class=\"line\">spark.sql(<span class=\"hljs-string\">\"your SQL\"</span>).queryExecution</span><br><span class=\"line\"><span class=\"hljs-comment\">// for Physical Plan</span></span><br><span class=\"line\">spark.sql(<span class=\"hljs-string\">\"your SQL\"</span>).explain</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h2><ul>\n<li><a href=\"https://databricks.com/blog/2015/04/13/deep-dive-into-spark-sqls-catalyst-optimizer.html\" target=\"_blank\" rel=\"noopener\">Deep Dive into Spark SQL’s Catalyst Optimizer</a></li>\n<li><a href=\"https://data-flair.training/blogs/spark-sql-optimization-catalyst-optimizer/\" target=\"_blank\" rel=\"noopener\">Spark SQL Optimization – Understanding the Catalyst Optimizer</a></li>\n<li><a href=\"https://github.com/apache/spark/tree/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst\" target=\"_blank\" rel=\"noopener\">Catalyst Source Code</a></li>\n<li><a href=\"https://docs.scala-lang.org/overviews/quasiquotes/intro.html\" target=\"_blank\" rel=\"noopener\">Quasiquotes Introduction</a></li>\n</ul>\n","site":{"data":{}},"_categories":[{"name":"spark","path":"categories/spark/"}],"_tags":[{"name":"spark","path":"tags/spark/"}],"excerpt":"<p><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/background/app-applications-apps.jpg\" alt=\"\"><br>Spark SQL is one of the most important components of Apache Spark and has become the fundation of Structure Streaming, ML Pipeline, GraphFrames and so on since Spark 2.0. Also, Spark SQL provides SQL queries and DataFrame/Dataset API, both of which are optimized by Catalyst. Let’s talk about Catalyst today.<br></p>","more":"<br>Catalyst Optimizer is a great optimization framework to improve Spark SQL performance, especially query capabilities of Spark SQL and it is written by functional programming construct in Scala. RBO(Rule Based Optimization) and CBO(Cost Based Optimization) are the optimization principles of Catalyst. The concept of Catalyst is familiar to many other query optimizers, so understanding Catalyst may help you learn the working pipeline of other query optimizers.</p>\n<h2 id=\"Trees-And-Rules\"><a href=\"#Trees-And-Rules\" class=\"headerlink\" title=\"Trees And Rules\"></a>Trees And Rules</h2><p>We will have a quick review of trees and rules. You can learn more about them by the references. </p>\n<h3 id=\"Trees\"><a href=\"#Trees\" class=\"headerlink\" title=\"Trees\"></a>Trees</h3><p>The tree is the basic datatype in Catalyst, which is consisted of node objects. Each node has a node type, and some children, which could be none. Let’s have an example, the tree for expression x+(1+2) could be translated in Scala as:</p>\n<p><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/18/18-3.png\" alt=\"\"><br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"type\">Add</span>(<span class=\"type\">Attribute</span>(x),<span class=\"type\">Add</span>(<span class=\"type\">Literal</span>(<span class=\"number\">1</span>),<span class=\"type\">Literal</span>(<span class=\"number\">2</span>)))</span><br></pre></td></tr></table></figure></p>\n<p>Actually, most of the SQL query optimization, like Catalyst, would transform the SQL to a huge tree structure as the first step of the optimization. Tree structure could be modified, tailored and optimized easily, also it represents the query plan briefly. What’s more, the data can be thrown to every node of the tree by the query plan iteratively. That’s why tree datatype is used and introduced firstly in Catalyst.</p>\n<h3 id=\"Rules\"><a href=\"#Rules\" class=\"headerlink\" title=\"Rules\"></a>Rules</h3><p>Trees can be manipulated by the rules. The optimization can be treated as some transformations from one tree to another under some rules we provide. With the help of rules, we can run arbitrary code and many other operations on the input tree. And the Catalyst will know which part of the tree that rules could match or apply, and will automatically skip over the tree that does not match. The rules are all applied by Scala case class, in other words, one rule can match multiple patterns. Let’s see an example.<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">tree.transform &#123;</span><br><span class=\"line\">  <span class=\"keyword\">case</span> <span class=\"type\">Add</span>(<span class=\"type\">Literal</span>(c1),<span class=\"type\">Literal</span>(c2)) =&gt; <span class=\"type\">Literal</span>(c1+c2)</span><br><span class=\"line\">  <span class=\"keyword\">case</span> <span class=\"type\">Add</span>(left, <span class=\"type\">Literal</span>(<span class=\"number\">0</span>)) =&gt; left</span><br><span class=\"line\">  <span class=\"keyword\">case</span> <span class=\"type\">Add</span>(<span class=\"type\">Literal</span>(<span class=\"number\">0</span>), right) =&gt; right</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"Catalyst\"><a href=\"#Catalyst\" class=\"headerlink\" title=\"Catalyst\"></a>Catalyst</h2><p>From this section, we will know how Catalyst optimizes Spark SQL, which might improve your understanding of how Spark SQL works and how to manipulate it better. There are four plan through the Catalyst, which are Parsed(Unresolved) Logical Plan, Analyzed Logical Plan, Optimized Logical Plan and Physical Plan. You can find all the four plans in your Spark UI. The figure below presents the workflow of Spark SQL, and the Analysis, Logical Optimization and Physical Planning are all working under the Catalyst, and we will go over one by one.</p>\n<p><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/18/18-2.png\" alt=\"\"></p>\n<h3 id=\"Parser\"><a href=\"#Parser\" class=\"headerlink\" title=\"Parser\"></a>Parser</h3><p>The first stage in the figure above is called Parser. Despite not a part of Catalyst, Parser is also very crucial in the whole optimization. Spark SQL transform SQL queries to an <a href=\"http://ns.inria.fr/ast/sql/index.html\" target=\"_blank\" rel=\"noopener\">AST</a>(Abstract Syntax Tree), also called Parsed Logical Plan in Catalyst,  by <a href=\"https://www.antlr.org/\" target=\"_blank\" rel=\"noopener\">ANTLR</a>, which is also used in the Hive, presto and so on. However, DataFrame/Dataset object can be transformed to Parsed Logical Plan by the API.</p>\n<h3 id=\"Analysis\"><a href=\"#Analysis\" class=\"headerlink\" title=\"Analysis\"></a>Analysis</h3><p>Returned by the Parser, Parsed Logical Plan contains many unresolved attribute reference of relation. For example, you have no idea whether the column name provided is correct or type of the column, either. Spark SQL use Catalyst and Catalog object that tracks the data all the time to resolve the attributes. Looking up relations by name from Catalog, mapping all the named attributes to the input, checking the attributes which match to the same value and giving the unique ID and pushing the type information of the attributes, the Parsed Logical Plan is transformed to Analyzed Logical Plan by Catalyst.</p>\n<blockquote>\n<p>Catalogs are named collections of schemas in an SQL-environment. An SQL-environment contains zero or more catalogs. A catalog contains one or more schemas, but always contains a schema named INFORMATION_SCHEMA that contains the views and domains of the Information Schema.</p>\n</blockquote>\n<h3 id=\"Logcial-Optimization\"><a href=\"#Logcial-Optimization\" class=\"headerlink\" title=\"Logcial Optimization\"></a>Logcial Optimization</h3><p>Logical Optimization is mainly based on the rules, which is also called RBO(Rules Based Optimization). Lots of rules are provided in this step to accomplish RBO, including constant folding, predicate pushdown, project pruning and so on. As results, the Optimized Logical Plan is returned by Logical Optimization from Analyzed Logical Plan. Some figures below describe these ROB mentioned.<br><strong>Predicate pushdown</strong> can reduce the computation of join operation by filtering unnecessary data before join.</p>\n<p><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/18/18-4.png\" alt=\"\"><br><strong>Constant folding</strong> avoids calculating the same operation between constants for each record.</p>\n<p><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/18/18-5.png\" alt=\"\"><br><strong>Column Pruning</strong> makes Spark SQL only load data which would be used in the table.</p>\n<p><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/18/18-6.png\" alt=\"\"></p>\n<h3 id=\"Physical-Planning\"><a href=\"#Physical-Planning\" class=\"headerlink\" title=\"Physical Planning\"></a>Physical Planning</h3><p>Since we get the Optimized Logical Plan, Spark still doesn’t know how to execute the it. For instance, Spark knows that there is a join operation, while whether sortMerge Join or Hash Shuffle Join should be invoked. So all the operation would be mapped into a real exection through <a href=\"https://github.com/apache/spark/blob/v2.3.2/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala\" target=\"_blank\" rel=\"noopener\">SparkStrategy</a>. Transformed from Opktimized Logical Plan by Physical Planning, Physical Plan would guide the Spark on how to handle the data. In Physical Planning, several Physical Plans are generated from Logical Plan, using physical operator matches the Spark execution engine. Using CBO(Cost Based Optimization), Catalyst will select the best performance one as the Physical Plan. However, the CBO is now only used for join and aggregation algorithms selection. Also, CBO is used in Physical Planning to pipelining projections or filters into single Spark <em>map()</em> transformation. What’s more, all</p>\n<h3 id=\"Code-Generation\"><a href=\"#Code-Generation\" class=\"headerlink\" title=\"Code Generation\"></a>Code Generation</h3><p>Getting the Physical Plan from the Physical Planning stage, Spark will translate the AST into Java bytecode to run on each machine. Thanks to quasiquotes, a greate feature of Scala, ASTs would be built by Scala language based on the Physical Plan. And the AST will be send to Scala Compile at runtime to generate<br>ava bytecode. </p>\n<blockquote>\n<p>We use Catalyst to transform a tree representing an expression in SQL to an AST for Scala code to evaluate that expression, and then compile and run the generated code.</p>\n</blockquote>\n<p>In Spark 2.x, the WholeStageCodeGen has been used in code generation stage, you can read more from <a href=\"https://joeasir.github.io/2018/11/14/spark-second-generation-tungsten-in-spark\" target=\"_blank\" rel=\"noopener\">here</a>.</p>\n<p>At last, if you want to see the Logical Plan and Physical Plan of your own Spark SQL, you may do as follows:<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// for Logical Plan</span></span><br><span class=\"line\">spark.sql(<span class=\"string\">\"your SQL\"</span>).queryExecution</span><br><span class=\"line\"><span class=\"comment\">// for Physical Plan</span></span><br><span class=\"line\">spark.sql(<span class=\"string\">\"your SQL\"</span>).explain</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h2><ul>\n<li><a href=\"https://databricks.com/blog/2015/04/13/deep-dive-into-spark-sqls-catalyst-optimizer.html\" target=\"_blank\" rel=\"noopener\">Deep Dive into Spark SQL’s Catalyst Optimizer</a></li>\n<li><a href=\"https://data-flair.training/blogs/spark-sql-optimization-catalyst-optimizer/\" target=\"_blank\" rel=\"noopener\">Spark SQL Optimization – Understanding the Catalyst Optimizer</a></li>\n<li><a href=\"https://github.com/apache/spark/tree/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst\" target=\"_blank\" rel=\"noopener\">Catalyst Source Code</a></li>\n<li><a href=\"https://docs.scala-lang.org/overviews/quasiquotes/intro.html\" target=\"_blank\" rel=\"noopener\">Quasiquotes Introduction</a></li>\n</ul>"},{"title":"From Spark RDD to DataFrame/Dataset","date":"2018-09-22T08:49:15.000Z","toc":true,"_content":"![](https://github.com/JoeAsir/blog-image/raw/master/blog/background/blur-close-up-code.jpg)\nThis article presents the relationship between Spark RDD, DataFrame and Dataset, and talks about both the advantages and disadvantages of them. RDD is the fundamental API since the inception of Spark and DataFrame/Dataset API is also pretty popular since Spark 2.0. What's the differences between them and how to decide which API to be imported, let's have a quick look.\n<!--more-->\n## RDD\nRDD (aka Resilient Distributed Dataset) is the most fundamental API, it's so critical that all the computation in Spark are based on it. The RDD is distributed, immutable, type-safed, unstructured, partitioned and low-leveled API, which offers transformations and actions. Let's have a quick review to the attributes of RDD. RDD has some awesome characteristics which make it the foundation of the Apache Spark. Let's take a look and learn about the details one by one.\n### Distributed data abstraction\nThe first attribute of RDD is the logical distributed abstraction, which makes RDD can run over the entire clusters. The RDD can be set across the storage and divided into many partitions so that the lambda function or any computation function you provide will execute on each partition separately. That's really awesome as RDD is the logical distributed abstraction, the computation will be much faster as data is divided and run in parallel on several executors.\n### Resilient and immutable\nRDD is also resilient and immutable. When an RDD is transformed to the next RDD, and then to the third one, and all the RDDs get recorded as a lineage of where they come from. The lineage can also be recorded as the acyclic graph of the RDDs, from which we can recreate any RDD in it when something goes wrong, and that's why RDD is resilient.\nAs for the immutability, when you make a transformation, the original RDD remains unaltered so that you can go back through the acyclic graph and recreate it at any time and point during the execution. Remember, the transformation operation creates a new RDD from the previous one instead of altering the previous one. \n### Compile-time type-safe\nRDD is compiled type-safe and you can name the RDD with particular type briefly. Spark application could be complicated and debugging in the distributed environment could be cumbersome, the compiled type-safe really save time as it could find the type error in the compile time.\n### Unstructured/Structured data\nThe fourth attribute is that data can be unstructured, streaming data from the media for instance, or semi-structured, log files with some particular date, time or url information for example. Since RDD would not care about the structure or schema of the data, it's good for those data without structures. Also, RDD can manipulate structured data, though it doesn't understand the different kinds of types and all depends on how you parse the data.\n### Lazy evaluation\nLazy evaluation in Apache Spark means the execution will not start until an action is triggered. In another word, the RDD will not be loaded and computed until it is necessary. And there are some benefits of lazy evaluation. Lazy evaluation really reduces the time and space complexities and speeds up the whole execution. \n## DataFrame/Dataset\nDataFrame/Dataset, unlike RDD, in high-level API dealing with structured data. Data is organized into named columns in DataFrame and can be manipulated type-safely. In Scala, DataFrame is just an alias for *Dataset[Row]*, and in Java, there is only Dataset API, as for Python and R, there is only DataFrame API provided since Python and R have no compile-time type-safety.\nThere are several benefits of DataFrame/Dataset API. I just want to talk about two of them, which I think are pretty awesome.\n### Static-typing and runtime type-safety\nDataFrame/Dataset presents static-typing and runtime type-safety. You may have a spelling error when you are typing a SQL such as typing *form* rather than *from*, and you would not find the syntax errors until the runtime, however, you will catch these errors at compile time in the DataFrame/Dataset API. Also, as for some analysis errors, the column you queried is not in the schema for example, you can catch these errors when compiling in Dataset while until running in SQL and DataFrame.\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/17/17-1.png)\n### Nice performance\nDataFrame/Dataset API can make the execution more intelligent and efficient. You are telling Spark how-to-do a operation when using RDD, while what-to-do using DataFrame/Dataset. Let's have a look at the example.\n```scala\nrdd.filter{case(project, page, numRequests) => project=='en'}.\n    map{case(_,page,numRequests) => (page, numRequests)}.\n    reduceByKey(_+_).\n    filter{case(page,_) => !isSpecialPage(page)}.\n    take(100).foreach {case (project, requests) => println(s\"projec:$requests\"\")}\n```\nThe code above can be run perfectly without any bug. But think about it, the RDD execute a *filter* followed by *reduceByKey* transformation, which means we filter some data after shuffling the entire data. That's really a waste because shuffle operation is very expensive. However, this problem will be solved in DataFrame/Dataset. Based on the Catalyst, DataFrame/Dataset will optimize the query operation by rules and cost, thus the execution is much smarter than the raw RDD.\n## When to Use\nSince we take a look at the RDD and DataFrame/Dataset, I make a list about when to use RDD and when to use DataFrame/Dataset.\n### When to use RDD\n* When you want more about the low-level control of dataset\n* When you are dealing with some unstructred data\n* When you prefer manipulate data with lambda function\n* When you don't care about schema or structure of data\n\n### When to use DataFrame/Dataset\n* When you are dealing with structured data\n* When you want more code optimization and better performance\n\nAll in all, I do recommend you to use DataFrame/Dataset API as you can for their easy-using and better optimization. Supporting by Catalyst and Tungsten, DataFrame/Dataset can reduce your time of optimization, thus you can pay more attention to the data itself. \n\n## References\n* [A Tale of Three Apache Spark APIs: RDDs, DataFrames, and Datasets](https://databricks.com/blog/2016/07/14/a-tale-of-three-apache-spark-apis-rdds-dataframes-and-datasets.html)\n* [Apache Spark RDD vs DataFrame vs DataSet](https://data-flair.training/blogs/apache-spark-rdd-vs-dataframe-vs-dataset/)\n","source":"_posts/spark-from-rdd-to-dataframe-dataset.md","raw":"---\ntitle: From Spark RDD to DataFrame/Dataset\ndate: 2018-09-22 16:49:15\ntags: spark\ncategories: spark\ntoc: true\n---\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/background/blur-close-up-code.jpg)\nThis article presents the relationship between Spark RDD, DataFrame and Dataset, and talks about both the advantages and disadvantages of them. RDD is the fundamental API since the inception of Spark and DataFrame/Dataset API is also pretty popular since Spark 2.0. What's the differences between them and how to decide which API to be imported, let's have a quick look.\n<!--more-->\n## RDD\nRDD (aka Resilient Distributed Dataset) is the most fundamental API, it's so critical that all the computation in Spark are based on it. The RDD is distributed, immutable, type-safed, unstructured, partitioned and low-leveled API, which offers transformations and actions. Let's have a quick review to the attributes of RDD. RDD has some awesome characteristics which make it the foundation of the Apache Spark. Let's take a look and learn about the details one by one.\n### Distributed data abstraction\nThe first attribute of RDD is the logical distributed abstraction, which makes RDD can run over the entire clusters. The RDD can be set across the storage and divided into many partitions so that the lambda function or any computation function you provide will execute on each partition separately. That's really awesome as RDD is the logical distributed abstraction, the computation will be much faster as data is divided and run in parallel on several executors.\n### Resilient and immutable\nRDD is also resilient and immutable. When an RDD is transformed to the next RDD, and then to the third one, and all the RDDs get recorded as a lineage of where they come from. The lineage can also be recorded as the acyclic graph of the RDDs, from which we can recreate any RDD in it when something goes wrong, and that's why RDD is resilient.\nAs for the immutability, when you make a transformation, the original RDD remains unaltered so that you can go back through the acyclic graph and recreate it at any time and point during the execution. Remember, the transformation operation creates a new RDD from the previous one instead of altering the previous one. \n### Compile-time type-safe\nRDD is compiled type-safe and you can name the RDD with particular type briefly. Spark application could be complicated and debugging in the distributed environment could be cumbersome, the compiled type-safe really save time as it could find the type error in the compile time.\n### Unstructured/Structured data\nThe fourth attribute is that data can be unstructured, streaming data from the media for instance, or semi-structured, log files with some particular date, time or url information for example. Since RDD would not care about the structure or schema of the data, it's good for those data without structures. Also, RDD can manipulate structured data, though it doesn't understand the different kinds of types and all depends on how you parse the data.\n### Lazy evaluation\nLazy evaluation in Apache Spark means the execution will not start until an action is triggered. In another word, the RDD will not be loaded and computed until it is necessary. And there are some benefits of lazy evaluation. Lazy evaluation really reduces the time and space complexities and speeds up the whole execution. \n## DataFrame/Dataset\nDataFrame/Dataset, unlike RDD, in high-level API dealing with structured data. Data is organized into named columns in DataFrame and can be manipulated type-safely. In Scala, DataFrame is just an alias for *Dataset[Row]*, and in Java, there is only Dataset API, as for Python and R, there is only DataFrame API provided since Python and R have no compile-time type-safety.\nThere are several benefits of DataFrame/Dataset API. I just want to talk about two of them, which I think are pretty awesome.\n### Static-typing and runtime type-safety\nDataFrame/Dataset presents static-typing and runtime type-safety. You may have a spelling error when you are typing a SQL such as typing *form* rather than *from*, and you would not find the syntax errors until the runtime, however, you will catch these errors at compile time in the DataFrame/Dataset API. Also, as for some analysis errors, the column you queried is not in the schema for example, you can catch these errors when compiling in Dataset while until running in SQL and DataFrame.\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/17/17-1.png)\n### Nice performance\nDataFrame/Dataset API can make the execution more intelligent and efficient. You are telling Spark how-to-do a operation when using RDD, while what-to-do using DataFrame/Dataset. Let's have a look at the example.\n```scala\nrdd.filter{case(project, page, numRequests) => project=='en'}.\n    map{case(_,page,numRequests) => (page, numRequests)}.\n    reduceByKey(_+_).\n    filter{case(page,_) => !isSpecialPage(page)}.\n    take(100).foreach {case (project, requests) => println(s\"projec:$requests\"\")}\n```\nThe code above can be run perfectly without any bug. But think about it, the RDD execute a *filter* followed by *reduceByKey* transformation, which means we filter some data after shuffling the entire data. That's really a waste because shuffle operation is very expensive. However, this problem will be solved in DataFrame/Dataset. Based on the Catalyst, DataFrame/Dataset will optimize the query operation by rules and cost, thus the execution is much smarter than the raw RDD.\n## When to Use\nSince we take a look at the RDD and DataFrame/Dataset, I make a list about when to use RDD and when to use DataFrame/Dataset.\n### When to use RDD\n* When you want more about the low-level control of dataset\n* When you are dealing with some unstructred data\n* When you prefer manipulate data with lambda function\n* When you don't care about schema or structure of data\n\n### When to use DataFrame/Dataset\n* When you are dealing with structured data\n* When you want more code optimization and better performance\n\nAll in all, I do recommend you to use DataFrame/Dataset API as you can for their easy-using and better optimization. Supporting by Catalyst and Tungsten, DataFrame/Dataset can reduce your time of optimization, thus you can pay more attention to the data itself. \n\n## References\n* [A Tale of Three Apache Spark APIs: RDDs, DataFrames, and Datasets](https://databricks.com/blog/2016/07/14/a-tale-of-three-apache-spark-apis-rdds-dataframes-and-datasets.html)\n* [Apache Spark RDD vs DataFrame vs DataSet](https://data-flair.training/blogs/apache-spark-rdd-vs-dataframe-vs-dataset/)\n","slug":"spark-from-rdd-to-dataframe-dataset","published":1,"updated":"2020-05-10T06:50:12.531Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cka0wnkws001mqxotqva303mg","content":"<p><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/background/blur-close-up-code.jpg\" alt=\"\"><br>This article presents the relationship between Spark RDD, DataFrame and Dataset, and talks about both the advantages and disadvantages of them. RDD is the fundamental API since the inception of Spark and DataFrame/Dataset API is also pretty popular since Spark 2.0. What’s the differences between them and how to decide which API to be imported, let’s have a quick look.<br><a id=\"more\"></a></p>\n<h2 id=\"RDD\"><a href=\"#RDD\" class=\"headerlink\" title=\"RDD\"></a>RDD</h2><p>RDD (aka Resilient Distributed Dataset) is the most fundamental API, it’s so critical that all the computation in Spark are based on it. The RDD is distributed, immutable, type-safed, unstructured, partitioned and low-leveled API, which offers transformations and actions. Let’s have a quick review to the attributes of RDD. RDD has some awesome characteristics which make it the foundation of the Apache Spark. Let’s take a look and learn about the details one by one.</p>\n<h3 id=\"Distributed-data-abstraction\"><a href=\"#Distributed-data-abstraction\" class=\"headerlink\" title=\"Distributed data abstraction\"></a>Distributed data abstraction</h3><p>The first attribute of RDD is the logical distributed abstraction, which makes RDD can run over the entire clusters. The RDD can be set across the storage and divided into many partitions so that the lambda function or any computation function you provide will execute on each partition separately. That’s really awesome as RDD is the logical distributed abstraction, the computation will be much faster as data is divided and run in parallel on several executors.</p>\n<h3 id=\"Resilient-and-immutable\"><a href=\"#Resilient-and-immutable\" class=\"headerlink\" title=\"Resilient and immutable\"></a>Resilient and immutable</h3><p>RDD is also resilient and immutable. When an RDD is transformed to the next RDD, and then to the third one, and all the RDDs get recorded as a lineage of where they come from. The lineage can also be recorded as the acyclic graph of the RDDs, from which we can recreate any RDD in it when something goes wrong, and that’s why RDD is resilient.<br>As for the immutability, when you make a transformation, the original RDD remains unaltered so that you can go back through the acyclic graph and recreate it at any time and point during the execution. Remember, the transformation operation creates a new RDD from the previous one instead of altering the previous one. </p>\n<h3 id=\"Compile-time-type-safe\"><a href=\"#Compile-time-type-safe\" class=\"headerlink\" title=\"Compile-time type-safe\"></a>Compile-time type-safe</h3><p>RDD is compiled type-safe and you can name the RDD with particular type briefly. Spark application could be complicated and debugging in the distributed environment could be cumbersome, the compiled type-safe really save time as it could find the type error in the compile time.</p>\n<h3 id=\"Unstructured-Structured-data\"><a href=\"#Unstructured-Structured-data\" class=\"headerlink\" title=\"Unstructured/Structured data\"></a>Unstructured/Structured data</h3><p>The fourth attribute is that data can be unstructured, streaming data from the media for instance, or semi-structured, log files with some particular date, time or url information for example. Since RDD would not care about the structure or schema of the data, it’s good for those data without structures. Also, RDD can manipulate structured data, though it doesn’t understand the different kinds of types and all depends on how you parse the data.</p>\n<h3 id=\"Lazy-evaluation\"><a href=\"#Lazy-evaluation\" class=\"headerlink\" title=\"Lazy evaluation\"></a>Lazy evaluation</h3><p>Lazy evaluation in Apache Spark means the execution will not start until an action is triggered. In another word, the RDD will not be loaded and computed until it is necessary. And there are some benefits of lazy evaluation. Lazy evaluation really reduces the time and space complexities and speeds up the whole execution. </p>\n<h2 id=\"DataFrame-Dataset\"><a href=\"#DataFrame-Dataset\" class=\"headerlink\" title=\"DataFrame/Dataset\"></a>DataFrame/Dataset</h2><p>DataFrame/Dataset, unlike RDD, in high-level API dealing with structured data. Data is organized into named columns in DataFrame and can be manipulated type-safely. In Scala, DataFrame is just an alias for <em>Dataset[Row]</em>, and in Java, there is only Dataset API, as for Python and R, there is only DataFrame API provided since Python and R have no compile-time type-safety.<br>There are several benefits of DataFrame/Dataset API. I just want to talk about two of them, which I think are pretty awesome.</p>\n<h3 id=\"Static-typing-and-runtime-type-safety\"><a href=\"#Static-typing-and-runtime-type-safety\" class=\"headerlink\" title=\"Static-typing and runtime type-safety\"></a>Static-typing and runtime type-safety</h3><p>DataFrame/Dataset presents static-typing and runtime type-safety. You may have a spelling error when you are typing a SQL such as typing <em>form</em> rather than <em>from</em>, and you would not find the syntax errors until the runtime, however, you will catch these errors at compile time in the DataFrame/Dataset API. Also, as for some analysis errors, the column you queried is not in the schema for example, you can catch these errors when compiling in Dataset while until running in SQL and DataFrame.<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/17/17-1.png\" alt=\"\"></p>\n<h3 id=\"Nice-performance\"><a href=\"#Nice-performance\" class=\"headerlink\" title=\"Nice performance\"></a>Nice performance</h3><p>DataFrame/Dataset API can make the execution more intelligent and efficient. You are telling Spark how-to-do a operation when using RDD, while what-to-do using DataFrame/Dataset. Let’s have a look at the example.<br><figure class=\"highlight scala hljs\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">rdd.filter&#123;<span class=\"hljs-keyword\">case</span>(project, page, numRequests) =&gt; project==<span class=\"hljs-symbol\">'e</span>n'&#125;.</span><br><span class=\"line\">    map&#123;<span class=\"hljs-keyword\">case</span>(_,page,numRequests) =&gt; (page, numRequests)&#125;.</span><br><span class=\"line\">    reduceByKey(_+_).</span><br><span class=\"line\">    filter&#123;<span class=\"hljs-keyword\">case</span>(page,_) =&gt; !isSpecialPage(page)&#125;.</span><br><span class=\"line\">    take(<span class=\"hljs-number\">100</span>).foreach &#123;<span class=\"hljs-keyword\">case</span> (project, requests) =&gt; println(<span class=\"hljs-string\">s\"projec:<span class=\"hljs-subst\">$requests</span>\"</span><span class=\"hljs-string\">\")&#125;</span></span><br></pre></td></tr></table></figure></p>\n<p>The code above can be run perfectly without any bug. But think about it, the RDD execute a <em>filter</em> followed by <em>reduceByKey</em> transformation, which means we filter some data after shuffling the entire data. That’s really a waste because shuffle operation is very expensive. However, this problem will be solved in DataFrame/Dataset. Based on the Catalyst, DataFrame/Dataset will optimize the query operation by rules and cost, thus the execution is much smarter than the raw RDD.</p>\n<h2 id=\"When-to-Use\"><a href=\"#When-to-Use\" class=\"headerlink\" title=\"When to Use\"></a>When to Use</h2><p>Since we take a look at the RDD and DataFrame/Dataset, I make a list about when to use RDD and when to use DataFrame/Dataset.</p>\n<h3 id=\"When-to-use-RDD\"><a href=\"#When-to-use-RDD\" class=\"headerlink\" title=\"When to use RDD\"></a>When to use RDD</h3><ul>\n<li>When you want more about the low-level control of dataset</li>\n<li>When you are dealing with some unstructred data</li>\n<li>When you prefer manipulate data with lambda function</li>\n<li>When you don’t care about schema or structure of data</li>\n</ul>\n<h3 id=\"When-to-use-DataFrame-Dataset\"><a href=\"#When-to-use-DataFrame-Dataset\" class=\"headerlink\" title=\"When to use DataFrame/Dataset\"></a>When to use DataFrame/Dataset</h3><ul>\n<li>When you are dealing with structured data</li>\n<li>When you want more code optimization and better performance</li>\n</ul>\n<p>All in all, I do recommend you to use DataFrame/Dataset API as you can for their easy-using and better optimization. Supporting by Catalyst and Tungsten, DataFrame/Dataset can reduce your time of optimization, thus you can pay more attention to the data itself. </p>\n<h2 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h2><ul>\n<li><a href=\"https://databricks.com/blog/2016/07/14/a-tale-of-three-apache-spark-apis-rdds-dataframes-and-datasets.html\" target=\"_blank\" rel=\"noopener\">A Tale of Three Apache Spark APIs: RDDs, DataFrames, and Datasets</a></li>\n<li><a href=\"https://data-flair.training/blogs/apache-spark-rdd-vs-dataframe-vs-dataset/\" target=\"_blank\" rel=\"noopener\">Apache Spark RDD vs DataFrame vs DataSet</a></li>\n</ul>\n","site":{"data":{}},"_categories":[{"name":"spark","path":"categories/spark/"}],"_tags":[{"name":"spark","path":"tags/spark/"}],"excerpt":"<p><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/background/blur-close-up-code.jpg\" alt=\"\"><br>This article presents the relationship between Spark RDD, DataFrame and Dataset, and talks about both the advantages and disadvantages of them. RDD is the fundamental API since the inception of Spark and DataFrame/Dataset API is also pretty popular since Spark 2.0. What’s the differences between them and how to decide which API to be imported, let’s have a quick look.<br></p>","more":"</p>\n<h2 id=\"RDD\"><a href=\"#RDD\" class=\"headerlink\" title=\"RDD\"></a>RDD</h2><p>RDD (aka Resilient Distributed Dataset) is the most fundamental API, it’s so critical that all the computation in Spark are based on it. The RDD is distributed, immutable, type-safed, unstructured, partitioned and low-leveled API, which offers transformations and actions. Let’s have a quick review to the attributes of RDD. RDD has some awesome characteristics which make it the foundation of the Apache Spark. Let’s take a look and learn about the details one by one.</p>\n<h3 id=\"Distributed-data-abstraction\"><a href=\"#Distributed-data-abstraction\" class=\"headerlink\" title=\"Distributed data abstraction\"></a>Distributed data abstraction</h3><p>The first attribute of RDD is the logical distributed abstraction, which makes RDD can run over the entire clusters. The RDD can be set across the storage and divided into many partitions so that the lambda function or any computation function you provide will execute on each partition separately. That’s really awesome as RDD is the logical distributed abstraction, the computation will be much faster as data is divided and run in parallel on several executors.</p>\n<h3 id=\"Resilient-and-immutable\"><a href=\"#Resilient-and-immutable\" class=\"headerlink\" title=\"Resilient and immutable\"></a>Resilient and immutable</h3><p>RDD is also resilient and immutable. When an RDD is transformed to the next RDD, and then to the third one, and all the RDDs get recorded as a lineage of where they come from. The lineage can also be recorded as the acyclic graph of the RDDs, from which we can recreate any RDD in it when something goes wrong, and that’s why RDD is resilient.<br>As for the immutability, when you make a transformation, the original RDD remains unaltered so that you can go back through the acyclic graph and recreate it at any time and point during the execution. Remember, the transformation operation creates a new RDD from the previous one instead of altering the previous one. </p>\n<h3 id=\"Compile-time-type-safe\"><a href=\"#Compile-time-type-safe\" class=\"headerlink\" title=\"Compile-time type-safe\"></a>Compile-time type-safe</h3><p>RDD is compiled type-safe and you can name the RDD with particular type briefly. Spark application could be complicated and debugging in the distributed environment could be cumbersome, the compiled type-safe really save time as it could find the type error in the compile time.</p>\n<h3 id=\"Unstructured-Structured-data\"><a href=\"#Unstructured-Structured-data\" class=\"headerlink\" title=\"Unstructured/Structured data\"></a>Unstructured/Structured data</h3><p>The fourth attribute is that data can be unstructured, streaming data from the media for instance, or semi-structured, log files with some particular date, time or url information for example. Since RDD would not care about the structure or schema of the data, it’s good for those data without structures. Also, RDD can manipulate structured data, though it doesn’t understand the different kinds of types and all depends on how you parse the data.</p>\n<h3 id=\"Lazy-evaluation\"><a href=\"#Lazy-evaluation\" class=\"headerlink\" title=\"Lazy evaluation\"></a>Lazy evaluation</h3><p>Lazy evaluation in Apache Spark means the execution will not start until an action is triggered. In another word, the RDD will not be loaded and computed until it is necessary. And there are some benefits of lazy evaluation. Lazy evaluation really reduces the time and space complexities and speeds up the whole execution. </p>\n<h2 id=\"DataFrame-Dataset\"><a href=\"#DataFrame-Dataset\" class=\"headerlink\" title=\"DataFrame/Dataset\"></a>DataFrame/Dataset</h2><p>DataFrame/Dataset, unlike RDD, in high-level API dealing with structured data. Data is organized into named columns in DataFrame and can be manipulated type-safely. In Scala, DataFrame is just an alias for <em>Dataset[Row]</em>, and in Java, there is only Dataset API, as for Python and R, there is only DataFrame API provided since Python and R have no compile-time type-safety.<br>There are several benefits of DataFrame/Dataset API. I just want to talk about two of them, which I think are pretty awesome.</p>\n<h3 id=\"Static-typing-and-runtime-type-safety\"><a href=\"#Static-typing-and-runtime-type-safety\" class=\"headerlink\" title=\"Static-typing and runtime type-safety\"></a>Static-typing and runtime type-safety</h3><p>DataFrame/Dataset presents static-typing and runtime type-safety. You may have a spelling error when you are typing a SQL such as typing <em>form</em> rather than <em>from</em>, and you would not find the syntax errors until the runtime, however, you will catch these errors at compile time in the DataFrame/Dataset API. Also, as for some analysis errors, the column you queried is not in the schema for example, you can catch these errors when compiling in Dataset while until running in SQL and DataFrame.<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/17/17-1.png\" alt=\"\"></p>\n<h3 id=\"Nice-performance\"><a href=\"#Nice-performance\" class=\"headerlink\" title=\"Nice performance\"></a>Nice performance</h3><p>DataFrame/Dataset API can make the execution more intelligent and efficient. You are telling Spark how-to-do a operation when using RDD, while what-to-do using DataFrame/Dataset. Let’s have a look at the example.<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">rdd.filter&#123;<span class=\"keyword\">case</span>(project, page, numRequests) =&gt; project==<span class=\"symbol\">'e</span>n'&#125;.</span><br><span class=\"line\">    map&#123;<span class=\"keyword\">case</span>(_,page,numRequests) =&gt; (page, numRequests)&#125;.</span><br><span class=\"line\">    reduceByKey(_+_).</span><br><span class=\"line\">    filter&#123;<span class=\"keyword\">case</span>(page,_) =&gt; !isSpecialPage(page)&#125;.</span><br><span class=\"line\">    take(<span class=\"number\">100</span>).foreach &#123;<span class=\"keyword\">case</span> (project, requests) =&gt; println(<span class=\"string\">s\"projec:<span class=\"subst\">$requests</span>\"</span><span class=\"string\">\")&#125;</span></span><br></pre></td></tr></table></figure></p>\n<p>The code above can be run perfectly without any bug. But think about it, the RDD execute a <em>filter</em> followed by <em>reduceByKey</em> transformation, which means we filter some data after shuffling the entire data. That’s really a waste because shuffle operation is very expensive. However, this problem will be solved in DataFrame/Dataset. Based on the Catalyst, DataFrame/Dataset will optimize the query operation by rules and cost, thus the execution is much smarter than the raw RDD.</p>\n<h2 id=\"When-to-Use\"><a href=\"#When-to-Use\" class=\"headerlink\" title=\"When to Use\"></a>When to Use</h2><p>Since we take a look at the RDD and DataFrame/Dataset, I make a list about when to use RDD and when to use DataFrame/Dataset.</p>\n<h3 id=\"When-to-use-RDD\"><a href=\"#When-to-use-RDD\" class=\"headerlink\" title=\"When to use RDD\"></a>When to use RDD</h3><ul>\n<li>When you want more about the low-level control of dataset</li>\n<li>When you are dealing with some unstructred data</li>\n<li>When you prefer manipulate data with lambda function</li>\n<li>When you don’t care about schema or structure of data</li>\n</ul>\n<h3 id=\"When-to-use-DataFrame-Dataset\"><a href=\"#When-to-use-DataFrame-Dataset\" class=\"headerlink\" title=\"When to use DataFrame/Dataset\"></a>When to use DataFrame/Dataset</h3><ul>\n<li>When you are dealing with structured data</li>\n<li>When you want more code optimization and better performance</li>\n</ul>\n<p>All in all, I do recommend you to use DataFrame/Dataset API as you can for their easy-using and better optimization. Supporting by Catalyst and Tungsten, DataFrame/Dataset can reduce your time of optimization, thus you can pay more attention to the data itself. </p>\n<h2 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h2><ul>\n<li><a href=\"https://databricks.com/blog/2016/07/14/a-tale-of-three-apache-spark-apis-rdds-dataframes-and-datasets.html\" target=\"_blank\" rel=\"noopener\">A Tale of Three Apache Spark APIs: RDDs, DataFrames, and Datasets</a></li>\n<li><a href=\"https://data-flair.training/blogs/apache-spark-rdd-vs-dataframe-vs-dataset/\" target=\"_blank\" rel=\"noopener\">Apache Spark RDD vs DataFrame vs DataSet</a></li>\n</ul>"},{"title":"Spark Tuning","date":"2018-02-23T05:10:32.000Z","_content":"Hi, all, 最近一直在研究spark tuning方面的问题，深感这是一个经验活，也是一个技术活，查阅和很多资料，在这里mark一下。\n<!--more-->\n上次我们review了一下spark的work-flow，主要是基于spark on yarn的，同样的，我们在这里探讨的也主要是基于spark on yarn。\n## Resource Allocation\n### Some Configuration\nResource allocation是spark中一个非常重要的环节，给予一个application过少的resource会带了执行效率的低下和执行速度的缓慢；相反，过多的resource则会带来资源浪费，影响cluster上其他appllication的运行，因此，一个合适的resource allocation是非常非常重要的，我们来看看几个比较重要的parameter：\n* num-executors: 表明spark申请executors的数目，我们可以通过设置spark.dynamicAllocation.enabled来让spark根据数据动态的分配executors，这样可以有效的提高资源利用率；\n* executor-cores: 指定每一个executor的core数目，core数目决定了每个executor的最大并行task数目\n* executor-memory: 指定分配给每一个executor的内存大小。\n\n### Some Tips\n* 对于executor来说，在过于大的memory上运行可能会带来比较高的GC(gabage collection) time，对于一个executor来说，建议给出的上限memory是64G；\n* 由于HDFS在并行读写的时候存在一些瓶颈，因此每一个executor中最好不要超过5个并行任务，即cores数不要超过5个，有实验可以证明，spark在多executor少core的配置下执行效率更高；\n* 相反的，对于executor来说，过分少的core，例如1个，将会使得executors数目变多，例如某个broadcast过程，需要传播到所有的executors上，那么过分多的executors会降低执行的效率。\n\n## Memory Mangement\n关于spark中的memory management，我们先来看一张图：\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/16/16-1.png)\n在图中我们可以看到，spark把memory分成了三部分，即spark memory、user memory和reserved memory，我们顺次来看看：\n### Reserved Memory\n所谓reserved memory，它就是系统预留下的一部分memory，用于存储spark的内部对象，默认大小为300m，绝大部分情况下，我们都不会修改这些参数。值得注意的是，当executor被分配的memory小于1.5倍的reserved memory时，将会抛出“please use larger heap size”的错误。\n### User Memory\nUser memory用于储存spark的transfermation的一些信息，比如RDD之间的依赖信息等等，这部分内存默认大小为(Java Heap - 300M)*0.25，其中的300M其实就是上面提到的reserved memory.具体的大小要依赖于spark.memory.fraction参数，这个参数决定了user 和 下面要讲到的spark memory的分配比例。\n### Spark Memory\n上文已经到了，spark memory主要是spark自己使用的memory部分，这部分的大小依赖于spark.memory.fraction参数，即(Java Heap - 300M)*spark.memory.fraction，其中fraction的default为0.75。\n\nSpark memory主要有两个用途，一是用于spark的shuffle等操作，而是用来cache spark中的RDD，因此spark memory也自然而然的分成了两部分，即负责shuffle操作的execution memory和负责cache的storage memory，两者的大小通过spark.memory.storageFraction参数来分割，默认值是0.5。\n\n在spark memory中，还有一个重要的性质，那就是storage 和 execution memory的共享机制，说的简单一些就是，当一边内存空闲而另一方内存紧张的时候，可以借用对方的内存，我们下面看看在内存出现冲突的时候，spark怎么协调：\n* 当storage占用execution memory的时候，发生execution memory使用紧张的情况时，强制将storage占有的内存释放并归还execution，丢失的数据将会后续重新计算；\n* 当execution占用storage memory的时候，发生storage memory紧张的情况，被占用的内存不会被强制释放，因为这会带来任务丢失，storage会耐心等待知道execution执行完释放出内存。\n\n## Data Serialization\n在整个spark任务中，数据传输都是经过序列化后(serialization)之后传输的，因此数据的序列化是很重要的，冗余的序列化过程会让整个spark任务变慢，spark提供两种序列化方式：\n* Java serialization：这是spark默认的序列化方式，java序列化是一种很经典和稳定的序列化方法，但是最大的缺点就是——慢！\n* Kryo serialization：Kryo 序列化可以让spark任务更加快速，甚至10倍于java序列化；但是它不支持所有的Serializable类型，同时需要为用户自己开发的class进行注册后，才可以使用Kyo.\n\n关于Kryo的详细信息，可以查看[spark documentation](https://spark.apache.org/docs/latest/tuning.html#data-serialization)，或者[Kryo documentation](https://github.com/EsotericSoftware/kryo)\n\n## Summary\n关于spark调优的问题，有很多因素，我也是简单的做了一些了解并分享给大家，除了我提到的，还有诸如GC等等因素，大家可以根据我给出的references做进一步的了解。\n\n## References\n* [How-to: Tune Your Apache Spark Jobs (Part 2)](http://blog.cloudera.com/blog/2015/03/how-to-tune-your-apache-spark-jobs-part-2/)\n* [Spark Documentation-Tuning](https://spark.apache.org/docs/latest/tuning.html)\n* [Karau, Holden, et al. Learning spark: lightning-fast big data analysis. \" O'Reilly Media, Inc.\", 2015.](http://shop.oreilly.com/product/0636920028512.do)\n","source":"_posts/spark-spark-tune.md","raw":"---\ntitle: Spark Tuning\ndate: 2018-02-23 13:10:32\ntags: spark\ncategories: spark\n---\nHi, all, 最近一直在研究spark tuning方面的问题，深感这是一个经验活，也是一个技术活，查阅和很多资料，在这里mark一下。\n<!--more-->\n上次我们review了一下spark的work-flow，主要是基于spark on yarn的，同样的，我们在这里探讨的也主要是基于spark on yarn。\n## Resource Allocation\n### Some Configuration\nResource allocation是spark中一个非常重要的环节，给予一个application过少的resource会带了执行效率的低下和执行速度的缓慢；相反，过多的resource则会带来资源浪费，影响cluster上其他appllication的运行，因此，一个合适的resource allocation是非常非常重要的，我们来看看几个比较重要的parameter：\n* num-executors: 表明spark申请executors的数目，我们可以通过设置spark.dynamicAllocation.enabled来让spark根据数据动态的分配executors，这样可以有效的提高资源利用率；\n* executor-cores: 指定每一个executor的core数目，core数目决定了每个executor的最大并行task数目\n* executor-memory: 指定分配给每一个executor的内存大小。\n\n### Some Tips\n* 对于executor来说，在过于大的memory上运行可能会带来比较高的GC(gabage collection) time，对于一个executor来说，建议给出的上限memory是64G；\n* 由于HDFS在并行读写的时候存在一些瓶颈，因此每一个executor中最好不要超过5个并行任务，即cores数不要超过5个，有实验可以证明，spark在多executor少core的配置下执行效率更高；\n* 相反的，对于executor来说，过分少的core，例如1个，将会使得executors数目变多，例如某个broadcast过程，需要传播到所有的executors上，那么过分多的executors会降低执行的效率。\n\n## Memory Mangement\n关于spark中的memory management，我们先来看一张图：\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/16/16-1.png)\n在图中我们可以看到，spark把memory分成了三部分，即spark memory、user memory和reserved memory，我们顺次来看看：\n### Reserved Memory\n所谓reserved memory，它就是系统预留下的一部分memory，用于存储spark的内部对象，默认大小为300m，绝大部分情况下，我们都不会修改这些参数。值得注意的是，当executor被分配的memory小于1.5倍的reserved memory时，将会抛出“please use larger heap size”的错误。\n### User Memory\nUser memory用于储存spark的transfermation的一些信息，比如RDD之间的依赖信息等等，这部分内存默认大小为(Java Heap - 300M)*0.25，其中的300M其实就是上面提到的reserved memory.具体的大小要依赖于spark.memory.fraction参数，这个参数决定了user 和 下面要讲到的spark memory的分配比例。\n### Spark Memory\n上文已经到了，spark memory主要是spark自己使用的memory部分，这部分的大小依赖于spark.memory.fraction参数，即(Java Heap - 300M)*spark.memory.fraction，其中fraction的default为0.75。\n\nSpark memory主要有两个用途，一是用于spark的shuffle等操作，而是用来cache spark中的RDD，因此spark memory也自然而然的分成了两部分，即负责shuffle操作的execution memory和负责cache的storage memory，两者的大小通过spark.memory.storageFraction参数来分割，默认值是0.5。\n\n在spark memory中，还有一个重要的性质，那就是storage 和 execution memory的共享机制，说的简单一些就是，当一边内存空闲而另一方内存紧张的时候，可以借用对方的内存，我们下面看看在内存出现冲突的时候，spark怎么协调：\n* 当storage占用execution memory的时候，发生execution memory使用紧张的情况时，强制将storage占有的内存释放并归还execution，丢失的数据将会后续重新计算；\n* 当execution占用storage memory的时候，发生storage memory紧张的情况，被占用的内存不会被强制释放，因为这会带来任务丢失，storage会耐心等待知道execution执行完释放出内存。\n\n## Data Serialization\n在整个spark任务中，数据传输都是经过序列化后(serialization)之后传输的，因此数据的序列化是很重要的，冗余的序列化过程会让整个spark任务变慢，spark提供两种序列化方式：\n* Java serialization：这是spark默认的序列化方式，java序列化是一种很经典和稳定的序列化方法，但是最大的缺点就是——慢！\n* Kryo serialization：Kryo 序列化可以让spark任务更加快速，甚至10倍于java序列化；但是它不支持所有的Serializable类型，同时需要为用户自己开发的class进行注册后，才可以使用Kyo.\n\n关于Kryo的详细信息，可以查看[spark documentation](https://spark.apache.org/docs/latest/tuning.html#data-serialization)，或者[Kryo documentation](https://github.com/EsotericSoftware/kryo)\n\n## Summary\n关于spark调优的问题，有很多因素，我也是简单的做了一些了解并分享给大家，除了我提到的，还有诸如GC等等因素，大家可以根据我给出的references做进一步的了解。\n\n## References\n* [How-to: Tune Your Apache Spark Jobs (Part 2)](http://blog.cloudera.com/blog/2015/03/how-to-tune-your-apache-spark-jobs-part-2/)\n* [Spark Documentation-Tuning](https://spark.apache.org/docs/latest/tuning.html)\n* [Karau, Holden, et al. Learning spark: lightning-fast big data analysis. \" O'Reilly Media, Inc.\", 2015.](http://shop.oreilly.com/product/0636920028512.do)\n","slug":"spark-spark-tune","published":1,"updated":"2020-05-10T06:50:12.531Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cka0wnkwu001qqxot0pfge5oz","content":"<p>Hi, all, 最近一直在研究spark tuning方面的问题，深感这是一个经验活，也是一个技术活，查阅和很多资料，在这里mark一下。<br><a id=\"more\"></a><br>上次我们review了一下spark的work-flow，主要是基于spark on yarn的，同样的，我们在这里探讨的也主要是基于spark on yarn。</p>\n<h2 id=\"Resource-Allocation\"><a href=\"#Resource-Allocation\" class=\"headerlink\" title=\"Resource Allocation\"></a>Resource Allocation</h2><h3 id=\"Some-Configuration\"><a href=\"#Some-Configuration\" class=\"headerlink\" title=\"Some Configuration\"></a>Some Configuration</h3><p>Resource allocation是spark中一个非常重要的环节，给予一个application过少的resource会带了执行效率的低下和执行速度的缓慢；相反，过多的resource则会带来资源浪费，影响cluster上其他appllication的运行，因此，一个合适的resource allocation是非常非常重要的，我们来看看几个比较重要的parameter：</p>\n<ul>\n<li>num-executors: 表明spark申请executors的数目，我们可以通过设置spark.dynamicAllocation.enabled来让spark根据数据动态的分配executors，这样可以有效的提高资源利用率；</li>\n<li>executor-cores: 指定每一个executor的core数目，core数目决定了每个executor的最大并行task数目</li>\n<li>executor-memory: 指定分配给每一个executor的内存大小。</li>\n</ul>\n<h3 id=\"Some-Tips\"><a href=\"#Some-Tips\" class=\"headerlink\" title=\"Some Tips\"></a>Some Tips</h3><ul>\n<li>对于executor来说，在过于大的memory上运行可能会带来比较高的GC(gabage collection) time，对于一个executor来说，建议给出的上限memory是64G；</li>\n<li>由于HDFS在并行读写的时候存在一些瓶颈，因此每一个executor中最好不要超过5个并行任务，即cores数不要超过5个，有实验可以证明，spark在多executor少core的配置下执行效率更高；</li>\n<li>相反的，对于executor来说，过分少的core，例如1个，将会使得executors数目变多，例如某个broadcast过程，需要传播到所有的executors上，那么过分多的executors会降低执行的效率。</li>\n</ul>\n<h2 id=\"Memory-Mangement\"><a href=\"#Memory-Mangement\" class=\"headerlink\" title=\"Memory Mangement\"></a>Memory Mangement</h2><p>关于spark中的memory management，我们先来看一张图：<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/16/16-1.png\" alt=\"\"><br>在图中我们可以看到，spark把memory分成了三部分，即spark memory、user memory和reserved memory，我们顺次来看看：</p>\n<h3 id=\"Reserved-Memory\"><a href=\"#Reserved-Memory\" class=\"headerlink\" title=\"Reserved Memory\"></a>Reserved Memory</h3><p>所谓reserved memory，它就是系统预留下的一部分memory，用于存储spark的内部对象，默认大小为300m，绝大部分情况下，我们都不会修改这些参数。值得注意的是，当executor被分配的memory小于1.5倍的reserved memory时，将会抛出“please use larger heap size”的错误。</p>\n<h3 id=\"User-Memory\"><a href=\"#User-Memory\" class=\"headerlink\" title=\"User Memory\"></a>User Memory</h3><p>User memory用于储存spark的transfermation的一些信息，比如RDD之间的依赖信息等等，这部分内存默认大小为(Java Heap - 300M)*0.25，其中的300M其实就是上面提到的reserved memory.具体的大小要依赖于spark.memory.fraction参数，这个参数决定了user 和 下面要讲到的spark memory的分配比例。</p>\n<h3 id=\"Spark-Memory\"><a href=\"#Spark-Memory\" class=\"headerlink\" title=\"Spark Memory\"></a>Spark Memory</h3><p>上文已经到了，spark memory主要是spark自己使用的memory部分，这部分的大小依赖于spark.memory.fraction参数，即(Java Heap - 300M)*spark.memory.fraction，其中fraction的default为0.75。</p>\n<p>Spark memory主要有两个用途，一是用于spark的shuffle等操作，而是用来cache spark中的RDD，因此spark memory也自然而然的分成了两部分，即负责shuffle操作的execution memory和负责cache的storage memory，两者的大小通过spark.memory.storageFraction参数来分割，默认值是0.5。</p>\n<p>在spark memory中，还有一个重要的性质，那就是storage 和 execution memory的共享机制，说的简单一些就是，当一边内存空闲而另一方内存紧张的时候，可以借用对方的内存，我们下面看看在内存出现冲突的时候，spark怎么协调：</p>\n<ul>\n<li>当storage占用execution memory的时候，发生execution memory使用紧张的情况时，强制将storage占有的内存释放并归还execution，丢失的数据将会后续重新计算；</li>\n<li>当execution占用storage memory的时候，发生storage memory紧张的情况，被占用的内存不会被强制释放，因为这会带来任务丢失，storage会耐心等待知道execution执行完释放出内存。</li>\n</ul>\n<h2 id=\"Data-Serialization\"><a href=\"#Data-Serialization\" class=\"headerlink\" title=\"Data Serialization\"></a>Data Serialization</h2><p>在整个spark任务中，数据传输都是经过序列化后(serialization)之后传输的，因此数据的序列化是很重要的，冗余的序列化过程会让整个spark任务变慢，spark提供两种序列化方式：</p>\n<ul>\n<li>Java serialization：这是spark默认的序列化方式，java序列化是一种很经典和稳定的序列化方法，但是最大的缺点就是——慢！</li>\n<li>Kryo serialization：Kryo 序列化可以让spark任务更加快速，甚至10倍于java序列化；但是它不支持所有的Serializable类型，同时需要为用户自己开发的class进行注册后，才可以使用Kyo.</li>\n</ul>\n<p>关于Kryo的详细信息，可以查看<a href=\"https://spark.apache.org/docs/latest/tuning.html#data-serialization\" target=\"_blank\" rel=\"noopener\">spark documentation</a>，或者<a href=\"https://github.com/EsotericSoftware/kryo\" target=\"_blank\" rel=\"noopener\">Kryo documentation</a></p>\n<h2 id=\"Summary\"><a href=\"#Summary\" class=\"headerlink\" title=\"Summary\"></a>Summary</h2><p>关于spark调优的问题，有很多因素，我也是简单的做了一些了解并分享给大家，除了我提到的，还有诸如GC等等因素，大家可以根据我给出的references做进一步的了解。</p>\n<h2 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h2><ul>\n<li><a href=\"http://blog.cloudera.com/blog/2015/03/how-to-tune-your-apache-spark-jobs-part-2/\" target=\"_blank\" rel=\"noopener\">How-to: Tune Your Apache Spark Jobs (Part 2)</a></li>\n<li><a href=\"https://spark.apache.org/docs/latest/tuning.html\" target=\"_blank\" rel=\"noopener\">Spark Documentation-Tuning</a></li>\n<li><a href=\"http://shop.oreilly.com/product/0636920028512.do\" target=\"_blank\" rel=\"noopener\">Karau, Holden, et al. Learning spark: lightning-fast big data analysis. “ O’Reilly Media, Inc.”, 2015.</a></li>\n</ul>\n","site":{"data":{}},"_categories":[{"name":"spark","path":"categories/spark/"}],"_tags":[{"name":"spark","path":"tags/spark/"}],"excerpt":"<p>Hi, all, 最近一直在研究spark tuning方面的问题，深感这是一个经验活，也是一个技术活，查阅和很多资料，在这里mark一下。<br></p>","more":"<br>上次我们review了一下spark的work-flow，主要是基于spark on yarn的，同样的，我们在这里探讨的也主要是基于spark on yarn。</p>\n<h2 id=\"Resource-Allocation\"><a href=\"#Resource-Allocation\" class=\"headerlink\" title=\"Resource Allocation\"></a>Resource Allocation</h2><h3 id=\"Some-Configuration\"><a href=\"#Some-Configuration\" class=\"headerlink\" title=\"Some Configuration\"></a>Some Configuration</h3><p>Resource allocation是spark中一个非常重要的环节，给予一个application过少的resource会带了执行效率的低下和执行速度的缓慢；相反，过多的resource则会带来资源浪费，影响cluster上其他appllication的运行，因此，一个合适的resource allocation是非常非常重要的，我们来看看几个比较重要的parameter：</p>\n<ul>\n<li>num-executors: 表明spark申请executors的数目，我们可以通过设置spark.dynamicAllocation.enabled来让spark根据数据动态的分配executors，这样可以有效的提高资源利用率；</li>\n<li>executor-cores: 指定每一个executor的core数目，core数目决定了每个executor的最大并行task数目</li>\n<li>executor-memory: 指定分配给每一个executor的内存大小。</li>\n</ul>\n<h3 id=\"Some-Tips\"><a href=\"#Some-Tips\" class=\"headerlink\" title=\"Some Tips\"></a>Some Tips</h3><ul>\n<li>对于executor来说，在过于大的memory上运行可能会带来比较高的GC(gabage collection) time，对于一个executor来说，建议给出的上限memory是64G；</li>\n<li>由于HDFS在并行读写的时候存在一些瓶颈，因此每一个executor中最好不要超过5个并行任务，即cores数不要超过5个，有实验可以证明，spark在多executor少core的配置下执行效率更高；</li>\n<li>相反的，对于executor来说，过分少的core，例如1个，将会使得executors数目变多，例如某个broadcast过程，需要传播到所有的executors上，那么过分多的executors会降低执行的效率。</li>\n</ul>\n<h2 id=\"Memory-Mangement\"><a href=\"#Memory-Mangement\" class=\"headerlink\" title=\"Memory Mangement\"></a>Memory Mangement</h2><p>关于spark中的memory management，我们先来看一张图：<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/16/16-1.png\" alt=\"\"><br>在图中我们可以看到，spark把memory分成了三部分，即spark memory、user memory和reserved memory，我们顺次来看看：</p>\n<h3 id=\"Reserved-Memory\"><a href=\"#Reserved-Memory\" class=\"headerlink\" title=\"Reserved Memory\"></a>Reserved Memory</h3><p>所谓reserved memory，它就是系统预留下的一部分memory，用于存储spark的内部对象，默认大小为300m，绝大部分情况下，我们都不会修改这些参数。值得注意的是，当executor被分配的memory小于1.5倍的reserved memory时，将会抛出“please use larger heap size”的错误。</p>\n<h3 id=\"User-Memory\"><a href=\"#User-Memory\" class=\"headerlink\" title=\"User Memory\"></a>User Memory</h3><p>User memory用于储存spark的transfermation的一些信息，比如RDD之间的依赖信息等等，这部分内存默认大小为(Java Heap - 300M)*0.25，其中的300M其实就是上面提到的reserved memory.具体的大小要依赖于spark.memory.fraction参数，这个参数决定了user 和 下面要讲到的spark memory的分配比例。</p>\n<h3 id=\"Spark-Memory\"><a href=\"#Spark-Memory\" class=\"headerlink\" title=\"Spark Memory\"></a>Spark Memory</h3><p>上文已经到了，spark memory主要是spark自己使用的memory部分，这部分的大小依赖于spark.memory.fraction参数，即(Java Heap - 300M)*spark.memory.fraction，其中fraction的default为0.75。</p>\n<p>Spark memory主要有两个用途，一是用于spark的shuffle等操作，而是用来cache spark中的RDD，因此spark memory也自然而然的分成了两部分，即负责shuffle操作的execution memory和负责cache的storage memory，两者的大小通过spark.memory.storageFraction参数来分割，默认值是0.5。</p>\n<p>在spark memory中，还有一个重要的性质，那就是storage 和 execution memory的共享机制，说的简单一些就是，当一边内存空闲而另一方内存紧张的时候，可以借用对方的内存，我们下面看看在内存出现冲突的时候，spark怎么协调：</p>\n<ul>\n<li>当storage占用execution memory的时候，发生execution memory使用紧张的情况时，强制将storage占有的内存释放并归还execution，丢失的数据将会后续重新计算；</li>\n<li>当execution占用storage memory的时候，发生storage memory紧张的情况，被占用的内存不会被强制释放，因为这会带来任务丢失，storage会耐心等待知道execution执行完释放出内存。</li>\n</ul>\n<h2 id=\"Data-Serialization\"><a href=\"#Data-Serialization\" class=\"headerlink\" title=\"Data Serialization\"></a>Data Serialization</h2><p>在整个spark任务中，数据传输都是经过序列化后(serialization)之后传输的，因此数据的序列化是很重要的，冗余的序列化过程会让整个spark任务变慢，spark提供两种序列化方式：</p>\n<ul>\n<li>Java serialization：这是spark默认的序列化方式，java序列化是一种很经典和稳定的序列化方法，但是最大的缺点就是——慢！</li>\n<li>Kryo serialization：Kryo 序列化可以让spark任务更加快速，甚至10倍于java序列化；但是它不支持所有的Serializable类型，同时需要为用户自己开发的class进行注册后，才可以使用Kyo.</li>\n</ul>\n<p>关于Kryo的详细信息，可以查看<a href=\"https://spark.apache.org/docs/latest/tuning.html#data-serialization\" target=\"_blank\" rel=\"noopener\">spark documentation</a>，或者<a href=\"https://github.com/EsotericSoftware/kryo\" target=\"_blank\" rel=\"noopener\">Kryo documentation</a></p>\n<h2 id=\"Summary\"><a href=\"#Summary\" class=\"headerlink\" title=\"Summary\"></a>Summary</h2><p>关于spark调优的问题，有很多因素，我也是简单的做了一些了解并分享给大家，除了我提到的，还有诸如GC等等因素，大家可以根据我给出的references做进一步的了解。</p>\n<h2 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h2><ul>\n<li><a href=\"http://blog.cloudera.com/blog/2015/03/how-to-tune-your-apache-spark-jobs-part-2/\" target=\"_blank\" rel=\"noopener\">How-to: Tune Your Apache Spark Jobs (Part 2)</a></li>\n<li><a href=\"https://spark.apache.org/docs/latest/tuning.html\" target=\"_blank\" rel=\"noopener\">Spark Documentation-Tuning</a></li>\n<li><a href=\"http://shop.oreilly.com/product/0636920028512.do\" target=\"_blank\" rel=\"noopener\">Karau, Holden, et al. Learning spark: lightning-fast big data analysis. “ O’Reilly Media, Inc.”, 2015.</a></li>\n</ul>"},{"title":"Second Generation Tungsten Engine in Spark 2.x","date":"2018-11-14T07:41:15.000Z","toc":true,"_content":"![](https://github.com/JoeAsir/blog-image/raw/master/blog/background/adventure-climb-691668.jpg)\nThis article is about the 2nd generation Tungsten engine, which is the core project to optimize Spark performance. Compared with the 1st generation Tungsten engine, the 2nd one mainly focuses on optimizing query plan and speeding up query execution, which is a pretty aggressive goal to get orders of magnitude faster performance. Let's take a look!\n<!--more-->\n## Project Tungsten\nIn the past several years, both storage and network IO bandwidth have been largely improved, while CPU efficiency bound has not. As results, CPU now becomes the new bottleneck and we have to substantially try to improve the efficiency of memory and CPU and push the performance of Spark closer to the limits of modern hardware, which is the main propose of Tungsten. To achieve it, three initiatives in 1st generation Tungsten engine are proposed, including:\n> * Memory Management and Binary Processing: leveraging application semantics to manage memory explicitly and eliminate the overhead of JVM object model and garbage collection\n* Cache-aware computation: algorithms and data structures to exploit memory hierarchy\n* Code generation: using code generation to exploit modern compilers and CPUs\n\nAs we can see, the first two initiatives mainly foucus on memory, and the last one are for CPUs. In the 2nd generation Tungsten engine, rather than code generation, WholeStageCodeGen and Vectorization are proposed for the order of magnitude faster.\n## WholeStageCodeGen\nAs the name presents, WholeStageCodeGen, aka whole-stage-code-generation, collapses the entire query into a single stage, or maybe a single function. So why combining all the query into a single stage could significantly improve the CPU efficiency and gain performance? We may take a quick look at what it looks like in 1st Tungsten engine.\n### Volcano Iterator Model\nWhat a vividly name! Volcano iterator model, as presents in the figure, would generate an interface for each operator, and the each operator would get the results from its parent one by one, just like the volcano eruption from bottom to top. \n![](https://github.com/JoeAsir/blog-image/raw/master/blog/19/19-1.png)\nAlthough Volcano Model can combine arbitrary operators together without worry about the data type of each operator provides, which makes it a popular classic query evaluation strategy in the past 20 years, there are still many downsides and we will take about it later. \n### Bottom-up Model\nIn [this blog](https://databricks.com/blog/2016/05/23/apache-spark-as-a-compiler-joining-a-billion-rows-per-second-on-a-laptop.html), a hand-written code is proposed to implement the query in the figure above, it's just a so simple for-loop that even a college freshman can complete, which is:\n```scala\nvar count = 0\nfor (ss_item_sk in store_sales) {\n  if (ss_item_sk == 1000) {\n      count += 1\n  }\n}        \n```\nEven though the code is pretty simple, the comparison of performance  between Volcano Iterator Model and Bottom-up Model will do shake you. \n![](https://github.com/JoeAsir/blog-image/raw/master/blog/19/19-2.png)\nBut why is that? Seems it turn out it to be caused by following downsides of Volcano Iterator Model:\n* Too many virtual functions calls:\nIn Volcano Iterator Model, when one operator call for the next operator, a virtual function **next()** would be called. But in Bottom-up Model, there is no virtual function called because all the operations are combined in a single loop function.\n* Intermediate data of Volcano Iterator Model are in memory while of Bottom-up Model are in CPU registers:\nAs one operator transforms data to another one in Volcano Iterator Model, the intermediate data can only be cached in Memory. But for Bottom-up Model, as there is no need to be transformed, data are always in CPU registers.\n* Volcano Iterator Model don't take advantage of modern techniques, which Bottom-up Model do, such as loop-pipelining, loop-unrolling and so on:\nAs Bottom-up Model evaluates the whole query into a single loop function, some modern technique can be used in the Bottom-up Model, I will show you two of them, which are loop-pipelining and loop-unrolling.\n\n#### Loop-pipelining \nIn a loop iteration function, one iteration of loop usually begins when the previous one is complete, which means the iteration of the loop should be executed sequentially one by one. While loop-pipelining can make some differences. The figure below could learn about it clearly. Loop-pipelining increases the parallelism of the loop iteration by implementing a concurrent manner.\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/19/19-3.png)\n#### Loop-unrolling\nLoop-unrolling is another technique to exploit parallelism between loop iterations. Let's learn about it by the code:\n```java\n// without loop-unrolling\nint sum=0;\nfor (int i=0; i<10; i++) {\n    sum+=a[i];\n}\n// with loop-unrolling\nint sum = 0;\nfor (int i=0; i<10; i+=2) {\n    sum += a[i];\n    sum += a[i+1];\n}\n```\nAs shown above, loop-unrolling creates multiple copies of the loop body and also changes the loop iteration counter. Reducing loop iteration number, loop-unrolling also create more operation in each loop iteration and, as results, more parallisim could be exploited. \n\n### Whole Stage Code Generation\nFusing operators together to make the generated code looks like the hand-writing bottom-up model, WholeStageCodeGen makes chains of operators as a single stage, and it has been the alternatives for the Code Generation in Catalyst Optimization. It takes advantages of hand-writing and significantly optimizes the query evaluation and can be easily found in the DAG of your Spark application.\n## Vectorization\nAlthough the WholeStageCodeGen makes a huge optimization of the query plan, there are still some problems. For example, when we import some external integrations, such as tensorflow, scikit-learn, and some python packages, these code cannot be optimized by the WholeStageCodeGen cause they cannot be merged in our code. What's more, the complicated IO cannot be fused, reading Parquet or ORC for instance. How to speed up this excution? The answer is improving the in-core parallelism for an operation of data, so **Vector Processing** and **Column Format** are used in 2nd generation Tungsten engine.\n### Vector Processing\n> In computing, a vector processor or array processor is a central processing unit (CPU) that implements an instruction set containing instructions that operate on one-dimensional arrays of data called vectors, compared to scalar processors, whose instructions operate on single data items.\n\nThe following figure presents the differences between Scalar and Vector Processing. \n![](https://github.com/JoeAsir/blog-image/raw/master/blog/19/19-4.png)\n\nAnd we find out that Vector Processing really fasten the operation. How does Vector Processing be implemented? We may need to learn something about SIMD(Single Instruction, Multiple Data)\n> Single instruction, multiple data (SIMD) is a class of parallel computers in Flynn's taxonomy. It describes computers with multiple processing elements that perform the same operation on multiple data points simultaneously.\n\nLet me show one figure to show what's SIMD breifly.\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/19/19-5.png)\nAs presented above, SIMD can process multuple data via single instruction, and the data are all in an one-dimesional vector. As results, take advantage of SMID, Vector Processing can improve the in-core parallelism and thus make the operation faster. For the sake of the data are all in an one-dimesonal vector in SIMD, Colunm Format could be a better choice for Spark.\n\n### Column Format\nColumn Format has been widely used in many fields, such as disk storage. The following figure shows the differences between Row Format and Column Format.\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/19/19-6.png)\n\n## Summary\nWholeStageCodeGen and Vectorization in 2nd generation Tungsten engine really optimize the query plan and speed up the query execution. Compared with 1st generation Tungsten engine, the 2nd one mainly foucses on improving the CPU parallelism to take advtange of some modern techniques. There are many terms and konwledges about CPU exection, which I learn so little that may make some mistakes in this blog, so it would be very nice of you to figure out any single mistake.\n## References\n* [Project Tungsten: Bringing Apache Spark Closer to Bare Metal](https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html)\n* [Apache Spark as a Compiler: Joining a Billion Rows per Second on a Laptop\nDeep dive into the new Tungsten execution engine](https://databricks.com/blog/2016/05/23/apache-spark-as-a-compiler-joining-a-billion-rows-per-second-on-a-laptop.html)\n* [Spark 2.x - 2nd generation Tungsten Engine](https://spoddutur.github.io/spark-notes/second_generation_tungsten_engine.html)\n* [Loop Pipelining and Loop Unrolling](https://www.xilinx.com/support/documentation/sw_manuals/xilinx2015_2/sdsoc_doc/topics/calling-coding-guidelines/concept_pipelining_loop_unrolling.html#)\n* [Vectorization: Ranger to Stampede Transition](http://www.cac.cornell.edu/education/training/ParallelFall2012/Vectorization.pdf)\n","source":"_posts/spark-second-generation-tungsten-in-spark.md","raw":"---\ntitle: Second Generation Tungsten Engine in Spark 2.x\ndate: 2018-11-14 15:41:15\ntags: spark\ncategories: spark\ntoc: true\n---\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/background/adventure-climb-691668.jpg)\nThis article is about the 2nd generation Tungsten engine, which is the core project to optimize Spark performance. Compared with the 1st generation Tungsten engine, the 2nd one mainly focuses on optimizing query plan and speeding up query execution, which is a pretty aggressive goal to get orders of magnitude faster performance. Let's take a look!\n<!--more-->\n## Project Tungsten\nIn the past several years, both storage and network IO bandwidth have been largely improved, while CPU efficiency bound has not. As results, CPU now becomes the new bottleneck and we have to substantially try to improve the efficiency of memory and CPU and push the performance of Spark closer to the limits of modern hardware, which is the main propose of Tungsten. To achieve it, three initiatives in 1st generation Tungsten engine are proposed, including:\n> * Memory Management and Binary Processing: leveraging application semantics to manage memory explicitly and eliminate the overhead of JVM object model and garbage collection\n* Cache-aware computation: algorithms and data structures to exploit memory hierarchy\n* Code generation: using code generation to exploit modern compilers and CPUs\n\nAs we can see, the first two initiatives mainly foucus on memory, and the last one are for CPUs. In the 2nd generation Tungsten engine, rather than code generation, WholeStageCodeGen and Vectorization are proposed for the order of magnitude faster.\n## WholeStageCodeGen\nAs the name presents, WholeStageCodeGen, aka whole-stage-code-generation, collapses the entire query into a single stage, or maybe a single function. So why combining all the query into a single stage could significantly improve the CPU efficiency and gain performance? We may take a quick look at what it looks like in 1st Tungsten engine.\n### Volcano Iterator Model\nWhat a vividly name! Volcano iterator model, as presents in the figure, would generate an interface for each operator, and the each operator would get the results from its parent one by one, just like the volcano eruption from bottom to top. \n![](https://github.com/JoeAsir/blog-image/raw/master/blog/19/19-1.png)\nAlthough Volcano Model can combine arbitrary operators together without worry about the data type of each operator provides, which makes it a popular classic query evaluation strategy in the past 20 years, there are still many downsides and we will take about it later. \n### Bottom-up Model\nIn [this blog](https://databricks.com/blog/2016/05/23/apache-spark-as-a-compiler-joining-a-billion-rows-per-second-on-a-laptop.html), a hand-written code is proposed to implement the query in the figure above, it's just a so simple for-loop that even a college freshman can complete, which is:\n```scala\nvar count = 0\nfor (ss_item_sk in store_sales) {\n  if (ss_item_sk == 1000) {\n      count += 1\n  }\n}        \n```\nEven though the code is pretty simple, the comparison of performance  between Volcano Iterator Model and Bottom-up Model will do shake you. \n![](https://github.com/JoeAsir/blog-image/raw/master/blog/19/19-2.png)\nBut why is that? Seems it turn out it to be caused by following downsides of Volcano Iterator Model:\n* Too many virtual functions calls:\nIn Volcano Iterator Model, when one operator call for the next operator, a virtual function **next()** would be called. But in Bottom-up Model, there is no virtual function called because all the operations are combined in a single loop function.\n* Intermediate data of Volcano Iterator Model are in memory while of Bottom-up Model are in CPU registers:\nAs one operator transforms data to another one in Volcano Iterator Model, the intermediate data can only be cached in Memory. But for Bottom-up Model, as there is no need to be transformed, data are always in CPU registers.\n* Volcano Iterator Model don't take advantage of modern techniques, which Bottom-up Model do, such as loop-pipelining, loop-unrolling and so on:\nAs Bottom-up Model evaluates the whole query into a single loop function, some modern technique can be used in the Bottom-up Model, I will show you two of them, which are loop-pipelining and loop-unrolling.\n\n#### Loop-pipelining \nIn a loop iteration function, one iteration of loop usually begins when the previous one is complete, which means the iteration of the loop should be executed sequentially one by one. While loop-pipelining can make some differences. The figure below could learn about it clearly. Loop-pipelining increases the parallelism of the loop iteration by implementing a concurrent manner.\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/19/19-3.png)\n#### Loop-unrolling\nLoop-unrolling is another technique to exploit parallelism between loop iterations. Let's learn about it by the code:\n```java\n// without loop-unrolling\nint sum=0;\nfor (int i=0; i<10; i++) {\n    sum+=a[i];\n}\n// with loop-unrolling\nint sum = 0;\nfor (int i=0; i<10; i+=2) {\n    sum += a[i];\n    sum += a[i+1];\n}\n```\nAs shown above, loop-unrolling creates multiple copies of the loop body and also changes the loop iteration counter. Reducing loop iteration number, loop-unrolling also create more operation in each loop iteration and, as results, more parallisim could be exploited. \n\n### Whole Stage Code Generation\nFusing operators together to make the generated code looks like the hand-writing bottom-up model, WholeStageCodeGen makes chains of operators as a single stage, and it has been the alternatives for the Code Generation in Catalyst Optimization. It takes advantages of hand-writing and significantly optimizes the query evaluation and can be easily found in the DAG of your Spark application.\n## Vectorization\nAlthough the WholeStageCodeGen makes a huge optimization of the query plan, there are still some problems. For example, when we import some external integrations, such as tensorflow, scikit-learn, and some python packages, these code cannot be optimized by the WholeStageCodeGen cause they cannot be merged in our code. What's more, the complicated IO cannot be fused, reading Parquet or ORC for instance. How to speed up this excution? The answer is improving the in-core parallelism for an operation of data, so **Vector Processing** and **Column Format** are used in 2nd generation Tungsten engine.\n### Vector Processing\n> In computing, a vector processor or array processor is a central processing unit (CPU) that implements an instruction set containing instructions that operate on one-dimensional arrays of data called vectors, compared to scalar processors, whose instructions operate on single data items.\n\nThe following figure presents the differences between Scalar and Vector Processing. \n![](https://github.com/JoeAsir/blog-image/raw/master/blog/19/19-4.png)\n\nAnd we find out that Vector Processing really fasten the operation. How does Vector Processing be implemented? We may need to learn something about SIMD(Single Instruction, Multiple Data)\n> Single instruction, multiple data (SIMD) is a class of parallel computers in Flynn's taxonomy. It describes computers with multiple processing elements that perform the same operation on multiple data points simultaneously.\n\nLet me show one figure to show what's SIMD breifly.\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/19/19-5.png)\nAs presented above, SIMD can process multuple data via single instruction, and the data are all in an one-dimesional vector. As results, take advantage of SMID, Vector Processing can improve the in-core parallelism and thus make the operation faster. For the sake of the data are all in an one-dimesonal vector in SIMD, Colunm Format could be a better choice for Spark.\n\n### Column Format\nColumn Format has been widely used in many fields, such as disk storage. The following figure shows the differences between Row Format and Column Format.\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/19/19-6.png)\n\n## Summary\nWholeStageCodeGen and Vectorization in 2nd generation Tungsten engine really optimize the query plan and speed up the query execution. Compared with 1st generation Tungsten engine, the 2nd one mainly foucses on improving the CPU parallelism to take advtange of some modern techniques. There are many terms and konwledges about CPU exection, which I learn so little that may make some mistakes in this blog, so it would be very nice of you to figure out any single mistake.\n## References\n* [Project Tungsten: Bringing Apache Spark Closer to Bare Metal](https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html)\n* [Apache Spark as a Compiler: Joining a Billion Rows per Second on a Laptop\nDeep dive into the new Tungsten execution engine](https://databricks.com/blog/2016/05/23/apache-spark-as-a-compiler-joining-a-billion-rows-per-second-on-a-laptop.html)\n* [Spark 2.x - 2nd generation Tungsten Engine](https://spoddutur.github.io/spark-notes/second_generation_tungsten_engine.html)\n* [Loop Pipelining and Loop Unrolling](https://www.xilinx.com/support/documentation/sw_manuals/xilinx2015_2/sdsoc_doc/topics/calling-coding-guidelines/concept_pipelining_loop_unrolling.html#)\n* [Vectorization: Ranger to Stampede Transition](http://www.cac.cornell.edu/education/training/ParallelFall2012/Vectorization.pdf)\n","slug":"spark-second-generation-tungsten-in-spark","published":1,"updated":"2020-05-10T06:50:12.531Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cka0wnkwv001rqxot88ak2bd8","content":"<p><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/background/adventure-climb-691668.jpg\" alt=\"\"><br>This article is about the 2nd generation Tungsten engine, which is the core project to optimize Spark performance. Compared with the 1st generation Tungsten engine, the 2nd one mainly focuses on optimizing query plan and speeding up query execution, which is a pretty aggressive goal to get orders of magnitude faster performance. Let’s take a look!<br><a id=\"more\"></a></p>\n<h2 id=\"Project-Tungsten\"><a href=\"#Project-Tungsten\" class=\"headerlink\" title=\"Project Tungsten\"></a>Project Tungsten</h2><p>In the past several years, both storage and network IO bandwidth have been largely improved, while CPU efficiency bound has not. As results, CPU now becomes the new bottleneck and we have to substantially try to improve the efficiency of memory and CPU and push the performance of Spark closer to the limits of modern hardware, which is the main propose of Tungsten. To achieve it, three initiatives in 1st generation Tungsten engine are proposed, including:</p>\n<blockquote>\n<ul>\n<li>Memory Management and Binary Processing: leveraging application semantics to manage memory explicitly and eliminate the overhead of JVM object model and garbage collection</li>\n<li>Cache-aware computation: algorithms and data structures to exploit memory hierarchy</li>\n<li>Code generation: using code generation to exploit modern compilers and CPUs</li>\n</ul>\n</blockquote>\n<p>As we can see, the first two initiatives mainly foucus on memory, and the last one are for CPUs. In the 2nd generation Tungsten engine, rather than code generation, WholeStageCodeGen and Vectorization are proposed for the order of magnitude faster.</p>\n<h2 id=\"WholeStageCodeGen\"><a href=\"#WholeStageCodeGen\" class=\"headerlink\" title=\"WholeStageCodeGen\"></a>WholeStageCodeGen</h2><p>As the name presents, WholeStageCodeGen, aka whole-stage-code-generation, collapses the entire query into a single stage, or maybe a single function. So why combining all the query into a single stage could significantly improve the CPU efficiency and gain performance? We may take a quick look at what it looks like in 1st Tungsten engine.</p>\n<h3 id=\"Volcano-Iterator-Model\"><a href=\"#Volcano-Iterator-Model\" class=\"headerlink\" title=\"Volcano Iterator Model\"></a>Volcano Iterator Model</h3><p>What a vividly name! Volcano iterator model, as presents in the figure, would generate an interface for each operator, and the each operator would get the results from its parent one by one, just like the volcano eruption from bottom to top.<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/19/19-1.png\" alt=\"\"><br>Although Volcano Model can combine arbitrary operators together without worry about the data type of each operator provides, which makes it a popular classic query evaluation strategy in the past 20 years, there are still many downsides and we will take about it later. </p>\n<h3 id=\"Bottom-up-Model\"><a href=\"#Bottom-up-Model\" class=\"headerlink\" title=\"Bottom-up Model\"></a>Bottom-up Model</h3><p>In <a href=\"https://databricks.com/blog/2016/05/23/apache-spark-as-a-compiler-joining-a-billion-rows-per-second-on-a-laptop.html\" target=\"_blank\" rel=\"noopener\">this blog</a>, a hand-written code is proposed to implement the query in the figure above, it’s just a so simple for-loop that even a college freshman can complete, which is:<br><figure class=\"highlight scala hljs\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"hljs-keyword\">var</span> count = <span class=\"hljs-number\">0</span></span><br><span class=\"line\"><span class=\"hljs-keyword\">for</span> (ss_item_sk in store_sales) &#123;</span><br><span class=\"line\">  <span class=\"hljs-keyword\">if</span> (ss_item_sk == <span class=\"hljs-number\">1000</span>) &#123;</span><br><span class=\"line\">      count += <span class=\"hljs-number\">1</span></span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>Even though the code is pretty simple, the comparison of performance  between Volcano Iterator Model and Bottom-up Model will do shake you.<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/19/19-2.png\" alt=\"\"><br>But why is that? Seems it turn out it to be caused by following downsides of Volcano Iterator Model:</p>\n<ul>\n<li>Too many virtual functions calls:<br>In Volcano Iterator Model, when one operator call for the next operator, a virtual function <strong>next()</strong> would be called. But in Bottom-up Model, there is no virtual function called because all the operations are combined in a single loop function.</li>\n<li>Intermediate data of Volcano Iterator Model are in memory while of Bottom-up Model are in CPU registers:<br>As one operator transforms data to another one in Volcano Iterator Model, the intermediate data can only be cached in Memory. But for Bottom-up Model, as there is no need to be transformed, data are always in CPU registers.</li>\n<li>Volcano Iterator Model don’t take advantage of modern techniques, which Bottom-up Model do, such as loop-pipelining, loop-unrolling and so on:<br>As Bottom-up Model evaluates the whole query into a single loop function, some modern technique can be used in the Bottom-up Model, I will show you two of them, which are loop-pipelining and loop-unrolling.</li>\n</ul>\n<h4 id=\"Loop-pipelining\"><a href=\"#Loop-pipelining\" class=\"headerlink\" title=\"Loop-pipelining\"></a>Loop-pipelining</h4><p>In a loop iteration function, one iteration of loop usually begins when the previous one is complete, which means the iteration of the loop should be executed sequentially one by one. While loop-pipelining can make some differences. The figure below could learn about it clearly. Loop-pipelining increases the parallelism of the loop iteration by implementing a concurrent manner.<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/19/19-3.png\" alt=\"\"></p>\n<h4 id=\"Loop-unrolling\"><a href=\"#Loop-unrolling\" class=\"headerlink\" title=\"Loop-unrolling\"></a>Loop-unrolling</h4><p>Loop-unrolling is another technique to exploit parallelism between loop iterations. Let’s learn about it by the code:<br><figure class=\"highlight java hljs\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"hljs-comment\">// without loop-unrolling</span></span><br><span class=\"line\"><span class=\"hljs-keyword\">int</span> sum=<span class=\"hljs-number\">0</span>;</span><br><span class=\"line\"><span class=\"hljs-keyword\">for</span> (<span class=\"hljs-keyword\">int</span> i=<span class=\"hljs-number\">0</span>; i&lt;<span class=\"hljs-number\">10</span>; i++) &#123;</span><br><span class=\"line\">    sum+=a[i];</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"hljs-comment\">// with loop-unrolling</span></span><br><span class=\"line\"><span class=\"hljs-keyword\">int</span> sum = <span class=\"hljs-number\">0</span>;</span><br><span class=\"line\"><span class=\"hljs-keyword\">for</span> (<span class=\"hljs-keyword\">int</span> i=<span class=\"hljs-number\">0</span>; i&lt;<span class=\"hljs-number\">10</span>; i+=<span class=\"hljs-number\">2</span>) &#123;</span><br><span class=\"line\">    sum += a[i];</span><br><span class=\"line\">    sum += a[i+<span class=\"hljs-number\">1</span>];</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>As shown above, loop-unrolling creates multiple copies of the loop body and also changes the loop iteration counter. Reducing loop iteration number, loop-unrolling also create more operation in each loop iteration and, as results, more parallisim could be exploited. </p>\n<h3 id=\"Whole-Stage-Code-Generation\"><a href=\"#Whole-Stage-Code-Generation\" class=\"headerlink\" title=\"Whole Stage Code Generation\"></a>Whole Stage Code Generation</h3><p>Fusing operators together to make the generated code looks like the hand-writing bottom-up model, WholeStageCodeGen makes chains of operators as a single stage, and it has been the alternatives for the Code Generation in Catalyst Optimization. It takes advantages of hand-writing and significantly optimizes the query evaluation and can be easily found in the DAG of your Spark application.</p>\n<h2 id=\"Vectorization\"><a href=\"#Vectorization\" class=\"headerlink\" title=\"Vectorization\"></a>Vectorization</h2><p>Although the WholeStageCodeGen makes a huge optimization of the query plan, there are still some problems. For example, when we import some external integrations, such as tensorflow, scikit-learn, and some python packages, these code cannot be optimized by the WholeStageCodeGen cause they cannot be merged in our code. What’s more, the complicated IO cannot be fused, reading Parquet or ORC for instance. How to speed up this excution? The answer is improving the in-core parallelism for an operation of data, so <strong>Vector Processing</strong> and <strong>Column Format</strong> are used in 2nd generation Tungsten engine.</p>\n<h3 id=\"Vector-Processing\"><a href=\"#Vector-Processing\" class=\"headerlink\" title=\"Vector Processing\"></a>Vector Processing</h3><blockquote>\n<p>In computing, a vector processor or array processor is a central processing unit (CPU) that implements an instruction set containing instructions that operate on one-dimensional arrays of data called vectors, compared to scalar processors, whose instructions operate on single data items.</p>\n</blockquote>\n<p>The following figure presents the differences between Scalar and Vector Processing.<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/19/19-4.png\" alt=\"\"></p>\n<p>And we find out that Vector Processing really fasten the operation. How does Vector Processing be implemented? We may need to learn something about SIMD(Single Instruction, Multiple Data)</p>\n<blockquote>\n<p>Single instruction, multiple data (SIMD) is a class of parallel computers in Flynn’s taxonomy. It describes computers with multiple processing elements that perform the same operation on multiple data points simultaneously.</p>\n</blockquote>\n<p>Let me show one figure to show what’s SIMD breifly.<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/19/19-5.png\" alt=\"\"><br>As presented above, SIMD can process multuple data via single instruction, and the data are all in an one-dimesional vector. As results, take advantage of SMID, Vector Processing can improve the in-core parallelism and thus make the operation faster. For the sake of the data are all in an one-dimesonal vector in SIMD, Colunm Format could be a better choice for Spark.</p>\n<h3 id=\"Column-Format\"><a href=\"#Column-Format\" class=\"headerlink\" title=\"Column Format\"></a>Column Format</h3><p>Column Format has been widely used in many fields, such as disk storage. The following figure shows the differences between Row Format and Column Format.<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/19/19-6.png\" alt=\"\"></p>\n<h2 id=\"Summary\"><a href=\"#Summary\" class=\"headerlink\" title=\"Summary\"></a>Summary</h2><p>WholeStageCodeGen and Vectorization in 2nd generation Tungsten engine really optimize the query plan and speed up the query execution. Compared with 1st generation Tungsten engine, the 2nd one mainly foucses on improving the CPU parallelism to take advtange of some modern techniques. There are many terms and konwledges about CPU exection, which I learn so little that may make some mistakes in this blog, so it would be very nice of you to figure out any single mistake.</p>\n<h2 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h2><ul>\n<li><a href=\"https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html\" target=\"_blank\" rel=\"noopener\">Project Tungsten: Bringing Apache Spark Closer to Bare Metal</a></li>\n<li><a href=\"https://databricks.com/blog/2016/05/23/apache-spark-as-a-compiler-joining-a-billion-rows-per-second-on-a-laptop.html\" target=\"_blank\" rel=\"noopener\">Apache Spark as a Compiler: Joining a Billion Rows per Second on a Laptop<br>Deep dive into the new Tungsten execution engine</a></li>\n<li><a href=\"https://spoddutur.github.io/spark-notes/second_generation_tungsten_engine.html\" target=\"_blank\" rel=\"noopener\">Spark 2.x - 2nd generation Tungsten Engine</a></li>\n<li><a href=\"https://www.xilinx.com/support/documentation/sw_manuals/xilinx2015_2/sdsoc_doc/topics/calling-coding-guidelines/concept_pipelining_loop_unrolling.html#\" target=\"_blank\" rel=\"noopener\">Loop Pipelining and Loop Unrolling</a></li>\n<li><a href=\"http://www.cac.cornell.edu/education/training/ParallelFall2012/Vectorization.pdf\" target=\"_blank\" rel=\"noopener\">Vectorization: Ranger to Stampede Transition</a></li>\n</ul>\n","site":{"data":{}},"_categories":[{"name":"spark","path":"categories/spark/"}],"_tags":[{"name":"spark","path":"tags/spark/"}],"excerpt":"<p><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/background/adventure-climb-691668.jpg\" alt=\"\"><br>This article is about the 2nd generation Tungsten engine, which is the core project to optimize Spark performance. Compared with the 1st generation Tungsten engine, the 2nd one mainly focuses on optimizing query plan and speeding up query execution, which is a pretty aggressive goal to get orders of magnitude faster performance. Let’s take a look!<br></p>","more":"</p>\n<h2 id=\"Project-Tungsten\"><a href=\"#Project-Tungsten\" class=\"headerlink\" title=\"Project Tungsten\"></a>Project Tungsten</h2><p>In the past several years, both storage and network IO bandwidth have been largely improved, while CPU efficiency bound has not. As results, CPU now becomes the new bottleneck and we have to substantially try to improve the efficiency of memory and CPU and push the performance of Spark closer to the limits of modern hardware, which is the main propose of Tungsten. To achieve it, three initiatives in 1st generation Tungsten engine are proposed, including:</p>\n<blockquote>\n<ul>\n<li>Memory Management and Binary Processing: leveraging application semantics to manage memory explicitly and eliminate the overhead of JVM object model and garbage collection</li>\n<li>Cache-aware computation: algorithms and data structures to exploit memory hierarchy</li>\n<li>Code generation: using code generation to exploit modern compilers and CPUs</li>\n</ul>\n</blockquote>\n<p>As we can see, the first two initiatives mainly foucus on memory, and the last one are for CPUs. In the 2nd generation Tungsten engine, rather than code generation, WholeStageCodeGen and Vectorization are proposed for the order of magnitude faster.</p>\n<h2 id=\"WholeStageCodeGen\"><a href=\"#WholeStageCodeGen\" class=\"headerlink\" title=\"WholeStageCodeGen\"></a>WholeStageCodeGen</h2><p>As the name presents, WholeStageCodeGen, aka whole-stage-code-generation, collapses the entire query into a single stage, or maybe a single function. So why combining all the query into a single stage could significantly improve the CPU efficiency and gain performance? We may take a quick look at what it looks like in 1st Tungsten engine.</p>\n<h3 id=\"Volcano-Iterator-Model\"><a href=\"#Volcano-Iterator-Model\" class=\"headerlink\" title=\"Volcano Iterator Model\"></a>Volcano Iterator Model</h3><p>What a vividly name! Volcano iterator model, as presents in the figure, would generate an interface for each operator, and the each operator would get the results from its parent one by one, just like the volcano eruption from bottom to top.<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/19/19-1.png\" alt=\"\"><br>Although Volcano Model can combine arbitrary operators together without worry about the data type of each operator provides, which makes it a popular classic query evaluation strategy in the past 20 years, there are still many downsides and we will take about it later. </p>\n<h3 id=\"Bottom-up-Model\"><a href=\"#Bottom-up-Model\" class=\"headerlink\" title=\"Bottom-up Model\"></a>Bottom-up Model</h3><p>In <a href=\"https://databricks.com/blog/2016/05/23/apache-spark-as-a-compiler-joining-a-billion-rows-per-second-on-a-laptop.html\" target=\"_blank\" rel=\"noopener\">this blog</a>, a hand-written code is proposed to implement the query in the figure above, it’s just a so simple for-loop that even a college freshman can complete, which is:<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">var</span> count = <span class=\"number\">0</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> (ss_item_sk in store_sales) &#123;</span><br><span class=\"line\">  <span class=\"keyword\">if</span> (ss_item_sk == <span class=\"number\">1000</span>) &#123;</span><br><span class=\"line\">      count += <span class=\"number\">1</span></span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>Even though the code is pretty simple, the comparison of performance  between Volcano Iterator Model and Bottom-up Model will do shake you.<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/19/19-2.png\" alt=\"\"><br>But why is that? Seems it turn out it to be caused by following downsides of Volcano Iterator Model:</p>\n<ul>\n<li>Too many virtual functions calls:<br>In Volcano Iterator Model, when one operator call for the next operator, a virtual function <strong>next()</strong> would be called. But in Bottom-up Model, there is no virtual function called because all the operations are combined in a single loop function.</li>\n<li>Intermediate data of Volcano Iterator Model are in memory while of Bottom-up Model are in CPU registers:<br>As one operator transforms data to another one in Volcano Iterator Model, the intermediate data can only be cached in Memory. But for Bottom-up Model, as there is no need to be transformed, data are always in CPU registers.</li>\n<li>Volcano Iterator Model don’t take advantage of modern techniques, which Bottom-up Model do, such as loop-pipelining, loop-unrolling and so on:<br>As Bottom-up Model evaluates the whole query into a single loop function, some modern technique can be used in the Bottom-up Model, I will show you two of them, which are loop-pipelining and loop-unrolling.</li>\n</ul>\n<h4 id=\"Loop-pipelining\"><a href=\"#Loop-pipelining\" class=\"headerlink\" title=\"Loop-pipelining\"></a>Loop-pipelining</h4><p>In a loop iteration function, one iteration of loop usually begins when the previous one is complete, which means the iteration of the loop should be executed sequentially one by one. While loop-pipelining can make some differences. The figure below could learn about it clearly. Loop-pipelining increases the parallelism of the loop iteration by implementing a concurrent manner.<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/19/19-3.png\" alt=\"\"></p>\n<h4 id=\"Loop-unrolling\"><a href=\"#Loop-unrolling\" class=\"headerlink\" title=\"Loop-unrolling\"></a>Loop-unrolling</h4><p>Loop-unrolling is another technique to exploit parallelism between loop iterations. Let’s learn about it by the code:<br><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// without loop-unrolling</span></span><br><span class=\"line\"><span class=\"keyword\">int</span> sum=<span class=\"number\">0</span>;</span><br><span class=\"line\"><span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> i=<span class=\"number\">0</span>; i&lt;<span class=\"number\">10</span>; i++) &#123;</span><br><span class=\"line\">    sum+=a[i];</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"comment\">// with loop-unrolling</span></span><br><span class=\"line\"><span class=\"keyword\">int</span> sum = <span class=\"number\">0</span>;</span><br><span class=\"line\"><span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> i=<span class=\"number\">0</span>; i&lt;<span class=\"number\">10</span>; i+=<span class=\"number\">2</span>) &#123;</span><br><span class=\"line\">    sum += a[i];</span><br><span class=\"line\">    sum += a[i+<span class=\"number\">1</span>];</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>As shown above, loop-unrolling creates multiple copies of the loop body and also changes the loop iteration counter. Reducing loop iteration number, loop-unrolling also create more operation in each loop iteration and, as results, more parallisim could be exploited. </p>\n<h3 id=\"Whole-Stage-Code-Generation\"><a href=\"#Whole-Stage-Code-Generation\" class=\"headerlink\" title=\"Whole Stage Code Generation\"></a>Whole Stage Code Generation</h3><p>Fusing operators together to make the generated code looks like the hand-writing bottom-up model, WholeStageCodeGen makes chains of operators as a single stage, and it has been the alternatives for the Code Generation in Catalyst Optimization. It takes advantages of hand-writing and significantly optimizes the query evaluation and can be easily found in the DAG of your Spark application.</p>\n<h2 id=\"Vectorization\"><a href=\"#Vectorization\" class=\"headerlink\" title=\"Vectorization\"></a>Vectorization</h2><p>Although the WholeStageCodeGen makes a huge optimization of the query plan, there are still some problems. For example, when we import some external integrations, such as tensorflow, scikit-learn, and some python packages, these code cannot be optimized by the WholeStageCodeGen cause they cannot be merged in our code. What’s more, the complicated IO cannot be fused, reading Parquet or ORC for instance. How to speed up this excution? The answer is improving the in-core parallelism for an operation of data, so <strong>Vector Processing</strong> and <strong>Column Format</strong> are used in 2nd generation Tungsten engine.</p>\n<h3 id=\"Vector-Processing\"><a href=\"#Vector-Processing\" class=\"headerlink\" title=\"Vector Processing\"></a>Vector Processing</h3><blockquote>\n<p>In computing, a vector processor or array processor is a central processing unit (CPU) that implements an instruction set containing instructions that operate on one-dimensional arrays of data called vectors, compared to scalar processors, whose instructions operate on single data items.</p>\n</blockquote>\n<p>The following figure presents the differences between Scalar and Vector Processing.<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/19/19-4.png\" alt=\"\"></p>\n<p>And we find out that Vector Processing really fasten the operation. How does Vector Processing be implemented? We may need to learn something about SIMD(Single Instruction, Multiple Data)</p>\n<blockquote>\n<p>Single instruction, multiple data (SIMD) is a class of parallel computers in Flynn’s taxonomy. It describes computers with multiple processing elements that perform the same operation on multiple data points simultaneously.</p>\n</blockquote>\n<p>Let me show one figure to show what’s SIMD breifly.<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/19/19-5.png\" alt=\"\"><br>As presented above, SIMD can process multuple data via single instruction, and the data are all in an one-dimesional vector. As results, take advantage of SMID, Vector Processing can improve the in-core parallelism and thus make the operation faster. For the sake of the data are all in an one-dimesonal vector in SIMD, Colunm Format could be a better choice for Spark.</p>\n<h3 id=\"Column-Format\"><a href=\"#Column-Format\" class=\"headerlink\" title=\"Column Format\"></a>Column Format</h3><p>Column Format has been widely used in many fields, such as disk storage. The following figure shows the differences between Row Format and Column Format.<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/19/19-6.png\" alt=\"\"></p>\n<h2 id=\"Summary\"><a href=\"#Summary\" class=\"headerlink\" title=\"Summary\"></a>Summary</h2><p>WholeStageCodeGen and Vectorization in 2nd generation Tungsten engine really optimize the query plan and speed up the query execution. Compared with 1st generation Tungsten engine, the 2nd one mainly foucses on improving the CPU parallelism to take advtange of some modern techniques. There are many terms and konwledges about CPU exection, which I learn so little that may make some mistakes in this blog, so it would be very nice of you to figure out any single mistake.</p>\n<h2 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h2><ul>\n<li><a href=\"https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html\" target=\"_blank\" rel=\"noopener\">Project Tungsten: Bringing Apache Spark Closer to Bare Metal</a></li>\n<li><a href=\"https://databricks.com/blog/2016/05/23/apache-spark-as-a-compiler-joining-a-billion-rows-per-second-on-a-laptop.html\" target=\"_blank\" rel=\"noopener\">Apache Spark as a Compiler: Joining a Billion Rows per Second on a Laptop<br>Deep dive into the new Tungsten execution engine</a></li>\n<li><a href=\"https://spoddutur.github.io/spark-notes/second_generation_tungsten_engine.html\" target=\"_blank\" rel=\"noopener\">Spark 2.x - 2nd generation Tungsten Engine</a></li>\n<li><a href=\"https://www.xilinx.com/support/documentation/sw_manuals/xilinx2015_2/sdsoc_doc/topics/calling-coding-guidelines/concept_pipelining_loop_unrolling.html#\" target=\"_blank\" rel=\"noopener\">Loop Pipelining and Loop Unrolling</a></li>\n<li><a href=\"http://www.cac.cornell.edu/education/training/ParallelFall2012/Vectorization.pdf\" target=\"_blank\" rel=\"noopener\">Vectorization: Ranger to Stampede Transition</a></li>\n</ul>"},{"title":"Spark工作流程简析","date":"2018-01-07T10:44:37.000Z","_content":"Hello，有一个月没写blog了感觉很自责，必须整起来！最近由于工作上遇到的一些调优困难，让我对Spark有些敬畏，所以集中的研究了下鬼魅玄学Spark，和大家分享一下。首先先来看看spark的基本工作流程。\n<!--more-->\n## Work Flow\n和hadoop一样，spark也是master-slave机制，Spark通过driver进程，将task分发到多个executors上并发进行计算。整个driver和所有的executors组成了一个spark application，每一个application是运行在cluster manager上的，Spark本身集成了standalone cluster，当然，Spark还可以运行在赫赫有名的YARN和Mesos上。我平时使用的公司集群都是基于YARN cluster manager的，因此本文重点探讨基于YARN的spark。\n\n下图就是spark在cluster manager下的整体工作流程。\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/15/15-1.png)\n\n## The Driver\nDriver是整个application最核心的部分，他运行的是application的main方法，它伴随这整个application的生命周期，driver进程的结束就会带来整个application的结束。\n\n对于所有的Spark任务，他们其实都是实现RDD的transformation和action操作，而这些操作，最后是需要driver将他们转化和分发成tasks，然后才可以去执行。所有的user program都会被driver通过DAG(directed acyclic graph)转化成实际的tasks执行计划，除此之外，driver还会在tasks执行的期间，监控executor上的tasks，并且保证他们拥有健康而合理的资源。\n\n## Executors\nExecutors是Spark application的执行者，他们也是伴随着application的生命周期而存在的，值得注意的是，**Spark job在executors执行失败的情况下依然可以继续进行**。Executors会对具体的tasks的执行结果返回给driver，同时给缓存的RDD提供存储空间。\n\n## Some terms\n* Job: Job是executor层面最大的执行单元，job通过RDD的action操作来分割，每一个action操作就会进行一次job的划分；\n* Stage: Stage是包含在job中的执行单元，stage通过RDD的shuffle操作来分割，每进行一次shuffle操作，就会进行一次stage的划分；\n* Task: Task是executor执行中最细的执行单元，task的数目取和parent RDD的partition数目是一一对应的。\n\n## Spark on Yarn-cluster\n下面，我们一起看看整个Spark application中，driver和executors的都会起到什么作用。我以基于yarn-cluster的YARN的Spark作为例子来简述整个流程，先看一张图：\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/15/15-2.png)\n首先我们要明确一些YARN的概念，YARN是与master-slaver的一个Cluster Manager， 在YARN中，RM(ResourseManager)负责整个调度分发，即我们常说的master；而NM(NodeManager)任务分发的接受者，负责执行具体的任务，也就是我们所说的worker。这些概念后续我专门介绍YARN的时候会详细的说明，他们的作用都是实现spark和YARN之间诸如资源申请等操作。\n\n首先Client向ResourceManager发出提交application的请求，ResourseManager会在某一个NodeManager上启动AppManager进程，AppManager会随后启动driver，并将driver申请containers资源的信息发给ResourceManager，申请完成后，ResourceManager将资源分配消息传递给AppManager并由它启动container，每一个container中只运行一个spark executor，由此完成了资源的申请和分配。\n\n然后整个application开始执行，在这个过程中，根据RDD的transformation或者action，driver把这些任务以tasks的形式，源源不断的传送给executors，于是executors不停地进行计算和存储的任务。当driver结束的时候，他会结束掉executors并且释放掉资源。这就是yarn-cluster上spark的整体工作流程。\n\n除了yarn-cluster，还有一种yarn-client的方法，这种方法唯一的区别在于，他的driver并非运行在某个NodeManager上，而是一直运行在client中。这样的问题就是client一旦关闭，那么整个任务也就随之停止执行。因此相较而言，yarn-cluster更适合线上任务，而yarn-client更适合调试模式。\n\n## Reference\n* [Karau, Holden, et al. Learning spark: lightning-fast big data analysis. \" O'Reilly Media, Inc.\", 2015.](http://shop.oreilly.com/product/0636920028512.do)\n* [Spark:Yarn-cluster和Yarn-client区别与联系](https://www.iteblog.com/archives/1223.html)\n","source":"_posts/spark-spark-workflow.md","raw":"---\ntitle: Spark工作流程简析\ndate: 2018-01-07 18:44:37\ntags: spark\ncategories: spark\n---\nHello，有一个月没写blog了感觉很自责，必须整起来！最近由于工作上遇到的一些调优困难，让我对Spark有些敬畏，所以集中的研究了下鬼魅玄学Spark，和大家分享一下。首先先来看看spark的基本工作流程。\n<!--more-->\n## Work Flow\n和hadoop一样，spark也是master-slave机制，Spark通过driver进程，将task分发到多个executors上并发进行计算。整个driver和所有的executors组成了一个spark application，每一个application是运行在cluster manager上的，Spark本身集成了standalone cluster，当然，Spark还可以运行在赫赫有名的YARN和Mesos上。我平时使用的公司集群都是基于YARN cluster manager的，因此本文重点探讨基于YARN的spark。\n\n下图就是spark在cluster manager下的整体工作流程。\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/15/15-1.png)\n\n## The Driver\nDriver是整个application最核心的部分，他运行的是application的main方法，它伴随这整个application的生命周期，driver进程的结束就会带来整个application的结束。\n\n对于所有的Spark任务，他们其实都是实现RDD的transformation和action操作，而这些操作，最后是需要driver将他们转化和分发成tasks，然后才可以去执行。所有的user program都会被driver通过DAG(directed acyclic graph)转化成实际的tasks执行计划，除此之外，driver还会在tasks执行的期间，监控executor上的tasks，并且保证他们拥有健康而合理的资源。\n\n## Executors\nExecutors是Spark application的执行者，他们也是伴随着application的生命周期而存在的，值得注意的是，**Spark job在executors执行失败的情况下依然可以继续进行**。Executors会对具体的tasks的执行结果返回给driver，同时给缓存的RDD提供存储空间。\n\n## Some terms\n* Job: Job是executor层面最大的执行单元，job通过RDD的action操作来分割，每一个action操作就会进行一次job的划分；\n* Stage: Stage是包含在job中的执行单元，stage通过RDD的shuffle操作来分割，每进行一次shuffle操作，就会进行一次stage的划分；\n* Task: Task是executor执行中最细的执行单元，task的数目取和parent RDD的partition数目是一一对应的。\n\n## Spark on Yarn-cluster\n下面，我们一起看看整个Spark application中，driver和executors的都会起到什么作用。我以基于yarn-cluster的YARN的Spark作为例子来简述整个流程，先看一张图：\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/15/15-2.png)\n首先我们要明确一些YARN的概念，YARN是与master-slaver的一个Cluster Manager， 在YARN中，RM(ResourseManager)负责整个调度分发，即我们常说的master；而NM(NodeManager)任务分发的接受者，负责执行具体的任务，也就是我们所说的worker。这些概念后续我专门介绍YARN的时候会详细的说明，他们的作用都是实现spark和YARN之间诸如资源申请等操作。\n\n首先Client向ResourceManager发出提交application的请求，ResourseManager会在某一个NodeManager上启动AppManager进程，AppManager会随后启动driver，并将driver申请containers资源的信息发给ResourceManager，申请完成后，ResourceManager将资源分配消息传递给AppManager并由它启动container，每一个container中只运行一个spark executor，由此完成了资源的申请和分配。\n\n然后整个application开始执行，在这个过程中，根据RDD的transformation或者action，driver把这些任务以tasks的形式，源源不断的传送给executors，于是executors不停地进行计算和存储的任务。当driver结束的时候，他会结束掉executors并且释放掉资源。这就是yarn-cluster上spark的整体工作流程。\n\n除了yarn-cluster，还有一种yarn-client的方法，这种方法唯一的区别在于，他的driver并非运行在某个NodeManager上，而是一直运行在client中。这样的问题就是client一旦关闭，那么整个任务也就随之停止执行。因此相较而言，yarn-cluster更适合线上任务，而yarn-client更适合调试模式。\n\n## Reference\n* [Karau, Holden, et al. Learning spark: lightning-fast big data analysis. \" O'Reilly Media, Inc.\", 2015.](http://shop.oreilly.com/product/0636920028512.do)\n* [Spark:Yarn-cluster和Yarn-client区别与联系](https://www.iteblog.com/archives/1223.html)\n","slug":"spark-spark-workflow","published":1,"updated":"2020-05-10T06:50:12.532Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cka0wnkww001uqxotc2k6t9ja","content":"<p>Hello，有一个月没写blog了感觉很自责，必须整起来！最近由于工作上遇到的一些调优困难，让我对Spark有些敬畏，所以集中的研究了下鬼魅玄学Spark，和大家分享一下。首先先来看看spark的基本工作流程。<br><a id=\"more\"></a></p>\n<h2 id=\"Work-Flow\"><a href=\"#Work-Flow\" class=\"headerlink\" title=\"Work Flow\"></a>Work Flow</h2><p>和hadoop一样，spark也是master-slave机制，Spark通过driver进程，将task分发到多个executors上并发进行计算。整个driver和所有的executors组成了一个spark application，每一个application是运行在cluster manager上的，Spark本身集成了standalone cluster，当然，Spark还可以运行在赫赫有名的YARN和Mesos上。我平时使用的公司集群都是基于YARN cluster manager的，因此本文重点探讨基于YARN的spark。</p>\n<p>下图就是spark在cluster manager下的整体工作流程。<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/15/15-1.png\" alt=\"\"></p>\n<h2 id=\"The-Driver\"><a href=\"#The-Driver\" class=\"headerlink\" title=\"The Driver\"></a>The Driver</h2><p>Driver是整个application最核心的部分，他运行的是application的main方法，它伴随这整个application的生命周期，driver进程的结束就会带来整个application的结束。</p>\n<p>对于所有的Spark任务，他们其实都是实现RDD的transformation和action操作，而这些操作，最后是需要driver将他们转化和分发成tasks，然后才可以去执行。所有的user program都会被driver通过DAG(directed acyclic graph)转化成实际的tasks执行计划，除此之外，driver还会在tasks执行的期间，监控executor上的tasks，并且保证他们拥有健康而合理的资源。</p>\n<h2 id=\"Executors\"><a href=\"#Executors\" class=\"headerlink\" title=\"Executors\"></a>Executors</h2><p>Executors是Spark application的执行者，他们也是伴随着application的生命周期而存在的，值得注意的是，<strong>Spark job在executors执行失败的情况下依然可以继续进行</strong>。Executors会对具体的tasks的执行结果返回给driver，同时给缓存的RDD提供存储空间。</p>\n<h2 id=\"Some-terms\"><a href=\"#Some-terms\" class=\"headerlink\" title=\"Some terms\"></a>Some terms</h2><ul>\n<li>Job: Job是executor层面最大的执行单元，job通过RDD的action操作来分割，每一个action操作就会进行一次job的划分；</li>\n<li>Stage: Stage是包含在job中的执行单元，stage通过RDD的shuffle操作来分割，每进行一次shuffle操作，就会进行一次stage的划分；</li>\n<li>Task: Task是executor执行中最细的执行单元，task的数目取和parent RDD的partition数目是一一对应的。</li>\n</ul>\n<h2 id=\"Spark-on-Yarn-cluster\"><a href=\"#Spark-on-Yarn-cluster\" class=\"headerlink\" title=\"Spark on Yarn-cluster\"></a>Spark on Yarn-cluster</h2><p>下面，我们一起看看整个Spark application中，driver和executors的都会起到什么作用。我以基于yarn-cluster的YARN的Spark作为例子来简述整个流程，先看一张图：<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/15/15-2.png\" alt=\"\"><br>首先我们要明确一些YARN的概念，YARN是与master-slaver的一个Cluster Manager， 在YARN中，RM(ResourseManager)负责整个调度分发，即我们常说的master；而NM(NodeManager)任务分发的接受者，负责执行具体的任务，也就是我们所说的worker。这些概念后续我专门介绍YARN的时候会详细的说明，他们的作用都是实现spark和YARN之间诸如资源申请等操作。</p>\n<p>首先Client向ResourceManager发出提交application的请求，ResourseManager会在某一个NodeManager上启动AppManager进程，AppManager会随后启动driver，并将driver申请containers资源的信息发给ResourceManager，申请完成后，ResourceManager将资源分配消息传递给AppManager并由它启动container，每一个container中只运行一个spark executor，由此完成了资源的申请和分配。</p>\n<p>然后整个application开始执行，在这个过程中，根据RDD的transformation或者action，driver把这些任务以tasks的形式，源源不断的传送给executors，于是executors不停地进行计算和存储的任务。当driver结束的时候，他会结束掉executors并且释放掉资源。这就是yarn-cluster上spark的整体工作流程。</p>\n<p>除了yarn-cluster，还有一种yarn-client的方法，这种方法唯一的区别在于，他的driver并非运行在某个NodeManager上，而是一直运行在client中。这样的问题就是client一旦关闭，那么整个任务也就随之停止执行。因此相较而言，yarn-cluster更适合线上任务，而yarn-client更适合调试模式。</p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ul>\n<li><a href=\"http://shop.oreilly.com/product/0636920028512.do\" target=\"_blank\" rel=\"noopener\">Karau, Holden, et al. Learning spark: lightning-fast big data analysis. “ O’Reilly Media, Inc.”, 2015.</a></li>\n<li><a href=\"https://www.iteblog.com/archives/1223.html\" target=\"_blank\" rel=\"noopener\">Spark:Yarn-cluster和Yarn-client区别与联系</a></li>\n</ul>\n","site":{"data":{}},"_categories":[{"name":"spark","path":"categories/spark/"}],"_tags":[{"name":"spark","path":"tags/spark/"}],"excerpt":"<p>Hello，有一个月没写blog了感觉很自责，必须整起来！最近由于工作上遇到的一些调优困难，让我对Spark有些敬畏，所以集中的研究了下鬼魅玄学Spark，和大家分享一下。首先先来看看spark的基本工作流程。<br></p>","more":"</p>\n<h2 id=\"Work-Flow\"><a href=\"#Work-Flow\" class=\"headerlink\" title=\"Work Flow\"></a>Work Flow</h2><p>和hadoop一样，spark也是master-slave机制，Spark通过driver进程，将task分发到多个executors上并发进行计算。整个driver和所有的executors组成了一个spark application，每一个application是运行在cluster manager上的，Spark本身集成了standalone cluster，当然，Spark还可以运行在赫赫有名的YARN和Mesos上。我平时使用的公司集群都是基于YARN cluster manager的，因此本文重点探讨基于YARN的spark。</p>\n<p>下图就是spark在cluster manager下的整体工作流程。<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/15/15-1.png\" alt=\"\"></p>\n<h2 id=\"The-Driver\"><a href=\"#The-Driver\" class=\"headerlink\" title=\"The Driver\"></a>The Driver</h2><p>Driver是整个application最核心的部分，他运行的是application的main方法，它伴随这整个application的生命周期，driver进程的结束就会带来整个application的结束。</p>\n<p>对于所有的Spark任务，他们其实都是实现RDD的transformation和action操作，而这些操作，最后是需要driver将他们转化和分发成tasks，然后才可以去执行。所有的user program都会被driver通过DAG(directed acyclic graph)转化成实际的tasks执行计划，除此之外，driver还会在tasks执行的期间，监控executor上的tasks，并且保证他们拥有健康而合理的资源。</p>\n<h2 id=\"Executors\"><a href=\"#Executors\" class=\"headerlink\" title=\"Executors\"></a>Executors</h2><p>Executors是Spark application的执行者，他们也是伴随着application的生命周期而存在的，值得注意的是，<strong>Spark job在executors执行失败的情况下依然可以继续进行</strong>。Executors会对具体的tasks的执行结果返回给driver，同时给缓存的RDD提供存储空间。</p>\n<h2 id=\"Some-terms\"><a href=\"#Some-terms\" class=\"headerlink\" title=\"Some terms\"></a>Some terms</h2><ul>\n<li>Job: Job是executor层面最大的执行单元，job通过RDD的action操作来分割，每一个action操作就会进行一次job的划分；</li>\n<li>Stage: Stage是包含在job中的执行单元，stage通过RDD的shuffle操作来分割，每进行一次shuffle操作，就会进行一次stage的划分；</li>\n<li>Task: Task是executor执行中最细的执行单元，task的数目取和parent RDD的partition数目是一一对应的。</li>\n</ul>\n<h2 id=\"Spark-on-Yarn-cluster\"><a href=\"#Spark-on-Yarn-cluster\" class=\"headerlink\" title=\"Spark on Yarn-cluster\"></a>Spark on Yarn-cluster</h2><p>下面，我们一起看看整个Spark application中，driver和executors的都会起到什么作用。我以基于yarn-cluster的YARN的Spark作为例子来简述整个流程，先看一张图：<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/15/15-2.png\" alt=\"\"><br>首先我们要明确一些YARN的概念，YARN是与master-slaver的一个Cluster Manager， 在YARN中，RM(ResourseManager)负责整个调度分发，即我们常说的master；而NM(NodeManager)任务分发的接受者，负责执行具体的任务，也就是我们所说的worker。这些概念后续我专门介绍YARN的时候会详细的说明，他们的作用都是实现spark和YARN之间诸如资源申请等操作。</p>\n<p>首先Client向ResourceManager发出提交application的请求，ResourseManager会在某一个NodeManager上启动AppManager进程，AppManager会随后启动driver，并将driver申请containers资源的信息发给ResourceManager，申请完成后，ResourceManager将资源分配消息传递给AppManager并由它启动container，每一个container中只运行一个spark executor，由此完成了资源的申请和分配。</p>\n<p>然后整个application开始执行，在这个过程中，根据RDD的transformation或者action，driver把这些任务以tasks的形式，源源不断的传送给executors，于是executors不停地进行计算和存储的任务。当driver结束的时候，他会结束掉executors并且释放掉资源。这就是yarn-cluster上spark的整体工作流程。</p>\n<p>除了yarn-cluster，还有一种yarn-client的方法，这种方法唯一的区别在于，他的driver并非运行在某个NodeManager上，而是一直运行在client中。这样的问题就是client一旦关闭，那么整个任务也就随之停止执行。因此相较而言，yarn-cluster更适合线上任务，而yarn-client更适合调试模式。</p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ul>\n<li><a href=\"http://shop.oreilly.com/product/0636920028512.do\" target=\"_blank\" rel=\"noopener\">Karau, Holden, et al. Learning spark: lightning-fast big data analysis. “ O’Reilly Media, Inc.”, 2015.</a></li>\n<li><a href=\"https://www.iteblog.com/archives/1223.html\" target=\"_blank\" rel=\"noopener\">Spark:Yarn-cluster和Yarn-client区别与联系</a></li>\n</ul>"},{"title":"Spark Tips Sum-up Part-1","date":"2018-09-15T14:15:40.000Z","toc":"ture","_content":"![](https://github.com/JoeAsir/blog-image/raw/master/blog/background/blurred-background-close-up-coffee-cup.jpg)\nThis article is about things I learned about Apache Spark recently. I've been struggling with Spark tuning for more than one month, including shuffle tuning, GC time tuning and so on. Honestly speaking, Apache Spark is just like an untamed horse, it could be a beauty if you tune it well while a nightmare if not. So let me show you some tips I've learned from my work. This article is part 1 and here we go.\n<!--more-->\n## RDD vs DataFrame Partition Number in Shuffle \nShuffle plays a pretty crucial role in MapReduce and Spark, which can influence a lot on the performance of the whole job. The shuffle would not only separate one stage but bring much more storage and I/O delay. However, is usually hard to avoid shuffle occurring, so how to make shuffle faster could be the key point.\nDuring my work in tuning the Spark SQL project, the join operation is unavoidable because it is the fundamental process in all the SQL program. While when I try to run the Spark SQL saving the joined DataFrame into a Hive table, I found the number of table partition is constant no matter how many partitions of two joining DataFrame, oh-gee! it would never happen in RDD join process. That makes the size of each file of Hive table on the HDFS very large, and I have to fix it up.\nAfter searching from the Internet, I found that the shuffle of DataFrame in Spark, join included, would have a partition number according to the configuration *spark.sql.shuffle.partitions* with 200 as default. And the problem above can be solved as this configuration changed. So, when you join two DataFrames or other process causing shuffle, remember the configuration *spark.sql.shuffle.partitions* when you want to modify the DataFrame partition number. \nWhat if I want to modify the partition number of a RDD? Actually, the configuration *spark.default.parallelism*, which seems to only working for raw RDD and is ignored when working with DataFrames in Spark SQL, may be a good choice. Alternatively, you can set the partition number by calling *repartition( )* or *coalese( )*, which is effective for both DataFrames and RDDs.\n\n## Smart Action\nAs we know, RDDs support two types of operation, which are action and transformation. Transformation creates a new RDD from an existing one, and action returns a value to the Spark driver from computing on a RDD. Based on the lazy evaluation, all the transformations would run as a lineage when an action is triggered by the Spark. A lineage of transformation followed by an action consist of one job in Spark, and one wide-dependency between two transformations separate the job into two stages, which is aka shuffle.\nWhen I'm tuning a Spark project, I found that different action following the same lineage of transformation takes different period of time. For example, the action *show( )* takes shorter time than *createOrReplaceTempView( )*, which makes me confuse a lot. After a long time thinking and searching, the answer finally comes out. Spark would draw a DAGSchedule when the program is submitted, the data would run through all the DAGSchedual and the result is sent to the Spark driver. Different action may generate different DAGSchedule even the the transformations are same, Spark is smart enough to know whether it needs to run everything in the RDD. Showed above, Spark may run less data for the *show( )* action than those for *createOrReplaceTempView( )*.\n\n## Broadcast Joins\nBroadcast joins (aka map-side joins) is a good way to abort shuffle and reduce cost. Spark SQL provides two ways for developers, you can not only write SQL but also use DataFrame/Dataset API. When a large table joins a smaller one, a threshold defined by *spark.sql.autoBroadcastJoinThreshold*, with 10M as default value, determines whether the smaller one will be broadcast or not. If you use SQL in Spark SQL, as the smaller table size below the threshold, Spark would automatically broadcast it to all executors. However, if you use DataFrame/Dataset API, the *broadcast* function must be imported or Spark wouldn't broadcast data even if the size is below the threshold.\n```scala\nval df = largeDF.join(broadcast(smallDF),Seq(\"col1\",\"col2\"),\"left\")\n```\nAlso, you can enlarge the value of *spark.sql.autoBroadcastJoinThreshold* so that larger table can also be broadcast, but the memory of your application should be paid attention. \nBroadcast joins is really an awesome solution to optimize Spark SQL joins, after using broadcast joins instead of default SortMerge joins, my application runs more than 10 times faster. Avoiding shuffle is quite important for Spark tuning, and broadcast is born to kill shuffle. You will fall in love with her as long as you have a try!\n\n## References\n* [Spark SQL Programming guide](http://spark.apache.org/docs/latest/sql-programming-guide.html#other-configuration-options)\n* [Mastering Spark SQL](https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-joins-broadcast.html)\n","source":"_posts/spark-sumup-part-1.md","raw":"---\ntitle: Spark Tips Sum-up Part-1\ndate: 2018-09-15 22:15:40\ntags: spark\ncategories: spark\ntoc: ture\n---\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/background/blurred-background-close-up-coffee-cup.jpg)\nThis article is about things I learned about Apache Spark recently. I've been struggling with Spark tuning for more than one month, including shuffle tuning, GC time tuning and so on. Honestly speaking, Apache Spark is just like an untamed horse, it could be a beauty if you tune it well while a nightmare if not. So let me show you some tips I've learned from my work. This article is part 1 and here we go.\n<!--more-->\n## RDD vs DataFrame Partition Number in Shuffle \nShuffle plays a pretty crucial role in MapReduce and Spark, which can influence a lot on the performance of the whole job. The shuffle would not only separate one stage but bring much more storage and I/O delay. However, is usually hard to avoid shuffle occurring, so how to make shuffle faster could be the key point.\nDuring my work in tuning the Spark SQL project, the join operation is unavoidable because it is the fundamental process in all the SQL program. While when I try to run the Spark SQL saving the joined DataFrame into a Hive table, I found the number of table partition is constant no matter how many partitions of two joining DataFrame, oh-gee! it would never happen in RDD join process. That makes the size of each file of Hive table on the HDFS very large, and I have to fix it up.\nAfter searching from the Internet, I found that the shuffle of DataFrame in Spark, join included, would have a partition number according to the configuration *spark.sql.shuffle.partitions* with 200 as default. And the problem above can be solved as this configuration changed. So, when you join two DataFrames or other process causing shuffle, remember the configuration *spark.sql.shuffle.partitions* when you want to modify the DataFrame partition number. \nWhat if I want to modify the partition number of a RDD? Actually, the configuration *spark.default.parallelism*, which seems to only working for raw RDD and is ignored when working with DataFrames in Spark SQL, may be a good choice. Alternatively, you can set the partition number by calling *repartition( )* or *coalese( )*, which is effective for both DataFrames and RDDs.\n\n## Smart Action\nAs we know, RDDs support two types of operation, which are action and transformation. Transformation creates a new RDD from an existing one, and action returns a value to the Spark driver from computing on a RDD. Based on the lazy evaluation, all the transformations would run as a lineage when an action is triggered by the Spark. A lineage of transformation followed by an action consist of one job in Spark, and one wide-dependency between two transformations separate the job into two stages, which is aka shuffle.\nWhen I'm tuning a Spark project, I found that different action following the same lineage of transformation takes different period of time. For example, the action *show( )* takes shorter time than *createOrReplaceTempView( )*, which makes me confuse a lot. After a long time thinking and searching, the answer finally comes out. Spark would draw a DAGSchedule when the program is submitted, the data would run through all the DAGSchedual and the result is sent to the Spark driver. Different action may generate different DAGSchedule even the the transformations are same, Spark is smart enough to know whether it needs to run everything in the RDD. Showed above, Spark may run less data for the *show( )* action than those for *createOrReplaceTempView( )*.\n\n## Broadcast Joins\nBroadcast joins (aka map-side joins) is a good way to abort shuffle and reduce cost. Spark SQL provides two ways for developers, you can not only write SQL but also use DataFrame/Dataset API. When a large table joins a smaller one, a threshold defined by *spark.sql.autoBroadcastJoinThreshold*, with 10M as default value, determines whether the smaller one will be broadcast or not. If you use SQL in Spark SQL, as the smaller table size below the threshold, Spark would automatically broadcast it to all executors. However, if you use DataFrame/Dataset API, the *broadcast* function must be imported or Spark wouldn't broadcast data even if the size is below the threshold.\n```scala\nval df = largeDF.join(broadcast(smallDF),Seq(\"col1\",\"col2\"),\"left\")\n```\nAlso, you can enlarge the value of *spark.sql.autoBroadcastJoinThreshold* so that larger table can also be broadcast, but the memory of your application should be paid attention. \nBroadcast joins is really an awesome solution to optimize Spark SQL joins, after using broadcast joins instead of default SortMerge joins, my application runs more than 10 times faster. Avoiding shuffle is quite important for Spark tuning, and broadcast is born to kill shuffle. You will fall in love with her as long as you have a try!\n\n## References\n* [Spark SQL Programming guide](http://spark.apache.org/docs/latest/sql-programming-guide.html#other-configuration-options)\n* [Mastering Spark SQL](https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-joins-broadcast.html)\n","slug":"spark-sumup-part-1","published":1,"updated":"2020-05-10T06:50:12.532Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cka0wnkwx001xqxotypc6rjlo","content":"<p><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/background/blurred-background-close-up-coffee-cup.jpg\" alt=\"\"><br>This article is about things I learned about Apache Spark recently. I’ve been struggling with Spark tuning for more than one month, including shuffle tuning, GC time tuning and so on. Honestly speaking, Apache Spark is just like an untamed horse, it could be a beauty if you tune it well while a nightmare if not. So let me show you some tips I’ve learned from my work. This article is part 1 and here we go.<br><a id=\"more\"></a></p>\n<h2 id=\"RDD-vs-DataFrame-Partition-Number-in-Shuffle\"><a href=\"#RDD-vs-DataFrame-Partition-Number-in-Shuffle\" class=\"headerlink\" title=\"RDD vs DataFrame Partition Number in Shuffle\"></a>RDD vs DataFrame Partition Number in Shuffle</h2><p>Shuffle plays a pretty crucial role in MapReduce and Spark, which can influence a lot on the performance of the whole job. The shuffle would not only separate one stage but bring much more storage and I/O delay. However, is usually hard to avoid shuffle occurring, so how to make shuffle faster could be the key point.<br>During my work in tuning the Spark SQL project, the join operation is unavoidable because it is the fundamental process in all the SQL program. While when I try to run the Spark SQL saving the joined DataFrame into a Hive table, I found the number of table partition is constant no matter how many partitions of two joining DataFrame, oh-gee! it would never happen in RDD join process. That makes the size of each file of Hive table on the HDFS very large, and I have to fix it up.<br>After searching from the Internet, I found that the shuffle of DataFrame in Spark, join included, would have a partition number according to the configuration <em>spark.sql.shuffle.partitions</em> with 200 as default. And the problem above can be solved as this configuration changed. So, when you join two DataFrames or other process causing shuffle, remember the configuration <em>spark.sql.shuffle.partitions</em> when you want to modify the DataFrame partition number.<br>What if I want to modify the partition number of a RDD? Actually, the configuration <em>spark.default.parallelism</em>, which seems to only working for raw RDD and is ignored when working with DataFrames in Spark SQL, may be a good choice. Alternatively, you can set the partition number by calling <em>repartition( )</em> or <em>coalese( )</em>, which is effective for both DataFrames and RDDs.</p>\n<h2 id=\"Smart-Action\"><a href=\"#Smart-Action\" class=\"headerlink\" title=\"Smart Action\"></a>Smart Action</h2><p>As we know, RDDs support two types of operation, which are action and transformation. Transformation creates a new RDD from an existing one, and action returns a value to the Spark driver from computing on a RDD. Based on the lazy evaluation, all the transformations would run as a lineage when an action is triggered by the Spark. A lineage of transformation followed by an action consist of one job in Spark, and one wide-dependency between two transformations separate the job into two stages, which is aka shuffle.<br>When I’m tuning a Spark project, I found that different action following the same lineage of transformation takes different period of time. For example, the action <em>show( )</em> takes shorter time than <em>createOrReplaceTempView( )</em>, which makes me confuse a lot. After a long time thinking and searching, the answer finally comes out. Spark would draw a DAGSchedule when the program is submitted, the data would run through all the DAGSchedual and the result is sent to the Spark driver. Different action may generate different DAGSchedule even the the transformations are same, Spark is smart enough to know whether it needs to run everything in the RDD. Showed above, Spark may run less data for the <em>show( )</em> action than those for <em>createOrReplaceTempView( )</em>.</p>\n<h2 id=\"Broadcast-Joins\"><a href=\"#Broadcast-Joins\" class=\"headerlink\" title=\"Broadcast Joins\"></a>Broadcast Joins</h2><p>Broadcast joins (aka map-side joins) is a good way to abort shuffle and reduce cost. Spark SQL provides two ways for developers, you can not only write SQL but also use DataFrame/Dataset API. When a large table joins a smaller one, a threshold defined by <em>spark.sql.autoBroadcastJoinThreshold</em>, with 10M as default value, determines whether the smaller one will be broadcast or not. If you use SQL in Spark SQL, as the smaller table size below the threshold, Spark would automatically broadcast it to all executors. However, if you use DataFrame/Dataset API, the <em>broadcast</em> function must be imported or Spark wouldn’t broadcast data even if the size is below the threshold.<br><figure class=\"highlight scala hljs\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"hljs-keyword\">val</span> df = largeDF.join(broadcast(smallDF),<span class=\"hljs-type\">Seq</span>(<span class=\"hljs-string\">\"col1\"</span>,<span class=\"hljs-string\">\"col2\"</span>),<span class=\"hljs-string\">\"left\"</span>)</span><br></pre></td></tr></table></figure></p>\n<p>Also, you can enlarge the value of <em>spark.sql.autoBroadcastJoinThreshold</em> so that larger table can also be broadcast, but the memory of your application should be paid attention.<br>Broadcast joins is really an awesome solution to optimize Spark SQL joins, after using broadcast joins instead of default SortMerge joins, my application runs more than 10 times faster. Avoiding shuffle is quite important for Spark tuning, and broadcast is born to kill shuffle. You will fall in love with her as long as you have a try!</p>\n<h2 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h2><ul>\n<li><a href=\"http://spark.apache.org/docs/latest/sql-programming-guide.html#other-configuration-options\" target=\"_blank\" rel=\"noopener\">Spark SQL Programming guide</a></li>\n<li><a href=\"https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-joins-broadcast.html\" target=\"_blank\" rel=\"noopener\">Mastering Spark SQL</a></li>\n</ul>\n","site":{"data":{}},"_categories":[{"name":"spark","path":"categories/spark/"}],"_tags":[{"name":"spark","path":"tags/spark/"}],"excerpt":"<p><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/background/blurred-background-close-up-coffee-cup.jpg\" alt=\"\"><br>This article is about things I learned about Apache Spark recently. I’ve been struggling with Spark tuning for more than one month, including shuffle tuning, GC time tuning and so on. Honestly speaking, Apache Spark is just like an untamed horse, it could be a beauty if you tune it well while a nightmare if not. So let me show you some tips I’ve learned from my work. This article is part 1 and here we go.<br></p>","more":"</p>\n<h2 id=\"RDD-vs-DataFrame-Partition-Number-in-Shuffle\"><a href=\"#RDD-vs-DataFrame-Partition-Number-in-Shuffle\" class=\"headerlink\" title=\"RDD vs DataFrame Partition Number in Shuffle\"></a>RDD vs DataFrame Partition Number in Shuffle</h2><p>Shuffle plays a pretty crucial role in MapReduce and Spark, which can influence a lot on the performance of the whole job. The shuffle would not only separate one stage but bring much more storage and I/O delay. However, is usually hard to avoid shuffle occurring, so how to make shuffle faster could be the key point.<br>During my work in tuning the Spark SQL project, the join operation is unavoidable because it is the fundamental process in all the SQL program. While when I try to run the Spark SQL saving the joined DataFrame into a Hive table, I found the number of table partition is constant no matter how many partitions of two joining DataFrame, oh-gee! it would never happen in RDD join process. That makes the size of each file of Hive table on the HDFS very large, and I have to fix it up.<br>After searching from the Internet, I found that the shuffle of DataFrame in Spark, join included, would have a partition number according to the configuration <em>spark.sql.shuffle.partitions</em> with 200 as default. And the problem above can be solved as this configuration changed. So, when you join two DataFrames or other process causing shuffle, remember the configuration <em>spark.sql.shuffle.partitions</em> when you want to modify the DataFrame partition number.<br>What if I want to modify the partition number of a RDD? Actually, the configuration <em>spark.default.parallelism</em>, which seems to only working for raw RDD and is ignored when working with DataFrames in Spark SQL, may be a good choice. Alternatively, you can set the partition number by calling <em>repartition( )</em> or <em>coalese( )</em>, which is effective for both DataFrames and RDDs.</p>\n<h2 id=\"Smart-Action\"><a href=\"#Smart-Action\" class=\"headerlink\" title=\"Smart Action\"></a>Smart Action</h2><p>As we know, RDDs support two types of operation, which are action and transformation. Transformation creates a new RDD from an existing one, and action returns a value to the Spark driver from computing on a RDD. Based on the lazy evaluation, all the transformations would run as a lineage when an action is triggered by the Spark. A lineage of transformation followed by an action consist of one job in Spark, and one wide-dependency between two transformations separate the job into two stages, which is aka shuffle.<br>When I’m tuning a Spark project, I found that different action following the same lineage of transformation takes different period of time. For example, the action <em>show( )</em> takes shorter time than <em>createOrReplaceTempView( )</em>, which makes me confuse a lot. After a long time thinking and searching, the answer finally comes out. Spark would draw a DAGSchedule when the program is submitted, the data would run through all the DAGSchedual and the result is sent to the Spark driver. Different action may generate different DAGSchedule even the the transformations are same, Spark is smart enough to know whether it needs to run everything in the RDD. Showed above, Spark may run less data for the <em>show( )</em> action than those for <em>createOrReplaceTempView( )</em>.</p>\n<h2 id=\"Broadcast-Joins\"><a href=\"#Broadcast-Joins\" class=\"headerlink\" title=\"Broadcast Joins\"></a>Broadcast Joins</h2><p>Broadcast joins (aka map-side joins) is a good way to abort shuffle and reduce cost. Spark SQL provides two ways for developers, you can not only write SQL but also use DataFrame/Dataset API. When a large table joins a smaller one, a threshold defined by <em>spark.sql.autoBroadcastJoinThreshold</em>, with 10M as default value, determines whether the smaller one will be broadcast or not. If you use SQL in Spark SQL, as the smaller table size below the threshold, Spark would automatically broadcast it to all executors. However, if you use DataFrame/Dataset API, the <em>broadcast</em> function must be imported or Spark wouldn’t broadcast data even if the size is below the threshold.<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">val</span> df = largeDF.join(broadcast(smallDF),<span class=\"type\">Seq</span>(<span class=\"string\">\"col1\"</span>,<span class=\"string\">\"col2\"</span>),<span class=\"string\">\"left\"</span>)</span><br></pre></td></tr></table></figure></p>\n<p>Also, you can enlarge the value of <em>spark.sql.autoBroadcastJoinThreshold</em> so that larger table can also be broadcast, but the memory of your application should be paid attention.<br>Broadcast joins is really an awesome solution to optimize Spark SQL joins, after using broadcast joins instead of default SortMerge joins, my application runs more than 10 times faster. Avoiding shuffle is quite important for Spark tuning, and broadcast is born to kill shuffle. You will fall in love with her as long as you have a try!</p>\n<h2 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h2><ul>\n<li><a href=\"http://spark.apache.org/docs/latest/sql-programming-guide.html#other-configuration-options\" target=\"_blank\" rel=\"noopener\">Spark SQL Programming guide</a></li>\n<li><a href=\"https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-joins-broadcast.html\" target=\"_blank\" rel=\"noopener\">Mastering Spark SQL</a></li>\n</ul>"},{"title":"Spark Tips Sum-up Part-2","date":"2018-10-13T01:53:02.000Z","toc":true,"_content":"![](https://github.com/JoeAsir/blog-image/raw/master/blog/background/boulders-calm-conifers-426894.jpg)\nThis article presents some tips of Apache Spark, which is part 2 of the series. All the tips below are based on the real problems which I met. Despite the background, the tips below are of valuable reference. I've tried a lot to learn about Apache Spark but can't know the detail of every part of it. I'd appreciate it if you figure out the mistakes in this article.\n<!--more-->\n## Coalesce\nChanging the number of partitions could benefit the performance of your Spark application. A large number of partitions may increase the parallelism of the process while too many tasks would be executed on a single executor, which could cost more running time. In contrast, a small number of partition might improve the complexity of each task and slow down the whole process. To solve this trade-off problem, *repartition()* and *coalesce()* is proposed in Apache Spark. \nBefore talking about the detail about *coalesce()*, let's review the concept of transformation with wide-dependencies and narrow-dependencies.\n> * Wide-dependencies: each partition of the parent RDD is used by at most one partition of the child RDD.\n* Narrow-dependencies: multiple child partitions may depend on each partition in the parent RDD.\n\nAccording to the definition above, *repartition()* is a typical transformation with wide-dependencies and thus can bring in shuffle operation when triggered. But what about *coalesce()*? To find out more about it, let's see the definition first.\n```scala\n// coalesce() for RDD is defined in org.apache.spark.rdd.RDD\ndef coalesce(numPartitions: Int, shuffle: Boolean = false,\n    partitionCoalescer: Option[PartitionCoalescer] = Option.empty) \n    (implicit ord: Ordering[T] = null)\n   : RDD[T] = withScope {...}\n\n// coalesce() for Dataset is defined in org.apache.spark.sql.Dataset\ndef coalesce(numPartitions: Int): Dataset[T] = withTypedPlan {\n    Repartition(numPartitions, shuffle = false, logicalPlan)\n}\n```\nFor RDDs, *coalesce()* has a boolean typed parameter called *shuffle*. The *coalesce()* can be treated as *repartition()* as *shuffle* is set to *True*, which is a transformation with wide-dependencies. In contrast, when *shuffle* is *False*, *coalesce()* is a transformation with narrow-dependencies and only can reduce the number of partitions, which means you cannot increase the number of partitions by setting the parameter *numPartitions*. As for DataFrame/Dataset API, *coalesce()* is a transformation with narrow-dependencies as there is no shuffle operation at all. As results, *coalesce()* cannot increase the number of DataFrame/Dataset's partitions and can only be used to reduce DataFrame/Dataset's partitions.\nAfter understanding the above, there is a crucial tip for you. When you use *coalesce()* and reduce the number of partitions, which may cause a problem that the partition of the whole stage would be decreased and the computation could be even slower than you expect. since no shuffle operation performed, the stage executes with the level of parallelism assigned by *coalesce()*. To avoid this problem, you can set *shuffle=True* for RDDs or use *repartition()* instead for DataFrame/Dataset to split the whole stage by a shuffle.\n## Read ORC Table\nReading data from and writing data to HIVE tables is quite often when I use Apache Spark. And we are now using ROC format to store HIVE table.\n> ORC is a self-describing type-aware columnar file format designed for Hadoop workloads. It is optimized for large streaming reads, but with integrated support for finding required rows quickly. \n\nGenerally, when Spark reads data from HDFS, the initial number of partitions are determined by the number of blocks of data store in HDFS, with one block represents one partition in Spark. However, when I read the ORC format HIVE table, the partition number is equal to the number of files stored on HDFS, not number block. That's caused by ORC split strategy set by *hive.exec.orc.split.strategy*, which determines what strategy ORC should use to create splits for execution. The available option includes \"BI\", \"ETL\" and \"HYBRID\"\n> The HYBRID mode reads the footers for all files if there are fewer files than expected mapper count, switching over to generating 1 split per file if the average file sizes are smaller than the default HDFS block size. ETL strategy always reads the ORC footers before generating splits, while the BI strategy generates per-file splits fast without reading any data from HDFS.\n\nAs results, when the split strategy is set to ETL, Spark would take more time to generate tasks and the number of partitions is based on the size of HDFS blocks. In contrast, BI strategy would make Spark generate tasks immediately and the number of tasks is equal to the number of files of HIVE table. These two strategies seem to be a trade-off for us and it's better for us to decide by the actual situation.  \n\n## Reference\n* [Managing Spark Partitions with Coalesce and Repartition](https://hackernoon.com/managing-spark-partitions-with-coalesce-and-repartition-4050c57ad5c4)\n* [High Performence Spark](http://opencarts.org/sachlaptrinh/pdf/28044.pdf)\n* [Apache ORC](https://orc.apache.org/)\n* [Apache ORC Configuration Properties](https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties#ConfigurationProperties-hive.exec.orc.split.strategy)\n","source":"_posts/spark-sumup-part-2.md","raw":"---\ntitle: Spark Tips Sum-up Part-2\ndate: 2018-10-13 09:53:02\ntags: spark\ncategories: spark\ntoc: true\n---\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/background/boulders-calm-conifers-426894.jpg)\nThis article presents some tips of Apache Spark, which is part 2 of the series. All the tips below are based on the real problems which I met. Despite the background, the tips below are of valuable reference. I've tried a lot to learn about Apache Spark but can't know the detail of every part of it. I'd appreciate it if you figure out the mistakes in this article.\n<!--more-->\n## Coalesce\nChanging the number of partitions could benefit the performance of your Spark application. A large number of partitions may increase the parallelism of the process while too many tasks would be executed on a single executor, which could cost more running time. In contrast, a small number of partition might improve the complexity of each task and slow down the whole process. To solve this trade-off problem, *repartition()* and *coalesce()* is proposed in Apache Spark. \nBefore talking about the detail about *coalesce()*, let's review the concept of transformation with wide-dependencies and narrow-dependencies.\n> * Wide-dependencies: each partition of the parent RDD is used by at most one partition of the child RDD.\n* Narrow-dependencies: multiple child partitions may depend on each partition in the parent RDD.\n\nAccording to the definition above, *repartition()* is a typical transformation with wide-dependencies and thus can bring in shuffle operation when triggered. But what about *coalesce()*? To find out more about it, let's see the definition first.\n```scala\n// coalesce() for RDD is defined in org.apache.spark.rdd.RDD\ndef coalesce(numPartitions: Int, shuffle: Boolean = false,\n    partitionCoalescer: Option[PartitionCoalescer] = Option.empty) \n    (implicit ord: Ordering[T] = null)\n   : RDD[T] = withScope {...}\n\n// coalesce() for Dataset is defined in org.apache.spark.sql.Dataset\ndef coalesce(numPartitions: Int): Dataset[T] = withTypedPlan {\n    Repartition(numPartitions, shuffle = false, logicalPlan)\n}\n```\nFor RDDs, *coalesce()* has a boolean typed parameter called *shuffle*. The *coalesce()* can be treated as *repartition()* as *shuffle* is set to *True*, which is a transformation with wide-dependencies. In contrast, when *shuffle* is *False*, *coalesce()* is a transformation with narrow-dependencies and only can reduce the number of partitions, which means you cannot increase the number of partitions by setting the parameter *numPartitions*. As for DataFrame/Dataset API, *coalesce()* is a transformation with narrow-dependencies as there is no shuffle operation at all. As results, *coalesce()* cannot increase the number of DataFrame/Dataset's partitions and can only be used to reduce DataFrame/Dataset's partitions.\nAfter understanding the above, there is a crucial tip for you. When you use *coalesce()* and reduce the number of partitions, which may cause a problem that the partition of the whole stage would be decreased and the computation could be even slower than you expect. since no shuffle operation performed, the stage executes with the level of parallelism assigned by *coalesce()*. To avoid this problem, you can set *shuffle=True* for RDDs or use *repartition()* instead for DataFrame/Dataset to split the whole stage by a shuffle.\n## Read ORC Table\nReading data from and writing data to HIVE tables is quite often when I use Apache Spark. And we are now using ROC format to store HIVE table.\n> ORC is a self-describing type-aware columnar file format designed for Hadoop workloads. It is optimized for large streaming reads, but with integrated support for finding required rows quickly. \n\nGenerally, when Spark reads data from HDFS, the initial number of partitions are determined by the number of blocks of data store in HDFS, with one block represents one partition in Spark. However, when I read the ORC format HIVE table, the partition number is equal to the number of files stored on HDFS, not number block. That's caused by ORC split strategy set by *hive.exec.orc.split.strategy*, which determines what strategy ORC should use to create splits for execution. The available option includes \"BI\", \"ETL\" and \"HYBRID\"\n> The HYBRID mode reads the footers for all files if there are fewer files than expected mapper count, switching over to generating 1 split per file if the average file sizes are smaller than the default HDFS block size. ETL strategy always reads the ORC footers before generating splits, while the BI strategy generates per-file splits fast without reading any data from HDFS.\n\nAs results, when the split strategy is set to ETL, Spark would take more time to generate tasks and the number of partitions is based on the size of HDFS blocks. In contrast, BI strategy would make Spark generate tasks immediately and the number of tasks is equal to the number of files of HIVE table. These two strategies seem to be a trade-off for us and it's better for us to decide by the actual situation.  \n\n## Reference\n* [Managing Spark Partitions with Coalesce and Repartition](https://hackernoon.com/managing-spark-partitions-with-coalesce-and-repartition-4050c57ad5c4)\n* [High Performence Spark](http://opencarts.org/sachlaptrinh/pdf/28044.pdf)\n* [Apache ORC](https://orc.apache.org/)\n* [Apache ORC Configuration Properties](https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties#ConfigurationProperties-hive.exec.orc.split.strategy)\n","slug":"spark-sumup-part-2","published":1,"updated":"2020-05-10T06:50:12.532Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cka0wnkxo004lqxoteqn397m0","content":"<p><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/background/boulders-calm-conifers-426894.jpg\" alt=\"\"><br>This article presents some tips of Apache Spark, which is part 2 of the series. All the tips below are based on the real problems which I met. Despite the background, the tips below are of valuable reference. I’ve tried a lot to learn about Apache Spark but can’t know the detail of every part of it. I’d appreciate it if you figure out the mistakes in this article.<br><a id=\"more\"></a></p>\n<h2 id=\"Coalesce\"><a href=\"#Coalesce\" class=\"headerlink\" title=\"Coalesce\"></a>Coalesce</h2><p>Changing the number of partitions could benefit the performance of your Spark application. A large number of partitions may increase the parallelism of the process while too many tasks would be executed on a single executor, which could cost more running time. In contrast, a small number of partition might improve the complexity of each task and slow down the whole process. To solve this trade-off problem, <em>repartition()</em> and <em>coalesce()</em> is proposed in Apache Spark.<br>Before talking about the detail about <em>coalesce()</em>, let’s review the concept of transformation with wide-dependencies and narrow-dependencies.</p>\n<blockquote>\n<ul>\n<li>Wide-dependencies: each partition of the parent RDD is used by at most one partition of the child RDD.</li>\n<li>Narrow-dependencies: multiple child partitions may depend on each partition in the parent RDD.</li>\n</ul>\n</blockquote>\n<p>According to the definition above, <em>repartition()</em> is a typical transformation with wide-dependencies and thus can bring in shuffle operation when triggered. But what about <em>coalesce()</em>? To find out more about it, let’s see the definition first.<br><figure class=\"highlight scala hljs\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"hljs-comment\">// coalesce() for RDD is defined in org.apache.spark.rdd.RDD</span></span><br><span class=\"line\"><span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">coalesce</span></span>(numPartitions: <span class=\"hljs-type\">Int</span>, shuffle: <span class=\"hljs-type\">Boolean</span> = <span class=\"hljs-literal\">false</span>,</span><br><span class=\"line\">    partitionCoalescer: <span class=\"hljs-type\">Option</span>[<span class=\"hljs-type\">PartitionCoalescer</span>] = <span class=\"hljs-type\">Option</span>.empty) </span><br><span class=\"line\">    (<span class=\"hljs-keyword\">implicit</span> ord: <span class=\"hljs-type\">Ordering</span>[<span class=\"hljs-type\">T</span>] = <span class=\"hljs-literal\">null</span>)</span><br><span class=\"line\">   : <span class=\"hljs-type\">RDD</span>[<span class=\"hljs-type\">T</span>] = withScope &#123;...&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"hljs-comment\">// coalesce() for Dataset is defined in org.apache.spark.sql.Dataset</span></span><br><span class=\"line\"><span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">coalesce</span></span>(numPartitions: <span class=\"hljs-type\">Int</span>): <span class=\"hljs-type\">Dataset</span>[<span class=\"hljs-type\">T</span>] = withTypedPlan &#123;</span><br><span class=\"line\">    <span class=\"hljs-type\">Repartition</span>(numPartitions, shuffle = <span class=\"hljs-literal\">false</span>, logicalPlan)</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>For RDDs, <em>coalesce()</em> has a boolean typed parameter called <em>shuffle</em>. The <em>coalesce()</em> can be treated as <em>repartition()</em> as <em>shuffle</em> is set to <em>True</em>, which is a transformation with wide-dependencies. In contrast, when <em>shuffle</em> is <em>False</em>, <em>coalesce()</em> is a transformation with narrow-dependencies and only can reduce the number of partitions, which means you cannot increase the number of partitions by setting the parameter <em>numPartitions</em>. As for DataFrame/Dataset API, <em>coalesce()</em> is a transformation with narrow-dependencies as there is no shuffle operation at all. As results, <em>coalesce()</em> cannot increase the number of DataFrame/Dataset’s partitions and can only be used to reduce DataFrame/Dataset’s partitions.<br>After understanding the above, there is a crucial tip for you. When you use <em>coalesce()</em> and reduce the number of partitions, which may cause a problem that the partition of the whole stage would be decreased and the computation could be even slower than you expect. since no shuffle operation performed, the stage executes with the level of parallelism assigned by <em>coalesce()</em>. To avoid this problem, you can set <em>shuffle=True</em> for RDDs or use <em>repartition()</em> instead for DataFrame/Dataset to split the whole stage by a shuffle.</p>\n<h2 id=\"Read-ORC-Table\"><a href=\"#Read-ORC-Table\" class=\"headerlink\" title=\"Read ORC Table\"></a>Read ORC Table</h2><p>Reading data from and writing data to HIVE tables is quite often when I use Apache Spark. And we are now using ROC format to store HIVE table.</p>\n<blockquote>\n<p>ORC is a self-describing type-aware columnar file format designed for Hadoop workloads. It is optimized for large streaming reads, but with integrated support for finding required rows quickly. </p>\n</blockquote>\n<p>Generally, when Spark reads data from HDFS, the initial number of partitions are determined by the number of blocks of data store in HDFS, with one block represents one partition in Spark. However, when I read the ORC format HIVE table, the partition number is equal to the number of files stored on HDFS, not number block. That’s caused by ORC split strategy set by <em>hive.exec.orc.split.strategy</em>, which determines what strategy ORC should use to create splits for execution. The available option includes “BI”, “ETL” and “HYBRID”</p>\n<blockquote>\n<p>The HYBRID mode reads the footers for all files if there are fewer files than expected mapper count, switching over to generating 1 split per file if the average file sizes are smaller than the default HDFS block size. ETL strategy always reads the ORC footers before generating splits, while the BI strategy generates per-file splits fast without reading any data from HDFS.</p>\n</blockquote>\n<p>As results, when the split strategy is set to ETL, Spark would take more time to generate tasks and the number of partitions is based on the size of HDFS blocks. In contrast, BI strategy would make Spark generate tasks immediately and the number of tasks is equal to the number of files of HIVE table. These two strategies seem to be a trade-off for us and it’s better for us to decide by the actual situation.  </p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ul>\n<li><a href=\"https://hackernoon.com/managing-spark-partitions-with-coalesce-and-repartition-4050c57ad5c4\" target=\"_blank\" rel=\"noopener\">Managing Spark Partitions with Coalesce and Repartition</a></li>\n<li><a href=\"http://opencarts.org/sachlaptrinh/pdf/28044.pdf\" target=\"_blank\" rel=\"noopener\">High Performence Spark</a></li>\n<li><a href=\"https://orc.apache.org/\" target=\"_blank\" rel=\"noopener\">Apache ORC</a></li>\n<li><a href=\"https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties#ConfigurationProperties-hive.exec.orc.split.strategy\" target=\"_blank\" rel=\"noopener\">Apache ORC Configuration Properties</a></li>\n</ul>\n","site":{"data":{}},"_categories":[{"name":"spark","path":"categories/spark/"}],"_tags":[{"name":"spark","path":"tags/spark/"}],"excerpt":"<p><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/background/boulders-calm-conifers-426894.jpg\" alt=\"\"><br>This article presents some tips of Apache Spark, which is part 2 of the series. All the tips below are based on the real problems which I met. Despite the background, the tips below are of valuable reference. I’ve tried a lot to learn about Apache Spark but can’t know the detail of every part of it. I’d appreciate it if you figure out the mistakes in this article.<br></p>","more":"</p>\n<h2 id=\"Coalesce\"><a href=\"#Coalesce\" class=\"headerlink\" title=\"Coalesce\"></a>Coalesce</h2><p>Changing the number of partitions could benefit the performance of your Spark application. A large number of partitions may increase the parallelism of the process while too many tasks would be executed on a single executor, which could cost more running time. In contrast, a small number of partition might improve the complexity of each task and slow down the whole process. To solve this trade-off problem, <em>repartition()</em> and <em>coalesce()</em> is proposed in Apache Spark.<br>Before talking about the detail about <em>coalesce()</em>, let’s review the concept of transformation with wide-dependencies and narrow-dependencies.</p>\n<blockquote>\n<ul>\n<li>Wide-dependencies: each partition of the parent RDD is used by at most one partition of the child RDD.</li>\n<li>Narrow-dependencies: multiple child partitions may depend on each partition in the parent RDD.</li>\n</ul>\n</blockquote>\n<p>According to the definition above, <em>repartition()</em> is a typical transformation with wide-dependencies and thus can bring in shuffle operation when triggered. But what about <em>coalesce()</em>? To find out more about it, let’s see the definition first.<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// coalesce() for RDD is defined in org.apache.spark.rdd.RDD</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">coalesce</span></span>(numPartitions: <span class=\"type\">Int</span>, shuffle: <span class=\"type\">Boolean</span> = <span class=\"literal\">false</span>,</span><br><span class=\"line\">    partitionCoalescer: <span class=\"type\">Option</span>[<span class=\"type\">PartitionCoalescer</span>] = <span class=\"type\">Option</span>.empty) </span><br><span class=\"line\">    (<span class=\"keyword\">implicit</span> ord: <span class=\"type\">Ordering</span>[<span class=\"type\">T</span>] = <span class=\"literal\">null</span>)</span><br><span class=\"line\">   : <span class=\"type\">RDD</span>[<span class=\"type\">T</span>] = withScope &#123;...&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// coalesce() for Dataset is defined in org.apache.spark.sql.Dataset</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">coalesce</span></span>(numPartitions: <span class=\"type\">Int</span>): <span class=\"type\">Dataset</span>[<span class=\"type\">T</span>] = withTypedPlan &#123;</span><br><span class=\"line\">    <span class=\"type\">Repartition</span>(numPartitions, shuffle = <span class=\"literal\">false</span>, logicalPlan)</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>For RDDs, <em>coalesce()</em> has a boolean typed parameter called <em>shuffle</em>. The <em>coalesce()</em> can be treated as <em>repartition()</em> as <em>shuffle</em> is set to <em>True</em>, which is a transformation with wide-dependencies. In contrast, when <em>shuffle</em> is <em>False</em>, <em>coalesce()</em> is a transformation with narrow-dependencies and only can reduce the number of partitions, which means you cannot increase the number of partitions by setting the parameter <em>numPartitions</em>. As for DataFrame/Dataset API, <em>coalesce()</em> is a transformation with narrow-dependencies as there is no shuffle operation at all. As results, <em>coalesce()</em> cannot increase the number of DataFrame/Dataset’s partitions and can only be used to reduce DataFrame/Dataset’s partitions.<br>After understanding the above, there is a crucial tip for you. When you use <em>coalesce()</em> and reduce the number of partitions, which may cause a problem that the partition of the whole stage would be decreased and the computation could be even slower than you expect. since no shuffle operation performed, the stage executes with the level of parallelism assigned by <em>coalesce()</em>. To avoid this problem, you can set <em>shuffle=True</em> for RDDs or use <em>repartition()</em> instead for DataFrame/Dataset to split the whole stage by a shuffle.</p>\n<h2 id=\"Read-ORC-Table\"><a href=\"#Read-ORC-Table\" class=\"headerlink\" title=\"Read ORC Table\"></a>Read ORC Table</h2><p>Reading data from and writing data to HIVE tables is quite often when I use Apache Spark. And we are now using ROC format to store HIVE table.</p>\n<blockquote>\n<p>ORC is a self-describing type-aware columnar file format designed for Hadoop workloads. It is optimized for large streaming reads, but with integrated support for finding required rows quickly. </p>\n</blockquote>\n<p>Generally, when Spark reads data from HDFS, the initial number of partitions are determined by the number of blocks of data store in HDFS, with one block represents one partition in Spark. However, when I read the ORC format HIVE table, the partition number is equal to the number of files stored on HDFS, not number block. That’s caused by ORC split strategy set by <em>hive.exec.orc.split.strategy</em>, which determines what strategy ORC should use to create splits for execution. The available option includes “BI”, “ETL” and “HYBRID”</p>\n<blockquote>\n<p>The HYBRID mode reads the footers for all files if there are fewer files than expected mapper count, switching over to generating 1 split per file if the average file sizes are smaller than the default HDFS block size. ETL strategy always reads the ORC footers before generating splits, while the BI strategy generates per-file splits fast without reading any data from HDFS.</p>\n</blockquote>\n<p>As results, when the split strategy is set to ETL, Spark would take more time to generate tasks and the number of partitions is based on the size of HDFS blocks. In contrast, BI strategy would make Spark generate tasks immediately and the number of tasks is equal to the number of files of HIVE table. These two strategies seem to be a trade-off for us and it’s better for us to decide by the actual situation.  </p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ul>\n<li><a href=\"https://hackernoon.com/managing-spark-partitions-with-coalesce-and-repartition-4050c57ad5c4\" target=\"_blank\" rel=\"noopener\">Managing Spark Partitions with Coalesce and Repartition</a></li>\n<li><a href=\"http://opencarts.org/sachlaptrinh/pdf/28044.pdf\" target=\"_blank\" rel=\"noopener\">High Performence Spark</a></li>\n<li><a href=\"https://orc.apache.org/\" target=\"_blank\" rel=\"noopener\">Apache ORC</a></li>\n<li><a href=\"https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties#ConfigurationProperties-hive.exec.orc.split.strategy\" target=\"_blank\" rel=\"noopener\">Apache ORC Configuration Properties</a></li>\n</ul>"},{"title":"Spark Tips Sum-up Part-3","date":"2019-03-06T10:24:15.000Z","toc":true,"_content":"![](https://github.com/JoeAsir/blog-image/raw/master/blog/background/beach-dawn-dusk-185927.jpg)\nThis blog is the third part of Apache Spark tips sum-up learnt from my programing and debugging. Have been busy for such a long time, I have some time to carry on my personal blog. Today I'll show you some tips about some functions in Spark SQL, and there may be some mistakes caused by my misunderstanding. Anyway, thanks if you figure out anything incorrect!  \n<!--more-->\n## Explode\nSpark SQL provides a varority of functions in *org.apache.spark.sql.functions* for you to restruct your data, one of which is the *explode()* function. Since Spark 2.3, *explode()* function has been optimized and it has been much more faster than it in the previous Spark versions. For more details about this optimization, [this issue](https://issues.apache.org/jira/browse/SPARK-21657) may help you a lot.\nHowever, what I want to share about is the number of partitions when you use *explode()*. It's easy to understand that the number of rows grows several times after the exploding process, as a result, the size of data for each partition has become larger than ever, which may cause the jobs taking more time and more memory. So, it could be a wise choice to enlarge the number of partitions by *repartition()*, especially when the rows explode more than 10 times than previous, each task would process much more data and that's possible to get an OOM error, or high GC time.\n\n## Foreach vs ForeachPartition, Map vs MapPartition\nYeah, *foreach()* vs *foreachPartition()* and *map()* vs *mapPartition()*, these four method do confuse me for a long time and let me share you my understanding about them. \nFirst of all, *foreach()* and *foreachPartition()* are actiona in Spark, while *map()* and *mapPartition()* are transformations. If you have no ideas about the defination of action and transformation, it's better to read about my previous blog or just ask help for dear *google*. *foreach()* and *foreachPartition()* are often used for writing data to external database while *map()* and *mapPartition()* are used to modify the data of each row in the RDD, also DataFrame or DataSet.\nSeondly, *foreachPartition()* and *mapPartitionn()* are respectively based on *foreach()* and *map()*. Instead of invoking function for each element, *foreachPartition()* and *mapPartition()* calls for each partition and provide an iterator to invoke the function. So what's the advantages of invoking function for each partition? For example, when you invoke a function connecting to your database, redis for example, *foreach()* will build a connection for each row of your data, which means, number of connection could be pretty huge and you may get connection failed errors. Instead of invokeing for each element, *foreachPartition()* could invoke the function building connection to redis for each partition, and data in each partition would be written to redis iteratively. Amazing, isn't it!\n\n## Reading ORC Table\nActually speaking, I have already talked about reading data from ORC table in Apache Spark in the previous blog, but I find something about reading from ORC table pretty awesome since Spark 2.3. Let's have a look. Spark supports a vectorized ORC reader with a new ORC file format for ORC files since 2.3 version, which means reading from and writing to ORC fromat file could be much faster!\nTo enbale the vectotized ORC reader, you just need to set these configuration:\n* --conf spark.sql.orc.impl=native \n* --conf spark.sql.orc.enableVectorizedReader=true \n* --conf spark.sql.hive.convertMetastoreOrc=true\n\nFor more information, you can read the [Spark Doc](https://spark.apache.org/docs/latest/sql-data-sources-orc.html).\n\n## References\n* [Apache Spark - foreach Vs foreachPartitions When to use What?](https://stackoverflow.com/questions/30484701/apache-spark-foreach-vs-foreachpartitions-when-to-use-what)\n* [Spark SQL Guide - ORC File](https://spark.apache.org/docs/latest/sql-data-sources-orc.html)\n","source":"_posts/spark-sumup-part-3.md","raw":"---\ntitle: Spark Tips Sum-up Part-3\ndate: 2019-03-06 18:24:15\ntags: spark\ncategories: spark\ntoc: true\n---\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/background/beach-dawn-dusk-185927.jpg)\nThis blog is the third part of Apache Spark tips sum-up learnt from my programing and debugging. Have been busy for such a long time, I have some time to carry on my personal blog. Today I'll show you some tips about some functions in Spark SQL, and there may be some mistakes caused by my misunderstanding. Anyway, thanks if you figure out anything incorrect!  \n<!--more-->\n## Explode\nSpark SQL provides a varority of functions in *org.apache.spark.sql.functions* for you to restruct your data, one of which is the *explode()* function. Since Spark 2.3, *explode()* function has been optimized and it has been much more faster than it in the previous Spark versions. For more details about this optimization, [this issue](https://issues.apache.org/jira/browse/SPARK-21657) may help you a lot.\nHowever, what I want to share about is the number of partitions when you use *explode()*. It's easy to understand that the number of rows grows several times after the exploding process, as a result, the size of data for each partition has become larger than ever, which may cause the jobs taking more time and more memory. So, it could be a wise choice to enlarge the number of partitions by *repartition()*, especially when the rows explode more than 10 times than previous, each task would process much more data and that's possible to get an OOM error, or high GC time.\n\n## Foreach vs ForeachPartition, Map vs MapPartition\nYeah, *foreach()* vs *foreachPartition()* and *map()* vs *mapPartition()*, these four method do confuse me for a long time and let me share you my understanding about them. \nFirst of all, *foreach()* and *foreachPartition()* are actiona in Spark, while *map()* and *mapPartition()* are transformations. If you have no ideas about the defination of action and transformation, it's better to read about my previous blog or just ask help for dear *google*. *foreach()* and *foreachPartition()* are often used for writing data to external database while *map()* and *mapPartition()* are used to modify the data of each row in the RDD, also DataFrame or DataSet.\nSeondly, *foreachPartition()* and *mapPartitionn()* are respectively based on *foreach()* and *map()*. Instead of invoking function for each element, *foreachPartition()* and *mapPartition()* calls for each partition and provide an iterator to invoke the function. So what's the advantages of invoking function for each partition? For example, when you invoke a function connecting to your database, redis for example, *foreach()* will build a connection for each row of your data, which means, number of connection could be pretty huge and you may get connection failed errors. Instead of invokeing for each element, *foreachPartition()* could invoke the function building connection to redis for each partition, and data in each partition would be written to redis iteratively. Amazing, isn't it!\n\n## Reading ORC Table\nActually speaking, I have already talked about reading data from ORC table in Apache Spark in the previous blog, but I find something about reading from ORC table pretty awesome since Spark 2.3. Let's have a look. Spark supports a vectorized ORC reader with a new ORC file format for ORC files since 2.3 version, which means reading from and writing to ORC fromat file could be much faster!\nTo enbale the vectotized ORC reader, you just need to set these configuration:\n* --conf spark.sql.orc.impl=native \n* --conf spark.sql.orc.enableVectorizedReader=true \n* --conf spark.sql.hive.convertMetastoreOrc=true\n\nFor more information, you can read the [Spark Doc](https://spark.apache.org/docs/latest/sql-data-sources-orc.html).\n\n## References\n* [Apache Spark - foreach Vs foreachPartitions When to use What?](https://stackoverflow.com/questions/30484701/apache-spark-foreach-vs-foreachpartitions-when-to-use-what)\n* [Spark SQL Guide - ORC File](https://spark.apache.org/docs/latest/sql-data-sources-orc.html)\n","slug":"spark-sumup-part-3","published":1,"updated":"2020-05-10T06:50:12.532Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cka0wnkxp004nqxotgx2ivlsf","content":"<p><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/background/beach-dawn-dusk-185927.jpg\" alt=\"\"><br>This blog is the third part of Apache Spark tips sum-up learnt from my programing and debugging. Have been busy for such a long time, I have some time to carry on my personal blog. Today I’ll show you some tips about some functions in Spark SQL, and there may be some mistakes caused by my misunderstanding. Anyway, thanks if you figure out anything incorrect!<br><a id=\"more\"></a></p>\n<h2 id=\"Explode\"><a href=\"#Explode\" class=\"headerlink\" title=\"Explode\"></a>Explode</h2><p>Spark SQL provides a varority of functions in <em>org.apache.spark.sql.functions</em> for you to restruct your data, one of which is the <em>explode()</em> function. Since Spark 2.3, <em>explode()</em> function has been optimized and it has been much more faster than it in the previous Spark versions. For more details about this optimization, <a href=\"https://issues.apache.org/jira/browse/SPARK-21657\" target=\"_blank\" rel=\"noopener\">this issue</a> may help you a lot.<br>However, what I want to share about is the number of partitions when you use <em>explode()</em>. It’s easy to understand that the number of rows grows several times after the exploding process, as a result, the size of data for each partition has become larger than ever, which may cause the jobs taking more time and more memory. So, it could be a wise choice to enlarge the number of partitions by <em>repartition()</em>, especially when the rows explode more than 10 times than previous, each task would process much more data and that’s possible to get an OOM error, or high GC time.</p>\n<h2 id=\"Foreach-vs-ForeachPartition-Map-vs-MapPartition\"><a href=\"#Foreach-vs-ForeachPartition-Map-vs-MapPartition\" class=\"headerlink\" title=\"Foreach vs ForeachPartition, Map vs MapPartition\"></a>Foreach vs ForeachPartition, Map vs MapPartition</h2><p>Yeah, <em>foreach()</em> vs <em>foreachPartition()</em> and <em>map()</em> vs <em>mapPartition()</em>, these four method do confuse me for a long time and let me share you my understanding about them.<br>First of all, <em>foreach()</em> and <em>foreachPartition()</em> are actiona in Spark, while <em>map()</em> and <em>mapPartition()</em> are transformations. If you have no ideas about the defination of action and transformation, it’s better to read about my previous blog or just ask help for dear <em>google</em>. <em>foreach()</em> and <em>foreachPartition()</em> are often used for writing data to external database while <em>map()</em> and <em>mapPartition()</em> are used to modify the data of each row in the RDD, also DataFrame or DataSet.<br>Seondly, <em>foreachPartition()</em> and <em>mapPartitionn()</em> are respectively based on <em>foreach()</em> and <em>map()</em>. Instead of invoking function for each element, <em>foreachPartition()</em> and <em>mapPartition()</em> calls for each partition and provide an iterator to invoke the function. So what’s the advantages of invoking function for each partition? For example, when you invoke a function connecting to your database, redis for example, <em>foreach()</em> will build a connection for each row of your data, which means, number of connection could be pretty huge and you may get connection failed errors. Instead of invokeing for each element, <em>foreachPartition()</em> could invoke the function building connection to redis for each partition, and data in each partition would be written to redis iteratively. Amazing, isn’t it!</p>\n<h2 id=\"Reading-ORC-Table\"><a href=\"#Reading-ORC-Table\" class=\"headerlink\" title=\"Reading ORC Table\"></a>Reading ORC Table</h2><p>Actually speaking, I have already talked about reading data from ORC table in Apache Spark in the previous blog, but I find something about reading from ORC table pretty awesome since Spark 2.3. Let’s have a look. Spark supports a vectorized ORC reader with a new ORC file format for ORC files since 2.3 version, which means reading from and writing to ORC fromat file could be much faster!<br>To enbale the vectotized ORC reader, you just need to set these configuration:</p>\n<ul>\n<li>–conf spark.sql.orc.impl=native </li>\n<li>–conf spark.sql.orc.enableVectorizedReader=true </li>\n<li>–conf spark.sql.hive.convertMetastoreOrc=true</li>\n</ul>\n<p>For more information, you can read the <a href=\"https://spark.apache.org/docs/latest/sql-data-sources-orc.html\" target=\"_blank\" rel=\"noopener\">Spark Doc</a>.</p>\n<h2 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h2><ul>\n<li><a href=\"https://stackoverflow.com/questions/30484701/apache-spark-foreach-vs-foreachpartitions-when-to-use-what\" target=\"_blank\" rel=\"noopener\">Apache Spark - foreach Vs foreachPartitions When to use What?</a></li>\n<li><a href=\"https://spark.apache.org/docs/latest/sql-data-sources-orc.html\" target=\"_blank\" rel=\"noopener\">Spark SQL Guide - ORC File</a></li>\n</ul>\n","site":{"data":{}},"_categories":[{"name":"spark","path":"categories/spark/"}],"_tags":[{"name":"spark","path":"tags/spark/"}],"excerpt":"<p><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/background/beach-dawn-dusk-185927.jpg\" alt=\"\"><br>This blog is the third part of Apache Spark tips sum-up learnt from my programing and debugging. Have been busy for such a long time, I have some time to carry on my personal blog. Today I’ll show you some tips about some functions in Spark SQL, and there may be some mistakes caused by my misunderstanding. Anyway, thanks if you figure out anything incorrect!<br></p>","more":"</p>\n<h2 id=\"Explode\"><a href=\"#Explode\" class=\"headerlink\" title=\"Explode\"></a>Explode</h2><p>Spark SQL provides a varority of functions in <em>org.apache.spark.sql.functions</em> for you to restruct your data, one of which is the <em>explode()</em> function. Since Spark 2.3, <em>explode()</em> function has been optimized and it has been much more faster than it in the previous Spark versions. For more details about this optimization, <a href=\"https://issues.apache.org/jira/browse/SPARK-21657\" target=\"_blank\" rel=\"noopener\">this issue</a> may help you a lot.<br>However, what I want to share about is the number of partitions when you use <em>explode()</em>. It’s easy to understand that the number of rows grows several times after the exploding process, as a result, the size of data for each partition has become larger than ever, which may cause the jobs taking more time and more memory. So, it could be a wise choice to enlarge the number of partitions by <em>repartition()</em>, especially when the rows explode more than 10 times than previous, each task would process much more data and that’s possible to get an OOM error, or high GC time.</p>\n<h2 id=\"Foreach-vs-ForeachPartition-Map-vs-MapPartition\"><a href=\"#Foreach-vs-ForeachPartition-Map-vs-MapPartition\" class=\"headerlink\" title=\"Foreach vs ForeachPartition, Map vs MapPartition\"></a>Foreach vs ForeachPartition, Map vs MapPartition</h2><p>Yeah, <em>foreach()</em> vs <em>foreachPartition()</em> and <em>map()</em> vs <em>mapPartition()</em>, these four method do confuse me for a long time and let me share you my understanding about them.<br>First of all, <em>foreach()</em> and <em>foreachPartition()</em> are actiona in Spark, while <em>map()</em> and <em>mapPartition()</em> are transformations. If you have no ideas about the defination of action and transformation, it’s better to read about my previous blog or just ask help for dear <em>google</em>. <em>foreach()</em> and <em>foreachPartition()</em> are often used for writing data to external database while <em>map()</em> and <em>mapPartition()</em> are used to modify the data of each row in the RDD, also DataFrame or DataSet.<br>Seondly, <em>foreachPartition()</em> and <em>mapPartitionn()</em> are respectively based on <em>foreach()</em> and <em>map()</em>. Instead of invoking function for each element, <em>foreachPartition()</em> and <em>mapPartition()</em> calls for each partition and provide an iterator to invoke the function. So what’s the advantages of invoking function for each partition? For example, when you invoke a function connecting to your database, redis for example, <em>foreach()</em> will build a connection for each row of your data, which means, number of connection could be pretty huge and you may get connection failed errors. Instead of invokeing for each element, <em>foreachPartition()</em> could invoke the function building connection to redis for each partition, and data in each partition would be written to redis iteratively. Amazing, isn’t it!</p>\n<h2 id=\"Reading-ORC-Table\"><a href=\"#Reading-ORC-Table\" class=\"headerlink\" title=\"Reading ORC Table\"></a>Reading ORC Table</h2><p>Actually speaking, I have already talked about reading data from ORC table in Apache Spark in the previous blog, but I find something about reading from ORC table pretty awesome since Spark 2.3. Let’s have a look. Spark supports a vectorized ORC reader with a new ORC file format for ORC files since 2.3 version, which means reading from and writing to ORC fromat file could be much faster!<br>To enbale the vectotized ORC reader, you just need to set these configuration:</p>\n<ul>\n<li>–conf spark.sql.orc.impl=native </li>\n<li>–conf spark.sql.orc.enableVectorizedReader=true </li>\n<li>–conf spark.sql.hive.convertMetastoreOrc=true</li>\n</ul>\n<p>For more information, you can read the <a href=\"https://spark.apache.org/docs/latest/sql-data-sources-orc.html\" target=\"_blank\" rel=\"noopener\">Spark Doc</a>.</p>\n<h2 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h2><ul>\n<li><a href=\"https://stackoverflow.com/questions/30484701/apache-spark-foreach-vs-foreachpartitions-when-to-use-what\" target=\"_blank\" rel=\"noopener\">Apache Spark - foreach Vs foreachPartitions When to use What?</a></li>\n<li><a href=\"https://spark.apache.org/docs/latest/sql-data-sources-orc.html\" target=\"_blank\" rel=\"noopener\">Spark SQL Guide - ORC File</a></li>\n</ul>"}],"PostAsset":[],"PostCategory":[{"post_id":"cka0wnkw10005qxotquuk2g90","category_id":"cka0wnkvy0002qxot8palizly","_id":"cka0wnkw7000bqxotakxlomvh"},{"post_id":"cka0wnkvt0000qxotbi9706ol","category_id":"cka0wnkvy0002qxot8palizly","_id":"cka0wnkw9000fqxot406azkyk"},{"post_id":"cka0wnkw20006qxot88x4of75","category_id":"cka0wnkvy0002qxot8palizly","_id":"cka0wnkwb000iqxotkw9mjrkw"},{"post_id":"cka0wnkw50009qxotd0ezka0w","category_id":"cka0wnkvy0002qxot8palizly","_id":"cka0wnkwd000nqxotbhucc2lp"},{"post_id":"cka0wnkvx0001qxotuhlk6rrc","category_id":"cka0wnkvy0002qxot8palizly","_id":"cka0wnkwe000pqxotny7iz881"},{"post_id":"cka0wnkw6000aqxotaaxbjq87","category_id":"cka0wnkvy0002qxot8palizly","_id":"cka0wnkwf000tqxotxx6bbg50"},{"post_id":"cka0wnkw00004qxotllo8f3to","category_id":"cka0wnkvy0002qxot8palizly","_id":"cka0wnkwg000vqxotuydu4i21"},{"post_id":"cka0wnkwd000oqxotjlkpcakg","category_id":"cka0wnkwc000lqxotc4a14ese","_id":"cka0wnkwh000yqxot7jgyew7a"},{"post_id":"cka0wnkw8000eqxotbcf0arls","category_id":"cka0wnkwc000lqxotc4a14ese","_id":"cka0wnkwj0012qxottuwreahe"},{"post_id":"cka0wnkwa000hqxotetgj0nh6","category_id":"cka0wnkwc000lqxotc4a14ese","_id":"cka0wnkwk0016qxott42zfzml"},{"post_id":"cka0wnkwc000mqxotj2kijogo","category_id":"cka0wnkwc000lqxotc4a14ese","_id":"cka0wnkwm001bqxotgawu5qrf"},{"post_id":"cka0wnkwe000rqxotr6psfc75","category_id":"cka0wnkwk0015qxotxmp0hl9w","_id":"cka0wnkwr001iqxotwez1cfh2"},{"post_id":"cka0wnkwg000uqxotxdkjaqia","category_id":"cka0wnkwm001cqxotb6ygxkwn","_id":"cka0wnkwt001oqxotnijbsvpx"},{"post_id":"cka0wnkwh000wqxotw5ylpaju","category_id":"cka0wnkwm001cqxotb6ygxkwn","_id":"cka0wnkwv001tqxotcnydm3n2"},{"post_id":"cka0wnkwi0011qxothcjkqnb7","category_id":"cka0wnkwm001cqxotb6ygxkwn","_id":"cka0wnkwy001zqxotrrvrwrj4"},{"post_id":"cka0wnkwj0014qxotqyqyyki0","category_id":"cka0wnkwm001cqxotb6ygxkwn","_id":"cka0wnkwz0023qxotgl4claes"},{"post_id":"cka0wnkwk0019qxotrz9wig2d","category_id":"cka0wnkwy001yqxot9idcrf97","_id":"cka0wnkwz0027qxotzdyzkica"},{"post_id":"cka0wnkwl001aqxotv7blftm0","category_id":"cka0wnkwz0024qxotkanmimmx","_id":"cka0wnkx0002dqxotsry57kmd"},{"post_id":"cka0wnkwn001eqxotxssjeddn","category_id":"cka0wnkwz0024qxotkanmimmx","_id":"cka0wnkx1002hqxotbb4drp8f"},{"post_id":"cka0wnkwn001gqxotznfx9std","category_id":"cka0wnkwz0024qxotkanmimmx","_id":"cka0wnkx1002jqxotzw6dloyb"},{"post_id":"cka0wnkwr001lqxot5g0nrxsf","category_id":"cka0wnkx1002gqxotzza6pdj4","_id":"cka0wnkx2002oqxot12c03sl9"},{"post_id":"cka0wnkws001mqxotqva303mg","category_id":"cka0wnkx1002gqxotzza6pdj4","_id":"cka0wnkx3002sqxota5azrily"},{"post_id":"cka0wnkwu001qqxot0pfge5oz","category_id":"cka0wnkx1002gqxotzza6pdj4","_id":"cka0wnkx4002wqxotrxj9cdl4"},{"post_id":"cka0wnkwv001rqxot88ak2bd8","category_id":"cka0wnkx1002gqxotzza6pdj4","_id":"cka0wnkx50030qxoti8pf1f7g"},{"post_id":"cka0wnkww001uqxotc2k6t9ja","category_id":"cka0wnkx1002gqxotzza6pdj4","_id":"cka0wnkx60033qxoto1w3sw66"},{"post_id":"cka0wnkwx001xqxotypc6rjlo","category_id":"cka0wnkx1002gqxotzza6pdj4","_id":"cka0wnkx60036qxotqiv8mqkc"},{"post_id":"cka0wnkxo004lqxoteqn397m0","category_id":"cka0wnkx1002gqxotzza6pdj4","_id":"cka0wnkxt004sqxotgz6003vn"},{"post_id":"cka0wnkxp004nqxotgx2ivlsf","category_id":"cka0wnkx1002gqxotzza6pdj4","_id":"cka0wnkxt004tqxotx2oku18j"}],"PostTag":[{"post_id":"cka0wnkvt0000qxotbi9706ol","tag_id":"cka0wnkw00003qxotpvn2s3jq","_id":"cka0wnkw9000gqxotpmk4lr0m"},{"post_id":"cka0wnkvt0000qxotbi9706ol","tag_id":"cka0wnkw40008qxotn0ilu17z","_id":"cka0wnkwb000jqxotqbsu78kg"},{"post_id":"cka0wnkvx0001qxotuhlk6rrc","tag_id":"cka0wnkw7000dqxotk5vy87x5","_id":"cka0wnkwi0010qxotjslmk0zk"},{"post_id":"cka0wnkvx0001qxotuhlk6rrc","tag_id":"cka0wnkwb000kqxot6kcm1g49","_id":"cka0wnkwj0013qxotjb4h6sjc"},{"post_id":"cka0wnkvx0001qxotuhlk6rrc","tag_id":"cka0wnkwe000qqxoth80pzu3l","_id":"cka0wnkwk0018qxotk9gckcnt"},{"post_id":"cka0wnkw00004qxotllo8f3to","tag_id":"cka0wnkw40008qxotn0ilu17z","_id":"cka0wnkwn001fqxotgvrypzg9"},{"post_id":"cka0wnkw00004qxotllo8f3to","tag_id":"cka0wnkwk0017qxotquz76rg0","_id":"cka0wnkwr001hqxotv9aivzrn"},{"post_id":"cka0wnkw10005qxotquuk2g90","tag_id":"cka0wnkwm001dqxot1u326b06","_id":"cka0wnkwx001wqxotm2ouhowv"},{"post_id":"cka0wnkw10005qxotquuk2g90","tag_id":"cka0wnkwr001kqxotzn9uzd50","_id":"cka0wnkwy0020qxot00o6z7x2"},{"post_id":"cka0wnkw10005qxotquuk2g90","tag_id":"cka0wnkwu001pqxotz4cp21co","_id":"cka0wnkwz0022qxot6bcuuww4"},{"post_id":"cka0wnkw20006qxot88x4of75","tag_id":"cka0wnkwm001dqxot1u326b06","_id":"cka0wnkwz0026qxot7a9x7ixy"},{"post_id":"cka0wnkw20006qxot88x4of75","tag_id":"cka0wnkwy0021qxotiynd18a0","_id":"cka0wnkwz0029qxotygyhtp0c"},{"post_id":"cka0wnkw50009qxotd0ezka0w","tag_id":"cka0wnkwz0025qxotcc6hjwy0","_id":"cka0wnkx0002bqxotqjon8gyu"},{"post_id":"cka0wnkw6000aqxotaaxbjq87","tag_id":"cka0wnkwz0025qxotcc6hjwy0","_id":"cka0wnkx1002fqxot9jd0uuzb"},{"post_id":"cka0wnkw8000eqxotbcf0arls","tag_id":"cka0wnkx1002eqxotoj62cc0e","_id":"cka0wnkx2002mqxottjzpdg58"},{"post_id":"cka0wnkw8000eqxotbcf0arls","tag_id":"cka0wnkx1002iqxotoy0bf57q","_id":"cka0wnkx3002pqxotin8w2d0u"},{"post_id":"cka0wnkwa000hqxotetgj0nh6","tag_id":"cka0wnkx1002eqxotoj62cc0e","_id":"cka0wnkx4002uqxotxd8rqwg6"},{"post_id":"cka0wnkwa000hqxotetgj0nh6","tag_id":"cka0wnkx3002qqxotfupooacg","_id":"cka0wnkx4002xqxot6v1f7v8s"},{"post_id":"cka0wnkwc000mqxotj2kijogo","tag_id":"cka0wnkx1002eqxotoj62cc0e","_id":"cka0wnkx60032qxotrsedkeeq"},{"post_id":"cka0wnkwc000mqxotj2kijogo","tag_id":"cka0wnkx1002iqxotoy0bf57q","_id":"cka0wnkx60034qxotnyw00sgj"},{"post_id":"cka0wnkwd000oqxotjlkpcakg","tag_id":"cka0wnkx1002eqxotoj62cc0e","_id":"cka0wnkx70038qxotv1jfmy8u"},{"post_id":"cka0wnkwd000oqxotjlkpcakg","tag_id":"cka0wnkx60035qxot4jcczys5","_id":"cka0wnkx70039qxot18j5134o"},{"post_id":"cka0wnkwe000rqxotr6psfc75","tag_id":"cka0wnkx60037qxotysmf6hlz","_id":"cka0wnkx7003bqxot3j408efg"},{"post_id":"cka0wnkwg000uqxotxdkjaqia","tag_id":"cka0wnkx7003aqxot2ymyskds","_id":"cka0wnkx7003eqxotkznlav2z"},{"post_id":"cka0wnkwg000uqxotxdkjaqia","tag_id":"cka0wnkw40008qxotn0ilu17z","_id":"cka0wnkx8003fqxotll2vwvpy"},{"post_id":"cka0wnkwg000uqxotxdkjaqia","tag_id":"cka0wnkx7003cqxotzb5jibpr","_id":"cka0wnkx8003hqxotvawws4fy"},{"post_id":"cka0wnkwh000wqxotw5ylpaju","tag_id":"cka0wnkx7003dqxotv5lx3y3b","_id":"cka0wnkx8003jqxotv19kw4sg"},{"post_id":"cka0wnkwh000wqxotw5ylpaju","tag_id":"cka0wnkw40008qxotn0ilu17z","_id":"cka0wnkx9003kqxotcg7fptvw"},{"post_id":"cka0wnkwh000wqxotw5ylpaju","tag_id":"cka0wnkx7003cqxotzb5jibpr","_id":"cka0wnkx9003mqxotggxmsu7o"},{"post_id":"cka0wnkwi0011qxothcjkqnb7","tag_id":"cka0wnkw00003qxotpvn2s3jq","_id":"cka0wnkxb003pqxot4ynxo2ls"},{"post_id":"cka0wnkwi0011qxothcjkqnb7","tag_id":"cka0wnkx8003iqxot8zsx8etw","_id":"cka0wnkxb003qqxothz6d913i"},{"post_id":"cka0wnkwi0011qxothcjkqnb7","tag_id":"cka0wnkx9003lqxotwh5kxwj0","_id":"cka0wnkxb003sqxot6hdn83ew"},{"post_id":"cka0wnkwi0011qxothcjkqnb7","tag_id":"cka0wnkxa003nqxot6acto4hz","_id":"cka0wnkxb003tqxotj2dssjlq"},{"post_id":"cka0wnkwj0014qxotqyqyyki0","tag_id":"cka0wnkxa003oqxotoqbirfvo","_id":"cka0wnkxb003vqxot72hbxxss"},{"post_id":"cka0wnkwk0019qxotrz9wig2d","tag_id":"cka0wnkxb003rqxotc3k9u7n0","_id":"cka0wnkxc003wqxot31ywdl9u"},{"post_id":"cka0wnkwl001aqxotv7blftm0","tag_id":"cka0wnkxb003uqxottyrkbc10","_id":"cka0wnkxc003zqxot726rf7ij"},{"post_id":"cka0wnkwl001aqxotv7blftm0","tag_id":"cka0wnkxc003xqxote9510yxj","_id":"cka0wnkxc0040qxot0sg86hc4"},{"post_id":"cka0wnkwl001aqxotv7blftm0","tag_id":"cka0wnkw40008qxotn0ilu17z","_id":"cka0wnkxd0042qxoteyrk9kqs"},{"post_id":"cka0wnkwn001eqxotxssjeddn","tag_id":"cka0wnkxc003yqxotkga9zwk6","_id":"cka0wnkxd0043qxot1scq8yqj"},{"post_id":"cka0wnkwn001gqxotznfx9std","tag_id":"cka0wnkxa003oqxotoqbirfvo","_id":"cka0wnkxf0047qxot6dyldyja"},{"post_id":"cka0wnkwn001gqxotznfx9std","tag_id":"cka0wnkxe0044qxotgsm0rts5","_id":"cka0wnkxf0048qxotj3n9jr28"},{"post_id":"cka0wnkwn001gqxotznfx9std","tag_id":"cka0wnkxe0045qxotdim923r5","_id":"cka0wnkxg004aqxot8flfnjtd"},{"post_id":"cka0wnkwr001lqxot5g0nrxsf","tag_id":"cka0wnkxe0046qxotya1bik22","_id":"cka0wnkxg004bqxot2a6ypctt"},{"post_id":"cka0wnkws001mqxotqva303mg","tag_id":"cka0wnkxe0046qxotya1bik22","_id":"cka0wnkxh004dqxotpaw69mnb"},{"post_id":"cka0wnkwu001qqxot0pfge5oz","tag_id":"cka0wnkxe0046qxotya1bik22","_id":"cka0wnkxh004fqxottupzr26l"},{"post_id":"cka0wnkwv001rqxot88ak2bd8","tag_id":"cka0wnkxe0046qxotya1bik22","_id":"cka0wnkxi004hqxotgimy9nu0"},{"post_id":"cka0wnkww001uqxotc2k6t9ja","tag_id":"cka0wnkxe0046qxotya1bik22","_id":"cka0wnkxi004jqxotjc5sgykl"},{"post_id":"cka0wnkwx001xqxotypc6rjlo","tag_id":"cka0wnkxe0046qxotya1bik22","_id":"cka0wnkxj004kqxotuv549mey"},{"post_id":"cka0wnkxo004lqxoteqn397m0","tag_id":"cka0wnkxe0046qxotya1bik22","_id":"cka0wnkxs004pqxoth3ee3a4s"},{"post_id":"cka0wnkxp004nqxotgx2ivlsf","tag_id":"cka0wnkxe0046qxotya1bik22","_id":"cka0wnkxs004rqxotktwsy4pc"}],"Tag":[{"name":"regularization","_id":"cka0wnkw00003qxotpvn2s3jq"},{"name":"gradient descent","_id":"cka0wnkw40008qxotn0ilu17z"},{"name":"hyperparameter","_id":"cka0wnkw7000dqxotk5vy87x5"},{"name":"batch norm","_id":"cka0wnkwb000kqxot6kcm1g49"},{"name":"covariate shift","_id":"cka0wnkwe000qqxoth80pzu3l"},{"name":"moving averages","_id":"cka0wnkwk0017qxotquz76rg0"},{"name":"learning strategy","_id":"cka0wnkwm001dqxot1u326b06"},{"name":"transfer learning","_id":"cka0wnkwr001kqxotzn9uzd50"},{"name":"multi-task learning","_id":"cka0wnkwu001pqxotz4cp21co"},{"name":"orthogonalization","_id":"cka0wnkwy0021qxotiynd18a0"},{"name":"CNN","_id":"cka0wnkwz0025qxotcc6hjwy0"},{"name":"hdfs","_id":"cka0wnkx1002eqxotoj62cc0e"},{"name":"hive","_id":"cka0wnkx1002iqxotoy0bf57q"},{"name":"yarn","_id":"cka0wnkx3002qqxotfupooacg"},{"name":"hadoop","_id":"cka0wnkx60035qxot4jcczys5"},{"name":"jvm","_id":"cka0wnkx60037qxotysmf6hlz"},{"name":"unconstrained optimization","_id":"cka0wnkx7003aqxot2ymyskds"},{"name":"newton's method","_id":"cka0wnkx7003cqxotzb5jibpr"},{"name":"convex optimization","_id":"cka0wnkx7003dqxotv5lx3y3b"},{"name":"MAP","_id":"cka0wnkx8003iqxot8zsx8etw"},{"name":"ridge regression","_id":"cka0wnkx9003lqxotwh5kxwj0"},{"name":"lasso regression","_id":"cka0wnkxa003nqxot6acto4hz"},{"name":"imbalanced data","_id":"cka0wnkxa003oqxotoqbirfvo"},{"name":"life","_id":"cka0wnkxb003rqxotc3k9u7n0"},{"name":"gbt","_id":"cka0wnkxb003uqxottyrkbc10"},{"name":"logistic regression","_id":"cka0wnkxc003xqxote9510yxj"},{"name":"activtion function","_id":"cka0wnkxc003yqxotkga9zwk6"},{"name":"undersampling","_id":"cka0wnkxe0044qxotgsm0rts5"},{"name":"bagging","_id":"cka0wnkxe0045qxotdim923r5"},{"name":"spark","_id":"cka0wnkxe0046qxotya1bik22"}]}}