<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>superAsir&#39;s Notes</title>
  
  <subtitle>Just 4 fun!</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://asirzhao.github.io/"/>
  <updated>2020-05-10T06:50:12.529Z</updated>
  <id>https://asirzhao.github.io/</id>
  
  <author>
    <name>Asir Zhao</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>ORC Stored HIVE Tips</title>
    <link href="https://asirzhao.github.io/2019/06/22/hdfs-orc-tips/"/>
    <id>https://asirzhao.github.io/2019/06/22/hdfs-orc-tips/</id>
    <published>2019-06-22T10:05:12.000Z</published>
    <updated>2020-05-10T06:50:12.529Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://github.com/JoeAsir/blog-image/raw/master/blog/background/balloon-bright-celebration-2388650.jpg" alt=""><br>ORC stored HIVE table has been very common in my daily work. Having a deep dive in ORC format recently, I realize that there are many awesome features in ORC format, and many of them have never been used before or somehow in the wrong way by me. So I take some time with some little tests and present two of the most exciting features of ORC stored HVE table in this blog. Let’s start this!<br>BTW, if you are not familiar with ORC, you can take a quick view from my <a href="https://joeasir.github.io/2019/05/05/hdfs-arvo-parquet-orc/#ORC" target="_blank" rel="noopener">previous blog</a>.<br><a id="more"></a></p><h2 id="Index-Filter"><a href="#Index-Filter" class="headerlink" title="Index Filter"></a>Index Filter</h2><p>ORC is a kind of columnar format, the basic structure and some terms are introduced in my <a href="https://joeasir.github.io/2019/05/05/hdfs-arvo-parquet-orc/#ORC" target="_blank" rel="noopener">previous blog</a>. There are three kinds of the index in ORC file, file level, stripe level, and row level, all of which stores some statistics values such as max/min, sum value, and so on. If you want to learn the specific content of these statistics, you can use <code>hive --orcfiledump {path-of-your-orc-file}</code> to lean everything, such as schema, compression, stripe statistics, file statistics, you name it, about your ORC file.<br>When the ORC file is been queried with a filter execution <em>WHERE Col1 &gt; 10</em>, based on the statistics information like max and min value of <em>Col1</em>, the stripes whose max value of <em>Col1</em> could be skipped and as the result, the input records and mappers could be reduced.<br>However, there is a little problem, if you want to take advantage of the index filter, you have to set <em>hive.optimize.index.filter=true</em>, but the good news is that this configuration is ignored in the HIVE 3.0.<br>What’s more, suppose the min/max value of the filtered column in each stripe is not overlapping, the index filter could be much more efficient, isn’t it? So there is a tip for you, if the column is the major filter, it could be better to sort it as soon as it’s inserted in the ORC table. I made a simple test and the status of non-sorted and sorted table present different performance under the same following query HIVEQL.<br><figure class="highlight sql hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-keyword">SELECT</span></span><br><span class="line">    <span class="hljs-keyword">SUM</span>(Col1),</span><br><span class="line">    <span class="hljs-keyword">COUNT</span>(<span class="hljs-number">1</span>)</span><br><span class="line"><span class="hljs-keyword">FROM</span> <span class="hljs-keyword">table</span></span><br><span class="line"><span class="hljs-keyword">WHERE</span> <span class="hljs-keyword">Col</span> &gt; <span class="hljs-number">1000</span></span><br></pre></td></tr></table></figure></p><p>There are significantly fewer input records and mappers when querying the sorted table, which is pretty awesome!<br><img src="https://github.com/JoeAsir/blog-image/raw/master/blog/24/24-1.png" alt=""><br><img src="https://github.com/JoeAsir/blog-image/raw/master/blog/24/24-2.png" alt=""></p><p>Last but not the least, <em>ORDER BY</em> in HIVEQL is such an expensive execution that all the records are put into a single reducer if you have a huge data, it’s also a good idea to use <em>DISTRIBUTE BY</em> to instead.</p><h2 id="Vectorization"><a href="#Vectorization" class="headerlink" title="Vectorization"></a>Vectorization</h2><p>Vectorization, aka as Vectorized Query Execution, is an awesome feature for ORC stored HIVE table. It reduces the CPU usage by processing a block of 1024 rows instead of row by row. You can find detail information <a href="https://cwiki.apache.org/confluence/display/Hive/Vectorized+Query+Execution" target="_blank" rel="noopener">here</a><br>To make leverage of Vectorization, we must set <em>hive.vectorized.execution.enabled=true</em>. I also make a comparison between the vectorized and standard execution. As the figures below, processing 1014 rows each time, the amount of input records is 1000 times less than the standard execution. However, I found hardly ever difference about the CPU time spent, maybe the execution of my SQL is too simple to distinguish the CPU status between vectorized and standard execution.<br><img src="https://github.com/JoeAsir/blog-image/raw/master/blog/24/24-2.png" alt=""><br><img src="https://github.com/JoeAsir/blog-image/raw/master/blog/24/24-3.png" alt=""></p><p>The vectorization in ORC is of good efficiency to reduce the input records in HIVE, and this feature has now been supported in Apache Spark since 2.3, see more <a href="https://spark.apache.org/docs/latest/sql-data-sources-orc.html" target="_blank" rel="noopener">here</a>.</p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul><li><a href="https://cwiki.apache.org/confluence/display/Hive/Vectorized+Query+Execution" target="_blank" rel="noopener">Vectorized Query Execution</a></li><li><a href="https://hortonworks.com/blog/5-ways-make-hive-queries-run-faster" target="_blank" rel="noopener">5 Ways to Make Your Hive Queries Run Faster</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;https://github.com/JoeAsir/blog-image/raw/master/blog/background/balloon-bright-celebration-2388650.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;ORC stored HIVE table has been very common in my daily work. Having a deep dive in ORC format recently, I realize that there are many awesome features in ORC format, and many of them have never been used before or somehow in the wrong way by me. So I take some time with some little tests and present two of the most exciting features of ORC stored HVE table in this blog. Let’s start this!&lt;br&gt;BTW, if you are not familiar with ORC, you can take a quick view from my &lt;a href=&quot;https://joeasir.github.io/2019/05/05/hdfs-arvo-parquet-orc/#ORC&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;previous blog&lt;/a&gt;.&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="hadoop" scheme="https://asirzhao.github.io/categories/hadoop/"/>
    
    
      <category term="hdfs" scheme="https://asirzhao.github.io/tags/hdfs/"/>
    
      <category term="hive" scheme="https://asirzhao.github.io/tags/hive/"/>
    
  </entry>
  
  <entry>
    <title>YARN Architecture</title>
    <link href="https://asirzhao.github.io/2019/06/11/hdfs-yarn-architecture/"/>
    <id>https://asirzhao.github.io/2019/06/11/hdfs-yarn-architecture/</id>
    <published>2019-06-11T12:18:54.000Z</published>
    <updated>2020-05-10T06:50:12.529Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://github.com/JoeAsir/blog-image/raw/master/blog/background/aerial-shot-bird-s-eye-view-forest-2415927.jpg" alt=""><br>YARN, the abbreviation of <em>Yet Another Resource Negotiator</em>, is introduced in Hadoop 2.0. Compared with MRV1(MapReduce Version 1), YARN takes over the responsibility of <strong>resource management and job scheduling</strong> in MRV1, and make <strong>non-MapReduce</strong> jobs run on the Hadoop, Apache Spark for example. Although there are some rising technology that been treated as alternative of YARN by more and more developers, Kubernetes for instance, YARN is still widely used. Let’s talk about YARN today.<br><a id="more"></a></p><h2 id="Components"><a href="#Components" class="headerlink" title="Components"></a>Components</h2><p>YARN consists of one Resource Manager as master deamon, several slave daemon called Node Manager for each slave node and Application Master for each application.</p><h3 id="Resource-Master-RM"><a href="#Resource-Master-RM" class="headerlink" title="Resource Master(RM)"></a>Resource Master(RM)</h3><p>Resouce Manager(RM) is replacement of <strong>JobTracker</strong> in MapReduce Version 1 and it is the master deamon in YARN and manages the global resources among all the applications. Resource Master has two main components, Scheduler and Application Manager</p><h4 id="Scheduler"><a href="#Scheduler" class="headerlink" title="Scheduler"></a>Scheduler</h4><p>Scheduler is mainly desgined to negotiate resource and it is responsible for allocating the resource for the running applications based on the abstract notion of a resource Container, such as memory, CPU etc. Notice that Container is the basic unit of resource in YARN. Scheduler allocate the resource based on the resource requirment of the application and the resource availabality. Scheduler doesn’t care too much about the application monitoring or tracking, even the restarting failure no matter application or hardware failure.</p><h4 id="Applicaiton-Manager"><a href="#Applicaiton-Manager" class="headerlink" title="Applicaiton Manager"></a>Applicaiton Manager</h4><p>Application Manager is responsible for the maintaining a collection of submitted applications. Application Manager accept the submission of one application from the client and allocate one Container for the application to load its Application Master. What’s more, Application Manager provides the monitoring and tracking service for the running applications and also restarting them if any failure come out.</p><h3 id="Node-Manager-NM"><a href="#Node-Manager-NM" class="headerlink" title="Node Manager(NM)"></a>Node Manager(NM)</h3><p>Node Manager, the update version of <strong>TaskTrack</strong> in MRV1, is the slave deamon in YARN. Node Manager is mainly responsible for the individual Containers management and monitoring. Node Manager tracks the usage of resource among all the Containers in the node and replay the detailed information to Resource Manager and Node Manager also monitors the heath of the running Containers(Node Manager only manage the resource and running status rather than application context information).<br>Node Manager always keep in touch with Resouce Manager with heartbeats, </p><h3 id="Application-Master-AM"><a href="#Application-Master-AM" class="headerlink" title="Application Master(AM)"></a>Application Master(AM)</h3><p>One Application Mater runs for one application in YARN, which means each application has a unique Applicaiton Master. Application Master is responsible to send request to Resouce Manager and acquire resource from the Scheduler. What’s more, Application Master is the process that coordinates the application’s execution and manage the applicaiton faults along with the Node Manager. Also, Application Master sends heartbeats to the Resource Manager periodly to update the health condiction and resource demands.<br>Notice that there is a main difference between the NM and AM, NM only monitors and tracks the status of the dividul Containers while AM the application itself. When an application is running, both NM and AM contributes to the monitoring and tracking, but they focus on the different part.</p><h2 id="Applicaiton-Submission-on-YARN"><a href="#Applicaiton-Submission-on-YARN" class="headerlink" title="Applicaiton Submission on YARN"></a>Applicaiton Submission on YARN</h2><p>As we’ve learned each component of the YARN, let’s take look at how these components work together to submit an application. The workflow is presented below.<br><img src="https://github.com/JoeAsir/blog-image/raw/master/blog/23/23-1.png" alt=""></p><ul><li>Client submits an application to the Resource Manager and request for building an Application Master;</li><li>Rsource Manager, Application Manager actually, allocates a free Container to load the Application Master for the submitted application and then the Application Master then will start;</li><li>Application Master registers to the Resource Manager, after which the client can communicate with the Application Master directly;</li><li>Application Master send request to acquire resource Container for the application to Resouce Manager;</li><li>Resource Manager(Scheduler) notify the Node Manager to launch Containers, and the launched Containers can communicate with Application Master;</li><li>Application code will executed in the Container, and the status can be updated to the Application Master by the <em>application-specific</em>;</li><li>While the application is running, client contacts to the Resource Manager(Application Manager) to get the application’s running status, health condiction and so on;</li><li>As soon as the application finishes, Application Master unregisters to the Resource Manager and shut down the connection, and all the Containers will also be free.</li></ul><h2 id="Scheduling-in-YARN"><a href="#Scheduling-in-YARN" class="headerlink" title="Scheduling in YARN"></a>Scheduling in YARN</h2><p>Scheduling is one of the most important points in YARN. As we talked about in the Resouce Manager, the Scheduler in RM is responsible for scheduling, and actully there are mainly three kinds of scheduler in YARN, which are <strong>FIFO</strong>, <strong>capacity</strong> and <strong>fair</strong> scheduler. The figure below presents the differences among them.<br><img src="https://github.com/JoeAsir/blog-image/raw/master/blog/23/23-2.png" alt=""></p><h3 id="FIFO-Scheduler"><a href="#FIFO-Scheduler" class="headerlink" title="FIFO Scheduler"></a>FIFO Scheduler</h3><p>As name presents, FIFO, abbr of <em>firt in first out</em>, is the most basic scheduler, the submitted applications are all in a queue and executed one by one in order. The drawback is pretty significant, as shown in the figure, the whloe queue could be easily blocked by the <strong>BIG</strong> applications. Big application always need more time to be executed and the whole queue have to be waiting for the big application execution.</p><h3 id="Capacity-Scheduler"><a href="#Capacity-Scheduler" class="headerlink" title="Capacity Scheduler"></a>Capacity Scheduler</h3><p>Capacity schdeuler can solve the blocked queue problem in FIFO in some terms. Capacity Scheduler devide the source into multiple queues and some of them are for big applications specifically. Big applications in Capacity take less resource than in the FIFO, which makes the big application take longer time to execute in Capacity Scheduler.</p><h3 id="Fair-Scheduler"><a href="#Fair-Scheduler" class="headerlink" title="Fair Scheduler"></a>Fair Scheduler</h3><p>Hence neither FIFO nor Capacity are good enough for Scheduling, the Fair Scheduler are proposed and it’s the most popular scheduler in YARN at present. Similar to the Capacity Scheduler, Fari also divide the cluster into multiple queues. However, the capacity, or the resource, of each queues is dynamical. Also the fraction of the queue source deviding can be modified in configuration.<br>Suppose we have two queues, which are A and B, and the YARN is configured to Fari Scheduler. A start the application when B is free, then A can take over all the resource of queue B, but when B start an application while A is free, B can also take over all the resource. The key point of Fair Scheduler is that the resouce is fairly shared for all queues. </p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul><li><a href="https://www.edureka.co/blog/hadoop-yarn-tutorial/" target="_blank" rel="noopener">Hadoop YARN Tutorial – Learn the Fundamentals of YARN Architecture</a></li><li><a href="https://data-flair.training/blogs/hadoop-yarn-resource-manager/" target="_blank" rel="noopener">Hadoop YARN Resource Manager – A Yarn Tutorial</a></li><li><a href="http://timepasstechies.com/hadoop-yarn-concept-fifocapacity-fair-scheduling/" target="_blank" rel="noopener">Hadoop Yarn Concept with Fifo, Capacity and Fair Scheduling</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;https://github.com/JoeAsir/blog-image/raw/master/blog/background/aerial-shot-bird-s-eye-view-forest-2415927.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;YARN, the abbreviation of &lt;em&gt;Yet Another Resource Negotiator&lt;/em&gt;, is introduced in Hadoop 2.0. Compared with MRV1(MapReduce Version 1), YARN takes over the responsibility of &lt;strong&gt;resource management and job scheduling&lt;/strong&gt; in MRV1, and make &lt;strong&gt;non-MapReduce&lt;/strong&gt; jobs run on the Hadoop, Apache Spark for example. Although there are some rising technology that been treated as alternative of YARN by more and more developers, Kubernetes for instance, YARN is still widely used. Let’s talk about YARN today.&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="hadoop" scheme="https://asirzhao.github.io/categories/hadoop/"/>
    
    
      <category term="hdfs" scheme="https://asirzhao.github.io/tags/hdfs/"/>
    
      <category term="yarn" scheme="https://asirzhao.github.io/tags/yarn/"/>
    
  </entry>
  
  <entry>
    <title>Arvo, Parquet and ORC</title>
    <link href="https://asirzhao.github.io/2019/05/05/hdfs-arvo-parquet-orc/"/>
    <id>https://asirzhao.github.io/2019/05/05/hdfs-arvo-parquet-orc/</id>
    <published>2019-05-05T08:34:37.000Z</published>
    <updated>2020-05-10T06:50:12.529Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://github.com/JoeAsir/blog-image/raw/master/blog/background/architecture-body-of-water-buildings-2255985.jpg" alt=""><br>Dealing with HIVE is one of my daily work with which I read data from and write back to the HDFS. There are many storage formats in HIVE, such as textFile, Avro,  and so on. Today we will talk about three popular formats that are widely use in HIVE world, also in Spark and even the entire distributed file system world. Not talking about some classical formats like textFile , SequenceFile, RCFile, doesn’t mean that they are not important or good enough, so you’d better have some look at them to help you make sense of the storage formats in HIVE.<br><a id="more"></a></p><h2 id="Formats-Categories"><a href="#Formats-Categories" class="headerlink" title="Formats Categories"></a>Formats Categories</h2><h3 id="Row-Formats-amp-Columnar-Formats"><a href="#Row-Formats-amp-Columnar-Formats" class="headerlink" title="Row Formats &amp; Columnar Formats"></a>Row Formats &amp; Columnar Formats</h3><p>Row formats and Columnar formats are two methods of seriallizing and storing a table in Database. And the following figure presents the difference of data stored in Row formats and in Columnar fromats.<br><img src="https://github.com/JoeAsir/blog-image/raw/master/blog/22/22-1.png" alt=""></p><h3 id="Text-formats-amp-Binary-formats"><a href="#Text-formats-amp-Binary-formats" class="headerlink" title="Text formats &amp; Binary formats"></a>Text formats &amp; Binary formats</h3><p>Text and Binary Formats are two methods of storing data in another dimension. Text formats are human-readable, easy to generate and easy to parse, however, they occupy a lot of disk space because of the readablity and redundancy. The most popular Text formats includs CSV, TSV, JSON and so on. And, Binary formats, occupy much less storage space than Test formats while they are machine-readable rather than human-readable. However, saving in  Binary formats can make all the storage in HDFS more effcient. Also, the formats we are going to talk about, Avro, Parquet and ORC formats, are all Binary formats.</p><h2 id="Avro"><a href="#Avro" class="headerlink" title="Avro"></a>Avro</h2><p>Avor is one of <strong>row-based</strong> formats, and boldly speaking, Avro is a binary alternative to JSON. Avor provides rich data strucures with the <strong>schema</strong>, and the schema file is separately stored from the data file. Also, Avro can generate the serialization and deserialization code form the schema. The figure below presents the sturcture of Avro.<br><img src="https://github.com/JoeAsir/blog-image/raw/master/blog/22/22-2.png" alt=""></p><p>As presented, Avor firstly splits data into multiple <strong>blocks</strong> rather than stores data record by record. The schema infomation is stored in the header and the data information are in blocks followed by a <strong>sync marker</strong> for each block. The sync markers make the Avor format splittable and the schema make it extensibility.  </p><h2 id="Parquet"><a href="#Parquet" class="headerlink" title="Parquet"></a>Parquet</h2><p>Parquet is a <strong>Columnar format</strong>, which is based on the Google Dremel paper, and it’s one of the most popular Columnar formats in Hadoop ecosystem and it’s well integrated with Apache Spark. What’s more, Parquet can easily deal with the nested schema. The following figure shows the structure of Parquet.<br><img src="https://github.com/JoeAsir/blog-image/raw/master/blog/22/22-3.png" alt=""></p><p>First we need to learn some terms about Parquet:</p><ul><li>Blocks: The block is actually the block in HDFS, which is unchanged for Parquet;</li><li>File: The HDFS file with metadata;</li><li>Row group: A logical horizontal partitioning of the data into rows. A row group consists of a column chunk for each column in the dataset;</li><li>Column chunk: A chunk of data for a specific column in one Row group. A column chunk consists of one or more than one page;</li><li>Page: Column chunks are devided into pages. And it is conceptually indivisible unit.</li></ul><p>All the metadata for file, row groups, and column chunks are all stored in a footer, which is the tail of the whole file. The figure presents the structure of Parquet clearly and if you want to learn the detail, you can take a look at the <a href="https://parquet.apache.org/documentation/latest/" target="_blank" rel="noopener">offcial doc</a></p><h2 id="ORC"><a href="#ORC" class="headerlink" title="ORC"></a>ORC</h2><p>ORC(Optimized Row Columnar) is also a <strong>Columnar format</strong> and it is optimized from RCFile format, which is also a Columar format. ORC is now widely used in Hadoop ecosystem especially for HIVE. Let’s start from the ORC structure.<br><img src="https://github.com/JoeAsir/blog-image/raw/master/blog/22/22-4.png" alt=""></p><p>First we have to make some terms clearly:</p><ul><li>Stripe: Simialr to the row groups in Parquet, stripe in ORC is a group of row data. Each stripe holds index data, row data and a stripe footer;</li><li>Index data: Include the min and max values and row positions within each column in one stripe;</li><li>Row data: The real data value;</li><li>Stripe footer: Contains the directoty of stream location.</li></ul><p>Now let’s talk about an awesome feature in ORC, which is based on the index.</p><h3 id="Indexes"><a href="#Indexes" class="headerlink" title="Indexes"></a>Indexes</h3><p>There are three level of indexes in ORC, which are file, stride and row level index. They contain some statistics like the max/min value and the position information of each vale for specific dimesion. </p><blockquote><ul><li>file level - statistics about the values in each column across the entire file</li><li>stripe level - statistics about the values in each column for each stripe</li><li>row level - statistics about the values in each column for each set of 10,000 rows within a stripe(the value can be modified by <em>orc.row.index.stride</em>)</li></ul></blockquote><p>File and stride index are in the file footer at the tail of the whole file, and as soon as the file is read, the information can be loaded directly and the useless part of data would be ignored immediately. The row level index is stored in the Index data in each stride and it contains the count of the values and whether there are null value present, and also the min/max values. Besides, the row index can include bloom filter, which will be talked in the next sub-section.<br>To make leverage of ORC index, we’d better insert the data into ORC table with sorting first, usually <em>distributed by</em> and <em>sort by</em>, to make the data are sorted well in each reducer result. Let’s have a simple example. Suppose we have a query with <em>WHERE id &gt; 10</em>, the entired file would be filter by the file level index, stride index and row level index. If the file is inserted sorted by <em>id</em>, the min/max value in each index would not be overlapping.</p><h3 id="Bloom-Filter"><a href="#Bloom-Filter" class="headerlink" title="Bloom Filter"></a>Bloom Filter</h3><p>Since Hive 1.2, the bloom filter is supported in row index. If you’ve never heared about bloom filter, don’t worry about it and click <a href="https://en.wikipedia.org/wiki/Bloom_filter" target="_blank" rel="noopener">here</a> to have a quick look.<br>As the last sub-section, the indexes are in good use to filter data and make the query more efficient. But the statistics value like min/max are more efficient for those continuously numerical type columns. When you have some operation like <em>=</em> or <em>in</em>, bloom filter would be a better choice.<br>With the configuration <em>orc.bloom.filter.columns</em> and <em>orc.bloom.filter.fpp</em>, we can figure out the colunms(splitted by comma) with bloom filter and specify the positive probability for the bloom filter. </p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul><li><a href="https://parquet.apache.org/documentation/latest/" target="_blank" rel="noopener">Parquet Documentation</a></li><li><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+ORC#LanguageManualORC-orc-spec" target="_blank" rel="noopener">ORC Documentation</a></li><li><a href="https://www.youtube.com/watch?v=rVC9F1y38oU" target="_blank" rel="noopener">Apache Parquet: Parquet File Internals and Inspecting Parquet File Structure</a></li><li><a href="https://orc.apache.org/specification/ORCv1/" target="_blank" rel="noopener">ORC Specification v1</a></li><li><a href="https://orc.apache.org/docs/indexes.html" target="_blank" rel="noopener">ORC Indexes</a></li><li><a href="https://snippetessay.wordpress.com/2015/07/25/hive-optimizations-with-indexes-bloom-filters-and-statistics/" target="_blank" rel="noopener">HIVE Optimizations with Indexes, BLOOM-FILTERS and Statistics</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;https://github.com/JoeAsir/blog-image/raw/master/blog/background/architecture-body-of-water-buildings-2255985.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;Dealing with HIVE is one of my daily work with which I read data from and write back to the HDFS. There are many storage formats in HIVE, such as textFile, Avro,  and so on. Today we will talk about three popular formats that are widely use in HIVE world, also in Spark and even the entire distributed file system world. Not talking about some classical formats like textFile , SequenceFile, RCFile, doesn’t mean that they are not important or good enough, so you’d better have some look at them to help you make sense of the storage formats in HIVE.&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="hadoop" scheme="https://asirzhao.github.io/categories/hadoop/"/>
    
    
      <category term="hdfs" scheme="https://asirzhao.github.io/tags/hdfs/"/>
    
      <category term="hive" scheme="https://asirzhao.github.io/tags/hive/"/>
    
  </entry>
  
  <entry>
    <title>HDFS Architecture</title>
    <link href="https://asirzhao.github.io/2019/04/28/hdfs-architecture/"/>
    <id>https://asirzhao.github.io/2019/04/28/hdfs-architecture/</id>
    <published>2019-04-28T10:25:30.000Z</published>
    <updated>2020-05-10T06:50:12.529Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://github.com/JoeAsir/blog-image/raw/master/blog/background/clouds-dark-dark-clouds-2308671.jpg" alt=""><br>I’ve been using HDFS as storage for almost 3 years reading data from and writing data to it by HIVE and Spark, but I’ve never learned the detail. Finally I have some time to watch the <a href="https://www.coursera.org/learn/big-data-essentials/" target="_blank" rel="noopener">Big Data Essentials</a> on <a href="https://www.coursera.org/" target="_blank" rel="noopener">Coursera</a>, which inspired me to have a deep dive in HDFS architecture. This blog contains so much about HDFS that I spent 3 days to sum up and mark them down. If anything is worng, it’s very nice of you to tell me and I’ll figure it out! Let’s take a look.<br><a id="more"></a></p><h2 id="Namenode"><a href="#Namenode" class="headerlink" title="Namenode"></a>Namenode</h2><p>Namenode is the <strong>master node</strong> of the HDFS, it contains the metadata of the filesystem, such as the number and location of the block, replica and so on. Notice the metadata is stored in-memory for the fast retrival of data.</p><blockquote><p>Task of Namenode</p><ul><li>Manage file system namespace.</li><li>Regulates client’s access to files.</li><li>It also executes file system execution such as naming, closing, opening files/directories.</li><li>All DataNodes sends a Heartbeat and block report to the NameNode in the Hadoop cluster. It ensures that the DataNodes are alive. A block report contains a list of all blocks on a datanode.</li><li>NameNode is also responsible for taking care of the Replication Factor of all the blocks.</li></ul></blockquote><h3 id="FsImage-amp-EditLogs"><a href="#FsImage-amp-EditLogs" class="headerlink" title="FsImage &amp; EditLogs"></a>FsImage &amp; EditLogs</h3><p>The HDFS Namenade, as the Master Node, manages the whole architecture of the filesystem by the metadata. Data in the metadata are present as FsImage and EditLogs. Let’s the detail about them.</p><ul><li><p>FsImage: FsImage contains the image of the entire filesystem, including serialized form of all the <strong>directories</strong> and file <strong>inodes</strong>. The FsImage is actually stored as a <strong>local file</strong> in the filesystem in Namenode and actually, you can treat FsImage as a snapshot of the present filesystem architecture.</p><blockquote><p>Each inode is an internal representation of file or directory’s metadata.  </p></blockquote><ul><li>EditLogs: Editlogs contains the modification made to the entire filesystem on the most recently FsImage, such as creating, moving, updating, deleting and so on. Also, EditLogs is stored on the Namenode as a local file, similar to the FsImage.</li></ul></li></ul><p>As you may get it, FsImage and EditLogs, one for storing the present situation and one for the modification. With the help of these two files, Namenode could can recover the matadata in case of something unexpected. Let’s go ahead to the specific strategy.</p><h3 id="Secondary-Namenode-Checkpoint-Node-amp-Backup-Node"><a href="#Secondary-Namenode-Checkpoint-Node-amp-Backup-Node" class="headerlink" title="Secondary Namenode(Checkpoint Node) &amp; Backup Node"></a>Secondary Namenode(Checkpoint Node) &amp; Backup Node</h3><p>FsImage and EditLogs can help us recover the entire HDFS, but is there any problem? Suppose the Namenode has been running for a month, and once it restarted, the Namenode would read the FsImage and EditLogs to rebuild the state of the HDFS. However, since the Namenode has been running for such a long time that the EditLogs could be so large that it would spend a long time to load and parse, even for few hours, which is unacceptable for us. To solve this problem, let’s have a look at the Secondary Namenode and Backup Node.</p><ul><li>Secondary Namenode(Checkpoint Node): Secondary Namenode runs on another machine apart from the Namenode, it <strong>fetches the FsImage and EditLogs periodically</strong> from the Namenode and merge them to a start-of-art FsImage, aka a checkpoint, and push it back to the Namenode(it may be little confusing that so-called Secondary Namenode would not upload the checkpoint automaticlly while the checkpoint Node would, I’m not quite sure about it actually). </li><li>Backup Node: Take care that Backup Node is a different term from Secondary Namenode or Checkpoint Node. Backup Node doesn’t fetch the FsImage and EditLogs periodically because it receive a filesystem edit stream from the Namenode. As a result, the state image is always stored <strong>in-memory</strong> on the Backup Node.<blockquote><p>The Backup Node provides the same functionality as the Checkpoint Node, but is synchronized with the NameNode. </p></blockquote></li></ul><h2 id="Datanode"><a href="#Datanode" class="headerlink" title="Datanode"></a>Datanode</h2><p>Datanode is the <strong>slaver node</strong> of HDFS, which is actually where the data stored.</p><blockquote><p>Tasks of Datanode </p><ul><li>Block replica creation, deletion, and replication according to the instruction of Namenode.</li><li>DataNode manages data storage of the system.</li><li>DataNodes send heartbeat to the NameNode to report the health of HDFS. By default, this frequency is set to 3 seconds.</li></ul></blockquote><h2 id="Block-amp-Replica"><a href="#Block-amp-Replica" class="headerlink" title="Block &amp; Replica"></a>Block &amp; Replica</h2><h3 id="Block"><a href="#Block" class="headerlink" title="Block"></a>Block</h3><p>Block is unit of data stored on HDFS, which cannot be controlled by us and the value is often 128M by default. Why we need data block? Suppose we have two files stored on the HDFS, one is bigger, 1G for instance, and another is 129M. As these two files been read synchronously, it could be an imbalanced progress. But what if we split all the files to a same unit size and these splitted units would be read in balance. That’s why we need data block in HDFS.<br>Since we’ve learnt why we need data block, you may ask, why is data block 128M? When files splitted by data block size, instead of one huge single file, few small chunks are stored on HDFS, and the main information of these chunks, aka metadata, are stored in-memory on the Namenode, including block size, block location and so on. So if the block size is too small that the chunks would be too many, then the Namenode in-memory stroage would be under great pressure. And that’s also why the <strong>small files problem</strong> damage to your HDFS. However, on the other hand, too large block size would make the reading data process on datanode slow, which is not a good situation for HDFS. It’s a trade-off, and that’s why we choose 128M as an eclectic solution for HDFS. </p><h3 id="Replication-Management-via-Rack-Awareness"><a href="#Replication-Management-via-Rack-Awareness" class="headerlink" title="Replication Management via Rack Awareness"></a>Replication Management via Rack Awareness</h3><p>As written to the HDFS, a single file would be divided into many blocks and these blocks would be stored across the cluster, at the same time, the replica of each block is created and there are 3 replicas by default which is can be modified in setting. The replica is actually the backup data from the blocks in case of the potentially unfavorable conditions of the Datanode, aka HDFS fault tolerance. Let’s take a look at how HDFS manage the replication under the rack awareness.<br>Every Datanode in a cluster is actually a single machine, and several Datanodes are put on one rack for better management and they share the network. Several racks are set in one data center and one cluster may be built across several data centers, which could be in different areas even counties, as results, the network distance between each nodes are different. See the figure below.<br>When the cluster start to write data to the HDFS, Namenode chooses the Datanode which is closer to the same rack or nearby rack to the write request. This distance is calculated by the rules below. Rack Awareness will choose the Datanode which is closer to get rid of too much network commuication cost.</p><ul><li>Distance is 0 when data in the same node;</li><li>Distance is 2 when data in two different nodes but the same rack;</li><li>Distance is 4 when data in two different racks but the same data center;</li><li>Distance is 6 when data in two different data centers.<br><img src="https://github.com/JoeAsir/blog-image/raw/master/blog/21/21-1.png" alt=""></li></ul><p>Via Rack Awareness, Namenode will not only choose the namenode to store the data, but also the replicas. Let’s make a sample with one data block, once the data block has been already stored on the Datanode, the first replica will be stored in the local Datanode, then the second replica will be cast to another Datanode in the different rack and the third replica will be stored at the different Datanode on the local rack of the second replica.<br><img src="https://github.com/JoeAsir/blog-image/raw/master/blog/21/21-2.png" alt=""></p><blockquote><p>A simple but nonoptimal policy is to place replicas on the different racks. This prevents losing data when an entire rack fails and allows us to use bandwidth from multiple racks while reading the data. This policy evenly distributes the data among replicas in the cluster which makes it easy to balance load in case of component failure. But the biggest drawback of this policy is that it will increase the cost of write operation because a writer needs to transfer blocks to multiple racks and communication between the two nodes in different racks has to go through switches.</p></blockquote><h2 id="Read-amp-Write"><a href="#Read-amp-Write" class="headerlink" title="Read &amp; Write"></a>Read &amp; Write</h2><h3 id="Read-Operation"><a href="#Read-Operation" class="headerlink" title="Read Operation"></a>Read Operation</h3><ol><li>Client opens the file by the <em>DistributedFileSystem</em> object;</li><li><em>DistributedFileSystem</em> calls the Namenode via RPC and get the blocks and replicas location according to the distance between datanode and client, and a <em>FSDataInputStream</em> is also returned;</li><li>With the address of Datanotes, <em>FSDataInputStream</em> open the I/O stream and bring data from Datanodes back to the client;</li><li>Once the reading is finished, client will call <em>close()</em> to end up the stream.<br><img src="https://github.com/JoeAsir/blog-image/raw/master/blog/21/21-3.png" alt=""></li></ol><blockquote><p>If the <em>DFSInputStream</em> encounters an error while communicating with a datanode, it will try the next closest one for that block. It will also remember datanodes that have failed so that it doesn’t needlessly retry them for later blocks. The <em>DFSInputStream</em> also verifies checksums for the data transferred to it from the datanode. If it finds a corrupt block, it reports this to the namenode before the <em>DFSInputStream</em> attempts to read a replica of the block from another datanode.</p></blockquote><h3 id="Write-Operation"><a href="#Write-Operation" class="headerlink" title="Write Operation"></a>Write Operation</h3><ol><li>Client sends a <strong>create</strong> request on the <em>DistributedFileSystem</em>, and <em>DistributedFileSystem</em> makes a RPC call to the Namenode to create a new file in the filesystem’s namespace, and Namenode would check for the file names, permission and so on. And a <em>FSDataOutputStream</em> containing the Datanode location is returned by the Namenode if everything is OK;</li><li><em>FSDataOutputStream</em> would split the data into packets and make them a queue, aka data queue, consumed by the <em>DataStreamer</em>, which would allocate new blocks by picking a list of suitable Datanodes to store the replica from the Namenode;</li><li>Assume that the replication factor is set to 3, the list of Datanodes form a pipeline containing 3 Datanodes(the first replica is stored in the local Datanode, so there are 3 Datanodes instead of 4);</li><li><em>DataStreamer</em> streams the packet to the first Datanode in the pipeline and then forwards it to the second one, then the third one.</li><li><em>FSDataOutputStream</em> also maintains an interal queue of packets waiting for the acknowledge by Datanodes. Once the acknowledge is send from Datanode in the pipeline, which is sent when the block is stored and the replicas are created, the packet is removed from the packet queue.</li><li>All the blocks are stored and replicated on the different datanodes, the data blocks are copied in parallel.</li><li><p>Client calls <em>close()</em> when writing operation finished.<br><img src="https://github.com/JoeAsir/blog-image/raw/master/blog/21/21-4.png" alt=""></p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul><li><a href="https://morrisjobke.de/2013/12/11/Hadoop-NameNode-and-siblings/" target="_blank" rel="noopener">Hadoop - NameNode, Checkpoint Node and Backup Node</a></li><li><a href="https://data-flair.training/blogs/hadoop-hdfs-architecture/" target="_blank" rel="noopener">Hadoop HDFS Architecture Explanation and Assumptions</a></li><li><a href="https://www.coursera.org/learn/big-data-essentials/" target="_blank" rel="noopener">Big Data Essentials: HDFS, MapReduce and Spark RDD</a></li></ul></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;https://github.com/JoeAsir/blog-image/raw/master/blog/background/clouds-dark-dark-clouds-2308671.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;I’ve been using HDFS as storage for almost 3 years reading data from and writing data to it by HIVE and Spark, but I’ve never learned the detail. Finally I have some time to watch the &lt;a href=&quot;https://www.coursera.org/learn/big-data-essentials/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Big Data Essentials&lt;/a&gt; on &lt;a href=&quot;https://www.coursera.org/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Coursera&lt;/a&gt;, which inspired me to have a deep dive in HDFS architecture. This blog contains so much about HDFS that I spent 3 days to sum up and mark them down. If anything is worng, it’s very nice of you to tell me and I’ll figure it out! Let’s take a look.&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="hadoop" scheme="https://asirzhao.github.io/categories/hadoop/"/>
    
    
      <category term="hdfs" scheme="https://asirzhao.github.io/tags/hdfs/"/>
    
      <category term="hadoop" scheme="https://asirzhao.github.io/tags/hadoop/"/>
    
  </entry>
  
  <entry>
    <title>Spark Tips Sum-up Part-3</title>
    <link href="https://asirzhao.github.io/2019/03/06/spark-sumup-part-3/"/>
    <id>https://asirzhao.github.io/2019/03/06/spark-sumup-part-3/</id>
    <published>2019-03-06T10:24:15.000Z</published>
    <updated>2020-05-10T06:50:12.532Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://github.com/JoeAsir/blog-image/raw/master/blog/background/beach-dawn-dusk-185927.jpg" alt=""><br>This blog is the third part of Apache Spark tips sum-up learnt from my programing and debugging. Have been busy for such a long time, I have some time to carry on my personal blog. Today I’ll show you some tips about some functions in Spark SQL, and there may be some mistakes caused by my misunderstanding. Anyway, thanks if you figure out anything incorrect!<br><a id="more"></a></p><h2 id="Explode"><a href="#Explode" class="headerlink" title="Explode"></a>Explode</h2><p>Spark SQL provides a varority of functions in <em>org.apache.spark.sql.functions</em> for you to restruct your data, one of which is the <em>explode()</em> function. Since Spark 2.3, <em>explode()</em> function has been optimized and it has been much more faster than it in the previous Spark versions. For more details about this optimization, <a href="https://issues.apache.org/jira/browse/SPARK-21657" target="_blank" rel="noopener">this issue</a> may help you a lot.<br>However, what I want to share about is the number of partitions when you use <em>explode()</em>. It’s easy to understand that the number of rows grows several times after the exploding process, as a result, the size of data for each partition has become larger than ever, which may cause the jobs taking more time and more memory. So, it could be a wise choice to enlarge the number of partitions by <em>repartition()</em>, especially when the rows explode more than 10 times than previous, each task would process much more data and that’s possible to get an OOM error, or high GC time.</p><h2 id="Foreach-vs-ForeachPartition-Map-vs-MapPartition"><a href="#Foreach-vs-ForeachPartition-Map-vs-MapPartition" class="headerlink" title="Foreach vs ForeachPartition, Map vs MapPartition"></a>Foreach vs ForeachPartition, Map vs MapPartition</h2><p>Yeah, <em>foreach()</em> vs <em>foreachPartition()</em> and <em>map()</em> vs <em>mapPartition()</em>, these four method do confuse me for a long time and let me share you my understanding about them.<br>First of all, <em>foreach()</em> and <em>foreachPartition()</em> are actiona in Spark, while <em>map()</em> and <em>mapPartition()</em> are transformations. If you have no ideas about the defination of action and transformation, it’s better to read about my previous blog or just ask help for dear <em>google</em>. <em>foreach()</em> and <em>foreachPartition()</em> are often used for writing data to external database while <em>map()</em> and <em>mapPartition()</em> are used to modify the data of each row in the RDD, also DataFrame or DataSet.<br>Seondly, <em>foreachPartition()</em> and <em>mapPartitionn()</em> are respectively based on <em>foreach()</em> and <em>map()</em>. Instead of invoking function for each element, <em>foreachPartition()</em> and <em>mapPartition()</em> calls for each partition and provide an iterator to invoke the function. So what’s the advantages of invoking function for each partition? For example, when you invoke a function connecting to your database, redis for example, <em>foreach()</em> will build a connection for each row of your data, which means, number of connection could be pretty huge and you may get connection failed errors. Instead of invokeing for each element, <em>foreachPartition()</em> could invoke the function building connection to redis for each partition, and data in each partition would be written to redis iteratively. Amazing, isn’t it!</p><h2 id="Reading-ORC-Table"><a href="#Reading-ORC-Table" class="headerlink" title="Reading ORC Table"></a>Reading ORC Table</h2><p>Actually speaking, I have already talked about reading data from ORC table in Apache Spark in the previous blog, but I find something about reading from ORC table pretty awesome since Spark 2.3. Let’s have a look. Spark supports a vectorized ORC reader with a new ORC file format for ORC files since 2.3 version, which means reading from and writing to ORC fromat file could be much faster!<br>To enbale the vectotized ORC reader, you just need to set these configuration:</p><ul><li>–conf spark.sql.orc.impl=native </li><li>–conf spark.sql.orc.enableVectorizedReader=true </li><li>–conf spark.sql.hive.convertMetastoreOrc=true</li></ul><p>For more information, you can read the <a href="https://spark.apache.org/docs/latest/sql-data-sources-orc.html" target="_blank" rel="noopener">Spark Doc</a>.</p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul><li><a href="https://stackoverflow.com/questions/30484701/apache-spark-foreach-vs-foreachpartitions-when-to-use-what" target="_blank" rel="noopener">Apache Spark - foreach Vs foreachPartitions When to use What?</a></li><li><a href="https://spark.apache.org/docs/latest/sql-data-sources-orc.html" target="_blank" rel="noopener">Spark SQL Guide - ORC File</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;https://github.com/JoeAsir/blog-image/raw/master/blog/background/beach-dawn-dusk-185927.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;This blog is the third part of Apache Spark tips sum-up learnt from my programing and debugging. Have been busy for such a long time, I have some time to carry on my personal blog. Today I’ll show you some tips about some functions in Spark SQL, and there may be some mistakes caused by my misunderstanding. Anyway, thanks if you figure out anything incorrect!&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="spark" scheme="https://asirzhao.github.io/categories/spark/"/>
    
    
      <category term="spark" scheme="https://asirzhao.github.io/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>Java Garbage Collection Overview</title>
    <link href="https://asirzhao.github.io/2019/02/26/jvm-java-garbage-collection-overview/"/>
    <id>https://asirzhao.github.io/2019/02/26/jvm-java-garbage-collection-overview/</id>
    <published>2019-02-26T08:35:08.000Z</published>
    <updated>2020-05-10T06:50:12.529Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://github.com/JoeAsir/blog-image/raw/master/blog/background/beach-boats-clouds.jpg" alt=""><br>Java Garbage Collection has confused me for such a long time when I try to tune my Spark Application, but unfortunately I’m not a good Java developer. I really feel terrible when staring at the red blocks representing high GC time in my SparkUI while having no idea how to fix it up. So I spent some time digging in GC and finally got to learn about what GC is and how to analysis the GC logs. So today I’m sharing you something I learnt and let’s move on.<br><a id="more"></a><br>Just as name presenting, Java Garbage Collection is used to collect the garbage, which is actually the unused objects, in JVM Heap memory. If you don’t know what does Heap memory represents, you’d better make sense of JVM memory management first.</p><h2 id="GC-overview"><a href="#GC-overview" class="headerlink" title="GC overview"></a>GC overview</h2><p>Let’s firstly take a quick look at on how GC track and remove the so-called grabage. The whole porcess, consisting of two steps, marking and deletion, might be much simpler than you can imagine.</p><ul><li>Marking - It’s easy to understand what <strong>Marking</strong> does if you know what GC does. Yep! <strong>Marking</strong> is purposed to distingrish and mark down which objects are still referenced and which are to be collected. And it’s a time consuming process for all the objects all scanned.</li><li>Deletion - <strong>Deleteing</strong> removes the unreferenced objects by moving the referenced objects together, it compact the memory to make benefis to the further memory allocation.</li></ul><h2 id="GC-Process"><a href="#GC-Process" class="headerlink" title="GC Process"></a>GC Process</h2><h3 id="JVM-Generations"><a href="#JVM-Generations" class="headerlink" title="JVM Generations"></a>JVM Generations</h3><p>The Heap memory in JVM is devided into manly three parts, which are Eden, Survivor and Tenured Space(and Permanet space in formal JDK version). Eden and Survivor spaces are called Yong Generation while the Tenured space Old Generation. I have to say the names of JVM Generations are really vivid, the Young Generation is where all the new objects allocated, and Old Generation is used to store long-living objects.<br><img src="https://github.com/JoeAsir/blog-image/raw/master/blog/20/20-1.png" alt=""></p><h3 id="Young-Generation"><a href="#Young-Generation" class="headerlink" title="Young Generation"></a>Young Generation</h3><p>There are two parts of Heap spaces in Young Generation, which are Eden and Survivor spaces as we talked above. And the Survivor space is actually made up with two parts, Survivor 0 and Survivor 1, aka “from” Survivor and “to” Survivor, or s0 and s1. The key point is that, those two Survivors space are not immutable as Eden space, actually they are relative just as the name “from” and “to” shows. Once an object is first allocated, it wil be stored in the Eden space. Time goes by, the Eden space is gradually filled up with so many new-born objects, then the <strong>Minor Garbage Collection(Minor GC)</strong> is triggered. Hey, you see? the <strong>Minor GC</strong> finally comes out here and it’s actually the GC process in the Young Generation.<br>In the <strong>Minor GC</strong> process, all the unreferenced or unused objects are removed, and all the referenced ones are moved to one of the Survivor spaces, which is actually the “from” Survivor space. As new objects are continuously born(or allocated), with the Eden space gradually filled up with those objects again, the <strong>Minor GC</strong> is triggered another time, all the unreferenced objects in the Eden space and “from” Survivor space are all removed while the referenced ones are moved to the “to” Survivor space. That’s why the two Survivor spaces are called “from” and “to”, really vivid, isn’t it? Hahaha. The survived objects(stored in Survivor space) are moved between these two Survivors space gathering with new-born ones moved from the Eden It’s interesting to find out that the “from” Survivor space will be called as “to” Survivor space in the next data moving time, which always keep the survived objects are all stored in one Survivor, whil another empty.<br>Once the referenced objects are moved, they are also aged once. As soon as the age of the objects reach a certain threshold(15 as the default value, and you can custom it as you wish), they would be moved to a new world, that’s the Old Generation(this moving stage is aka so-called promotion), which I’m about to talk below.<br><img src="https://github.com/JoeAsir/blog-image/raw/master/blog/20/20-2.png" alt=""></p><h3 id="Old-Generation"><a href="#Old-Generation" class="headerlink" title="Old Generation"></a>Old Generation</h3><p>As we talked above, when referenced objects are “old” enough(couldn’t stop laughing), or one of the Survivor space is full-filled, they will be then promoted to the the Tenured space, which means the long-time survived objects will be stored in the Old Generation. As <strong>Minor GC</strong> keeps invoked once again and again, survived objects will be continuous promoted to the Old Generation. And the <strong>Major Garbage Collection(Major GC)</strong> would be triggered eventually on the Old Generation to clean up and compact that space.<br>What’s more, what if the Old Generation is full? Wow, that’s really not a good news for your JVM application because the <strong>Full Garbage Collection(Full GC)</strong> is triggered, which clean up all the objects on <strong>both</strong> the Young Generation and Old Generation.<br>I have to say that there is no brief definitions for <strong>Major GC</strong> and <strong>Full GC</strong>, which really confuses me, see more details in <a href="https://plumbr.io/blog/garbage-collection/minor-gc-vs-major-gc-vs-full-gc" target="_blank" rel="noopener">this blog</a>.</p><h3 id="Stop-the-wordld-STW"><a href="#Stop-the-wordld-STW" class="headerlink" title="Stop-the-wordld(STW)"></a>Stop-the-wordld(STW)</h3><p>Now we almostly get the generatinal garbage collection process of JVM GC, but why we foucs a lot on it, and how does it ruins our application? The answer is quite simple, that is <strong>Stop-the-world</strong> aka STW. Once a GC process triggered, the JVM has to move and update those references object before the application manipulate them, otherwise there could be something wrong. <a href="https://stackoverflow.com/questions/40182392/does-java-garbage-collect-always-has-to-stop-the-world" target="_blank" rel="noopener">See this answers on Stackoverflow to learn more</a>.<br>Actually, all the GC process could bring in STW problem, and especially for <strong>Full GC</strong> process, which could be a nightmare your application proformance. </p><h2 id="Java-Garbage-Collector"><a href="#Java-Garbage-Collector" class="headerlink" title="Java Garbage Collector"></a>Java Garbage Collector</h2><ul><li><p>Serial Garbage Collector<br>As name presents, Serial Garbage Collector is basicaly works with a single thread. Serial Garbage Collector would stop all the other application threads because of the memory compaction, also known as the <strong>STW</strong> we talked about.  As a result, Serial Garbage often be used in the applications without low pause time requirements and run on client-style machines.</p></li><li><p>Parallel Garbage Collector<br>Instead of the single thread in Serial Garbage Collector, Parallel Garbage Collector uses multiple threads to preform GC. And Parallel Collector could also bring the <strong>STW</strong> as the Serial Garbage Collector does. </p></li><li><p>CMS Garbage Collector<br>The Concurrent Mark Sweep Garbage Collector, aka CMS, uses multiple garbage collector threads to reduce the pauses time in application when collecting Old Generation. Normally the CMS Garbage Collector dosen’t compact the memory and move the referenced objects together after collection, thus that’s the mainly disadvtange of CMS.</p></li><li><p>G1 Garbage Collector<br>Garbage First or G1 Collector is purposed to be the alternative of CMS Garbage Collector. G1 Collector seems to be a perfect choice as it’s a parallel, concurrent and compacting low-pause collector.</p></li></ul><blockquote><p>Unlike other collectors, G1 collector partitions the heap into a set of equal-sized heap regions, each a contiguous range of virtual memory. When performing garbage collections, G1 shows a concurrent global marking phase (i.e. phase 1 known as Marking) to determine the liveness of objects throughout the heap.<br>After the mark phase is completed, G1 knows which regions are mostly empty. It collects in these areas first, which usually yields a significant amount of free space (i.e. phase 2 known as Sweeping). It is why this method of garbage collection is called Garbage-First.</p></blockquote><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>This blog mainly talks about the overview of the JVM Garbage Collection from the generation based process to garbage collector. Honestly speaking, GC is much more complex than presented in the blog, thus there is still long way to go to make a deep dive in GC, even JVM. </p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul><li><a href="https://codeahoy.com/2017/08/06/basics-of-java-garbage-collection/" target="_blank" rel="noopener">Basics of Java Garbage Collection</a></li><li><a href="https://www.baeldung.com/jvm-garbage-collectors" target="_blank" rel="noopener">JVM Garbage Collectors</a></li><li><a href="https://www.oracle.com/webfolder/technetwork/tutorials/obe/java/gc01/index.html" target="_blank" rel="noopener">Java Garbage Collection Basics</a></li><li><a href="https://plumbr.io/blog/garbage-collection/minor-gc-vs-major-gc-vs-full-gc" target="_blank" rel="noopener">Minor GC vs Major GC vs Full GC</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;https://github.com/JoeAsir/blog-image/raw/master/blog/background/beach-boats-clouds.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;Java Garbage Collection has confused me for such a long time when I try to tune my Spark Application, but unfortunately I’m not a good Java developer. I really feel terrible when staring at the red blocks representing high GC time in my SparkUI while having no idea how to fix it up. So I spent some time digging in GC and finally got to learn about what GC is and how to analysis the GC logs. So today I’m sharing you something I learnt and let’s move on.&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="jvm" scheme="https://asirzhao.github.io/categories/jvm/"/>
    
    
      <category term="jvm" scheme="https://asirzhao.github.io/tags/jvm/"/>
    
  </entry>
  
  <entry>
    <title>Second Generation Tungsten Engine in Spark 2.x</title>
    <link href="https://asirzhao.github.io/2018/11/14/spark-second-generation-tungsten-in-spark/"/>
    <id>https://asirzhao.github.io/2018/11/14/spark-second-generation-tungsten-in-spark/</id>
    <published>2018-11-14T07:41:15.000Z</published>
    <updated>2020-05-10T06:50:12.531Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://github.com/JoeAsir/blog-image/raw/master/blog/background/adventure-climb-691668.jpg" alt=""><br>This article is about the 2nd generation Tungsten engine, which is the core project to optimize Spark performance. Compared with the 1st generation Tungsten engine, the 2nd one mainly focuses on optimizing query plan and speeding up query execution, which is a pretty aggressive goal to get orders of magnitude faster performance. Let’s take a look!<br><a id="more"></a></p><h2 id="Project-Tungsten"><a href="#Project-Tungsten" class="headerlink" title="Project Tungsten"></a>Project Tungsten</h2><p>In the past several years, both storage and network IO bandwidth have been largely improved, while CPU efficiency bound has not. As results, CPU now becomes the new bottleneck and we have to substantially try to improve the efficiency of memory and CPU and push the performance of Spark closer to the limits of modern hardware, which is the main propose of Tungsten. To achieve it, three initiatives in 1st generation Tungsten engine are proposed, including:</p><blockquote><ul><li>Memory Management and Binary Processing: leveraging application semantics to manage memory explicitly and eliminate the overhead of JVM object model and garbage collection</li><li>Cache-aware computation: algorithms and data structures to exploit memory hierarchy</li><li>Code generation: using code generation to exploit modern compilers and CPUs</li></ul></blockquote><p>As we can see, the first two initiatives mainly foucus on memory, and the last one are for CPUs. In the 2nd generation Tungsten engine, rather than code generation, WholeStageCodeGen and Vectorization are proposed for the order of magnitude faster.</p><h2 id="WholeStageCodeGen"><a href="#WholeStageCodeGen" class="headerlink" title="WholeStageCodeGen"></a>WholeStageCodeGen</h2><p>As the name presents, WholeStageCodeGen, aka whole-stage-code-generation, collapses the entire query into a single stage, or maybe a single function. So why combining all the query into a single stage could significantly improve the CPU efficiency and gain performance? We may take a quick look at what it looks like in 1st Tungsten engine.</p><h3 id="Volcano-Iterator-Model"><a href="#Volcano-Iterator-Model" class="headerlink" title="Volcano Iterator Model"></a>Volcano Iterator Model</h3><p>What a vividly name! Volcano iterator model, as presents in the figure, would generate an interface for each operator, and the each operator would get the results from its parent one by one, just like the volcano eruption from bottom to top.<br><img src="https://github.com/JoeAsir/blog-image/raw/master/blog/19/19-1.png" alt=""><br>Although Volcano Model can combine arbitrary operators together without worry about the data type of each operator provides, which makes it a popular classic query evaluation strategy in the past 20 years, there are still many downsides and we will take about it later. </p><h3 id="Bottom-up-Model"><a href="#Bottom-up-Model" class="headerlink" title="Bottom-up Model"></a>Bottom-up Model</h3><p>In <a href="https://databricks.com/blog/2016/05/23/apache-spark-as-a-compiler-joining-a-billion-rows-per-second-on-a-laptop.html" target="_blank" rel="noopener">this blog</a>, a hand-written code is proposed to implement the query in the figure above, it’s just a so simple for-loop that even a college freshman can complete, which is:<br><figure class="highlight scala hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-keyword">var</span> count = <span class="hljs-number">0</span></span><br><span class="line"><span class="hljs-keyword">for</span> (ss_item_sk in store_sales) &#123;</span><br><span class="line">  <span class="hljs-keyword">if</span> (ss_item_sk == <span class="hljs-number">1000</span>) &#123;</span><br><span class="line">      count += <span class="hljs-number">1</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>Even though the code is pretty simple, the comparison of performance  between Volcano Iterator Model and Bottom-up Model will do shake you.<br><img src="https://github.com/JoeAsir/blog-image/raw/master/blog/19/19-2.png" alt=""><br>But why is that? Seems it turn out it to be caused by following downsides of Volcano Iterator Model:</p><ul><li>Too many virtual functions calls:<br>In Volcano Iterator Model, when one operator call for the next operator, a virtual function <strong>next()</strong> would be called. But in Bottom-up Model, there is no virtual function called because all the operations are combined in a single loop function.</li><li>Intermediate data of Volcano Iterator Model are in memory while of Bottom-up Model are in CPU registers:<br>As one operator transforms data to another one in Volcano Iterator Model, the intermediate data can only be cached in Memory. But for Bottom-up Model, as there is no need to be transformed, data are always in CPU registers.</li><li>Volcano Iterator Model don’t take advantage of modern techniques, which Bottom-up Model do, such as loop-pipelining, loop-unrolling and so on:<br>As Bottom-up Model evaluates the whole query into a single loop function, some modern technique can be used in the Bottom-up Model, I will show you two of them, which are loop-pipelining and loop-unrolling.</li></ul><h4 id="Loop-pipelining"><a href="#Loop-pipelining" class="headerlink" title="Loop-pipelining"></a>Loop-pipelining</h4><p>In a loop iteration function, one iteration of loop usually begins when the previous one is complete, which means the iteration of the loop should be executed sequentially one by one. While loop-pipelining can make some differences. The figure below could learn about it clearly. Loop-pipelining increases the parallelism of the loop iteration by implementing a concurrent manner.<br><img src="https://github.com/JoeAsir/blog-image/raw/master/blog/19/19-3.png" alt=""></p><h4 id="Loop-unrolling"><a href="#Loop-unrolling" class="headerlink" title="Loop-unrolling"></a>Loop-unrolling</h4><p>Loop-unrolling is another technique to exploit parallelism between loop iterations. Let’s learn about it by the code:<br><figure class="highlight java hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment">// without loop-unrolling</span></span><br><span class="line"><span class="hljs-keyword">int</span> sum=<span class="hljs-number">0</span>;</span><br><span class="line"><span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> i=<span class="hljs-number">0</span>; i&lt;<span class="hljs-number">10</span>; i++) &#123;</span><br><span class="line">    sum+=a[i];</span><br><span class="line">&#125;</span><br><span class="line"><span class="hljs-comment">// with loop-unrolling</span></span><br><span class="line"><span class="hljs-keyword">int</span> sum = <span class="hljs-number">0</span>;</span><br><span class="line"><span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> i=<span class="hljs-number">0</span>; i&lt;<span class="hljs-number">10</span>; i+=<span class="hljs-number">2</span>) &#123;</span><br><span class="line">    sum += a[i];</span><br><span class="line">    sum += a[i+<span class="hljs-number">1</span>];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>As shown above, loop-unrolling creates multiple copies of the loop body and also changes the loop iteration counter. Reducing loop iteration number, loop-unrolling also create more operation in each loop iteration and, as results, more parallisim could be exploited. </p><h3 id="Whole-Stage-Code-Generation"><a href="#Whole-Stage-Code-Generation" class="headerlink" title="Whole Stage Code Generation"></a>Whole Stage Code Generation</h3><p>Fusing operators together to make the generated code looks like the hand-writing bottom-up model, WholeStageCodeGen makes chains of operators as a single stage, and it has been the alternatives for the Code Generation in Catalyst Optimization. It takes advantages of hand-writing and significantly optimizes the query evaluation and can be easily found in the DAG of your Spark application.</p><h2 id="Vectorization"><a href="#Vectorization" class="headerlink" title="Vectorization"></a>Vectorization</h2><p>Although the WholeStageCodeGen makes a huge optimization of the query plan, there are still some problems. For example, when we import some external integrations, such as tensorflow, scikit-learn, and some python packages, these code cannot be optimized by the WholeStageCodeGen cause they cannot be merged in our code. What’s more, the complicated IO cannot be fused, reading Parquet or ORC for instance. How to speed up this excution? The answer is improving the in-core parallelism for an operation of data, so <strong>Vector Processing</strong> and <strong>Column Format</strong> are used in 2nd generation Tungsten engine.</p><h3 id="Vector-Processing"><a href="#Vector-Processing" class="headerlink" title="Vector Processing"></a>Vector Processing</h3><blockquote><p>In computing, a vector processor or array processor is a central processing unit (CPU) that implements an instruction set containing instructions that operate on one-dimensional arrays of data called vectors, compared to scalar processors, whose instructions operate on single data items.</p></blockquote><p>The following figure presents the differences between Scalar and Vector Processing.<br><img src="https://github.com/JoeAsir/blog-image/raw/master/blog/19/19-4.png" alt=""></p><p>And we find out that Vector Processing really fasten the operation. How does Vector Processing be implemented? We may need to learn something about SIMD(Single Instruction, Multiple Data)</p><blockquote><p>Single instruction, multiple data (SIMD) is a class of parallel computers in Flynn’s taxonomy. It describes computers with multiple processing elements that perform the same operation on multiple data points simultaneously.</p></blockquote><p>Let me show one figure to show what’s SIMD breifly.<br><img src="https://github.com/JoeAsir/blog-image/raw/master/blog/19/19-5.png" alt=""><br>As presented above, SIMD can process multuple data via single instruction, and the data are all in an one-dimesional vector. As results, take advantage of SMID, Vector Processing can improve the in-core parallelism and thus make the operation faster. For the sake of the data are all in an one-dimesonal vector in SIMD, Colunm Format could be a better choice for Spark.</p><h3 id="Column-Format"><a href="#Column-Format" class="headerlink" title="Column Format"></a>Column Format</h3><p>Column Format has been widely used in many fields, such as disk storage. The following figure shows the differences between Row Format and Column Format.<br><img src="https://github.com/JoeAsir/blog-image/raw/master/blog/19/19-6.png" alt=""></p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>WholeStageCodeGen and Vectorization in 2nd generation Tungsten engine really optimize the query plan and speed up the query execution. Compared with 1st generation Tungsten engine, the 2nd one mainly foucses on improving the CPU parallelism to take advtange of some modern techniques. There are many terms and konwledges about CPU exection, which I learn so little that may make some mistakes in this blog, so it would be very nice of you to figure out any single mistake.</p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul><li><a href="https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html" target="_blank" rel="noopener">Project Tungsten: Bringing Apache Spark Closer to Bare Metal</a></li><li><a href="https://databricks.com/blog/2016/05/23/apache-spark-as-a-compiler-joining-a-billion-rows-per-second-on-a-laptop.html" target="_blank" rel="noopener">Apache Spark as a Compiler: Joining a Billion Rows per Second on a Laptop<br>Deep dive into the new Tungsten execution engine</a></li><li><a href="https://spoddutur.github.io/spark-notes/second_generation_tungsten_engine.html" target="_blank" rel="noopener">Spark 2.x - 2nd generation Tungsten Engine</a></li><li><a href="https://www.xilinx.com/support/documentation/sw_manuals/xilinx2015_2/sdsoc_doc/topics/calling-coding-guidelines/concept_pipelining_loop_unrolling.html#" target="_blank" rel="noopener">Loop Pipelining and Loop Unrolling</a></li><li><a href="http://www.cac.cornell.edu/education/training/ParallelFall2012/Vectorization.pdf" target="_blank" rel="noopener">Vectorization: Ranger to Stampede Transition</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;https://github.com/JoeAsir/blog-image/raw/master/blog/background/adventure-climb-691668.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;This article is about the 2nd generation Tungsten engine, which is the core project to optimize Spark performance. Compared with the 1st generation Tungsten engine, the 2nd one mainly focuses on optimizing query plan and speeding up query execution, which is a pretty aggressive goal to get orders of magnitude faster performance. Let’s take a look!&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="spark" scheme="https://asirzhao.github.io/categories/spark/"/>
    
    
      <category term="spark" scheme="https://asirzhao.github.io/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark Tips Sum-up Part-2</title>
    <link href="https://asirzhao.github.io/2018/10/13/spark-sumup-part-2/"/>
    <id>https://asirzhao.github.io/2018/10/13/spark-sumup-part-2/</id>
    <published>2018-10-13T01:53:02.000Z</published>
    <updated>2020-05-10T06:50:12.532Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://github.com/JoeAsir/blog-image/raw/master/blog/background/boulders-calm-conifers-426894.jpg" alt=""><br>This article presents some tips of Apache Spark, which is part 2 of the series. All the tips below are based on the real problems which I met. Despite the background, the tips below are of valuable reference. I’ve tried a lot to learn about Apache Spark but can’t know the detail of every part of it. I’d appreciate it if you figure out the mistakes in this article.<br><a id="more"></a></p><h2 id="Coalesce"><a href="#Coalesce" class="headerlink" title="Coalesce"></a>Coalesce</h2><p>Changing the number of partitions could benefit the performance of your Spark application. A large number of partitions may increase the parallelism of the process while too many tasks would be executed on a single executor, which could cost more running time. In contrast, a small number of partition might improve the complexity of each task and slow down the whole process. To solve this trade-off problem, <em>repartition()</em> and <em>coalesce()</em> is proposed in Apache Spark.<br>Before talking about the detail about <em>coalesce()</em>, let’s review the concept of transformation with wide-dependencies and narrow-dependencies.</p><blockquote><ul><li>Wide-dependencies: each partition of the parent RDD is used by at most one partition of the child RDD.</li><li>Narrow-dependencies: multiple child partitions may depend on each partition in the parent RDD.</li></ul></blockquote><p>According to the definition above, <em>repartition()</em> is a typical transformation with wide-dependencies and thus can bring in shuffle operation when triggered. But what about <em>coalesce()</em>? To find out more about it, let’s see the definition first.<br><figure class="highlight scala hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment">// coalesce() for RDD is defined in org.apache.spark.rdd.RDD</span></span><br><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">coalesce</span></span>(numPartitions: <span class="hljs-type">Int</span>, shuffle: <span class="hljs-type">Boolean</span> = <span class="hljs-literal">false</span>,</span><br><span class="line">    partitionCoalescer: <span class="hljs-type">Option</span>[<span class="hljs-type">PartitionCoalescer</span>] = <span class="hljs-type">Option</span>.empty) </span><br><span class="line">    (<span class="hljs-keyword">implicit</span> ord: <span class="hljs-type">Ordering</span>[<span class="hljs-type">T</span>] = <span class="hljs-literal">null</span>)</span><br><span class="line">   : <span class="hljs-type">RDD</span>[<span class="hljs-type">T</span>] = withScope &#123;...&#125;</span><br><span class="line"></span><br><span class="line"><span class="hljs-comment">// coalesce() for Dataset is defined in org.apache.spark.sql.Dataset</span></span><br><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">coalesce</span></span>(numPartitions: <span class="hljs-type">Int</span>): <span class="hljs-type">Dataset</span>[<span class="hljs-type">T</span>] = withTypedPlan &#123;</span><br><span class="line">    <span class="hljs-type">Repartition</span>(numPartitions, shuffle = <span class="hljs-literal">false</span>, logicalPlan)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>For RDDs, <em>coalesce()</em> has a boolean typed parameter called <em>shuffle</em>. The <em>coalesce()</em> can be treated as <em>repartition()</em> as <em>shuffle</em> is set to <em>True</em>, which is a transformation with wide-dependencies. In contrast, when <em>shuffle</em> is <em>False</em>, <em>coalesce()</em> is a transformation with narrow-dependencies and only can reduce the number of partitions, which means you cannot increase the number of partitions by setting the parameter <em>numPartitions</em>. As for DataFrame/Dataset API, <em>coalesce()</em> is a transformation with narrow-dependencies as there is no shuffle operation at all. As results, <em>coalesce()</em> cannot increase the number of DataFrame/Dataset’s partitions and can only be used to reduce DataFrame/Dataset’s partitions.<br>After understanding the above, there is a crucial tip for you. When you use <em>coalesce()</em> and reduce the number of partitions, which may cause a problem that the partition of the whole stage would be decreased and the computation could be even slower than you expect. since no shuffle operation performed, the stage executes with the level of parallelism assigned by <em>coalesce()</em>. To avoid this problem, you can set <em>shuffle=True</em> for RDDs or use <em>repartition()</em> instead for DataFrame/Dataset to split the whole stage by a shuffle.</p><h2 id="Read-ORC-Table"><a href="#Read-ORC-Table" class="headerlink" title="Read ORC Table"></a>Read ORC Table</h2><p>Reading data from and writing data to HIVE tables is quite often when I use Apache Spark. And we are now using ROC format to store HIVE table.</p><blockquote><p>ORC is a self-describing type-aware columnar file format designed for Hadoop workloads. It is optimized for large streaming reads, but with integrated support for finding required rows quickly. </p></blockquote><p>Generally, when Spark reads data from HDFS, the initial number of partitions are determined by the number of blocks of data store in HDFS, with one block represents one partition in Spark. However, when I read the ORC format HIVE table, the partition number is equal to the number of files stored on HDFS, not number block. That’s caused by ORC split strategy set by <em>hive.exec.orc.split.strategy</em>, which determines what strategy ORC should use to create splits for execution. The available option includes “BI”, “ETL” and “HYBRID”</p><blockquote><p>The HYBRID mode reads the footers for all files if there are fewer files than expected mapper count, switching over to generating 1 split per file if the average file sizes are smaller than the default HDFS block size. ETL strategy always reads the ORC footers before generating splits, while the BI strategy generates per-file splits fast without reading any data from HDFS.</p></blockquote><p>As results, when the split strategy is set to ETL, Spark would take more time to generate tasks and the number of partitions is based on the size of HDFS blocks. In contrast, BI strategy would make Spark generate tasks immediately and the number of tasks is equal to the number of files of HIVE table. These two strategies seem to be a trade-off for us and it’s better for us to decide by the actual situation.  </p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li><a href="https://hackernoon.com/managing-spark-partitions-with-coalesce-and-repartition-4050c57ad5c4" target="_blank" rel="noopener">Managing Spark Partitions with Coalesce and Repartition</a></li><li><a href="http://opencarts.org/sachlaptrinh/pdf/28044.pdf" target="_blank" rel="noopener">High Performence Spark</a></li><li><a href="https://orc.apache.org/" target="_blank" rel="noopener">Apache ORC</a></li><li><a href="https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties#ConfigurationProperties-hive.exec.orc.split.strategy" target="_blank" rel="noopener">Apache ORC Configuration Properties</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;https://github.com/JoeAsir/blog-image/raw/master/blog/background/boulders-calm-conifers-426894.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;This article presents some tips of Apache Spark, which is part 2 of the series. All the tips below are based on the real problems which I met. Despite the background, the tips below are of valuable reference. I’ve tried a lot to learn about Apache Spark but can’t know the detail of every part of it. I’d appreciate it if you figure out the mistakes in this article.&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="spark" scheme="https://asirzhao.github.io/categories/spark/"/>
    
    
      <category term="spark" scheme="https://asirzhao.github.io/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>Catalyst Optimization in Spark SQL</title>
    <link href="https://asirzhao.github.io/2018/09/25/spark-catalyst-optimization/"/>
    <id>https://asirzhao.github.io/2018/09/25/spark-catalyst-optimization/</id>
    <published>2018-09-25T11:52:15.000Z</published>
    <updated>2020-05-10T06:50:12.531Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://github.com/JoeAsir/blog-image/raw/master/blog/background/app-applications-apps.jpg" alt=""><br>Spark SQL is one of the most important components of Apache Spark and has become the fundation of Structure Streaming, ML Pipeline, GraphFrames and so on since Spark 2.0. Also, Spark SQL provides SQL queries and DataFrame/Dataset API, both of which are optimized by Catalyst. Let’s talk about Catalyst today.<br><a id="more"></a><br>Catalyst Optimizer is a great optimization framework to improve Spark SQL performance, especially query capabilities of Spark SQL and it is written by functional programming construct in Scala. RBO(Rule Based Optimization) and CBO(Cost Based Optimization) are the optimization principles of Catalyst. The concept of Catalyst is familiar to many other query optimizers, so understanding Catalyst may help you learn the working pipeline of other query optimizers.</p><h2 id="Trees-And-Rules"><a href="#Trees-And-Rules" class="headerlink" title="Trees And Rules"></a>Trees And Rules</h2><p>We will have a quick review of trees and rules. You can learn more about them by the references. </p><h3 id="Trees"><a href="#Trees" class="headerlink" title="Trees"></a>Trees</h3><p>The tree is the basic datatype in Catalyst, which is consisted of node objects. Each node has a node type, and some children, which could be none. Let’s have an example, the tree for expression x+(1+2) could be translated in Scala as:</p><p><img src="https://github.com/JoeAsir/blog-image/raw/master/blog/18/18-3.png" alt=""><br><figure class="highlight scala hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-type">Add</span>(<span class="hljs-type">Attribute</span>(x),<span class="hljs-type">Add</span>(<span class="hljs-type">Literal</span>(<span class="hljs-number">1</span>),<span class="hljs-type">Literal</span>(<span class="hljs-number">2</span>)))</span><br></pre></td></tr></table></figure></p><p>Actually, most of the SQL query optimization, like Catalyst, would transform the SQL to a huge tree structure as the first step of the optimization. Tree structure could be modified, tailored and optimized easily, also it represents the query plan briefly. What’s more, the data can be thrown to every node of the tree by the query plan iteratively. That’s why tree datatype is used and introduced firstly in Catalyst.</p><h3 id="Rules"><a href="#Rules" class="headerlink" title="Rules"></a>Rules</h3><p>Trees can be manipulated by the rules. The optimization can be treated as some transformations from one tree to another under some rules we provide. With the help of rules, we can run arbitrary code and many other operations on the input tree. And the Catalyst will know which part of the tree that rules could match or apply, and will automatically skip over the tree that does not match. The rules are all applied by Scala case class, in other words, one rule can match multiple patterns. Let’s see an example.<br><figure class="highlight scala hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tree.transform &#123;</span><br><span class="line">  <span class="hljs-keyword">case</span> <span class="hljs-type">Add</span>(<span class="hljs-type">Literal</span>(c1),<span class="hljs-type">Literal</span>(c2)) =&gt; <span class="hljs-type">Literal</span>(c1+c2)</span><br><span class="line">  <span class="hljs-keyword">case</span> <span class="hljs-type">Add</span>(left, <span class="hljs-type">Literal</span>(<span class="hljs-number">0</span>)) =&gt; left</span><br><span class="line">  <span class="hljs-keyword">case</span> <span class="hljs-type">Add</span>(<span class="hljs-type">Literal</span>(<span class="hljs-number">0</span>), right) =&gt; right</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h2 id="Catalyst"><a href="#Catalyst" class="headerlink" title="Catalyst"></a>Catalyst</h2><p>From this section, we will know how Catalyst optimizes Spark SQL, which might improve your understanding of how Spark SQL works and how to manipulate it better. There are four plan through the Catalyst, which are Parsed(Unresolved) Logical Plan, Analyzed Logical Plan, Optimized Logical Plan and Physical Plan. You can find all the four plans in your Spark UI. The figure below presents the workflow of Spark SQL, and the Analysis, Logical Optimization and Physical Planning are all working under the Catalyst, and we will go over one by one.</p><p><img src="https://github.com/JoeAsir/blog-image/raw/master/blog/18/18-2.png" alt=""></p><h3 id="Parser"><a href="#Parser" class="headerlink" title="Parser"></a>Parser</h3><p>The first stage in the figure above is called Parser. Despite not a part of Catalyst, Parser is also very crucial in the whole optimization. Spark SQL transform SQL queries to an <a href="http://ns.inria.fr/ast/sql/index.html" target="_blank" rel="noopener">AST</a>(Abstract Syntax Tree), also called Parsed Logical Plan in Catalyst,  by <a href="https://www.antlr.org/" target="_blank" rel="noopener">ANTLR</a>, which is also used in the Hive, presto and so on. However, DataFrame/Dataset object can be transformed to Parsed Logical Plan by the API.</p><h3 id="Analysis"><a href="#Analysis" class="headerlink" title="Analysis"></a>Analysis</h3><p>Returned by the Parser, Parsed Logical Plan contains many unresolved attribute reference of relation. For example, you have no idea whether the column name provided is correct or type of the column, either. Spark SQL use Catalyst and Catalog object that tracks the data all the time to resolve the attributes. Looking up relations by name from Catalog, mapping all the named attributes to the input, checking the attributes which match to the same value and giving the unique ID and pushing the type information of the attributes, the Parsed Logical Plan is transformed to Analyzed Logical Plan by Catalyst.</p><blockquote><p>Catalogs are named collections of schemas in an SQL-environment. An SQL-environment contains zero or more catalogs. A catalog contains one or more schemas, but always contains a schema named INFORMATION_SCHEMA that contains the views and domains of the Information Schema.</p></blockquote><h3 id="Logcial-Optimization"><a href="#Logcial-Optimization" class="headerlink" title="Logcial Optimization"></a>Logcial Optimization</h3><p>Logical Optimization is mainly based on the rules, which is also called RBO(Rules Based Optimization). Lots of rules are provided in this step to accomplish RBO, including constant folding, predicate pushdown, project pruning and so on. As results, the Optimized Logical Plan is returned by Logical Optimization from Analyzed Logical Plan. Some figures below describe these ROB mentioned.<br><strong>Predicate pushdown</strong> can reduce the computation of join operation by filtering unnecessary data before join.</p><p><img src="https://github.com/JoeAsir/blog-image/raw/master/blog/18/18-4.png" alt=""><br><strong>Constant folding</strong> avoids calculating the same operation between constants for each record.</p><p><img src="https://github.com/JoeAsir/blog-image/raw/master/blog/18/18-5.png" alt=""><br><strong>Column Pruning</strong> makes Spark SQL only load data which would be used in the table.</p><p><img src="https://github.com/JoeAsir/blog-image/raw/master/blog/18/18-6.png" alt=""></p><h3 id="Physical-Planning"><a href="#Physical-Planning" class="headerlink" title="Physical Planning"></a>Physical Planning</h3><p>Since we get the Optimized Logical Plan, Spark still doesn’t know how to execute the it. For instance, Spark knows that there is a join operation, while whether sortMerge Join or Hash Shuffle Join should be invoked. So all the operation would be mapped into a real exection through <a href="https://github.com/apache/spark/blob/v2.3.2/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala" target="_blank" rel="noopener">SparkStrategy</a>. Transformed from Opktimized Logical Plan by Physical Planning, Physical Plan would guide the Spark on how to handle the data. In Physical Planning, several Physical Plans are generated from Logical Plan, using physical operator matches the Spark execution engine. Using CBO(Cost Based Optimization), Catalyst will select the best performance one as the Physical Plan. However, the CBO is now only used for join and aggregation algorithms selection. Also, CBO is used in Physical Planning to pipelining projections or filters into single Spark <em>map()</em> transformation. What’s more, all</p><h3 id="Code-Generation"><a href="#Code-Generation" class="headerlink" title="Code Generation"></a>Code Generation</h3><p>Getting the Physical Plan from the Physical Planning stage, Spark will translate the AST into Java bytecode to run on each machine. Thanks to quasiquotes, a greate feature of Scala, ASTs would be built by Scala language based on the Physical Plan. And the AST will be send to Scala Compile at runtime to generate<br>ava bytecode. </p><blockquote><p>We use Catalyst to transform a tree representing an expression in SQL to an AST for Scala code to evaluate that expression, and then compile and run the generated code.</p></blockquote><p>In Spark 2.x, the WholeStageCodeGen has been used in code generation stage, you can read more from <a href="https://joeasir.github.io/2018/11/14/spark-second-generation-tungsten-in-spark" target="_blank" rel="noopener">here</a>.</p><p>At last, if you want to see the Logical Plan and Physical Plan of your own Spark SQL, you may do as follows:<br><figure class="highlight scala hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment">// for Logical Plan</span></span><br><span class="line">spark.sql(<span class="hljs-string">"your SQL"</span>).queryExecution</span><br><span class="line"><span class="hljs-comment">// for Physical Plan</span></span><br><span class="line">spark.sql(<span class="hljs-string">"your SQL"</span>).explain</span><br></pre></td></tr></table></figure></p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul><li><a href="https://databricks.com/blog/2015/04/13/deep-dive-into-spark-sqls-catalyst-optimizer.html" target="_blank" rel="noopener">Deep Dive into Spark SQL’s Catalyst Optimizer</a></li><li><a href="https://data-flair.training/blogs/spark-sql-optimization-catalyst-optimizer/" target="_blank" rel="noopener">Spark SQL Optimization – Understanding the Catalyst Optimizer</a></li><li><a href="https://github.com/apache/spark/tree/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst" target="_blank" rel="noopener">Catalyst Source Code</a></li><li><a href="https://docs.scala-lang.org/overviews/quasiquotes/intro.html" target="_blank" rel="noopener">Quasiquotes Introduction</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;https://github.com/JoeAsir/blog-image/raw/master/blog/background/app-applications-apps.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;Spark SQL is one of the most important components of Apache Spark and has become the fundation of Structure Streaming, ML Pipeline, GraphFrames and so on since Spark 2.0. Also, Spark SQL provides SQL queries and DataFrame/Dataset API, both of which are optimized by Catalyst. Let’s talk about Catalyst today.&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="spark" scheme="https://asirzhao.github.io/categories/spark/"/>
    
    
      <category term="spark" scheme="https://asirzhao.github.io/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>From Spark RDD to DataFrame/Dataset</title>
    <link href="https://asirzhao.github.io/2018/09/22/spark-from-rdd-to-dataframe-dataset/"/>
    <id>https://asirzhao.github.io/2018/09/22/spark-from-rdd-to-dataframe-dataset/</id>
    <published>2018-09-22T08:49:15.000Z</published>
    <updated>2020-05-10T06:50:12.531Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://github.com/JoeAsir/blog-image/raw/master/blog/background/blur-close-up-code.jpg" alt=""><br>This article presents the relationship between Spark RDD, DataFrame and Dataset, and talks about both the advantages and disadvantages of them. RDD is the fundamental API since the inception of Spark and DataFrame/Dataset API is also pretty popular since Spark 2.0. What’s the differences between them and how to decide which API to be imported, let’s have a quick look.<br><a id="more"></a></p><h2 id="RDD"><a href="#RDD" class="headerlink" title="RDD"></a>RDD</h2><p>RDD (aka Resilient Distributed Dataset) is the most fundamental API, it’s so critical that all the computation in Spark are based on it. The RDD is distributed, immutable, type-safed, unstructured, partitioned and low-leveled API, which offers transformations and actions. Let’s have a quick review to the attributes of RDD. RDD has some awesome characteristics which make it the foundation of the Apache Spark. Let’s take a look and learn about the details one by one.</p><h3 id="Distributed-data-abstraction"><a href="#Distributed-data-abstraction" class="headerlink" title="Distributed data abstraction"></a>Distributed data abstraction</h3><p>The first attribute of RDD is the logical distributed abstraction, which makes RDD can run over the entire clusters. The RDD can be set across the storage and divided into many partitions so that the lambda function or any computation function you provide will execute on each partition separately. That’s really awesome as RDD is the logical distributed abstraction, the computation will be much faster as data is divided and run in parallel on several executors.</p><h3 id="Resilient-and-immutable"><a href="#Resilient-and-immutable" class="headerlink" title="Resilient and immutable"></a>Resilient and immutable</h3><p>RDD is also resilient and immutable. When an RDD is transformed to the next RDD, and then to the third one, and all the RDDs get recorded as a lineage of where they come from. The lineage can also be recorded as the acyclic graph of the RDDs, from which we can recreate any RDD in it when something goes wrong, and that’s why RDD is resilient.<br>As for the immutability, when you make a transformation, the original RDD remains unaltered so that you can go back through the acyclic graph and recreate it at any time and point during the execution. Remember, the transformation operation creates a new RDD from the previous one instead of altering the previous one. </p><h3 id="Compile-time-type-safe"><a href="#Compile-time-type-safe" class="headerlink" title="Compile-time type-safe"></a>Compile-time type-safe</h3><p>RDD is compiled type-safe and you can name the RDD with particular type briefly. Spark application could be complicated and debugging in the distributed environment could be cumbersome, the compiled type-safe really save time as it could find the type error in the compile time.</p><h3 id="Unstructured-Structured-data"><a href="#Unstructured-Structured-data" class="headerlink" title="Unstructured/Structured data"></a>Unstructured/Structured data</h3><p>The fourth attribute is that data can be unstructured, streaming data from the media for instance, or semi-structured, log files with some particular date, time or url information for example. Since RDD would not care about the structure or schema of the data, it’s good for those data without structures. Also, RDD can manipulate structured data, though it doesn’t understand the different kinds of types and all depends on how you parse the data.</p><h3 id="Lazy-evaluation"><a href="#Lazy-evaluation" class="headerlink" title="Lazy evaluation"></a>Lazy evaluation</h3><p>Lazy evaluation in Apache Spark means the execution will not start until an action is triggered. In another word, the RDD will not be loaded and computed until it is necessary. And there are some benefits of lazy evaluation. Lazy evaluation really reduces the time and space complexities and speeds up the whole execution. </p><h2 id="DataFrame-Dataset"><a href="#DataFrame-Dataset" class="headerlink" title="DataFrame/Dataset"></a>DataFrame/Dataset</h2><p>DataFrame/Dataset, unlike RDD, in high-level API dealing with structured data. Data is organized into named columns in DataFrame and can be manipulated type-safely. In Scala, DataFrame is just an alias for <em>Dataset[Row]</em>, and in Java, there is only Dataset API, as for Python and R, there is only DataFrame API provided since Python and R have no compile-time type-safety.<br>There are several benefits of DataFrame/Dataset API. I just want to talk about two of them, which I think are pretty awesome.</p><h3 id="Static-typing-and-runtime-type-safety"><a href="#Static-typing-and-runtime-type-safety" class="headerlink" title="Static-typing and runtime type-safety"></a>Static-typing and runtime type-safety</h3><p>DataFrame/Dataset presents static-typing and runtime type-safety. You may have a spelling error when you are typing a SQL such as typing <em>form</em> rather than <em>from</em>, and you would not find the syntax errors until the runtime, however, you will catch these errors at compile time in the DataFrame/Dataset API. Also, as for some analysis errors, the column you queried is not in the schema for example, you can catch these errors when compiling in Dataset while until running in SQL and DataFrame.<br><img src="https://github.com/JoeAsir/blog-image/raw/master/blog/17/17-1.png" alt=""></p><h3 id="Nice-performance"><a href="#Nice-performance" class="headerlink" title="Nice performance"></a>Nice performance</h3><p>DataFrame/Dataset API can make the execution more intelligent and efficient. You are telling Spark how-to-do a operation when using RDD, while what-to-do using DataFrame/Dataset. Let’s have a look at the example.<br><figure class="highlight scala hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">rdd.filter&#123;<span class="hljs-keyword">case</span>(project, page, numRequests) =&gt; project==<span class="hljs-symbol">'e</span>n'&#125;.</span><br><span class="line">    map&#123;<span class="hljs-keyword">case</span>(_,page,numRequests) =&gt; (page, numRequests)&#125;.</span><br><span class="line">    reduceByKey(_+_).</span><br><span class="line">    filter&#123;<span class="hljs-keyword">case</span>(page,_) =&gt; !isSpecialPage(page)&#125;.</span><br><span class="line">    take(<span class="hljs-number">100</span>).foreach &#123;<span class="hljs-keyword">case</span> (project, requests) =&gt; println(<span class="hljs-string">s"projec:<span class="hljs-subst">$requests</span>"</span><span class="hljs-string">")&#125;</span></span><br></pre></td></tr></table></figure></p><p>The code above can be run perfectly without any bug. But think about it, the RDD execute a <em>filter</em> followed by <em>reduceByKey</em> transformation, which means we filter some data after shuffling the entire data. That’s really a waste because shuffle operation is very expensive. However, this problem will be solved in DataFrame/Dataset. Based on the Catalyst, DataFrame/Dataset will optimize the query operation by rules and cost, thus the execution is much smarter than the raw RDD.</p><h2 id="When-to-Use"><a href="#When-to-Use" class="headerlink" title="When to Use"></a>When to Use</h2><p>Since we take a look at the RDD and DataFrame/Dataset, I make a list about when to use RDD and when to use DataFrame/Dataset.</p><h3 id="When-to-use-RDD"><a href="#When-to-use-RDD" class="headerlink" title="When to use RDD"></a>When to use RDD</h3><ul><li>When you want more about the low-level control of dataset</li><li>When you are dealing with some unstructred data</li><li>When you prefer manipulate data with lambda function</li><li>When you don’t care about schema or structure of data</li></ul><h3 id="When-to-use-DataFrame-Dataset"><a href="#When-to-use-DataFrame-Dataset" class="headerlink" title="When to use DataFrame/Dataset"></a>When to use DataFrame/Dataset</h3><ul><li>When you are dealing with structured data</li><li>When you want more code optimization and better performance</li></ul><p>All in all, I do recommend you to use DataFrame/Dataset API as you can for their easy-using and better optimization. Supporting by Catalyst and Tungsten, DataFrame/Dataset can reduce your time of optimization, thus you can pay more attention to the data itself. </p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul><li><a href="https://databricks.com/blog/2016/07/14/a-tale-of-three-apache-spark-apis-rdds-dataframes-and-datasets.html" target="_blank" rel="noopener">A Tale of Three Apache Spark APIs: RDDs, DataFrames, and Datasets</a></li><li><a href="https://data-flair.training/blogs/apache-spark-rdd-vs-dataframe-vs-dataset/" target="_blank" rel="noopener">Apache Spark RDD vs DataFrame vs DataSet</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;https://github.com/JoeAsir/blog-image/raw/master/blog/background/blur-close-up-code.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;This article presents the relationship between Spark RDD, DataFrame and Dataset, and talks about both the advantages and disadvantages of them. RDD is the fundamental API since the inception of Spark and DataFrame/Dataset API is also pretty popular since Spark 2.0. What’s the differences between them and how to decide which API to be imported, let’s have a quick look.&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="spark" scheme="https://asirzhao.github.io/categories/spark/"/>
    
    
      <category term="spark" scheme="https://asirzhao.github.io/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark Tips Sum-up Part-1</title>
    <link href="https://asirzhao.github.io/2018/09/15/spark-sumup-part-1/"/>
    <id>https://asirzhao.github.io/2018/09/15/spark-sumup-part-1/</id>
    <published>2018-09-15T14:15:40.000Z</published>
    <updated>2020-05-10T06:50:12.532Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://github.com/JoeAsir/blog-image/raw/master/blog/background/blurred-background-close-up-coffee-cup.jpg" alt=""><br>This article is about things I learned about Apache Spark recently. I’ve been struggling with Spark tuning for more than one month, including shuffle tuning, GC time tuning and so on. Honestly speaking, Apache Spark is just like an untamed horse, it could be a beauty if you tune it well while a nightmare if not. So let me show you some tips I’ve learned from my work. This article is part 1 and here we go.<br><a id="more"></a></p><h2 id="RDD-vs-DataFrame-Partition-Number-in-Shuffle"><a href="#RDD-vs-DataFrame-Partition-Number-in-Shuffle" class="headerlink" title="RDD vs DataFrame Partition Number in Shuffle"></a>RDD vs DataFrame Partition Number in Shuffle</h2><p>Shuffle plays a pretty crucial role in MapReduce and Spark, which can influence a lot on the performance of the whole job. The shuffle would not only separate one stage but bring much more storage and I/O delay. However, is usually hard to avoid shuffle occurring, so how to make shuffle faster could be the key point.<br>During my work in tuning the Spark SQL project, the join operation is unavoidable because it is the fundamental process in all the SQL program. While when I try to run the Spark SQL saving the joined DataFrame into a Hive table, I found the number of table partition is constant no matter how many partitions of two joining DataFrame, oh-gee! it would never happen in RDD join process. That makes the size of each file of Hive table on the HDFS very large, and I have to fix it up.<br>After searching from the Internet, I found that the shuffle of DataFrame in Spark, join included, would have a partition number according to the configuration <em>spark.sql.shuffle.partitions</em> with 200 as default. And the problem above can be solved as this configuration changed. So, when you join two DataFrames or other process causing shuffle, remember the configuration <em>spark.sql.shuffle.partitions</em> when you want to modify the DataFrame partition number.<br>What if I want to modify the partition number of a RDD? Actually, the configuration <em>spark.default.parallelism</em>, which seems to only working for raw RDD and is ignored when working with DataFrames in Spark SQL, may be a good choice. Alternatively, you can set the partition number by calling <em>repartition( )</em> or <em>coalese( )</em>, which is effective for both DataFrames and RDDs.</p><h2 id="Smart-Action"><a href="#Smart-Action" class="headerlink" title="Smart Action"></a>Smart Action</h2><p>As we know, RDDs support two types of operation, which are action and transformation. Transformation creates a new RDD from an existing one, and action returns a value to the Spark driver from computing on a RDD. Based on the lazy evaluation, all the transformations would run as a lineage when an action is triggered by the Spark. A lineage of transformation followed by an action consist of one job in Spark, and one wide-dependency between two transformations separate the job into two stages, which is aka shuffle.<br>When I’m tuning a Spark project, I found that different action following the same lineage of transformation takes different period of time. For example, the action <em>show( )</em> takes shorter time than <em>createOrReplaceTempView( )</em>, which makes me confuse a lot. After a long time thinking and searching, the answer finally comes out. Spark would draw a DAGSchedule when the program is submitted, the data would run through all the DAGSchedual and the result is sent to the Spark driver. Different action may generate different DAGSchedule even the the transformations are same, Spark is smart enough to know whether it needs to run everything in the RDD. Showed above, Spark may run less data for the <em>show( )</em> action than those for <em>createOrReplaceTempView( )</em>.</p><h2 id="Broadcast-Joins"><a href="#Broadcast-Joins" class="headerlink" title="Broadcast Joins"></a>Broadcast Joins</h2><p>Broadcast joins (aka map-side joins) is a good way to abort shuffle and reduce cost. Spark SQL provides two ways for developers, you can not only write SQL but also use DataFrame/Dataset API. When a large table joins a smaller one, a threshold defined by <em>spark.sql.autoBroadcastJoinThreshold</em>, with 10M as default value, determines whether the smaller one will be broadcast or not. If you use SQL in Spark SQL, as the smaller table size below the threshold, Spark would automatically broadcast it to all executors. However, if you use DataFrame/Dataset API, the <em>broadcast</em> function must be imported or Spark wouldn’t broadcast data even if the size is below the threshold.<br><figure class="highlight scala hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-keyword">val</span> df = largeDF.join(broadcast(smallDF),<span class="hljs-type">Seq</span>(<span class="hljs-string">"col1"</span>,<span class="hljs-string">"col2"</span>),<span class="hljs-string">"left"</span>)</span><br></pre></td></tr></table></figure></p><p>Also, you can enlarge the value of <em>spark.sql.autoBroadcastJoinThreshold</em> so that larger table can also be broadcast, but the memory of your application should be paid attention.<br>Broadcast joins is really an awesome solution to optimize Spark SQL joins, after using broadcast joins instead of default SortMerge joins, my application runs more than 10 times faster. Avoiding shuffle is quite important for Spark tuning, and broadcast is born to kill shuffle. You will fall in love with her as long as you have a try!</p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul><li><a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#other-configuration-options" target="_blank" rel="noopener">Spark SQL Programming guide</a></li><li><a href="https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-joins-broadcast.html" target="_blank" rel="noopener">Mastering Spark SQL</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;https://github.com/JoeAsir/blog-image/raw/master/blog/background/blurred-background-close-up-coffee-cup.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;This article is about things I learned about Apache Spark recently. I’ve been struggling with Spark tuning for more than one month, including shuffle tuning, GC time tuning and so on. Honestly speaking, Apache Spark is just like an untamed horse, it could be a beauty if you tune it well while a nightmare if not. So let me show you some tips I’ve learned from my work. This article is part 1 and here we go.&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="spark" scheme="https://asirzhao.github.io/categories/spark/"/>
    
    
      <category term="spark" scheme="https://asirzhao.github.io/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark Tuning</title>
    <link href="https://asirzhao.github.io/2018/02/23/spark-spark-tune/"/>
    <id>https://asirzhao.github.io/2018/02/23/spark-spark-tune/</id>
    <published>2018-02-23T05:10:32.000Z</published>
    <updated>2020-05-10T06:50:12.531Z</updated>
    
    <content type="html"><![CDATA[<p>Hi, all, 最近一直在研究spark tuning方面的问题，深感这是一个经验活，也是一个技术活，查阅和很多资料，在这里mark一下。<br><a id="more"></a><br>上次我们review了一下spark的work-flow，主要是基于spark on yarn的，同样的，我们在这里探讨的也主要是基于spark on yarn。</p><h2 id="Resource-Allocation"><a href="#Resource-Allocation" class="headerlink" title="Resource Allocation"></a>Resource Allocation</h2><h3 id="Some-Configuration"><a href="#Some-Configuration" class="headerlink" title="Some Configuration"></a>Some Configuration</h3><p>Resource allocation是spark中一个非常重要的环节，给予一个application过少的resource会带了执行效率的低下和执行速度的缓慢；相反，过多的resource则会带来资源浪费，影响cluster上其他appllication的运行，因此，一个合适的resource allocation是非常非常重要的，我们来看看几个比较重要的parameter：</p><ul><li>num-executors: 表明spark申请executors的数目，我们可以通过设置spark.dynamicAllocation.enabled来让spark根据数据动态的分配executors，这样可以有效的提高资源利用率；</li><li>executor-cores: 指定每一个executor的core数目，core数目决定了每个executor的最大并行task数目</li><li>executor-memory: 指定分配给每一个executor的内存大小。</li></ul><h3 id="Some-Tips"><a href="#Some-Tips" class="headerlink" title="Some Tips"></a>Some Tips</h3><ul><li>对于executor来说，在过于大的memory上运行可能会带来比较高的GC(gabage collection) time，对于一个executor来说，建议给出的上限memory是64G；</li><li>由于HDFS在并行读写的时候存在一些瓶颈，因此每一个executor中最好不要超过5个并行任务，即cores数不要超过5个，有实验可以证明，spark在多executor少core的配置下执行效率更高；</li><li>相反的，对于executor来说，过分少的core，例如1个，将会使得executors数目变多，例如某个broadcast过程，需要传播到所有的executors上，那么过分多的executors会降低执行的效率。</li></ul><h2 id="Memory-Mangement"><a href="#Memory-Mangement" class="headerlink" title="Memory Mangement"></a>Memory Mangement</h2><p>关于spark中的memory management，我们先来看一张图：<br><img src="https://github.com/JoeAsir/blog-image/raw/master/blog/16/16-1.png" alt=""><br>在图中我们可以看到，spark把memory分成了三部分，即spark memory、user memory和reserved memory，我们顺次来看看：</p><h3 id="Reserved-Memory"><a href="#Reserved-Memory" class="headerlink" title="Reserved Memory"></a>Reserved Memory</h3><p>所谓reserved memory，它就是系统预留下的一部分memory，用于存储spark的内部对象，默认大小为300m，绝大部分情况下，我们都不会修改这些参数。值得注意的是，当executor被分配的memory小于1.5倍的reserved memory时，将会抛出“please use larger heap size”的错误。</p><h3 id="User-Memory"><a href="#User-Memory" class="headerlink" title="User Memory"></a>User Memory</h3><p>User memory用于储存spark的transfermation的一些信息，比如RDD之间的依赖信息等等，这部分内存默认大小为(Java Heap - 300M)*0.25，其中的300M其实就是上面提到的reserved memory.具体的大小要依赖于spark.memory.fraction参数，这个参数决定了user 和 下面要讲到的spark memory的分配比例。</p><h3 id="Spark-Memory"><a href="#Spark-Memory" class="headerlink" title="Spark Memory"></a>Spark Memory</h3><p>上文已经到了，spark memory主要是spark自己使用的memory部分，这部分的大小依赖于spark.memory.fraction参数，即(Java Heap - 300M)*spark.memory.fraction，其中fraction的default为0.75。</p><p>Spark memory主要有两个用途，一是用于spark的shuffle等操作，而是用来cache spark中的RDD，因此spark memory也自然而然的分成了两部分，即负责shuffle操作的execution memory和负责cache的storage memory，两者的大小通过spark.memory.storageFraction参数来分割，默认值是0.5。</p><p>在spark memory中，还有一个重要的性质，那就是storage 和 execution memory的共享机制，说的简单一些就是，当一边内存空闲而另一方内存紧张的时候，可以借用对方的内存，我们下面看看在内存出现冲突的时候，spark怎么协调：</p><ul><li>当storage占用execution memory的时候，发生execution memory使用紧张的情况时，强制将storage占有的内存释放并归还execution，丢失的数据将会后续重新计算；</li><li>当execution占用storage memory的时候，发生storage memory紧张的情况，被占用的内存不会被强制释放，因为这会带来任务丢失，storage会耐心等待知道execution执行完释放出内存。</li></ul><h2 id="Data-Serialization"><a href="#Data-Serialization" class="headerlink" title="Data Serialization"></a>Data Serialization</h2><p>在整个spark任务中，数据传输都是经过序列化后(serialization)之后传输的，因此数据的序列化是很重要的，冗余的序列化过程会让整个spark任务变慢，spark提供两种序列化方式：</p><ul><li>Java serialization：这是spark默认的序列化方式，java序列化是一种很经典和稳定的序列化方法，但是最大的缺点就是——慢！</li><li>Kryo serialization：Kryo 序列化可以让spark任务更加快速，甚至10倍于java序列化；但是它不支持所有的Serializable类型，同时需要为用户自己开发的class进行注册后，才可以使用Kyo.</li></ul><p>关于Kryo的详细信息，可以查看<a href="https://spark.apache.org/docs/latest/tuning.html#data-serialization" target="_blank" rel="noopener">spark documentation</a>，或者<a href="https://github.com/EsotericSoftware/kryo" target="_blank" rel="noopener">Kryo documentation</a></p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>关于spark调优的问题，有很多因素，我也是简单的做了一些了解并分享给大家，除了我提到的，还有诸如GC等等因素，大家可以根据我给出的references做进一步的了解。</p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul><li><a href="http://blog.cloudera.com/blog/2015/03/how-to-tune-your-apache-spark-jobs-part-2/" target="_blank" rel="noopener">How-to: Tune Your Apache Spark Jobs (Part 2)</a></li><li><a href="https://spark.apache.org/docs/latest/tuning.html" target="_blank" rel="noopener">Spark Documentation-Tuning</a></li><li><a href="http://shop.oreilly.com/product/0636920028512.do" target="_blank" rel="noopener">Karau, Holden, et al. Learning spark: lightning-fast big data analysis. “ O’Reilly Media, Inc.”, 2015.</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Hi, all, 最近一直在研究spark tuning方面的问题，深感这是一个经验活，也是一个技术活，查阅和很多资料，在这里mark一下。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="spark" scheme="https://asirzhao.github.io/categories/spark/"/>
    
    
      <category term="spark" scheme="https://asirzhao.github.io/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark工作流程简析</title>
    <link href="https://asirzhao.github.io/2018/01/07/spark-spark-workflow/"/>
    <id>https://asirzhao.github.io/2018/01/07/spark-spark-workflow/</id>
    <published>2018-01-07T10:44:37.000Z</published>
    <updated>2020-05-10T06:50:12.532Z</updated>
    
    <content type="html"><![CDATA[<p>Hello，有一个月没写blog了感觉很自责，必须整起来！最近由于工作上遇到的一些调优困难，让我对Spark有些敬畏，所以集中的研究了下鬼魅玄学Spark，和大家分享一下。首先先来看看spark的基本工作流程。<br><a id="more"></a></p><h2 id="Work-Flow"><a href="#Work-Flow" class="headerlink" title="Work Flow"></a>Work Flow</h2><p>和hadoop一样，spark也是master-slave机制，Spark通过driver进程，将task分发到多个executors上并发进行计算。整个driver和所有的executors组成了一个spark application，每一个application是运行在cluster manager上的，Spark本身集成了standalone cluster，当然，Spark还可以运行在赫赫有名的YARN和Mesos上。我平时使用的公司集群都是基于YARN cluster manager的，因此本文重点探讨基于YARN的spark。</p><p>下图就是spark在cluster manager下的整体工作流程。<br><img src="https://github.com/JoeAsir/blog-image/raw/master/blog/15/15-1.png" alt=""></p><h2 id="The-Driver"><a href="#The-Driver" class="headerlink" title="The Driver"></a>The Driver</h2><p>Driver是整个application最核心的部分，他运行的是application的main方法，它伴随这整个application的生命周期，driver进程的结束就会带来整个application的结束。</p><p>对于所有的Spark任务，他们其实都是实现RDD的transformation和action操作，而这些操作，最后是需要driver将他们转化和分发成tasks，然后才可以去执行。所有的user program都会被driver通过DAG(directed acyclic graph)转化成实际的tasks执行计划，除此之外，driver还会在tasks执行的期间，监控executor上的tasks，并且保证他们拥有健康而合理的资源。</p><h2 id="Executors"><a href="#Executors" class="headerlink" title="Executors"></a>Executors</h2><p>Executors是Spark application的执行者，他们也是伴随着application的生命周期而存在的，值得注意的是，<strong>Spark job在executors执行失败的情况下依然可以继续进行</strong>。Executors会对具体的tasks的执行结果返回给driver，同时给缓存的RDD提供存储空间。</p><h2 id="Some-terms"><a href="#Some-terms" class="headerlink" title="Some terms"></a>Some terms</h2><ul><li>Job: Job是executor层面最大的执行单元，job通过RDD的action操作来分割，每一个action操作就会进行一次job的划分；</li><li>Stage: Stage是包含在job中的执行单元，stage通过RDD的shuffle操作来分割，每进行一次shuffle操作，就会进行一次stage的划分；</li><li>Task: Task是executor执行中最细的执行单元，task的数目取和parent RDD的partition数目是一一对应的。</li></ul><h2 id="Spark-on-Yarn-cluster"><a href="#Spark-on-Yarn-cluster" class="headerlink" title="Spark on Yarn-cluster"></a>Spark on Yarn-cluster</h2><p>下面，我们一起看看整个Spark application中，driver和executors的都会起到什么作用。我以基于yarn-cluster的YARN的Spark作为例子来简述整个流程，先看一张图：<br><img src="https://github.com/JoeAsir/blog-image/raw/master/blog/15/15-2.png" alt=""><br>首先我们要明确一些YARN的概念，YARN是与master-slaver的一个Cluster Manager， 在YARN中，RM(ResourseManager)负责整个调度分发，即我们常说的master；而NM(NodeManager)任务分发的接受者，负责执行具体的任务，也就是我们所说的worker。这些概念后续我专门介绍YARN的时候会详细的说明，他们的作用都是实现spark和YARN之间诸如资源申请等操作。</p><p>首先Client向ResourceManager发出提交application的请求，ResourseManager会在某一个NodeManager上启动AppManager进程，AppManager会随后启动driver，并将driver申请containers资源的信息发给ResourceManager，申请完成后，ResourceManager将资源分配消息传递给AppManager并由它启动container，每一个container中只运行一个spark executor，由此完成了资源的申请和分配。</p><p>然后整个application开始执行，在这个过程中，根据RDD的transformation或者action，driver把这些任务以tasks的形式，源源不断的传送给executors，于是executors不停地进行计算和存储的任务。当driver结束的时候，他会结束掉executors并且释放掉资源。这就是yarn-cluster上spark的整体工作流程。</p><p>除了yarn-cluster，还有一种yarn-client的方法，这种方法唯一的区别在于，他的driver并非运行在某个NodeManager上，而是一直运行在client中。这样的问题就是client一旦关闭，那么整个任务也就随之停止执行。因此相较而言，yarn-cluster更适合线上任务，而yarn-client更适合调试模式。</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li><a href="http://shop.oreilly.com/product/0636920028512.do" target="_blank" rel="noopener">Karau, Holden, et al. Learning spark: lightning-fast big data analysis. “ O’Reilly Media, Inc.”, 2015.</a></li><li><a href="https://www.iteblog.com/archives/1223.html" target="_blank" rel="noopener">Spark:Yarn-cluster和Yarn-client区别与联系</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Hello，有一个月没写blog了感觉很自责，必须整起来！最近由于工作上遇到的一些调优困难，让我对Spark有些敬畏，所以集中的研究了下鬼魅玄学Spark，和大家分享一下。首先先来看看spark的基本工作流程。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="spark" scheme="https://asirzhao.github.io/categories/spark/"/>
    
    
      <category term="spark" scheme="https://asirzhao.github.io/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>Learning Notes-Deep Learning, course4, week2</title>
    <link href="https://asirzhao.github.io/2017/11/29/course-deep-learning-course4-week2/"/>
    <id>https://asirzhao.github.io/2017/11/29/course-deep-learning-course4-week2/</id>
    <published>2017-11-29T08:26:12.000Z</published>
    <updated>2020-05-10T06:50:12.529Z</updated>
    
    <content type="html"><![CDATA[<p>我们继续来看看course4的week2，CNN的知识还是蛮丰富的，本周主要讲了一些经典的CNN结构以及一些computer vision的技巧和知识，一起recap一下。<br><a id="more"></a></p><h2 id="Classic-Networks"><a href="#Classic-Networks" class="headerlink" title="Classic Networks"></a>Classic Networks</h2><p>Ng一共给我们带来了3个最为经典的CNN网络，这里我会给出网络的截图和paper原文，抽空我也会看看原文，希望大家和我一起来看看。</p><h3 id="LeNet-5"><a href="#LeNet-5" class="headerlink" title="LeNet-5"></a>LeNet-5</h3><p><img src="https://github.com/JoeAsir/blog-image/raw/master/blog/14/14-1.png" alt=""><br><a href="http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf" target="_blank" rel="noopener">Lécun Y, Bottou L, Bengio Y, et al. Gradient-based learning applied to document recognition[J]. Proceedings of the IEEE, 1998, 86(11):2278-2324.</a></p><h3 id="AlexNet"><a href="#AlexNet" class="headerlink" title="AlexNet"></a>AlexNet</h3><p><img src="https://github.com/JoeAsir/blog-image/raw/master/blog/14/14-2.png" alt=""><br><a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" target="_blank" rel="noopener">Krizhevsky A, Sutskever I, Hinton G E. ImageNet classification with deep convolutional neural networks[J]. Communications of the Acm, 2012, 60(2):2012.</a></p><h3 id="VGG-16"><a href="#VGG-16" class="headerlink" title="VGG-16"></a>VGG-16</h3><p><img src="https://github.com/JoeAsir/blog-image/raw/master/blog/14/14-3.png" alt=""><br><a href="https://arxiv.org/pdf/1409.1556.pdf" target="_blank" rel="noopener">Simonyan K, Zisserman A. Very Deep Convolutional Networks for Large-Scale Image Recognition[J]. Computer Science, 2014.</a></p><p>以上可以说是最为经典的三个cnn网络了，大家可以通过阅读paper获得一些详细的知识，都是经典之作，推荐阅读。</p><h2 id="Residual-Networks-ResNets"><a href="#Residual-Networks-ResNets" class="headerlink" title="Residual Networks(ResNets)"></a>Residual Networks(ResNets)</h2><p>对于residual networks，我们在这里具体看一下，它的具体原理可以通过下图的residual block来看看：<br>其实，residual block是把\(a^{[l]}\)直接作为\(a^{[l+2]}\)输入，也就是说：<br><img src="https://github.com/JoeAsir/blog-image/raw/master/blog/14/14-4.png" alt=""><br>$$a^{[l+2]}=g(z^{[l+2]}+a^{[l]})$$<br>其中\(g\)是activation function，如ReLU等。这种思想也被称为short circuit或者skip connection。把上面的residual block串联起来，就变成了我们的residual networks，如下图：<br><img src="https://github.com/JoeAsir/blog-image/raw/master/blog/14/14-5.png" alt=""><br>Residual networks最大的特点就是，普通networks随着layer增大，training error理论上是会变小，但是实际上会在某个最小点后增大，但是residual networks则会严格的随着layer增多而减小training error，下面是原文：<br><a href="https://arxiv.org/pdf/1512.03385.pdf" target="_blank" rel="noopener">He K, Zhang X, Ren S, et al. Deep Residual Learning for Image Recognition[J]. 2015:770-778.</a></p><h2 id="Network-in-Network-and-1×1-Convolutions"><a href="#Network-in-Network-and-1×1-Convolutions" class="headerlink" title="Network in Network and 1×1 Convolutions"></a>Network in Network and 1×1 Convolutions</h2><p>通常我们使用的filter，都是奇数的kernel matrix，在某些情况下，1×1的filter也会被我们使用，它到底有什么作用呢？我们来看一张图：<br><img src="https://github.com/JoeAsir/blog-image/raw/master/blog/14/14-6.png" alt=""><br>从这张图中可以看出，1×1的filter可以压缩input的channel(depth)，因此1×1filter还是有一些意思的。下面是原文：<br><a href="https://arxiv.org/pdf/1312.4400.pdf" target="_blank" rel="noopener">Lin M, Chen Q, Yan S. Network In Network[J]. Computer Science, 2013.</a></p><h2 id="Inception-Network"><a href="#Inception-Network" class="headerlink" title="Inception Network"></a>Inception Network</h2><p>关于inception network，我们先来看一张图：<br><img src="https://github.com/JoeAsir/blog-image/raw/master/blog/14/14-7.png" alt=""><br>对于同一个input，我们分别采用不同的filter，甚至max pooling，在保证输出的hight和width一样的前提下，将结果堆叠起来，作为我们的输出，这样做的好处是，我们不需要自己挑选filter，我们将所有的可能都交给network，让它来决定去选择什么样子的结构。原文是：<br><a href="https://arxiv.org/pdf/1409.4842.pdf" target="_blank" rel="noopener">Szegedy C, Liu W, Jia Y, et al. Going deeper with convolutions[C].  Computer Vision and Pattern Recognition. IEEE, 2015:1-9.</a><br>同时，Ng在课程上说明，inception network 中大量使用了1×1filter来降低计算量，这一点值得我们注意。<br>我们来看看Inception 单元的图解：<br><img src="https://github.com/JoeAsir/blog-image/raw/master/blog/14/14-8.png" alt=""><br>这一周的课程感觉量很大，介绍了很多的网络，我准备下面慢慢的看看这些paper，站在巨人的肩上去看世界，一定会有别样的风景！</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li><a href="hhttps://www.coursera.org/learn/convolutional-neural-networks" target="_blank" rel="noopener">Deep learning-Coursera Andrew Ng</a></li><li><a href="https://mooc.study.163.com/course/2001281004#/info" target="_blank" rel="noopener">Deep learning-网易云课堂 Andrew Ng</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;我们继续来看看course4的week2，CNN的知识还是蛮丰富的，本周主要讲了一些经典的CNN结构以及一些computer vision的技巧和知识，一起recap一下。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="learning notes" scheme="https://asirzhao.github.io/categories/learning-notes/"/>
    
    
      <category term="CNN" scheme="https://asirzhao.github.io/tags/CNN/"/>
    
  </entry>
  
  <entry>
    <title>Learning Notes-Deep Learning, course4, week1</title>
    <link href="https://asirzhao.github.io/2017/11/26/course-deep-learning-course4-week1/"/>
    <id>https://asirzhao.github.io/2017/11/26/course-deep-learning-course4-week1/</id>
    <published>2017-11-26T12:30:47.000Z</published>
    <updated>2020-05-10T06:50:12.528Z</updated>
    
    <content type="html"><![CDATA[<p>Hi, all. 最近开始休假了，可以有空继续自己的学习，一方面补一补前面的作业，一方面继续自己的学习，今天我们来到了course4，也就是convolutional neural networks 的内容。我们一起来看看！<br><a id="more"></a></p><h2 id="Convolution"><a href="#Convolution" class="headerlink" title="Convolution"></a>Convolution</h2><p>在课程中，Ng从edge detection的角度来给大家讲了讲convolution，因为本人是image processing出身，所以认为Ng在这里讲的还是很浅显易懂的，我就不再专门的markdown。主要来看看convolution中的一些技巧。</p><h3 id="Padding"><a href="#Padding" class="headerlink" title="Padding"></a>Padding</h3><p>我们都知道，在最纯粹的convolution中，我们假设原image尺寸是\(n *n\)，convolution filter尺寸是\(f *f\)，那么最终的结果image尺寸应该是\( (n-f+1) *(n-f+1)\)，也就是说，结果的尺寸变小了。如果想让输出image的尺寸不发生改变，那么我们就要使用大名鼎鼎的padding了。</p><p>Padding其实就是表示，在原始image中，向外扩大多少尺寸，一般我们会使用简单复制相邻元素值的方法进行扩充。假设对于一个\(6 *6\)的原始image，采用\(3 *3\)的filter，加上\(p=1\)的padding，那么原始图像尺寸变成了\(8 * 8\)，结果变成了\(6 *6\)，原始image和结果image一模一样！于是加入了padding的convolution公式就成了\( (n+2p-f+1) *(n+2p-f+1)\).</p><p>在这里Ng引入了两个概念，valid和same convolutions，所谓valid convolution，就是没有padding 的convolution；所谓same convolution，就是输入输出的尺寸完全一样。</p><h3 id="Stride"><a href="#Stride" class="headerlink" title="Stride"></a>Stride</h3><p>继padding之后，还有一个很重要的参数，就是步长stride，步长stride决定了filter做convolution时候的步长，如果stride=1，那么filter就会挨着计算，如果stride=2，那么就会跳跃这进行计算。</p><p>总结一下，假设原image尺寸是\(n *n\)，convolution filter尺寸是\(f *f\)，padding值是\(p\)，stride值是\(s\)那么最终的结果image尺寸应该是\( ( \frac {n+2p-f}{s}+1) *( \frac {n+2p-f}{s}+1)\)，如果除不尽的话，我们选择向下取整，也就是不足以做convolution的区域，我们选择放弃。</p><p>##Convolution over Volume<br>对于一般的图像处理，我们使用的都是RGB图像，我们都知道，RGB图像有三个channel，这种情况下，convolution应该如何做，我们来看下面的图：<br><img src="https://github.com/JoeAsir/blog-image/raw/master/blog/13/13-1.png" alt=""><br>假设我们的图像是6×6×3，也就是hight×width×channel(depth)，因此对应的filter也要有3的channel(depth)，最后可以得到一个4×4的结果。</p><p>当然，我们可以采用不止一个filter，如图<br><img src="https://github.com/JoeAsir/blog-image/raw/master/blog/13/13-2.png" alt=""><br>我们加入了两个不同的filter，他们的大小都是3×3×3，于是最终的结果就是4×4×2，请注意：结果的channel数目取决于filter的个数，而和输入的channel没有任何关系。</p><h2 id="CNN"><a href="#CNN" class="headerlink" title="CNN"></a>CNN</h2><h3 id="Convolution-Layer"><a href="#Convolution-Layer" class="headerlink" title="Convolution Layer"></a>Convolution Layer</h3><p>下面我们来看看CNN网络中的一个layer的工作原理是什么，首先来看截图：<br><img src="https://github.com/JoeAsir/blog-image/raw/master/blog/13/13-3.png" alt=""><br>这张图十分复杂，我们一起仔细看看这张图，对于一个6×6×3的RGB图像，我们用了两个3×3×3的filter，我们可以把输入image看做\(x\)，也就是\(a ^{[0]}\)，filter看做\(w ^{[1]}\)，得到的结果就是\(w ^{[1]}a ^{[0]}\)，我们再加上一个bias项\(b^{[1]}\)，那么就获得了一个liner output\(w ^{[1]}a ^{[0]}+b^{[1]}\)，我们再使用一个non-liner function例如ReLU，如此获得一个4×4×2的output。如此就是CNN的一个layer.</p><p>如此我们可以看到，CNN和之前的DNN实质上都存在一种liner function到non-liner function的转化，通过non-liner function去classify线性不可分的data，另外，在CNN中，每一个filter就可以获得一个不同的feature，多个filter可以让我们从多个角度去classify data.</p><p>另外，相比较于fully connected 的DNN，CNN所需要的parameters也少了很多，这一点值得我们注意。</p><h3 id="Pooling"><a href="#Pooling" class="headerlink" title="Pooling"></a>Pooling</h3><p>Pooling原理还是很简单的，我们来看一张图：<br><img src="https://github.com/JoeAsir/blog-image/raw/master/blog/13/13-4.png" alt=""><br>首先我们来看看max pooling，如图，我们取filter尺寸\(f=2\)，stride大小\(s=2\)，对于一个filter中的元素，我们取max作为输出；相对应的，如果我们取average，那么就成了average pooling，pooling中的hyperparameter只有filter尺寸\(f\)和stride大小\(s\)，值得注意的是，pooling过程中不存在学习过程，no parameters to learn!</p><h3 id="Fully-Connected-layer"><a href="#Fully-Connected-layer" class="headerlink" title="Fully Connected layer"></a>Fully Connected layer</h3><p>Fully connected layer在CNN其实很简单，我们只需要将input展开，按照DNN的方法进行fully connected就可以了。</p><p>一般情况下，我们认为有prameter变化的才算一层，因此我们不认为pooling是一个layer，我们举个一个最简单的CNN做例子：CONV-POOL-CONV-POOL-FC-FC-Softmax，我们这就是一个简单的5层的CNN，在下周的课程中我们可以看到一些经典的CNN框架，这里就不再复述。</p><h2 id="Why-Convolutions"><a href="#Why-Convolutions" class="headerlink" title="Why Convolutions"></a>Why Convolutions</h2><p>关于这个问题，Ng给出了两个意见，我们一起看看：</p><blockquote><p><strong>Parameter sharing</strong>: A feature detector (such as a vertical edge detetor) that’s useful in one part of image is probably useful in another part of the image.<br><strong>Sparsity of connections</strong>: In  each layer, each output value depends only on a small number of inputs.</p></blockquote><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li><a href="hhttps://www.coursera.org/learn/convolutional-neural-networks" target="_blank" rel="noopener">Deep learning-Coursera Andrew Ng</a></li><li><a href="https://mooc.study.163.com/course/2001281004#/info" target="_blank" rel="noopener">Deep learning-网易云课堂 Andrew Ng</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Hi, all. 最近开始休假了，可以有空继续自己的学习，一方面补一补前面的作业，一方面继续自己的学习，今天我们来到了course4，也就是convolutional neural networks 的内容。我们一起来看看！&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="learning notes" scheme="https://asirzhao.github.io/categories/learning-notes/"/>
    
    
      <category term="CNN" scheme="https://asirzhao.github.io/tags/CNN/"/>
    
  </entry>
  
  <entry>
    <title>Imbalanced data 问题总结方法汇总</title>
    <link href="https://asirzhao.github.io/2017/11/11/ml-imbalanced-data-solution/"/>
    <id>https://asirzhao.github.io/2017/11/11/ml-imbalanced-data-solution/</id>
    <published>2017-11-11T15:01:09.000Z</published>
    <updated>2020-05-10T06:50:12.530Z</updated>
    
    <content type="html"><![CDATA[<p>Hello，大家好，双十一真的很累，一直在加班，忙里偷闲看了<a href="https://arxiv.org/pdf/1710.05381.pdf" target="_blank" rel="noopener">A systematic study of the class imbalance  problem in convolutional neural networks</a>，感觉paper呈现的研究内容感觉很一般，但是，paper中关于imbalanced data的solution方法倒是写的很不错，也勾起了我对于这一块总结的欲望。之前也写过一篇关于imbalanced data的paper notes，但是对于这一块的具体方法总结还不是很足够，于是用这篇paper为主线好好sum up一计。</p><p>我们来一起看看。<br><a id="more"></a></p><h2 id="Data-level-methods"><a href="#Data-level-methods" class="headerlink" title="Data level methods"></a>Data level methods</h2><p>首先我们来看一看data level methods，这类方法有一个共性，那就是通过改变data的数量来完成对imbalanced data problem的解决。</p><h3 id="Oversampling"><a href="#Oversampling" class="headerlink" title="Oversampling"></a>Oversampling</h3><p>Oversampling可以说是最直观的solution之一，它的核心思想是，对于较少一类别的samples，过此重复采样，以此让两种类别的样本接近平衡。<strong>但是，对于一个sample多次重复训练，很有可能带来overfitting</strong>，因此，简单粗暴的重复采样并不可取。因此，很多改进的版本应运而生：</p><h4 id="SMOTE"><a href="#SMOTE" class="headerlink" title="SMOTE"></a>SMOTE</h4><p>SMOTE算法是一种经典的oversampling方法，它的主要思想是对较少数目类别的样本，随机抽取\(m\)个样本，对于随机抽取出的样本，每个样本选取距离最近的\(n\)个样本，在他们的连线上随机选取一个点，作为较少类别的补充样本。假设原样本点为\(x\)，被选中的附近的点为\(x’\)，则新的样本点为：<br>$$x_{new}= x + rand(0,1) \cdot |x-x’|$$</p><p>通过这种方式，SMOTE可以对较少类别样本进行扩充，进而实现oversampling，平衡数据分布。</p><h4 id="Cluster-base-oversampling"><a href="#Cluster-base-oversampling" class="headerlink" title="Cluster-base oversampling"></a>Cluster-base oversampling</h4><p>Cluster-based方法的最大特点莫过于最开始对数据进行一个聚类分析，数据会变成数个cluster，然后对于每一个cluster在进行数据的oversampling，<strong>同时兼顾类别之间的between-class imbalance，还要考虑到类内部各个cluster的within-class imbalance</strong>.</p><blockquote><p>Its idea is to consider not only the between class imbalance (the imbalance occurring between the two classes) but also the with in class imbalance (the imbalance occurring between the sub clusters of each class) and to oversample the data set by rectifying these two types of imbalances simultaneously.</p></blockquote><p>原paper大致叙述了整个流程，首先我们对imbalanced data进行k-means(或者其他算法也可以)聚类，聚成多个cluster之后，我们开始进行oversampling，假设majority class有\(m\)个cluster，minority有\(n\)个cluster，我们以cluster最大的data数目\(k\)为标准，我们先对majority class中所有cluster，都进行oversampling，使得他们的数目都达到\(k\)，随后，对于minority中每个cluster进行oversampling，使得每一个cluster数目变成\(m * k /n\)，最终实现between-class balance和within-class balance.</p><h3 id="Undersampling"><a href="#Undersampling" class="headerlink" title="Undersampling"></a>Undersampling</h3><p>与oversampling相对应的则是undersampling，undersampling的核心思想是对于较多类别的samples抽样，使得两个类别数据趋于相近。但是，随机抽样获得会使得类别丧失很多的信息，甚至导致数据分布发生改变。</p><h4 id="One-sided-selection"><a href="#One-sided-selection" class="headerlink" title="One-sided selection"></a>One-sided selection</h4><p>one-sided selection的主要思想是，为了保证数据整体的分布，我们优先去除靠近边界的样本，这样可以保证较多分类的数据分布。</p><h2 id="Classifier-level-methods"><a href="#Classifier-level-methods" class="headerlink" title="Classifier level methods"></a>Classifier level methods</h2><p>下面我们来看看通过改变classifier level来解决imbalanced data的方法，这类方法侧重于分类器本身的一些性质而并非两类数据的个数。</p><h3 id="Thresholding"><a href="#Thresholding" class="headerlink" title="Thresholding"></a>Thresholding</h3><p>我在之前的博客中聊到过，imbalanced data的分类平面会倾向于较少数据的分类一侧，所以我们可以通过改变类别预测的probabilty的threshold来修正分类平面。常用的方法就是加入关于类别数目的prior probability：<br>$$y_i(x)=p(i|x)= \frac{p(i) \cdot p(x|i)}{p(x)}$$</p><h3 id="Cost-sensitive-learning"><a href="#Cost-sensitive-learning" class="headerlink" title="Cost sensitive learning"></a>Cost sensitive learning</h3><p>Thresholding方法其实对已经train好的模型的采取的一种方式。相应的，我们在模型训练的时候就来消除imbalanced data的一些影响，如何做到呢？答案就是cost function.</p><p>我们可以通过调整learning rate，加强对cost比较大的samples，并且最终的优化目标从标准的cost function变成misclassification cost，如此就可以解决imbalanced data的问题了</p><h3 id="One-class-classification"><a href="#One-class-classification" class="headerlink" title="One-class classification"></a>One-class classification</h3><p>该方法可以说是换了一种思维看问题，我们不再将classification作为我们的task，而是变成了对于一种异常检测的问题。我们只是着眼于较多samples的类别，认为另一类别的samples是一种异常值。</p><p>当然，这种方法适合那种极端的imbalanced data，对于一般的情况并不一定很适用。</p><h2 id="Recommendation"><a href="#Recommendation" class="headerlink" title="Recommendation"></a>Recommendation</h2><p>Projection:<a href="https://github.com/scikit-learn-contrib/imbalanced-learn" target="_blank" rel="noopener">Imbalanced-learn</a></p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li><a href="https://arxiv.org/pdf/1710.05381.pdf" target="_blank" rel="noopener">Buda, Mateusz, Atsuto Maki, and Maciej A. Mazurowski. “A systematic study of the class imbalance problem in convolutional neural networks.” arXiv preprint arXiv:1710.05381 (2017).</a></li><li><a href="https://www.jair.org/media/953/live-953-2037-jair.pdf" target="_blank" rel="noopener">Chawla, Nitesh V., et al. “SMOTE: synthetic minority over-sampling technique.” Journal of artificial intelligence research 16 (2002): 321-357.</a></li><li><a href="http://sci2s.ugr.es/keel/pdf/specific/articulo/jo.pdf" target="_blank" rel="noopener">Jo, Taeho, and Nathalie Japkowicz. “Class imbalances versus small disjuncts.” ACM Sigkdd Explorations Newsletter 6.1 (2004): 40-49.</a></li><li><a href="http://www.ee.iisc.ac.in/new/people/faculty/prasantg/downloads/NeuralNetworksPosteriors_Lippmann1991.pdf" target="_blank" rel="noopener">Richard, Michael D., and Richard P. Lippmann. “Neural network classifiers estimate Bayesian a posteriori probabilities.” Neural computation 3.4 (1991): 461-483.</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Hello，大家好，双十一真的很累，一直在加班，忙里偷闲看了&lt;a href=&quot;https://arxiv.org/pdf/1710.05381.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;A systematic study of the class imbalance  problem in convolutional neural networks&lt;/a&gt;，感觉paper呈现的研究内容感觉很一般，但是，paper中关于imbalanced data的solution方法倒是写的很不错，也勾起了我对于这一块总结的欲望。之前也写过一篇关于imbalanced data的paper notes，但是对于这一块的具体方法总结还不是很足够，于是用这篇paper为主线好好sum up一计。&lt;/p&gt;
&lt;p&gt;我们来一起看看。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="machine learning" scheme="https://asirzhao.github.io/categories/machine-learning/"/>
    
    
      <category term="imbalanced data" scheme="https://asirzhao.github.io/tags/imbalanced-data/"/>
    
  </entry>
  
  <entry>
    <title>Reading Notes-Swish：A Self-gated Activation Function</title>
    <link href="https://asirzhao.github.io/2017/10/22/paper-swish/"/>
    <id>https://asirzhao.github.io/2017/10/22/paper-swish/</id>
    <published>2017-10-22T08:13:30.000Z</published>
    <updated>2020-05-10T06:50:12.531Z</updated>
    
    <content type="html"><![CDATA[<p>Hi all，今天和大家分享一篇比较新的paper，是关于一种新的activation function，关于我们知道的activation function，有sigmoid，tanh，ReLU以及ReLU的一些变种，那我们今天来看看这种新提出的activation function到底有什么特色。<br><a id="more"></a></p><h2 id="Notes"><a href="#Notes" class="headerlink" title="Notes"></a>Notes</h2><p>首先定义，swish activation function \(f(x)=x \cdot \sigma (x)\)，其中\(\sigma(x)\)是sigmoid function，也就是\( \sigma(x)=1/(1+ e^{-x})\).Swish functin的图像如图所示：<br><img src="https://github.com/JoeAsir/blog-image/raw/master/blog/11/11-1.png" alt=""><br>我们再来看下swish function的1st and 2nd derivatives，<br><img src="https://github.com/JoeAsir/blog-image/raw/master/blog/11/11-2.png" alt=""><br>下面我们一起来集中看看swish function的优点都有什么，作者给出了以下几点：函数值没有上限，函数值有下限，函数不单调，函数光滑连续，我们一起看看：</p><h3 id="Unbounded-above"><a href="#Unbounded-above" class="headerlink" title="Unbounded above"></a>Unbounded above</h3><p>Unbounded above的实质，是防止activation function在bounded value处发生saturation. bounded above 带来的问题，就是越接近bounded value的时候，function gradient就会越小，逐渐接近0，这就导致gradient descent异常缓慢甚至无法converge。例如sigmoid 和tanh function，他们都是bounded below and above，当我们采用这两种activation function的时候，我们必须谨慎小心的让初始值尽量在function的接近liner的部分来避免上面问题的产生，因此，unbounded above是一个很好的优点，例如ReLU及其变种都采用了这一原则。</p><h3 id="Bounded-below-amp-non-monotonicity"><a href="#Bounded-below-amp-non-monotonicity" class="headerlink" title="Bounded below &amp; non-monotonicity"></a>Bounded below &amp; non-monotonicity</h3><p>Bounded below其实也是一种很好的方法，并且也有activation function已经采用了，采用该方法后，所有负数input都会得到相差无几的activation value，也就是说，-1000和-1的值几乎没有区别，按照author的话来讲，就是我们将</p><blockquote><p>make large negative input “fogotten”</p></blockquote><p>这其实也是regularzation的一种思想，这种方法在ReLU等方法中也有体现，但是，swish可以通过自身的非单调性质，将比较小的negative input仍然以negative value输出，non-monotonicity提供了更好的gradient flow.</p><h3 id="Smothness"><a href="#Smothness" class="headerlink" title="Smothness"></a>Smothness</h3><p>关于smoothness的优点，我们来看一张图：<br><img src="https://github.com/JoeAsir/blog-image/raw/master/blog/11/11-3.png" alt=""></p><p> 总而言之，个人感觉swish应该算是一个不错的activation，本人由于时间原因，还没有来得及自己测试它，但是据我所看到的讨论，swish的实际效果貌似不是十分稳定，所以我们可以持保留意见，进一步观察它的表现。</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li><a href="https://arxiv.org/pdf/1710.05941.pdf" target="_blank" rel="noopener">Ramachandran P, Zoph B, Le Q V. Swish：a Self-Gated Activation Function[J]. arXiv preprint arXiv:1710.05941, 2017.</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Hi all，今天和大家分享一篇比较新的paper，是关于一种新的activation function，关于我们知道的activation function，有sigmoid，tanh，ReLU以及ReLU的一些变种，那我们今天来看看这种新提出的activation function到底有什么特色。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="reading notes" scheme="https://asirzhao.github.io/categories/reading-notes/"/>
    
    
      <category term="activtion function" scheme="https://asirzhao.github.io/tags/activtion-function/"/>
    
  </entry>
  
  <entry>
    <title>Learning Notes-Deep Learning, course3, week2</title>
    <link href="https://asirzhao.github.io/2017/10/18/course-deep-learning-course3-week2/"/>
    <id>https://asirzhao.github.io/2017/10/18/course-deep-learning-course3-week2/</id>
    <published>2017-10-18T13:28:12.000Z</published>
    <updated>2020-05-10T06:50:12.528Z</updated>
    
    <content type="html"><![CDATA[<p>Hi all，course3来到了week2，本周的课程依然主要是关于一些learning strategy，这些方法相当实用。虽然不是什么具体的算法，但都都是Ng在科研和工作中积累下来的宝贵经验，对于实际问题十分有效。</p><p>我们一起来看看。<br><a id="more"></a></p><h2 id="Error-analysis"><a href="#Error-analysis" class="headerlink" title="Error analysis"></a>Error analysis</h2><h3 id="Carry-out-error-analysis"><a href="#Carry-out-error-analysis" class="headerlink" title="Carry out error analysis"></a>Carry out error analysis</h3><p>按照通常的流程，在进行training过程后，我们在dev set会进行模型的测试，如果dev error比training error大很多的话，我们应该去排查问题的症结所在呢？Ng给出了solution</p><p>例如在cat recognition中，我们发现错分的sample有很多dog图像，还有很多猫科动物的图像，还有一些是模糊的cat图像。于是我们自然而然的想到三种解决方案：</p><ul><li>解决狗错分为猫的问题</li><li>解决猫科动物被错分成猫的问题</li><li>提升模糊图像被误分的问题</li></ul><p>可是由于我们精力和时间都有限，需要找出误分最主要的问题，因此我们要做的，是把所有错分的图像罗列出来，或者随机抽样一定的图像，分析每种错误它有多少，占错分图像多少比例。我们来看截图<br><img src="https://github.com/JoeAsir/blog-image/raw/master/blog/10/10-1.png" alt=""><br>每一个错分的图像都会进行标签化的统计，最后通过统计每一个标签，找出影响错分最严重的因素，作为我们的改进方向。</p><h3 id="Clean-up-incorrectly-labeled-data"><a href="#Clean-up-incorrectly-labeled-data" class="headerlink" title="Clean up incorrectly labeled data"></a>Clean up incorrectly labeled data</h3><p>在常见的错误中，错误的label是一种很常见的问题，这种问题往往来自于标注时候，错误的label会对training造成误导。</p><p>首先，对于training set，来说，incorrectly labeled data应该怎么处理？首先，Ng告诉了我们一个性质：</p><blockquote><p>DL algorithms are quite robust to random errors in the training set</p></blockquote><p>DL因为其自身的robust性质，当training set中有少许的，随机产生的incorrectly labeled data时，效果并不会有多差，我们完全不需要去管他。但是，当这incorrectly labeled data很多时就不行了，因为它们带来的是systematic errors，极端的想，如果把所有的白狗都错误的标注成了猫，那么这个cat recognition系统一定不会好，因为它一定会把白色的狗判断成为猫。</p><p>再来看看dev/test set中的incorrectly labeled data，对于这个问题，我们要做的是，评估incorrectly labeled data对dev error带来了多少贡献，解决的过程也是类似的，来看截图：<br><img src="https://github.com/JoeAsir/blog-image/raw/master/blog/10/10-2.png" alt=""><br>我们把incorrectly labeled也作为一个要素或标签，放在错分图像分析的过程中，<br>看看最终的统计结果，再决定incorrectly labeled data是不是影响dev error的主要原因，是否值得我们去fix it up.</p><p>最后，关于correcting incorrect dev/test set example，Ng给出了一些建议：</p><blockquote><p>Apply same proces to your dev and test sets to make sure thry continue to come from the same distribution.</p></blockquote><p>在修正的过程中，一定要保证dev和test set同时被修正，如果他们不再符合同一distribution，那么会对于后续的评价带来一些问题。</p><blockquote><p>Consider examining examples your algorithm got right as well as ones it got wrong.</p></blockquote><p>我们在更正的时候，不能只是看被错分的图像，对于被正确分类的，也有可能存在incorrect labeled 的情况。</p><blockquote><p>Tran and dev/test data may now come from slightly different distribution</p></blockquote><p>正如刚才讲的，DL对于training有一定程度的robust性，incorrect labeled data可能不会对training set带来这些问题，在这种情况下，我们可以不用去更正training set，这种情况，我们是可以接受的。</p><h3 id="Build-up-quickly-and-iterate"><a href="#Build-up-quickly-and-iterate" class="headerlink" title="Build up quickly and iterate"></a>Build up quickly and iterate</h3><p>最后Ng用一个speech recognition作为例子，我们首先要分析出可能影响效果的一些因素：</p><ul><li>Noisy background</li><li>Accented speech</li><li>Far from microphone</li><li>Young children’s speech</li></ul><p>…<br>针对这些问题，我们该如何构造我们的模型呢，Ng给出了建议</p><ul><li>Set up dev/test set and metric</li><li>Build initial system quickly</li><li>Use bias/variance analysis &amp; error analysis to prioritize next steps.</li></ul><p>总而言之，guideline是</p><blockquote><p>Build your first system quickly, then iterate.</p></blockquote><h2 id="Mismatched-training-and-dev-test-data"><a href="#Mismatched-training-and-dev-test-data" class="headerlink" title="Mismatched training and dev/test data"></a>Mismatched training and dev/test data</h2><h3 id="Training-and-testing-on-different-distributions"><a href="#Training-and-testing-on-different-distributions" class="headerlink" title="Training and testing on different distributions"></a>Training and testing on different distributions</h3><p>之前我们再三强调过一个尖锐的问题，那就是training/dev/test set一定要在同一个distribution下，但是实际上，愿望总是美好的而现实很残酷，我们总是会面对一些training and testing on different distribution问题。</p><p>例如在猫识别的任务中，我们需要将这个模型部署到手机app上，我们手上的数据只有10k是从手机拍摄获得的，而有200k的数据是从网络上获得的，这两种图像显然不属于同一distribution，我们应该怎么办？</p><p>首先来看option1，我们将所有的210k数据充分混合在一起，其中205k作为training set，2.5k作为dev，2.5作为test set。这样看起来是一个很不错的方法，但是，确实很不好的一个方法，为什么这么说呢？</p><p>在整个过程中，dev/test set其实扮演了一个非常重要的角色，它决定了我们的target，也就是整体的优化方向。在这个例子中，我们要优化的方向是app上的图像，而这种data set分割方法和我们的task target并不符合，因此并不优秀。</p><p>我们再来看option2，我们将200k的来自网络的图片全部放入training set，然后将10k的app数据，5k放入training set，2.5k作为dev，2.5作为test，这样做的话，dev/test决定的target 和我们的task target是一致的，所以长远来看，虽然option2的training/dev set并不是同一distribution，但是从长远看它的效果还是很不错的。</p><h3 id="Bias-amp-variance-with-mismatched-data-distribution"><a href="#Bias-amp-variance-with-mismatched-data-distribution" class="headerlink" title="Bias &amp; variance with mismatched data distribution"></a>Bias &amp; variance with mismatched data distribution</h3><p>在training/dev/test set符合同一distribution的时候，我们通过比较training error和dev error就可以定性是否存在high variance的问题。但是，当training set和dev set不符合同一distribution的时候，这个判断就显得有些困难了。我们应该怎么处理呢？</p><p>这时候，我们可以从training set中取出一小部分数据，命名为training-dev set，这部分数据将不再进行training，而是作为评判training效果的一个set，此时我们就有了training error，training-dev error和dev error三个error，再结合human error，training error和training-dev error之间的差值可以反映出模型是否有high bias或者variance，这样可以更科学的来评判模型效果。相应的，training-dev error和dev error相差越多，data mismatch的程度越大。</p><h3 id="Addressing-data-mismatch"><a href="#Addressing-data-mismatch" class="headerlink" title="Addressing data mismatch"></a>Addressing data mismatch</h3><p>我们如何addressing data mismatch呢，首先我们来看看Ng的两条guideline：</p><ul><li>Carry out manual error analysis to try to understand difference between training and dev/test sets</li><li>Make training data more similar;  or collect more data similar to dev/test sets</li></ul><p>理解一下，首先我们要通过人工的analysis去分析出造成training set和dev set之间distribution不同的原因，比如语音识别中的有无汽车噪声等等；然后我们需要根据这些差别，让training set和dev set更加的相似，甚至相通。</p><p>但是要注意的是，我们在这个过程中，要避免出现overfitting的情况出现，例如Ng举出的例子，在识别车内的人声过程中，我们可以通过人工的合成汽车声音与人的声音让training set和dev set更加的相似，但是如果我们的只用一段汽车噪音循环往复的去做合成，例如吧1min的汽车噪声循环的合成到1h的人声中，那结果一定是不尽如人意的，因为出现了overfitting.</p><h2 id="Transfer-learning"><a href="#Transfer-learning" class="headerlink" title="Transfer learning"></a>Transfer learning</h2><p>下面我们一起来看看大名鼎鼎的transfer learning，所谓transfer，就是存在一种从A到B的转换，而且这种情况往往是B的数据量很少，需要通过A来做一个pre-training过程。假设我们有如下的neural networks<br><img src="https://github.com/JoeAsir/blog-image/raw/master/blog/10/10-3.png" alt=""><br>假设这个我们使用这个neural networks训练了一个image recognition模型，在训练完成后，我们将最后的output，以及output对应的的\(w\)和\(b\)也删除，更换成例如放射数据再进行训练，如下图：<br><img src="https://github.com/JoeAsir/blog-image/raw/master/blog/10/10-4.png" alt=""><br>我们不仅仅可以把output层更换成一个新的output层，还可以将output层更换成几个新层。我们甚至可以将transfer之前的训练认为是一种pre-training，但是transfer training需要有几个条件：</p><ul><li>Task A and B have the same input x.</li><li>You have a lot more data for Task A than Task B.</li><li>Low level features from A could be helpful for learning B.</li></ul><h2 id="Multi-task-learning"><a href="#Multi-task-learning" class="headerlink" title="Multi-task learning"></a>Multi-task learning</h2><p>现在假设我们有一个自动驾驶的场景，我们需要从视频中识别行人、车辆、停车标志和红绿灯，按照常理来说，我们可以单独的构建4个模型。但是，这4个模型的特征场景都是很相似的，构建4个模型稍微有一些浪费，于是我们可以把这四个任务合并在一起，这就是Multi-task learning.</p><p>在这里我们的标签\(y\)，就不再是一个m×1的矩阵了，而是一个m×4的矩阵，对于multi-task来说，在以下情况下是可行的：</p><ul><li>Training on a set of tasks that could benefit from having shared lower-level features.</li><li>Usually: Amount of data you have for eachtask is quite similar.</li><li>Can train a big enough neural network to do well on all the tasks.</li></ul><h2 id="End-to-end-learning"><a href="#End-to-end-learning" class="headerlink" title="End to end learning"></a>End to end learning</h2><p>End to end learning是随着DL兴起后而产生的一种learning方式，在end2end中，我们不再关注一些中间的步骤，例如feature selection或者image processing，我们只是把原始的数据和最后的结果告诉DL，它就可以自主的完成这个任务。</p><p>当然end2end 也是有一些优势和劣势的，我们来看一下：<br>Pros：</p><ul><li>Let the data speak.</li><li>Less hand-desgining of components needed.</li></ul><p>Cons:</p><ul><li>May need large amount of data.</li><li>Excludes potentially userful hand-designed components.</li></ul><p>总之，对于end2end来说，大数据量，一定是最重要的因素，基于这一点，我们才可以摆脱传统的中间步骤，彻底实现end to end learning.</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li><a href="https://www.coursera.org/specializations/deep-learning" target="_blank" rel="noopener">Deep learning-Coursera Andrew Ng</a></li><li><a href="https://mooc.study.163.com/course/deeplearning_ai-2001281003#/info" target="_blank" rel="noopener">Deep learning-网易云课堂 Andrew Ng</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Hi all，course3来到了week2，本周的课程依然主要是关于一些learning strategy，这些方法相当实用。虽然不是什么具体的算法，但都都是Ng在科研和工作中积累下来的宝贵经验，对于实际问题十分有效。&lt;/p&gt;
&lt;p&gt;我们一起来看看。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="learning notes" scheme="https://asirzhao.github.io/categories/learning-notes/"/>
    
    
      <category term="learning strategy" scheme="https://asirzhao.github.io/tags/learning-strategy/"/>
    
      <category term="transfer learning" scheme="https://asirzhao.github.io/tags/transfer-learning/"/>
    
      <category term="multi-task learning" scheme="https://asirzhao.github.io/tags/multi-task-learning/"/>
    
  </entry>
  
  <entry>
    <title>Learning Notes-Deep Learning, course3, week1</title>
    <link href="https://asirzhao.github.io/2017/10/12/course-deep-learning-course3-week1/"/>
    <id>https://asirzhao.github.io/2017/10/12/course-deep-learning-course3-week1/</id>
    <published>2017-10-12T04:34:05.000Z</published>
    <updated>2020-05-10T06:50:12.528Z</updated>
    
    <content type="html"><![CDATA[<p>课程3主要讲的是deep learning中的一些strategy，这些strategy可以帮助我们快速的分析模型所存在的问题，避免我们的优化方向有偏差而导致的人力以及时间的浪费，这一点对于团队尤其重要。</p><p>我们一起来recap一下week1的课程<br><a id="more"></a></p><h2 id="Orthogonalization"><a href="#Orthogonalization" class="headerlink" title="Orthogonalization"></a>Orthogonalization</h2><p>对于ML task来说，有众多因素影响最终的效果，这些因素相互犬牙交错，因此我们在提升模型效果的时候，一定要把所有的因素orthogonalization一下，Ng举的例子就很形象，就像显示器的控制按钮一样，每个按钮各司其职，一个控制高度，一个控制宽度，一个控制大小，一个控制梯度，通过各自调整每一个按钮，我们可以很好的完成画面调整。</p><p>对于orthogonalization优化模型，Ng给出了4方面的建议：</p><ul><li>Fit training set well in cost function</li><li>Fit development set well on cost function</li><li>Fit test set well on cost function</li><li>Performs well in real world</li></ul><p>我们详细来看看这四条：</p><p>对于第一条，首先模型必须要对于training set有良好的拟合效果，如果这点达不到的话，模型一定是<strong>high bias</strong>，也就是<strong>under fitting</strong>了，那么我们必须尝试通过more complex的模型，bigger neural networks或者是longer training time去更充分的拟合training set.</p><p>对于第二条，在很好的拟合training set的前提下，我们就要看看development set的效果了，如果对于development set fit效果不好的话，那基本上就是<strong>high varience</strong>，也就是<strong>over fitting</strong>的问题了，这时候，regularization或者more training data可以解决解决这个问题。</p><p>对于第三条，在符合上两条的前提下，如果模型在test set上表现不佳，我们就需要更大的development set去涵盖更多的情况，并通过扩充后的development set重复第二条的检验</p><p>对于最后一条，在符合上三条的前提下，如果模型在real world中表现不佳，那么很大程度上是因为我们的development 和test set与real world相差比较多，比如猫脸检测中，我们的data set都是高清的图像，但是real world 中，都是像素很低的图像。因此，我们需要让development 和test set更接近与real world，然后重复上面的步骤。</p><p>通过这四个步骤，我们就可以通过orthogonalization来完成模型的调整</p><h2 id="Metric"><a href="#Metric" class="headerlink" title="Metric"></a>Metric</h2><h3 id="Single-number-evaluation"><a href="#Single-number-evaluation" class="headerlink" title="Single number evaluation"></a>Single number evaluation</h3><p>Metric无疑是ML task中很重要的环节，通过metric，我们可以评估不同模型之间的优良差异，并且可以选择出最理想的模型。</p><p>但是metric指标琳郎满目，例如对于两个模型，模型A的precision高于B的，但是A的recall又低于B，这时候就不太好评价两个模型，在这种情况下，我们需要采用单一的数字评价指标，例如我们可以用F1-score来进行评估，single number evaluation metric是我们做metrics时一定要注意的</p><h3 id="Satisficing-and-optimizing"><a href="#Satisficing-and-optimizing" class="headerlink" title="Satisficing   and optimizing"></a>Satisficing   and optimizing</h3><p>在某些情况下，例如我们不仅仅要求模型的指标，还对其他的，例如模型时间会有要求，如果一个模型有很高的模型accuracy，但是却很耗费时间，那是我们不能接受的，如下图例子：<br><img src="https://github.com/JoeAsir/blog-image/raw/master/blog/9/9-1.png" alt=""><br>图中的accuracy是optimizing metric，通常更高的accuracy就代表这classifier更加的优秀；但是，这里还有一个必须低于100ms 的running time作为satisficing metric，通常来说，如果我们有\(N\)个metrics，那么我们的optimizing metric必须只有一个，剩下的\(N-1\)metrics 都是satisficing metrics，只要以threshold形式进行限定就可以了。</p><h2 id="Data-set"><a href="#Data-set" class="headerlink" title="Data set"></a>Data set</h2><h3 id="Distributions"><a href="#Distributions" class="headerlink" title="Distributions"></a>Distributions</h3><p>对于training set，dev set和test set来说，所有data一定要保证服从同一个data distribution，例如猫脸实验，如果training set是高清大图而dev set是模糊图像，那么最终一定很难获得理想的metric.</p><p>最重要的是，你所构建模型的data，一定和模型应用场景的data在同样的distribution下，Ng给出的guideline是</p><blockquote><p>Choose a development set and test set to reflect data you expect to get in  the future and consider important to do well.</p></blockquote><h3 id="Size-of-data-set"><a href="#Size-of-data-set" class="headerlink" title="Size of data set"></a>Size of data set</h3><p>传统的ML task中，dataset的分布一般如下图所示：<br><img src="https://github.com/JoeAsir/blog-image/raw/master/blog/9/9-2.png" alt=""><br>但是在big data时代，一般采用下图：<br><img src="https://github.com/JoeAsir/blog-image/raw/master/blog/9/9-3.png" alt=""><br>Ng同样给出了guideline：</p><ul><li>Set uop the size of test set to give a high confidence in the overall performance of the system.</li><li>Test set helps evaluate the proformance of the final classifier which could be less than 30% of the whole data set</li><li>The development set has to be big enough to evaluate different ideas</li></ul><h2 id="Change-data-set-and-metric"><a href="#Change-data-set-and-metric" class="headerlink" title="Change data set and metric"></a>Change data set and metric</h2><h3 id="Change-data-set"><a href="#Change-data-set" class="headerlink" title="Change data set"></a>Change data set</h3><p>这个问题的原因，其实就是data set不在同一distribution下的问题，如果模型在dev和test set 上都有很好的表现和metric，但是在real world中效果并不好，那么我们要做的一定就是改变data set，让dev/test set与real world在同一distribution下</p><h3 id="Change-metric"><a href="#Change-metric" class="headerlink" title="Change metric"></a>Change metric</h3><p>我们使用ML来解决现实中的问题时，metric也不是一成不变的，需要根据具体的情况做出一些改变，例如，色情图像识别中，我们误讲非色情识别维色情图像，是可以接受的，但是将色情图像识别成非色情图像则是不可接受的，相似的，风控系统中，将风险用户分类为正常用户的错误，比把正常用户分类为风险用户的错误要严重很多，因此，在类似的场景下，我们的metric需要随着业务场景做出一些改变。</p><p>正常情况下，我们计算的模型error是：<br>$$Error = \frac{1}{m_{dev}} \sum^{m_{dev}}<em>{i=1} \mathcal { \hat{y}^{(i)} \neq y^{(i)} }$$<br>但是在上述场景中，我们需要加入一个权重，来对两种不同错误加以区分<br>$$ w^{(i)}=\left{<br>\begin{aligned}<br>1 \quad x^{(i)}notpron \<br>10 \quad x^{(i)} pron \<br>\end{aligned}<br>\right.<br>$$<br>$$Error = \frac{1}{ \sum w^{(i)}} \sum^{m</em>{dev}}_{i=1} w^{(i)} \mathcal { \hat{y}^{(i)} \neq y^{(i)} }$$<br>这样就把两种不同的问题区分开了。</p><h2 id="Improve-model-performance"><a href="#Improve-model-performance" class="headerlink" title="Improve model performance"></a>Improve model performance</h2><p>模型performance的提升是模型的核心问题，我们如何确定模型调整的大体方向，Ng给出了如下的图<br><img src="https://github.com/JoeAsir/blog-image/raw/master/blog/9/9-4.png" alt=""><br>我们默认human-level是很接近理论误差，也就是Bayes error，我们需要比较human-level，training error和dev error这三者之间的关系，human-level和training error之间的差值更大的话，我们就需要去减小bias，反之，我们需要去减少variance，具体的方法，还是我们之前的老套路。</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li><a href="https://www.coursera.org/specializations/deep-learning" target="_blank" rel="noopener">Deep learning-Coursera Andrew Ng</a></li><li><a href="https://mooc.study.163.com/course/deeplearning_ai-2001281003#/info" target="_blank" rel="noopener">Deep learning-网易云课堂 Andrew Ng</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;课程3主要讲的是deep learning中的一些strategy，这些strategy可以帮助我们快速的分析模型所存在的问题，避免我们的优化方向有偏差而导致的人力以及时间的浪费，这一点对于团队尤其重要。&lt;/p&gt;
&lt;p&gt;我们一起来recap一下week1的课程&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="learning notes" scheme="https://asirzhao.github.io/categories/learning-notes/"/>
    
    
      <category term="learning strategy" scheme="https://asirzhao.github.io/tags/learning-strategy/"/>
    
      <category term="orthogonalization" scheme="https://asirzhao.github.io/tags/orthogonalization/"/>
    
  </entry>
  
  <entry>
    <title>Learning Notes-Deep Learning, course2, week3</title>
    <link href="https://asirzhao.github.io/2017/09/30/course-deep-learning-course2-week3/"/>
    <id>https://asirzhao.github.io/2017/09/30/course-deep-learning-course2-week3/</id>
    <published>2017-09-30T07:44:25.000Z</published>
    <updated>2020-05-10T06:50:12.528Z</updated>
    
    <content type="html"><![CDATA[<p>不知不觉来到第三周的课程了，大家加油！这周的主要内容是hyperparameter selection和batch normal的问题，我们一起来看看这一周的内容！<br><a id="more"></a></p><h2 id="Hyperparameter-selection"><a href="#Hyperparameter-selection" class="headerlink" title="Hyperparameter selection"></a>Hyperparameter selection</h2><p>Hyperparameter selection在machine learning中是一个非常重要的优化过程，例如gradient descent中的learning rate \(\alpha\)就是关乎算法结果的重要hyperparameter，那么我们应该怎么去选择呢？Ng给出了两个建议：</p><ul><li>构建多个hyperparameter交叉，随机选择大小，选择效果较好的范围，继续随机选择hyperparameter大小，观察结果。</li><li>选择参数的时候分段选择，并且使用log分段，例如在0.0001到1之间选择，将数轴分成0.0001，0.001，0.01，0.1和1，这样选择出的结果更好</li></ul><p>对于整体模型的hyperparameter selection，Ng也出了建议，那就是babysitting和parallel方法，一种是对一个模型多次调整，一种是同时启动多个不同hyperparameter的模型，最后取效果最好的。</p><p>两种方法殊途同归，可以根据自己的具体情况做出选择。</p><h2 id="Batch-norm"><a href="#Batch-norm" class="headerlink" title="Batch norm"></a>Batch norm</h2><h3 id="Normalization"><a href="#Normalization" class="headerlink" title="Normalization"></a>Normalization</h3><p>相信大家都听说过大名鼎鼎的normalization吧，这是一种很棒的数据预处理的方法，它可以很好的提升数据处理（例如gradient descent）的速度和效果，在引入batch norm之前，我也稍微提一下normalization，下面上公式：</p><p>对于输入数据来说，我们可以按以下方法来normalize<br>$$ \mu = \frac{1}{m} \sum_i x^{(i)}$$<br>$$X = X- \mu$$<br>$$ \sigma^2 = \frac{1}{m} \sum_i (x^{(i)})^2 $$<br>$$ X = X/ \sigma ^2$$<br>这样，我们就把输入数据转化成了符合期望为0，方差为1的Gaussian distribution的数据。</p><p>当然，这只是normaliztion中的一种方法，也是被称作z-score方法。</p><h3 id="Batch-norm-1"><a href="#Batch-norm-1" class="headerlink" title="Batch norm"></a>Batch norm</h3><p>上面说的normalization方法可以推广到neural networks中，对于nerual networks中的某一个layer来说，可以看做是一个孤立的计算过程，在这个过程中，我们可以引入normalization，对于\(z^{(i)}\)来说：<br>$$ \mu = \frac{1}{m} \sum_i z^{(i)}$$<br>$$ \sigma ^2= \frac{1}{m} \sum_i (z^{(i)}- \mu)^2 $$<br>$$z^{(i)}<em>{norm}= \frac{z^{(i)}- \mu}{ \sqrt{ \sigma^2 + \epsilon}}$$<br>$$z^{N(i)}= \gamma z^{(i)}</em>{norm} + \beta$$<br>然后我们用最终的\(z^{N[l](i)}\)来替换\(z^{[l](i)}\) 就可以，其中\( \gamma\)和\(\beta\)是两个parameter，可以通过gradient descent来更新，这两个parameter存在的意义，就是可以调整normalization映射的Gaussian distribution，而不是统统映射到Normal distribution，值得注意的是，\(\epsilon\)是一个很小的数，用来避免分母分0的情况。</p><p>如果\(\gamma = \sqrt{ \sigma^2 + \epsilon}\)且\( \beta = \mu\)的话，那么其实\(z^{N(i)}=z^(i)\)的，大家可以算算，这种情况下，就是相当于没做normalization.</p><h3 id="Batch-norm-on-neural-networks"><a href="#Batch-norm-on-neural-networks" class="headerlink" title="Batch norm on neural networks"></a>Batch norm on neural networks</h3><p>对于neural networks，输入\(X\)通过parameter\(w^{[1]}\)和\(b^{[1]}\)得到\(z^{[1]}\)，通过\(\beta\)和\(\gamma\)获得\(z^{N[1]}\)，经过active function后获得\(a^{[1]}\)，通过\(w^{[2]}\)和\(b^{[2]}\)获得\(z^{[2]}\)，如此下去，一直到最后的输出层，完成forward propagation.</p><p>在整个过程中，一共有四个parameters，分别是\(w^{[l]}\)，\(b^{[l]}\)，\( \beta^{[l]}\)，\( \gamma^{[l]}\)，我们都知道：<br>$$z^{[l]}=w^{[l]}a^{[l-1]}+b^{[l]}$$<br>但是，我们在做batch normal的时候，首先会把\(z^{[l]}\)映射到期望为1方差为0的Gaussian distribution上，这就意味着\(b^{[l]}\)是可以忽略掉的，因为即使保留，在batch normal的时候也会被减去，因此，我们的parameter只有三个，即：\(w^{[l]}\)，\( \beta^{[l]}\)，\( \gamma^{[l]}\)</p><p>在backforward的时候，我们和普通的neural networks一样，只是可以不用再去计算\(db\)</p><h3 id="Solve-covariate-shift"><a href="#Solve-covariate-shift" class="headerlink" title="Solve covariate shift"></a>Solve covariate shift</h3><p>什么是covariate shift？简单的理解，就是模型需要随着样本的变化而变化，Ng举的例子就很直观，在猫脸试验中，假设training set里都是黑猫，这样获得的模型，对于花猫识别就是不适用的，这就叫covariate shift. 其实，batch norm可以改善neural networks效果的原因，就可以理解为solve covariate shift的过程。</p><p>OK，我们来详细看看原因，假设我们有一个如图的neural networks：<br><img src="https://github.com/JoeAsir/blog-image/raw/master/blog/8/8-1.png" alt=""><br>在标示出的位置，有parameter\(w^{[3]}\)和\(b^{[3]}\)，如果我们盖住前面的部分，那么我们将获得如图的neural networks<br><img src="https://github.com/JoeAsir/blog-image/raw/master/blog/8/8-2.png" alt=""><br>对于neural networks来说，相当于获得了黑箱输出的\(a^{[2]}\)，而\(a^{[2]}\)的值其实并不是是固定的，每一次iteration后都有不一样的\(a^{[2]}\)，这就产生了covariate shift问题。</p><p>但是，batch norm可以将\(a^{[2]}\)的期望和方差限制到\(\beta\)和\(gamma\)控制的范围内，以此<strong>极大限度</strong>的缓解了covariate shift现象。</p><p>另外，batch norm还可以有一些regularization的作用，由于每次mini-batch gradient descent中batch norm作用的sample不一样，类似于dropout的效果，会给对应layer加入一些噪声，以此产生一些regularization的效果。但是，我们一般不会把batch norm列入regularization范畴内。</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li><a href="https://www.coursera.org/specializations/deep-learning" target="_blank" rel="noopener">Deep learning-Coursera Andrew Ng</a></li><li><a href="https://mooc.study.163.com/course/deeplearning_ai-2001281003#/info" target="_blank" rel="noopener">Deep learning-网易云课堂 Andrew Ng</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;不知不觉来到第三周的课程了，大家加油！这周的主要内容是hyperparameter selection和batch normal的问题，我们一起来看看这一周的内容！&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="learning notes" scheme="https://asirzhao.github.io/categories/learning-notes/"/>
    
    
      <category term="hyperparameter" scheme="https://asirzhao.github.io/tags/hyperparameter/"/>
    
      <category term="batch norm" scheme="https://asirzhao.github.io/tags/batch-norm/"/>
    
      <category term="covariate shift" scheme="https://asirzhao.github.io/tags/covariate-shift/"/>
    
  </entry>
  
</feed>
