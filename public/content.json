{"pages":[{"title":"About","text":"Welome to superAsir’s Notes.All the cover images are downloaded from PEXELS. 📖 M.E. from BUAA 💻 Software Developer/Data Scientist 🇨🇳 Beijing, China 💡 Machine learning, deep learning and distributed computing 🔴 COME ON YOU GOONERS! 🎸 Punk’s not dead 🥃 Prefer rum to vodka 🍺 Prefer Tsingtao to Corona ☕️ Could’t love Caffè Americano more","link":"/about/index.html"},{"title":"Tags","text":"","link":"/tags/index.html"},{"title":"Categories","text":"","link":"/categories/index.html"}],"posts":[{"title":"Learning Notes-Deep Learning, course2, week1","text":"大家好，最近在学习Andrew Ng的Deep learning课程，于是决定写一些learning notes来recap和mark一下学到的知识，避免遗忘。由于该课程的course1比较基础，我个人认为没有mark的必要，所以从course2开始，按照week来mark. Data set在machine learning中，data set可以说是最重要的部分，区别于传统machine learning，deep learning中的data set分布更侧重于training，Ng建议我们讲data set分为三部分： training set——训练数据集 dev/validation set——模型选择和参数调整，泛化能力测试 testing set——模型效果测试一定有很多人对于dev和testing set有一些疑问，最开始我也是懵逼的，来看看下面这段话 Dev/Validation Set: this data set is used to minimize overfitting. You’re not adjusting the weights of the network with this data set, you’re just verifying that any increase in accuracy over the training data set actually yields an increase in accuracy over a data set that has not been shown to the network before, or at least the network hasn’t trained on it (i.e. validation data set). If the accuracy over the training data set increases, but the accuracy over then validation data set stays the same or decreases, then you’re overfitting your neural network and you should stop training.Testing Set: this data set is used only for testing the final solution in order to confirm the actual predictive power of the network. 这三者的比例则是/99.5%/2.5%/2.5%/，这样的原因是因为deep learning中，数据量足够大而且deep learning的学习能力很强，大家一定注意这一点。当然，如果实在没有test set，但是有dev set也是可以接受的。 Bias and variance什么是bias和variancebias &amp; variance是machine learning 领域一个经典的辩证问题，在Ng经典的CS229中就重点的讲述过，具体的定义我不太想给出了，后续有时间可以专门写一篇，后面会给出一些资料链接。我们简单的看一幅图左图就是一个典型的high bias situation，模型没有办法很好的拟合数据，这也就是我们常说的under fitting，右图则是典型的high variance situation，模型过分的拟合了training set，这就是我们最需要防范的over fitting.当然，中间的则是比较理想的状况。 Solution在实际的工作中，我们应该怎么分析自己模型的bias和variance情况呢，Ng给了我们一个流程图，如下：首先检验是否存在high bias 情况，具体方法是在training set 和 dev set上计算error，对比training error和dev error，如果两者都很高，那么就是high bias，如果training error很小而dev error很高，那么一定是high variance，如果两者都很大，那么就是最差的情况了既high bias又high variance 对于high bias，我们可以通过更复杂的神经网络、更长的训练时间，更强的网络结构来解决这个问题；对于high variance，我们可以通过更多的数据，regularization的方法来解决。 RegularizationL1&amp;L2 regularization这部分内容我就不多说了，我之前专门详细深入的讲述过L1和L2 regularization，大家可以去看一看。 唯一需要明确的一点是，在加入L1或者L2 regularization之后，在观测cost function convergence 的时候，一定要带上regularization item，否则结果是很难看到convergence的，这和regularization性质有很大的关系。 DropoutDropout是neural network中一种经典的regularization方法，经典到什么程度呢，我当年毕设课题中都用到了这个方法，而且效果超赞 Dropout方法的实质是按比例随机隐藏掉neural network中layer里的某些units，也就是说，再一次epoch中，只有一部分的units对应的weights和bias会得到更新，而下一次epoch中，则是另一部分units对应的weights和bias得到更新，如下图那么为什么Dropout可以实现regularization效果呢，Ng告诉我们： Intuition:Can’t rely on any one feature, so have to spread out weights 如何理解呢？加入dropout后，每个unit对应的weights和bias不能完全依赖上层units，因为他并不是每一次epoch都可以work on，因此在学习的过程中，见笑了over fitting的风险。实际上，dropout可以产生shrink weights的效果，和L2 regularization相似，因此也是一种regularization方法。 但是，dropout和L2 regularization唯一的区别在于，他很难给出一个regularization item，所以你没有办法画出cost function convergence的轨迹。 Other methods除了经典的L1、L2 regularization和dropout方法，还有一些防止over fitting的方法，例如图像处理中，我们可以用data augmentation，旋转，翻转，加噪声等方法。 还有一个early stopping方法，我们都知道，随着training 的epoch增多，模型对training set拟合会越来越好，随之带来的问题就是可能over fitting，我们可以通过early stopping，让模型在没有产生over fitting的时候停下来，效果可能会更好。 Exploding/vanishing gradient什么是exploding/vanishing gradient对于deep learning，曾经最为棘手的问题就是exploding/vanishing gradient，甚至是限制deep learning发展的瓶颈，我们来一起看看。 假设我们有一个比较深的neural network，假设一共有\\(l\\)层，对应的weights是\\(W^{[1]}\\)到\\(W^{[l]}\\)，bias是\\(b^{[1]}\\)到\\(b^{[l]}\\)，我们为了计算方便，假设bias均为0，active function为\\(g(z)=z\\)，那么，\\(y\\)就等于$$\\hat{y}=W^{[l]}W^{[l-1]}W^{[l-2]} \\cdots W^{[3]}W^{[2]}W^{[1]}X$$大家感兴趣的话可以验证一下，很简单的。 那么现在问题来了，当\\(l\\)很大的情况下，如果\\(W\\)元素都大于1，那么最后的结果就会非常非常大，甚至到无限大，这种情况叫exploding gradient；相应的，如果\\(W\\)元素都小于1，那么最后的结果就会特别小，甚至为零，这就是vanishing gradient. Solution对于上面的问题，我们一般在weights初始化的时候做一些工作来解决可能出现的exploding or vanishing gradient。我们可以直观的理解一下，对于active function\\(g(z)\\)，假设bias为零，$$z=w_{1}x_1 +w_{2}x_2+ \\cdots +w_{n}x_n$$我们要我们可以看到，\\(n\\)的增大，\\(w\\)会变小，我们让\\(w\\)始终保持以0为mean，1为varance的Gaussian distribution下，就可以很好的控制\\(w\\)的大小，那么我们可以看到，\\(var(w)= \\frac{1}{n^{l-1}}\\) 在Ng的建议中，如果active function是sigmoid，我们一般取\\(var(w)= \\frac{1}{n^{l-1}}\\)，如果是reLu，我们取\\(var(w)= \\frac{2}{n^{l-1}}\\)，对于tanh，\\(var(w)= \\frac{1}{n^{l-1}}\\)或者\\(var(w)= \\frac{2}{n^{l-1}+n^{l}}\\)，这样可以很好的避免exploding or vanishing gradient. Gradient checkingGradient approximation在调试neural network的时候，我们会经常做gradient check的工作，以确定整个network正常的运行，Ng在这里建议我们使用双边逼近的方法去做gradient check，这里我不做太多描述，主要上一张图：通常来说，双边逼近的方法获得结果更加准确。 Gradient checking notes Don’t use in training-only to debug(too slow) If algorithm fails grad check, look at components to try to identify bug Remeber regularization Dosen’t wrok with dropout Run at random initialzation; perhaps again after some training. Reference Deep learning-Coursera Andrew Ng Deep learning-网易云课堂 Andrew Ng Bias and variance","link":"/2017/09/24/course-deep-learning-course2-week1/"},{"title":"Learning Notes-Deep Learning, course2, week3","text":"不知不觉来到第三周的课程了，大家加油！这周的主要内容是hyperparameter selection和batch normal的问题，我们一起来看看这一周的内容！ Hyperparameter selectionHyperparameter selection在machine learning中是一个非常重要的优化过程，例如gradient descent中的learning rate \\(\\alpha\\)就是关乎算法结果的重要hyperparameter，那么我们应该怎么去选择呢？Ng给出了两个建议： 构建多个hyperparameter交叉，随机选择大小，选择效果较好的范围，继续随机选择hyperparameter大小，观察结果。 选择参数的时候分段选择，并且使用log分段，例如在0.0001到1之间选择，将数轴分成0.0001，0.001，0.01，0.1和1，这样选择出的结果更好 对于整体模型的hyperparameter selection，Ng也出了建议，那就是babysitting和parallel方法，一种是对一个模型多次调整，一种是同时启动多个不同hyperparameter的模型，最后取效果最好的。 两种方法殊途同归，可以根据自己的具体情况做出选择。 Batch normNormalization相信大家都听说过大名鼎鼎的normalization吧，这是一种很棒的数据预处理的方法，它可以很好的提升数据处理（例如gradient descent）的速度和效果，在引入batch norm之前，我也稍微提一下normalization，下面上公式： 对于输入数据来说，我们可以按以下方法来normalize$$ \\mu = \\frac{1}{m} \\sum_i x^{(i)}$$$$X = X- \\mu$$$$ \\sigma^2 = \\frac{1}{m} \\sum_i (x^{(i)})^2 $$$$ X = X/ \\sigma ^2$$这样，我们就把输入数据转化成了符合期望为0，方差为1的Gaussian distribution的数据。 当然，这只是normaliztion中的一种方法，也是被称作z-score方法。 Batch norm上面说的normalization方法可以推广到neural networks中，对于nerual networks中的某一个layer来说，可以看做是一个孤立的计算过程，在这个过程中，我们可以引入normalization，对于\\(z^{(i)}\\)来说：$$ \\mu = \\frac{1}{m} \\sum_i z^{(i)}$$$$ \\sigma ^2= \\frac{1}{m} \\sum_i (z^{(i)}- \\mu)^2 $$$$z^{(i)}{norm}= \\frac{z^{(i)}- \\mu}{ \\sqrt{ \\sigma^2 + \\epsilon}}$$$$z^{N(i)}= \\gamma z^{(i)}{norm} + \\beta$$然后我们用最终的\\(z^{N[l](i)}\\)来替换\\(z^{[l](i)}\\) 就可以，其中\\( \\gamma\\)和\\(\\beta\\)是两个parameter，可以通过gradient descent来更新，这两个parameter存在的意义，就是可以调整normalization映射的Gaussian distribution，而不是统统映射到Normal distribution，值得注意的是，\\(\\epsilon\\)是一个很小的数，用来避免分母分0的情况。 如果\\(\\gamma = \\sqrt{ \\sigma^2 + \\epsilon}\\)且\\( \\beta = \\mu\\)的话，那么其实\\(z^{N(i)}=z^(i)\\)的，大家可以算算，这种情况下，就是相当于没做normalization. Batch norm on neural networks对于neural networks，输入\\(X\\)通过parameter\\(w^{[1]}\\)和\\(b^{[1]}\\)得到\\(z^{[1]}\\)，通过\\(\\beta\\)和\\(\\gamma\\)获得\\(z^{N[1]}\\)，经过active function后获得\\(a^{[1]}\\)，通过\\(w^{[2]}\\)和\\(b^{[2]}\\)获得\\(z^{[2]}\\)，如此下去，一直到最后的输出层，完成forward propagation. 在整个过程中，一共有四个parameters，分别是\\(w^{[l]}\\)，\\(b^{[l]}\\)，\\( \\beta^{[l]}\\)，\\( \\gamma^{[l]}\\)，我们都知道：$$z^{[l]}=w^{[l]}a^{[l-1]}+b^{[l]}$$但是，我们在做batch normal的时候，首先会把\\(z^{[l]}\\)映射到期望为1方差为0的Gaussian distribution上，这就意味着\\(b^{[l]}\\)是可以忽略掉的，因为即使保留，在batch normal的时候也会被减去，因此，我们的parameter只有三个，即：\\(w^{[l]}\\)，\\( \\beta^{[l]}\\)，\\( \\gamma^{[l]}\\) 在backforward的时候，我们和普通的neural networks一样，只是可以不用再去计算\\(db\\) Solve covariate shift什么是covariate shift？简单的理解，就是模型需要随着样本的变化而变化，Ng举的例子就很直观，在猫脸试验中，假设training set里都是黑猫，这样获得的模型，对于花猫识别就是不适用的，这就叫covariate shift. 其实，batch norm可以改善neural networks效果的原因，就可以理解为solve covariate shift的过程。 OK，我们来详细看看原因，假设我们有一个如图的neural networks：在标示出的位置，有parameter\\(w^{[3]}\\)和\\(b^{[3]}\\)，如果我们盖住前面的部分，那么我们将获得如图的neural networks对于neural networks来说，相当于获得了黑箱输出的\\(a^{[2]}\\)，而\\(a^{[2]}\\)的值其实并不是是固定的，每一次iteration后都有不一样的\\(a^{[2]}\\)，这就产生了covariate shift问题。 但是，batch norm可以将\\(a^{[2]}\\)的期望和方差限制到\\(\\beta\\)和\\(gamma\\)控制的范围内，以此极大限度的缓解了covariate shift现象。 另外，batch norm还可以有一些regularization的作用，由于每次mini-batch gradient descent中batch norm作用的sample不一样，类似于dropout的效果，会给对应layer加入一些噪声，以此产生一些regularization的效果。但是，我们一般不会把batch norm列入regularization范畴内。 Reference Deep learning-Coursera Andrew Ng Deep learning-网易云课堂 Andrew Ng","link":"/2017/09/30/course-deep-learning-course2-week3/"},{"title":"Learning Notes-Deep Learning, course2, week2","text":"大家好，课程来到了第二周，这周主要是一些优化方法，使得整个neural networks可以更快更好的工作，我们一起来recap一下。 Mini-batch gradient descent这一节我就不打算写了，比较基础，其实mini-batch gradient descent是batch gradient和stochastic gradient descent综合后的结果，一方面解决了batch gradient计算量大，容易converge到local minimum的问题，也解决了stochastic gradient descent epoch太多的弊端，是现在gradient descent使用最广泛的方法。 对于mini-batch gradient descent，Ng建议batch大小取决于数据量多少，在小数据量上完全没有必要做mini-batch，直接使用batch gradient descent就可以，对于大数据量的情况，最好选择2的乘方，如64,128,256,512来作为batch size. 当然，batch size也要满足CPU和GPU的内存大小。 Exponentially weighted averagesExponentially weighted averagesExponentially weighted averages，也被称为moving averages，是一种综合历史数据的加权平均方法，课程中Ng用了伦敦一年的气温变化曲线作为例子，对于固有变量\\(\\theta\\)来说，我们要求的平均值\\(v\\)应该是$$v_0 = 0$$$$v_1 = \\beta v_0 + (1- \\beta) \\theta_1 $$$$v_2 = \\beta v_1 + (1- \\beta) \\theta_2$$$$v_3= \\beta v_2 + (1- \\beta) \\theta_3$$$$\\cdots$$$$v_t= \\beta v_{t-1} + (1- \\beta) \\theta_t$$其中\\(\\beta\\)是一个因子，它决定了moving averages大约向前平均了\\(\\frac{1}{1- \\beta}\\)个\\(\\theta\\)值，例如\\(\\beta = 0.9\\)，那么大约向前平均了10个值，并且是向前按指数衰减加权获得的平均值。 Bias correction在exponentially weighted averages中，有一个问题很尖锐，那就是在最初的求解过程中，由于\\(v_0=0\\)，导致前面的数字结果距离正确结果较小，如下图所示，紫色曲线是获得的结果，而在起始位置的值明显是偏小的。此时，我们引入bias correction，原理也很简单，就是此处我们不使用\\(v_t\\)作为最终的结果，而是使用\\( \\frac{v_t}{1- \\beta^{t}}\\)作为最后的结果，通过bias correction，我们会获得绿色的曲线。 我们可以看到，起始绿色钱和紫色曲线在最后基本没有差别，几乎重合，但是在曲线开始的时候，绿色曲线比紫色曲线更加逼近真实情况，因此，Ng给我们以下建议： 当我们不关注moving averages initial value大小的时候，我们可以不使用bias correction Bias correction对于initial value效果更好 Gradient descent optimizationmomentum在gradient descent中，我们经常会遇到一种情况，如图所示：在水平方向上，我们希望更快的下降，而在垂直方向上我们希望更小的下降速率，以避免过多的iteration，针对这种情况，我们讲moving averages的思想带入进来，这就是momentum方法。 在momentum中，我们的每次迭代中：$$v_{dW}= \\beta v_{dW}+(1- \\beta)dW$$$$v_{db}= \\beta v_{db}+(1- \\beta)db$$$$W:=W- \\alpha v_{dW}$$$$b:=b - \\alpha v_{db}$$在很多的文献中，上面的\\(1- \\beta\\)项被省略掉了，这样做只是将等式等量做了缩放，并不影响实际的效果，Ng表示，两种方式的momentum几乎没有差别，大家可以放心使用。 实际上，momentum可以理解为将数次之前迭代过程中的gradient变化也带入到了这次迭代过程中，我个人认为就是带入了一种gradient变化的趋势，这样可以更好的控制gradient descent的方向和大小。例如上图中的纵向方向中，加入momentum后可以轻松的将纵向梯度正负抵消到近似0，这样就可以减少在gradient descent在纵向的反复迭代，因为，那既是无用功，也是我们不愿意看到的情况。 另外，Ng给我们了一个\\(\\beta\\)的理想取值，既0.9，这个值大约取了前10次迭代结果的moving averages。另外，Ng表示，在momen中很少使用bias correction，因为10次迭代之后，这种问题很快就会消除，而一般的gradient descent，iteration次数远远大于10次。 RMSprop除了momentum，还有一些类似的方法，例如大名鼎鼎的RMSprop(root means square prop)，在这个方法中，主要体现了square的应用，我们来看看，在每次的迭代中：$$S_{dW}= \\beta S_{dW} + (1- \\beta)(dW)^2$$$$S_{db}= \\beta S_{db}+(1- \\beta)(db)^2$$$$W:=W- \\alpha \\frac{dW}{\\sqrt {S_{dW}}+ \\epsilon}$$$$b:=b- \\alpha \\frac{db}{\\sqrt {S_{db}} + \\epsilon}$$\\(\\epsilon\\)是为了防止分母为0的一个item，取值建议为\\(10^{-8}\\) 假设在gradient descent中，\\(W\\)下降速率太低，也就是\\(dW\\)太小，那么在RMSprop的迭代中，\\(dW\\)将会除以一个很小的值\\( \\sqrt{S_{dW}}\\)，也就是\\(W\\)将会减去一个较大的值，反之亦然。 通过这种方式，我们缓解了上图所示的情况，改善了gradient descent的合理性，同时可以使用更大的\\(\\alpha\\)去实现更快的gradient descent. Adam我们看到了momentum和RMSprop优化方法的厉害之处，现在Adam方法横空出世，他融合了momentum和RMSprop，他是如何融合的呢，我们把momentum中的\\( \\beta\\)命名为\\( \\beta_1\\)，把RMSprop中的\\( \\beta\\)命名为\\( \\beta_2\\)，在第\\(t\\)次迭代中：$$v_{dW}= \\beta_1 v_{dW}+(1- \\beta_1)dW, \\qquad v_{db}= \\beta_1 v_{db}+(1- \\beta_1)db$$$$s_{dW}= \\beta_2 s_{dW}+(1- \\beta_2)(dW)^2, \\qquad s_{db}= \\beta_2 s_{db}+(1- \\beta_2)(db)^2$$加上bias correction后$$v^{corrected}{dW}= \\frac{v{dW}}{1- \\beta^t_1}, \\qquad v^{corrected}{db}= \\frac{v{db}}{1- \\beta^t_1}$$$$s^{corrected}{dW}= \\frac{s{dW}}{1- \\beta^t_2}, \\qquad s^{corrected}{db}= \\frac{s{db}}{1- \\beta^t_2}$$$$W:=W- \\alpha \\frac{v^{corrected}{dW}}{ \\sqrt{s^{corrected}{dW}}+ \\epsilon}$$$$W:=W- \\alpha \\frac{v^{corrected}{db}}{ \\sqrt{s^{corrected}{db}}+ \\epsilon}$$其中，\\(\\epsilon\\)是为了防止分母为0的一个item，取值建议为\\(10^{-8}\\)，\\(\\beta_1\\)建议取值0.9，\\(\\beta_2\\)建议取值0.999。 Learning rate decay在gradient descent中，随着迭代的深度，越来越靠近minimum，我们需要更小的learning rate，以避免越过minimum，常用的learning decay方法有：$$\\alpha = \\frac{1}{1+decayRateepochNum} \\alpha_0$$$$\\alpha = 0.95^{epochNum} \\alpha_0$$$$\\alpha = \\frac{k}{\\sqrt{epochNum}} \\alpha_0$$这些方法都可以让\\(\\alpha\\)随着迭代次数增加而慢慢变小，可以更好的逼近minimum. Reference Deep learning-Coursera Andrew Ng Deep learning-网易云课堂 Andrew Ng An overview of gradient descent optimization algorithms","link":"/2017/09/27/course-deep-learning-course2-week2/"},{"title":"Learning Notes-Deep Learning, course3, week1","text":"课程3主要讲的是deep learning中的一些strategy，这些strategy可以帮助我们快速的分析模型所存在的问题，避免我们的优化方向有偏差而导致的人力以及时间的浪费，这一点对于团队尤其重要。 我们一起来recap一下week1的课程 Orthogonalization对于ML task来说，有众多因素影响最终的效果，这些因素相互犬牙交错，因此我们在提升模型效果的时候，一定要把所有的因素orthogonalization一下，Ng举的例子就很形象，就像显示器的控制按钮一样，每个按钮各司其职，一个控制高度，一个控制宽度，一个控制大小，一个控制梯度，通过各自调整每一个按钮，我们可以很好的完成画面调整。 对于orthogonalization优化模型，Ng给出了4方面的建议： Fit training set well in cost function Fit development set well on cost function Fit test set well on cost function Performs well in real world 我们详细来看看这四条： 对于第一条，首先模型必须要对于training set有良好的拟合效果，如果这点达不到的话，模型一定是high bias，也就是under fitting了，那么我们必须尝试通过more complex的模型，bigger neural networks或者是longer training time去更充分的拟合training set. 对于第二条，在很好的拟合training set的前提下，我们就要看看development set的效果了，如果对于development set fit效果不好的话，那基本上就是high varience，也就是over fitting的问题了，这时候，regularization或者more training data可以解决解决这个问题。 对于第三条，在符合上两条的前提下，如果模型在test set上表现不佳，我们就需要更大的development set去涵盖更多的情况，并通过扩充后的development set重复第二条的检验 对于最后一条，在符合上三条的前提下，如果模型在real world中表现不佳，那么很大程度上是因为我们的development 和test set与real world相差比较多，比如猫脸检测中，我们的data set都是高清的图像，但是real world 中，都是像素很低的图像。因此，我们需要让development 和test set更接近与real world，然后重复上面的步骤。 通过这四个步骤，我们就可以通过orthogonalization来完成模型的调整 MetricSingle number evaluationMetric无疑是ML task中很重要的环节，通过metric，我们可以评估不同模型之间的优良差异，并且可以选择出最理想的模型。 但是metric指标琳郎满目，例如对于两个模型，模型A的precision高于B的，但是A的recall又低于B，这时候就不太好评价两个模型，在这种情况下，我们需要采用单一的数字评价指标，例如我们可以用F1-score来进行评估，single number evaluation metric是我们做metrics时一定要注意的 Satisficing and optimizing在某些情况下，例如我们不仅仅要求模型的指标，还对其他的，例如模型时间会有要求，如果一个模型有很高的模型accuracy，但是却很耗费时间，那是我们不能接受的，如下图例子：图中的accuracy是optimizing metric，通常更高的accuracy就代表这classifier更加的优秀；但是，这里还有一个必须低于100ms 的running time作为satisficing metric，通常来说，如果我们有\\(N\\)个metrics，那么我们的optimizing metric必须只有一个，剩下的\\(N-1\\)metrics 都是satisficing metrics，只要以threshold形式进行限定就可以了。 Data setDistributions对于training set，dev set和test set来说，所有data一定要保证服从同一个data distribution，例如猫脸实验，如果training set是高清大图而dev set是模糊图像，那么最终一定很难获得理想的metric. 最重要的是，你所构建模型的data，一定和模型应用场景的data在同样的distribution下，Ng给出的guideline是 Choose a development set and test set to reflect data you expect to get in the future and consider important to do well. Size of data set传统的ML task中，dataset的分布一般如下图所示：但是在big data时代，一般采用下图：Ng同样给出了guideline： Set uop the size of test set to give a high confidence in the overall performance of the system. Test set helps evaluate the proformance of the final classifier which could be less than 30% of the whole data set The development set has to be big enough to evaluate different ideas Change data set and metricChange data set这个问题的原因，其实就是data set不在同一distribution下的问题，如果模型在dev和test set 上都有很好的表现和metric，但是在real world中效果并不好，那么我们要做的一定就是改变data set，让dev/test set与real world在同一distribution下 Change metric我们使用ML来解决现实中的问题时，metric也不是一成不变的，需要根据具体的情况做出一些改变，例如，色情图像识别中，我们误讲非色情识别维色情图像，是可以接受的，但是将色情图像识别成非色情图像则是不可接受的，相似的，风控系统中，将风险用户分类为正常用户的错误，比把正常用户分类为风险用户的错误要严重很多，因此，在类似的场景下，我们的metric需要随着业务场景做出一些改变。 正常情况下，我们计算的模型error是：$$Error = \\frac{1}{m_{dev}} \\sum^{m_{dev}}{i=1} \\mathcal { \\hat{y}^{(i)} \\neq y^{(i)} }$$但是在上述场景中，我们需要加入一个权重，来对两种不同错误加以区分$$ w^{(i)}=\\left{\\begin{aligned}1 \\quad x^{(i)}notpron \\10 \\quad x^{(i)} pron \\\\end{aligned}\\right.$$$$Error = \\frac{1}{ \\sum w^{(i)}} \\sum^{m{dev}}_{i=1} w^{(i)} \\mathcal { \\hat{y}^{(i)} \\neq y^{(i)} }$$这样就把两种不同的问题区分开了。 Improve model performance模型performance的提升是模型的核心问题，我们如何确定模型调整的大体方向，Ng给出了如下的图我们默认human-level是很接近理论误差，也就是Bayes error，我们需要比较human-level，training error和dev error这三者之间的关系，human-level和training error之间的差值更大的话，我们就需要去减小bias，反之，我们需要去减少variance，具体的方法，还是我们之前的老套路。 Reference Deep learning-Coursera Andrew Ng Deep learning-网易云课堂 Andrew Ng","link":"/2017/10/12/course-deep-learning-course3-week1/"},{"title":"Learning Notes-Deep Learning, course3, week2","text":"Hi all，course3来到了week2，本周的课程依然主要是关于一些learning strategy，这些方法相当实用。虽然不是什么具体的算法，但都都是Ng在科研和工作中积累下来的宝贵经验，对于实际问题十分有效。 我们一起来看看。 Error analysisCarry out error analysis按照通常的流程，在进行training过程后，我们在dev set会进行模型的测试，如果dev error比training error大很多的话，我们应该去排查问题的症结所在呢？Ng给出了solution 例如在cat recognition中，我们发现错分的sample有很多dog图像，还有很多猫科动物的图像，还有一些是模糊的cat图像。于是我们自然而然的想到三种解决方案： 解决狗错分为猫的问题 解决猫科动物被错分成猫的问题 提升模糊图像被误分的问题 可是由于我们精力和时间都有限，需要找出误分最主要的问题，因此我们要做的，是把所有错分的图像罗列出来，或者随机抽样一定的图像，分析每种错误它有多少，占错分图像多少比例。我们来看截图每一个错分的图像都会进行标签化的统计，最后通过统计每一个标签，找出影响错分最严重的因素，作为我们的改进方向。 Clean up incorrectly labeled data在常见的错误中，错误的label是一种很常见的问题，这种问题往往来自于标注时候，错误的label会对training造成误导。 首先，对于training set，来说，incorrectly labeled data应该怎么处理？首先，Ng告诉了我们一个性质： DL algorithms are quite robust to random errors in the training set DL因为其自身的robust性质，当training set中有少许的，随机产生的incorrectly labeled data时，效果并不会有多差，我们完全不需要去管他。但是，当这incorrectly labeled data很多时就不行了，因为它们带来的是systematic errors，极端的想，如果把所有的白狗都错误的标注成了猫，那么这个cat recognition系统一定不会好，因为它一定会把白色的狗判断成为猫。 再来看看dev/test set中的incorrectly labeled data，对于这个问题，我们要做的是，评估incorrectly labeled data对dev error带来了多少贡献，解决的过程也是类似的，来看截图：我们把incorrectly labeled也作为一个要素或标签，放在错分图像分析的过程中，看看最终的统计结果，再决定incorrectly labeled data是不是影响dev error的主要原因，是否值得我们去fix it up. 最后，关于correcting incorrect dev/test set example，Ng给出了一些建议： Apply same proces to your dev and test sets to make sure thry continue to come from the same distribution. 在修正的过程中，一定要保证dev和test set同时被修正，如果他们不再符合同一distribution，那么会对于后续的评价带来一些问题。 Consider examining examples your algorithm got right as well as ones it got wrong. 我们在更正的时候，不能只是看被错分的图像，对于被正确分类的，也有可能存在incorrect labeled 的情况。 Tran and dev/test data may now come from slightly different distribution 正如刚才讲的，DL对于training有一定程度的robust性，incorrect labeled data可能不会对training set带来这些问题，在这种情况下，我们可以不用去更正training set，这种情况，我们是可以接受的。 Build up quickly and iterate最后Ng用一个speech recognition作为例子，我们首先要分析出可能影响效果的一些因素： Noisy background Accented speech Far from microphone Young children’s speech …针对这些问题，我们该如何构造我们的模型呢，Ng给出了建议 Set up dev/test set and metric Build initial system quickly Use bias/variance analysis &amp; error analysis to prioritize next steps. 总而言之，guideline是 Build your first system quickly, then iterate. Mismatched training and dev/test dataTraining and testing on different distributions之前我们再三强调过一个尖锐的问题，那就是training/dev/test set一定要在同一个distribution下，但是实际上，愿望总是美好的而现实很残酷，我们总是会面对一些training and testing on different distribution问题。 例如在猫识别的任务中，我们需要将这个模型部署到手机app上，我们手上的数据只有10k是从手机拍摄获得的，而有200k的数据是从网络上获得的，这两种图像显然不属于同一distribution，我们应该怎么办？ 首先来看option1，我们将所有的210k数据充分混合在一起，其中205k作为training set，2.5k作为dev，2.5作为test set。这样看起来是一个很不错的方法，但是，确实很不好的一个方法，为什么这么说呢？ 在整个过程中，dev/test set其实扮演了一个非常重要的角色，它决定了我们的target，也就是整体的优化方向。在这个例子中，我们要优化的方向是app上的图像，而这种data set分割方法和我们的task target并不符合，因此并不优秀。 我们再来看option2，我们将200k的来自网络的图片全部放入training set，然后将10k的app数据，5k放入training set，2.5k作为dev，2.5作为test，这样做的话，dev/test决定的target 和我们的task target是一致的，所以长远来看，虽然option2的training/dev set并不是同一distribution，但是从长远看它的效果还是很不错的。 Bias &amp; variance with mismatched data distribution在training/dev/test set符合同一distribution的时候，我们通过比较training error和dev error就可以定性是否存在high variance的问题。但是，当training set和dev set不符合同一distribution的时候，这个判断就显得有些困难了。我们应该怎么处理呢？ 这时候，我们可以从training set中取出一小部分数据，命名为training-dev set，这部分数据将不再进行training，而是作为评判training效果的一个set，此时我们就有了training error，training-dev error和dev error三个error，再结合human error，training error和training-dev error之间的差值可以反映出模型是否有high bias或者variance，这样可以更科学的来评判模型效果。相应的，training-dev error和dev error相差越多，data mismatch的程度越大。 Addressing data mismatch我们如何addressing data mismatch呢，首先我们来看看Ng的两条guideline： Carry out manual error analysis to try to understand difference between training and dev/test sets Make training data more similar; or collect more data similar to dev/test sets 理解一下，首先我们要通过人工的analysis去分析出造成training set和dev set之间distribution不同的原因，比如语音识别中的有无汽车噪声等等；然后我们需要根据这些差别，让training set和dev set更加的相似，甚至相通。 但是要注意的是，我们在这个过程中，要避免出现overfitting的情况出现，例如Ng举出的例子，在识别车内的人声过程中，我们可以通过人工的合成汽车声音与人的声音让training set和dev set更加的相似，但是如果我们的只用一段汽车噪音循环往复的去做合成，例如吧1min的汽车噪声循环的合成到1h的人声中，那结果一定是不尽如人意的，因为出现了overfitting. Transfer learning下面我们一起来看看大名鼎鼎的transfer learning，所谓transfer，就是存在一种从A到B的转换，而且这种情况往往是B的数据量很少，需要通过A来做一个pre-training过程。假设我们有如下的neural networks假设这个我们使用这个neural networks训练了一个image recognition模型，在训练完成后，我们将最后的output，以及output对应的的\\(w\\)和\\(b\\)也删除，更换成例如放射数据再进行训练，如下图：我们不仅仅可以把output层更换成一个新的output层，还可以将output层更换成几个新层。我们甚至可以将transfer之前的训练认为是一种pre-training，但是transfer training需要有几个条件： Task A and B have the same input x. You have a lot more data for Task A than Task B. Low level features from A could be helpful for learning B. Multi-task learning现在假设我们有一个自动驾驶的场景，我们需要从视频中识别行人、车辆、停车标志和红绿灯，按照常理来说，我们可以单独的构建4个模型。但是，这4个模型的特征场景都是很相似的，构建4个模型稍微有一些浪费，于是我们可以把这四个任务合并在一起，这就是Multi-task learning. 在这里我们的标签\\(y\\)，就不再是一个m×1的矩阵了，而是一个m×4的矩阵，对于multi-task来说，在以下情况下是可行的： Training on a set of tasks that could benefit from having shared lower-level features. Usually: Amount of data you have for eachtask is quite similar. Can train a big enough neural network to do well on all the tasks. End to end learningEnd to end learning是随着DL兴起后而产生的一种learning方式，在end2end中，我们不再关注一些中间的步骤，例如feature selection或者image processing，我们只是把原始的数据和最后的结果告诉DL，它就可以自主的完成这个任务。 当然end2end 也是有一些优势和劣势的，我们来看一下：Pros： Let the data speak. Less hand-desgining of components needed. Cons: May need large amount of data. Excludes potentially userful hand-designed components. 总之，对于end2end来说，大数据量，一定是最重要的因素，基于这一点，我们才可以摆脱传统的中间步骤，彻底实现end to end learning. Reference Deep learning-Coursera Andrew Ng Deep learning-网易云课堂 Andrew Ng","link":"/2017/10/18/course-deep-learning-course3-week2/"},{"title":"Learning Notes-Deep Learning, course4, week2","text":"我们继续来看看course4的week2，CNN的知识还是蛮丰富的，本周主要讲了一些经典的CNN结构以及一些computer vision的技巧和知识，一起recap一下。 Classic NetworksNg一共给我们带来了3个最为经典的CNN网络，这里我会给出网络的截图和paper原文，抽空我也会看看原文，希望大家和我一起来看看。 LeNet-5Lécun Y, Bottou L, Bengio Y, et al. Gradient-based learning applied to document recognition[J]. Proceedings of the IEEE, 1998, 86(11):2278-2324. AlexNetKrizhevsky A, Sutskever I, Hinton G E. ImageNet classification with deep convolutional neural networks[J]. Communications of the Acm, 2012, 60(2):2012. VGG-16Simonyan K, Zisserman A. Very Deep Convolutional Networks for Large-Scale Image Recognition[J]. Computer Science, 2014. 以上可以说是最为经典的三个cnn网络了，大家可以通过阅读paper获得一些详细的知识，都是经典之作，推荐阅读。 Residual Networks(ResNets)对于residual networks，我们在这里具体看一下，它的具体原理可以通过下图的residual block来看看：其实，residual block是把\\(a^{[l]}\\)直接作为\\(a^{[l+2]}\\)输入，也就是说：$$a^{[l+2]}=g(z^{[l+2]}+a^{[l]})$$其中\\(g\\)是activation function，如ReLU等。这种思想也被称为short circuit或者skip connection。把上面的residual block串联起来，就变成了我们的residual networks，如下图：Residual networks最大的特点就是，普通networks随着layer增大，training error理论上是会变小，但是实际上会在某个最小点后增大，但是residual networks则会严格的随着layer增多而减小training error，下面是原文：He K, Zhang X, Ren S, et al. Deep Residual Learning for Image Recognition[J]. 2015:770-778. Network in Network and 1×1 Convolutions通常我们使用的filter，都是奇数的kernel matrix，在某些情况下，1×1的filter也会被我们使用，它到底有什么作用呢？我们来看一张图：从这张图中可以看出，1×1的filter可以压缩input的channel(depth)，因此1×1filter还是有一些意思的。下面是原文：Lin M, Chen Q, Yan S. Network In Network[J]. Computer Science, 2013. Inception Network关于inception network，我们先来看一张图：对于同一个input，我们分别采用不同的filter，甚至max pooling，在保证输出的hight和width一样的前提下，将结果堆叠起来，作为我们的输出，这样做的好处是，我们不需要自己挑选filter，我们将所有的可能都交给network，让它来决定去选择什么样子的结构。原文是：Szegedy C, Liu W, Jia Y, et al. Going deeper with convolutions[C]. Computer Vision and Pattern Recognition. IEEE, 2015:1-9.同时，Ng在课程上说明，inception network 中大量使用了1×1filter来降低计算量，这一点值得我们注意。我们来看看Inception 单元的图解：这一周的课程感觉量很大，介绍了很多的网络，我准备下面慢慢的看看这些paper，站在巨人的肩上去看世界，一定会有别样的风景！ Reference Deep learning-Coursera Andrew Ng Deep learning-网易云课堂 Andrew Ng","link":"/2017/11/29/course-deep-learning-course4-week2/"},{"title":"Learning Notes-Deep Learning, course4, week1","text":"Hi, all. 最近开始休假了，可以有空继续自己的学习，一方面补一补前面的作业，一方面继续自己的学习，今天我们来到了course4，也就是convolutional neural networks 的内容。我们一起来看看！ Convolution在课程中，Ng从edge detection的角度来给大家讲了讲convolution，因为本人是image processing出身，所以认为Ng在这里讲的还是很浅显易懂的，我就不再专门的markdown。主要来看看convolution中的一些技巧。 Padding我们都知道，在最纯粹的convolution中，我们假设原image尺寸是\\(n *n\\)，convolution filter尺寸是\\(f *f\\)，那么最终的结果image尺寸应该是\\( (n-f+1) *(n-f+1)\\)，也就是说，结果的尺寸变小了。如果想让输出image的尺寸不发生改变，那么我们就要使用大名鼎鼎的padding了。 Padding其实就是表示，在原始image中，向外扩大多少尺寸，一般我们会使用简单复制相邻元素值的方法进行扩充。假设对于一个\\(6 *6\\)的原始image，采用\\(3 *3\\)的filter，加上\\(p=1\\)的padding，那么原始图像尺寸变成了\\(8 * 8\\)，结果变成了\\(6 *6\\)，原始image和结果image一模一样！于是加入了padding的convolution公式就成了\\( (n+2p-f+1) *(n+2p-f+1)\\). 在这里Ng引入了两个概念，valid和same convolutions，所谓valid convolution，就是没有padding 的convolution；所谓same convolution，就是输入输出的尺寸完全一样。 Stride继padding之后，还有一个很重要的参数，就是步长stride，步长stride决定了filter做convolution时候的步长，如果stride=1，那么filter就会挨着计算，如果stride=2，那么就会跳跃这进行计算。 总结一下，假设原image尺寸是\\(n *n\\)，convolution filter尺寸是\\(f *f\\)，padding值是\\(p\\)，stride值是\\(s\\)那么最终的结果image尺寸应该是\\( ( \\frac {n+2p-f}{s}+1) *( \\frac {n+2p-f}{s}+1)\\)，如果除不尽的话，我们选择向下取整，也就是不足以做convolution的区域，我们选择放弃。 ##Convolution over Volume对于一般的图像处理，我们使用的都是RGB图像，我们都知道，RGB图像有三个channel，这种情况下，convolution应该如何做，我们来看下面的图：假设我们的图像是6×6×3，也就是hight×width×channel(depth)，因此对应的filter也要有3的channel(depth)，最后可以得到一个4×4的结果。 当然，我们可以采用不止一个filter，如图我们加入了两个不同的filter，他们的大小都是3×3×3，于是最终的结果就是4×4×2，请注意：结果的channel数目取决于filter的个数，而和输入的channel没有任何关系。 CNNConvolution Layer下面我们来看看CNN网络中的一个layer的工作原理是什么，首先来看截图：这张图十分复杂，我们一起仔细看看这张图，对于一个6×6×3的RGB图像，我们用了两个3×3×3的filter，我们可以把输入image看做\\(x\\)，也就是\\(a ^{[0]}\\)，filter看做\\(w ^{[1]}\\)，得到的结果就是\\(w ^{[1]}a ^{[0]}\\)，我们再加上一个bias项\\(b^{[1]}\\)，那么就获得了一个liner output\\(w ^{[1]}a ^{[0]}+b^{[1]}\\)，我们再使用一个non-liner function例如ReLU，如此获得一个4×4×2的output。如此就是CNN的一个layer. 如此我们可以看到，CNN和之前的DNN实质上都存在一种liner function到non-liner function的转化，通过non-liner function去classify线性不可分的data，另外，在CNN中，每一个filter就可以获得一个不同的feature，多个filter可以让我们从多个角度去classify data. 另外，相比较于fully connected 的DNN，CNN所需要的parameters也少了很多，这一点值得我们注意。 PoolingPooling原理还是很简单的，我们来看一张图：首先我们来看看max pooling，如图，我们取filter尺寸\\(f=2\\)，stride大小\\(s=2\\)，对于一个filter中的元素，我们取max作为输出；相对应的，如果我们取average，那么就成了average pooling，pooling中的hyperparameter只有filter尺寸\\(f\\)和stride大小\\(s\\)，值得注意的是，pooling过程中不存在学习过程，no parameters to learn! Fully Connected layerFully connected layer在CNN其实很简单，我们只需要将input展开，按照DNN的方法进行fully connected就可以了。 一般情况下，我们认为有prameter变化的才算一层，因此我们不认为pooling是一个layer，我们举个一个最简单的CNN做例子：CONV-POOL-CONV-POOL-FC-FC-Softmax，我们这就是一个简单的5层的CNN，在下周的课程中我们可以看到一些经典的CNN框架，这里就不再复述。 Why Convolutions关于这个问题，Ng给出了两个意见，我们一起看看： Parameter sharing: A feature detector (such as a vertical edge detetor) that’s useful in one part of image is probably useful in another part of the image.Sparsity of connections: In each layer, each output value depends only on a small number of inputs. Reference Deep learning-Coursera Andrew Ng Deep learning-网易云课堂 Andrew Ng","link":"/2017/11/26/course-deep-learning-course4-week1/"},{"title":"从凸函数到梯度下降和牛顿法","text":"记得我在和优男一起研究logistic regression的时候，他问了我几个非常尖锐的问题，让我顿时哑口无言 怎么保证logistic regression通过gradient descent找到的是最优解； 为什么logistic regression可以用newton’s method呢？ Newton’s method中Hessian matrix必须positive definite有什么意义呢，log cost function能保证吗？ 这些细节问题，说实话我也没有认真的想过。在夸奖他之余，我们也一起开始了研究，希望从中学习到一些更深层的东西，趁着现在有个blog分享给大家 凸函数(Convex function)在开始之前，我有一个关于术语的倡议。中文里的“凸函数”，看上去是凹下去的，对应的，中文里的“凹函数”看上去凸起来的，amazing吧？这是有一定历史原因的，感兴趣的朋友可以去查阅下资料，这里我们不再复述。所以为了避免让大家产生误解，我鼓励大家使用英文，convex function和concave function.这样会避免很多不必要的麻烦。 OK，我们来一起看看，convex function 对于一维函数 \\(f(x)\\)来说，在定义域内的任意值 \\(a\\)和\\(b\\)，对于任意的 \\( 0 \\leq \\theta \\leq 1\\)，如果满足以下条件，则称为convex function$$f(\\theta a+(1-\\theta) b) \\leq \\theta f(a) + (1- \\theta)f(b)$$我们再用图片直观的感受一下 显而易见的是，当公式中等号去掉的时候，函数就是strictly convex function. Convex function具有一定的性质，我们简单的描述一下。 First order condition对于 function \\(f\\)，在定义域内一阶可导，且导数为$$ \\nabla f=( \\frac{\\partial f(x)}{x_1}, \\frac{\\partial f(x)}{x_2},…, \\frac{\\partial f(x)}{x_n})$$那么 \\(f\\)是convex function的充要条件是：对于定义域内任意 \\(x\\) 和 \\(y\\)$$ f(y) \\geq f(x) + \\nabla f(x)^T (y - x)$$OK，再来张图片直观感受一下： 其实简单的来讲，就是对于convex function \\(f\\)，它的函数值永远大于等于切线上的值！ Second order condition对于 function \\(f\\)，在定义域内二阶可导，且 \\(n\\) 维方阵Hessian matrix的元素为$$ \\nabla ^2 f(x) = \\frac{ \\partial ^2 f(x)}{ \\partial x_i \\partial x_j}, \\quad i,j = 1,2,…,n$$当且仅当Hessian matrix positive semi-defnite的时候，\\(f\\) 是convex function。以上互为充要条件。这里的证明我不想展开讲，在后面我会给出reference链接。 下面给出一些 \\(\\Bbb R\\) 空间下常见的convex function： 线性函数：\\(f(x) = ax+b\\) 指数函数：\\(f(x)=e^ {ax}\\) 负熵函数： \\(f(x)=xlogx \\quad on \\quad \\Bbb R_{++}\\) 对应的，一些常见的concave function： 线性函数：\\(f(x) = ax+b\\) 对数函数： \\(f(x)=logx \\quad on \\quad \\Bbb R_{++}\\) 其中大家可以看到，线性函数既是convex也是concave函数，比较特殊，这和它本身的first order condition为常数有关。 以上就是convex function的一个简单介绍，你也许会问，为什么花这么多力气来介绍convex function. 其实，在machine learning中，convex function的优化是非常重要的，很多算法说到底，都是要optimize一个convex function，我们会用liner regression和logistic regression为例子，进一步从convex function的简介过渡到gradient descent和newton’s method. 梯度下降法(Gradient descent)关于gradient descent，我们使用liner regression作为例子来讨论。Liner regression算法的实质是least square method，他的cost function是$$J( \\theta)= \\frac{1}{2} \\sum_{i=1} ^m (h_ \\theta (x ^{(i)})-y^{(i)})^2, \\quad h_ \\theta (x)= \\theta ^Tx $$对于liner regression来说，算法的实质就是去求出\\( J( \\theta) \\) 以\\( \\theta\\)为参数的minimum，gradient descent算法的作用就是去实现了这个过程，gradient descent的基础知识详见reference. 那么针对cost function，gradient descent是如何保证收敛的呢，我们一起来看看 对于\\(J( \\theta)\\)，我们将其带入convex function的定义公式中，注意这里我们的自变量是\\( \\theta\\)，我们可以通过推导证明该式成立，也就是说，least square cost function是convex function. 既然有这个结论了，那么我们可以想象一下，least square cost function作为convex function，是存在全局最小值的，也就是说，gradient descent不会出现陷入局部最优无法自拔的现象，只要gradient descent保证参数足够好的情况下，理论上，是完全可以很好的逼近全局最优的解的。 Gradien descent算法本身并不能保证获得全局最小值，只有在objective function是convex function的时候才可以保证 下图可以看出，右边的object function是non-convex function，因而很容易陷入到局部最小值无法自拔，而左边的objective function是一个标准的convex function，在gradient descent参数合理的前提下，可以逼近全局最优。 当然，gradien descent的一些改进方法，例如stochastic gradient descent在解决non-convex optimization上有一些帮助，但是我们在这里不做讨论，后面有时间我会专门再写。 由此，我们可以得出，gradient descent不仅仅是minimize liner regression的一个很好的方法，也是convex optimization的一种理想方法 牛顿法(Newton’s method)Newton’s method 这块内容，我们将会用logistic regression作为例子。同样，我们先来关注下log cost function，这里，我们取label为-1和+1，因为这样得到的cost function比0,1下的计算更加简单$$J( \\omega)= - \\frac{1}{m} \\sum_{i=1} ^{m} log(1+e^{-y^{(i)} \\omega^T x^{(i)}})$$这里我们采用了-1和+1作为标签值，和大多数教材中不一样，大家可以下来自己推导一下\\(J( \\omega)\\)，这种写法广泛的应用在了比较logistic regression和SVM两大分类器的文献中，希望大家熟知。 此处我们对原始的likehood function加上了 \\(- \\frac{1}{m}\\)的系数，同样，当我们把 \\(J( \\omega)\\)带入到convex function的定义中，可以验证上式为convex function，值得注意的是，\\(J( \\omega)\\)是\\( \\omega\\)的函数。 其实，我们也可以将log cost function展开后，利用最基本的函数convex和concave性质来获得上式是convex function的结论，碍于公式实在太难打，就留给大家去证明吧。 OK，既然log cost function是convex function，我们一定是可以用gradient descent去求解的。问题是，如果我们用newton’s method呢？ Newton’s method的基本原理详见reference，这里我们可以发现，既然log cost function是convex function，那么根据second order condition可以知道，它的Hessian matrix一定是positive semi-definite的。如果我们加上了L2 regularizer，由于L2 regularizer本身就是一个strict convex function，那么log cost function就一定是strict convex function了，也就是：$$J( \\omega)= - \\frac{1}{m} \\sum_{i=1} ^{m} log(1+e^{-y^{(i)} \\omega^T x^{(i)}})+ \\frac{1}{2}|| \\omega||^2$$因此，在log cost function中，Hessian matrix是positive definite的，完全满足newton’s method 的要求。同样，类似于上一部分，newton’s method也可以找到log cost function的全局最优。 Sum upOK，我们说到这里也确实讲了不少，这篇blog有些冗长，希望朋友们不要焦虑。总体来说，我想表达的是以下几个观点： Machine learning中我们寻求的其实就是objective function一个全局最优值，这些问题是通过gradient descent等方法解决的； Gradient descent和newton’s method都是convex optimization的好方法，他们都可以对于convex function获得全局最优； 对于non-convex optimization问题，stochastic gradient descent也很有效果，我们后续再慢慢学习。 好了，核心思想就这三点，今天先说这么多！ Reference EE364, Convex Optimization Stanford University Regularized Logistic Regression is Strictly Convex XinyiLI大神的blog Liner regression Logsitc regression Gradient descent Newton’s method","link":"/2017/08/02/ml-convex-opt/"},{"title":"再深入聊聊梯度下降和牛顿法","text":"上次我们一起聊到了gradient descent和newton’s method，而且我们已经知道了gradient descent和newton’s method都是convex optimization的好方法，这次我们就跳出convex optimization，从更大的unconstrained optimization角度来探讨下这两种方法之间的关联和区别。 假设我们现有一个的optimization task，要求objective function \\(f(x)\\)的最小值，我们一般有两种方案： 考虑到\\(f(x)\\)的最小值很有可能是全局最小值，那么我们可以通过寻找\\( \\nabla f(x)=0\\)的点来确定最小值，这就是newton’s method的思想 既然我们要寻找最小值，那我们可以顺着一条\\(f(x)\\)逐渐减小的路径，顺着这条路径一直走下去，直到不再变小，这就是gradient descent的思想 OK，简单的叙述之后，我们开始正题！ 泰勒级数(Taylor series)首先我们需要回忆一下高等数学中重要的Taylor series，如果\\( f(x)\\)在点\\( x_0\\)的领域内具有\\(n+1\\)阶导数，那么，在该领域内，\\( f(x)\\)可展开成\\(n\\)阶Taylor series，忽略无限大次项的形式就是$$f(x)=f(x_0)+ \\nabla f(x_0)(x-x_0) + \\frac{ \\nabla ^2 f(x_0)}{2!}(x-x_0)^2 +…+\\frac{ \\nabla ^n f(x_0)}{n!}(x-x_0)^n $$其实在高等数学中学到Taylor series的时候，我本人是十分无感的，我并不知道这个东西到底有什么用处，相信很多人和我有相似的经历。 In mathematics, a Taylor series is a representation of a function as an infinite sum of terms that are calculated from the values of the function’s derivatives at a single point. 事实上，Taylor series所表现的是，对于\\( f(x)\\)在点\\( x_0\\)附近的一个估计，也可以理解为，根据\\( x_0\\)点处的各阶derivatives之和构成一个新的function，这个function就是对\\(f(x)\\)的逼近和拟合，而且这种逼近和拟合，随着Taylor series阶数增加而更接近于真实的\\(f(x)\\)。如果我们使用0阶Taylor series来逼近的话，那我们就粗暴的认为，\\( f(x)\\)在点\\( x_0\\)附近的值就都是\\(x_0\\)，这当然太粗暴直接了，哈哈。 既然这太粗暴了，那么我们就用1st order Taylor series来做一个逼近和估计，这就是gradient descent的思想；如果我们用2nd order Taylor series来估计呢，那就成了newton’s method了 OK，我们继续娓娓道来。 1st order Taylor series &amp; gradient descent假设\\(x_k\\)是第k次gradient descent迭代后的\\(x\\)取值，那我们在此处的1st order Taylor series 就是$$f(x)=f(x_k)+ \\nabla f(x_k)(x-x_k)$$其中\\(x\\)是迭代的下一个方向，gradient descent的目标就是让\\(f(x)\\)达到局部甚至全局最小值，那么每一次迭代，也需要尽可能的减小更多以达到这个目的，那么$$f(x_k)-f(x)=- \\nabla f(x_k)(x-x_k)$$显然，上式应该尽可能的大，即\\(- \\nabla f(x_k)(x-x_k)\\)越大越好，我们现在把\\((x-x_k)\\)做一个替换，用单位向量\\(\\vec g\\)和标量\\( \\alpha\\)分别代表方向和大小，现在的任务就变成了$$ \\min \\nabla f(x_k)(x-x_k) = \\min( \\alpha \\vec{\\nabla f(x_k)}⋅ \\vec g)$$我们都知道，对于两个向量来说，当他们方向相反时，他们的内积是最小的。 梯度方向的定义是该点梯度在标量场增长最快的方向 因此当\\(\\vec g\\)的方向是\\( \\vec{\\nabla f(x_k)}\\)的反方向时，上式可以取到最小值，于是就有$$x-x_k=- \\alpha \\nabla f(x_k)$$$$x:=x_k- \\alpha \\nabla f(x_k)$$到这一步，是不是看到了熟悉的gradient descent呢，yeah mate！We make it! 2nd order Taylor series &amp; newton’s method和上面的gradient descent相似，假设\\(x_k\\)是第\\(k\\)次newton’s method迭代后的\\(x\\)取值，那我们在此处的2nd order Taylor series 是$$f(x)=f(x_k)+ \\nabla f(x_k)(x-x_k) + \\frac{1}{2}(x-x_k)^T \\nabla^2 f(x_k)(x-x_k) $$我们对等号两边同时对\\(x\\)求导，并令其为零$$ \\nabla f(x) = \\nabla f(x_k) + (x-x_k) \\nabla^2 f(x_k)=0$$由于newton’s method的原理就是通过\\(\\nabla f(x)=0\\)来寻找最小值，故上式为零的解\\(x\\)其实就是newton’s method在\\(k+1\\)次迭代后的新的\\(x\\)值。其中\\(\\nabla f(x_k)\\)是\\(x_k\\)处的一阶导数，\\( \\nabla^2 f(x_k)\\)是\\(x_k\\)处的二阶导数Hessian矩阵元素 我们令\\(\\nabla f(x_k)=g\\)，\\(\\nabla^2 f(x_k)=H\\)，则上式变成$$g+H(x-x_k)=0$$进一步的$$x=x_k-H^{-1}g$$由于\\(-g H^{-1} \\) 是优化的前进方向，在寻找最小值的过程中，这个方向一定是和梯度方向\\(g\\)相反才可以更快的下降，那么就有\\( g^T H^{-1} g &gt; 0\\)，这不就是positive definite的定义吗？也就是说，Hessian矩阵是positive definite的。 想象一下，如果Hessian是negative definite的话，参数更新的方向就成了和\\(g\\)相同的方向，newton’s method将会发散，这一点，也是newton’s method的缺点。在objective function是non-convex function的情况下，如果第\\(k\\)次迭代获得的\\(x_k\\)处的Hessian matrix negative definite，那么newton’s method将会发散，从而导致不收敛。当然，为了解决这种问题，后续有改进的BFGS等方法，我们在这里暂时不详细讨论。 Sum up下面我们再来总结性质的对比一下两种方法，来看一张图事实上，这两种方法都采用了一种逼近和拟合的思想。假设现在处于迭代\\(k\\)次之后的\\(x_k\\)点，对于objective function，我们用\\(x_k\\)点的Taylor series \\(f(x)\\)来逼近和拟合，当然了，上图我们看到，gradient descent是用一次function而newton’s method采用的是二次function，这是二者之间最显著的区别。 对于new’s method，在拟合之后，我们通过\\( \\nabla f(x)=0\\)求得的\\(x _{k+1}\\)点作为此次迭代的结果，下次迭代时候，又在\\(x _{k+1}\\)处次进行二次function的拟合，并如此迭代下去。 Newton’s method采用二次function来拟合，我们可以感性的理解为，newton’s method在寻找下降的方向时候，关注的不仅仅是此处objective function value是不是减小(一阶value)，还关注此处value下降的趋势如何(二阶value)，而gradient descent只关心此处function value是不是减小，因此newton’s method可以迭代更少次数获得最优解。对于标准二次型的objective function，newton’s method甚至可以一次迭代就找到全局最小值。 但是值得注意的是，上面所说的标准二次型function，实质上是convex function，在一般的unconstrained optimization中，更多的情况则是non-convex optimization，对于一般的non-convex optimization，newton’s method是相对不稳定的，因为我们很难保证Hessian matrix的positive definite。鉴于此，我们会加入步长\\(\\lambda\\)限制，防止其一次迭代过大而带来迭代后Hessian matrix negative definite的情况，即$$x:=x- \\lambda H^{-1} g$$对于这种思想，我个人认为，是在整体non-convex function中寻找一个局部的convex function，通过步长将newton’s method限制在这个局部中，最后收敛到局部最优中。由此可见，newton’s mtehod在non-convex中受限制比较大。 相比之下，由于gradient descent采用的一次function做拟合，只需要考虑沿着梯度反方向寻找最小值，因此gradient descent适用于各种场景，甚至是non-convex optimization，虽然不能保证是全局最优，但至少gradient descent是可以值得一试的方法。 下面来总结一下： Gradient descent 和 newton’s method都是利用Taylor series对objective function进行拟合来实现迭代的； Gradient descent 采用一次型function拟合而 newton’s method采用的是二次型function，因此newton’s method迭代更迅速； Newton’s method每次迭代都会计算Hessian matrix的逆，在高维feature情况下，这使得每次迭代会比较慢； Newton’s method在non-convex optimization中很受限制，而gradient descent则不受影响。 好了，先写这么多，这其中的知识量还是很深奥的，也不知道自己有没有叙述明白，欢迎大家一起来讨论！ 最后感谢优男的宝贵意见！ Reference UCLA courseware CCU courseware Taylor series","link":"/2017/08/11/ml-gd-and-nm/"},{"title":"深入聊聊正则化","text":"最近和优男一起聊到了L1和L2 regularization，期间遇到了很多没有想明白的问题，加上最近工作有些忙，空余时间用来倒腾新到货的小米路由器，只能趁周末自己研究研究，下面和大家分享一下regularization中一些深入的问题。不讨论基础知识，直接上干货。 MAP and regularization我们都知道，当cost function在没有加regularization的时候，我们对参数使用的是MLE(Maximum likelihood estimation)，对应频率学派所认为的参数本无分布规律的观点；在Andrew Ng经典的CS229中，这位AI大师曾经提到，regularization其实是对参数的MAP(Maximum a posteriori estimation)，是基于贝叶斯学派认为的参数本有priori distribution，同时吸纳了MLE的一种中间观点。 这里的priori distribution，就是根据经验，认为参数应该大致符合某个distribution，这样，最终获得的参数估计结果也会和这个被认为的distribution有一些相近 而我们所熟知的L1 regularization，其实就是认为参数的priori distribution是Laplacian distribution，而L2 regularization，则认为参数的priorit distribution是Gaussian distribution，相信大家对Gaussian distribution是很熟悉的，而对于Laplacian distribution，它的分布是$$p(x;a)= \\frac{a}{2} e^{-a|x|}$$下图就是两者的一个比较：在后文中，我们会看到，两种distribution的特点决定了两种regularization的性质。 Lasso regression在liner regression中，我们假设参数\\( \\theta\\)服从Laplacian distribution，cost function就成了$$J( \\theta) = \\frac{1}{2} \\sum^{m}{i=1} (y^{(i)}- \\theta^T x^{(i)})+ \\lambda \\sum^{d}{j=1}| \\theta^{(i)}|$$上式就是Lasso regression Ridge regression在liner regression中，我们假设参数\\( \\theta\\)服从Gaussian distribution，cost function就成了$$J( \\theta) = \\frac{1}{2} \\sum^{m}{i=1} (y^{(i)}- \\theta^T x^{(i)})+ \\lambda \\sum^{d}{j=1} ( \\theta^{(i)})^2$$上式就是Ridge regression或shrinkage geometry of error surfaces在不考虑参数priori distribution的时候，cost function的形式是$$J( \\theta) = \\frac{1}{2} \\sum^{m}_{i=1} (y^{(i)}- \\theta^T x^{(i)})^2$$用二维截面图展示就是图中只有objective function，横纵轴是参数\\( \\theta\\)，截取过来的图，所以上面的参数是\\(w\\)，\\(l\\)是loss值，箭头指向的点就是cost function的极小点。在不考虑参数priori distribution的时候，这个点就是我们的optimization target. 下面大家来我一起做一个头脑风暴，所谓参数的priori distribution，其实就是用来限制最后optimization结果的一个限定，那么我们其实就是在做一个受限制的的convex optimization，即：$$ \\theta=argmin \\frac{1}{2} \\sum^{m}{i=1} (y^{(i)}- \\theta^T x^{(i)})^2$$$$ s.t. \\sum^{d}{j=1}| \\theta^{(i)}|^p \\geq \\beta$$其中，\\( \\beta\\)是ridge或者lasso的最小值。那么此时的图就变成了：我们从图中可以看到，在加入了限制后，最终的optimization不是落在极小值点，而是落在图中所示的位置。从另一个角度来想，regularization item的加入，使得整个cost function在寻找最小值的时候，要均衡的考虑objective function和regularization item的大小. 在这个地方，我和优男讨论的时候有一个地方没有想通，例如使用gradient descent进行optimzation的时候，怎么保证优化可以落到图中的点呢，我是这么考虑的：当加入regularization后，cost function本身就有了变化，随之而来的是gradient也发生了变化，在gradient descent迭代过程中就已经把regularization的影响带了进去，因此在每一次迭代的时候，实际上应该都是按照上式的限制进行优化的。 当然，上图也可以用来就是为什么lasso可以获得稀疏特征，那就是因为lasso更可能在坐标轴上和objective function产生交点，进而使得一些特征变成0. Reference CS 195-5: Machine Learning STAT 897D","link":"/2017/08/27/ml-ridge-lasso/"},{"title":"Hello World","text":"我终于把blog搭建起来了! 这是一个属于Asir 自己的博客,在这里我会写一些技术分享,记录自己平时学到的东西,也会整点吐槽或者鸡汤.总之,有了一个真正的属于自己的天地,可以随便整,这种感觉非常棒. 其实自己在博客园也尝试过一次,可是体验并不是很理想,在这里我并没有抨击的意思,因为自己搭建起来的成就感那不是一点两点.希望后面可以趁热打铁,开启blog之旅. 在这里感谢下亲铁圈羊为我提供的完美设备和深夜技术支持,非常棒! 最后,作为一个coding man, 在所有事情的最开始,都不应该缺少这句话 Hello world!!!","link":"/2017/07/26/other-hello/"},{"title":"Imbalanced data 问题总结方法汇总","text":"Hello，大家好，双十一真的很累，一直在加班，忙里偷闲看了A systematic study of the class imbalance problem in convolutional neural networks，感觉paper呈现的研究内容感觉很一般，但是，paper中关于imbalanced data的solution方法倒是写的很不错，也勾起了我对于这一块总结的欲望。之前也写过一篇关于imbalanced data的paper notes，但是对于这一块的具体方法总结还不是很足够，于是用这篇paper为主线好好sum up一计。 我们来一起看看。 Data level methods首先我们来看一看data level methods，这类方法有一个共性，那就是通过改变data的数量来完成对imbalanced data problem的解决。 OversamplingOversampling可以说是最直观的solution之一，它的核心思想是，对于较少一类别的samples，过此重复采样，以此让两种类别的样本接近平衡。但是，对于一个sample多次重复训练，很有可能带来overfitting，因此，简单粗暴的重复采样并不可取。因此，很多改进的版本应运而生： SMOTESMOTE算法是一种经典的oversampling方法，它的主要思想是对较少数目类别的样本，随机抽取\\(m\\)个样本，对于随机抽取出的样本，每个样本选取距离最近的\\(n\\)个样本，在他们的连线上随机选取一个点，作为较少类别的补充样本。假设原样本点为\\(x\\)，被选中的附近的点为\\(x’\\)，则新的样本点为：$$x_{new}= x + rand(0,1) \\cdot |x-x’|$$ 通过这种方式，SMOTE可以对较少类别样本进行扩充，进而实现oversampling，平衡数据分布。 Cluster-base oversamplingCluster-based方法的最大特点莫过于最开始对数据进行一个聚类分析，数据会变成数个cluster，然后对于每一个cluster在进行数据的oversampling，同时兼顾类别之间的between-class imbalance，还要考虑到类内部各个cluster的within-class imbalance. Its idea is to consider not only the between class imbalance (the imbalance occurring between the two classes) but also the with in class imbalance (the imbalance occurring between the sub clusters of each class) and to oversample the data set by rectifying these two types of imbalances simultaneously. 原paper大致叙述了整个流程，首先我们对imbalanced data进行k-means(或者其他算法也可以)聚类，聚成多个cluster之后，我们开始进行oversampling，假设majority class有\\(m\\)个cluster，minority有\\(n\\)个cluster，我们以cluster最大的data数目\\(k\\)为标准，我们先对majority class中所有cluster，都进行oversampling，使得他们的数目都达到\\(k\\)，随后，对于minority中每个cluster进行oversampling，使得每一个cluster数目变成\\(m * k /n\\)，最终实现between-class balance和within-class balance. Undersampling与oversampling相对应的则是undersampling，undersampling的核心思想是对于较多类别的samples抽样，使得两个类别数据趋于相近。但是，随机抽样获得会使得类别丧失很多的信息，甚至导致数据分布发生改变。 One-sided selectionone-sided selection的主要思想是，为了保证数据整体的分布，我们优先去除靠近边界的样本，这样可以保证较多分类的数据分布。 Classifier level methods下面我们来看看通过改变classifier level来解决imbalanced data的方法，这类方法侧重于分类器本身的一些性质而并非两类数据的个数。 Thresholding我在之前的博客中聊到过，imbalanced data的分类平面会倾向于较少数据的分类一侧，所以我们可以通过改变类别预测的probabilty的threshold来修正分类平面。常用的方法就是加入关于类别数目的prior probability：$$y_i(x)=p(i|x)= \\frac{p(i) \\cdot p(x|i)}{p(x)}$$ Cost sensitive learningThresholding方法其实对已经train好的模型的采取的一种方式。相应的，我们在模型训练的时候就来消除imbalanced data的一些影响，如何做到呢？答案就是cost function. 我们可以通过调整learning rate，加强对cost比较大的samples，并且最终的优化目标从标准的cost function变成misclassification cost，如此就可以解决imbalanced data的问题了 One-class classification该方法可以说是换了一种思维看问题，我们不再将classification作为我们的task，而是变成了对于一种异常检测的问题。我们只是着眼于较多samples的类别，认为另一类别的samples是一种异常值。 当然，这种方法适合那种极端的imbalanced data，对于一般的情况并不一定很适用。 RecommendationProjection:Imbalanced-learn Reference Buda, Mateusz, Atsuto Maki, and Maciej A. Mazurowski. “A systematic study of the class imbalance problem in convolutional neural networks.” arXiv preprint arXiv:1710.05381 (2017). Chawla, Nitesh V., et al. “SMOTE: synthetic minority over-sampling technique.” Journal of artificial intelligence research 16 (2002): 321-357. Jo, Taeho, and Nathalie Japkowicz. “Class imbalances versus small disjuncts.” ACM Sigkdd Explorations Newsletter 6.1 (2004): 40-49. Richard, Michael D., and Richard P. Lippmann. “Neural network classifiers estimate Bayesian a posteriori probabilities.” Neural computation 3.4 (1991): 461-483.","link":"/2017/11/11/ml-imbalanced-data-solution/"},{"title":"Reading Notes-Practical lessons from predicting clicks on ads at facebook","text":"OK，今天我们来review一篇经典的paper，这篇paper是3年前facebook的研究成果，关于gbt和lr的结合，这个搭配对于近几年的CTR预测以及推荐系统的发展都产生了深远的影响。虽然已经很难被称为一篇新paper了，但是还是值得我们去看看。 我们一起简单看看这篇paper的核心point. Notes传统CTR预测中，logistic regression一直有着很好的效果，lr不仅可以线性分类，同时也可以给出样本属于该类别的posterior probability 但是传统的lr也有着本身的缺憾，lr本身就是liner分类器，对于线性不可分的features效果不是很理想。同时在对于连续feature离散化的时候，效果很大程度依赖于离散分桶的人为经验。 该paper提出了一种依靠gbt进行feature transform的方法，不多说废话，我们直接上图这就是这篇paper最最最核心的部分了。 Input features are transformed by means of boosted decision trees.The output of each individual tree is treated as a categorical input feature to a sparse linear classifier. Boosted decision trees prove to be very powerful feature transforms. 从图中我们可以看到，原始feature被gbt进行了transform，样本落入到哪个tree node，则该位置1，其他位置0，随后再进入线性分类器lr中进行最后的分类。 假设有一个sample，在图中所示的模型中，gbt有两棵树，从左到右是tree1和tree2，tree1中sample被分到了第一个tree node，tree2中sample被分到了第二个tree node，那么最终transform得到的new sample就变成了(1,0,0,0,1) 通过gbt的transform后，feature不仅从非线性转换成了线性（类似于SVM的kernel效果），而且feature被完全的离散成了0-1稀疏feature，无论从线性可分还是特征稀疏的角度上，都变得比原始feature更加理想！ 因为是一篇相对老一些的paper，所以我叙述的比较简单，大家可以get到gbt+lr这个模型的基本原理就可以了。我自己在私下也用python写了一个简单的demo，感兴趣的朋友可以看看，欢迎提出意见，欢迎folk！ Reference He, Xinran, et al. “Practical lessons from predicting clicks on ads at facebook.” Proceedings of the Eighth International Workshop on Data Mining for Online Advertising. ACM, 2014.","link":"/2017/08/23/paper-facebook/"},{"title":"Reading Notes-Class Imbalance, Redux","text":"再次感谢优男，向我提出了又一个尖锐的问题，使得我有机会思考和研究，并且最终可以看到这篇paper，并且最后可以分享给大家。 我个人在工作之中遇到过imbalanced data的问题，我只是直观的感受到，imbalanced data的最后效果往往不是很棒，网上也只是给出了oversampling和undersampling的建议，并没有提及这其中的一些缘故，今天我们一起通过这篇paper来学习学习。 Notes我们假设有positive和negative两类sample，其中positive samples符合\\(P(x)\\)的Guassian分布，negative samples符合\\(G(x)\\)的Guassian分布，分类平面将空间划分成positive region\\(\\cal R^{+} _{w}\\)和negative region\\(\\cal R^{-} _{w}\\)，如下图所示：图中\\(w^{ *}\\)是理想的分割平面，\\(w^{ *}\\) 应该是使loss最小的取值，即$$w^{}= \\arg\\underset{w}{\\min} \\cal L^{}(w)$$对于loss值，其实就是分类中被错分的fn(false negative)和fp(false positive)的期望值，显然，通过minimun该loss得到的会是图中的\\(w^{}\\)，因为这个分类平面所带来的error明显是最少的。$$\\cal L^{}(w) = \\cal C_{fn} \\int _{\\cal R^{w} {-}} \\it P(x)dx + \\cal C{fp} \\int _{\\cal R^{w} {+}} \\it G(x)dx$$对于整个数据集\\(\\cal D \\)来说，我们假设数据量较少的一类(paper中设定positive类较少)所占比例为\\(\\pi\\)(小于0.5)，那么对于带有比例\\(\\pi\\)的数据集\\(\\cal D{\\pi}\\)，全局期望是$$\\bf E_{\\cal D_{ \\pi}} [\\cal L(w)]=\\pi \\cal C_{fn} \\int _{\\cal R^{w} {-}} \\it P(x)dx + (1- \\pi) \\cal C{fp} \\int _{\\cal R^{w} _{+}} \\it G(x)dx$$此处，我个人的理解是，在两类数据均衡的情况下，全局情况下的期望其实是和上面的loss等价的，但是imbalanced data带来了不均衡的因子\\(\\pi\\)，因此，两个公式不再等价。 OK，既然不等价，那么问题就来了，paper上说，通过最小化全局期望获得的\\(\\hat w\\)，是向着较少数量类别的样本倾斜，也就是第一幅图中，向较少的postive那边skewed，原因是因为\\( \\cal R _{+} ^{ \\hat w} &lt; \\cal R _{+} ^{w^{*}}\\), 也就是说，\\(\\hat w\\)分割的positive region面积小于\\(w^{*}\\)分割出的面积，面积的减小势必导致分割平面向positive类别方向偏移。 遗憾的是，关于面积的证明我实在看不明白，也email了一些人，也没有得到一个满意的答案，如果有朋友看明白了的话，记得留言或者email我！ 到了这里，paper大概介绍了undersampling的裨益，undersampling的核心其实就是消除前面提到的比例\\(\\pi\\)，让它趋近于0.5后，分类平面\\(\\hat w\\)就会趋近于理想分类平面\\(w^{*}\\)。 这里，作者提出了一个bagging方法，就是多次做undersampling，最后最结果做bagging可以获得更好的效果，如下图paper还对比了其他的方法，比如Weighted Empirical Cost Minimization(如weighted SVM)和SMOTE方法效果不如bagging undersampling，我上一幅图说明下SMOTE的缺点，更多细节，大家可以详细看看paper，如图：SMOTE方法是随机选择方向生成新的sample，但是如果新的sample产生了图中位置，则效果不会很好。 OK，今天就这么多，记得看明白了中间的推导一起分享啊！ Reference Wallace, Byron C., et al. “Class imbalance, redux.” Data Mining (ICDM), 2011 IEEE 11th International Conference on. IEEE, 2011. PPT-Class Imbalance, Redux","link":"/2017/09/10/paper-imbalance/"},{"title":"Reading Notes-Swish：A Self-gated Activation Function","text":"Hi all，今天和大家分享一篇比较新的paper，是关于一种新的activation function，关于我们知道的activation function，有sigmoid，tanh，ReLU以及ReLU的一些变种，那我们今天来看看这种新提出的activation function到底有什么特色。 Notes首先定义，swish activation function \\(f(x)=x \\cdot \\sigma (x)\\)，其中\\(\\sigma(x)\\)是sigmoid function，也就是\\( \\sigma(x)=1/(1+ e^{-x})\\).Swish functin的图像如图所示：我们再来看下swish function的1st and 2nd derivatives，下面我们一起来集中看看swish function的优点都有什么，作者给出了以下几点：函数值没有上限，函数值有下限，函数不单调，函数光滑连续，我们一起看看： Unbounded aboveUnbounded above的实质，是防止activation function在bounded value处发生saturation. bounded above 带来的问题，就是越接近bounded value的时候，function gradient就会越小，逐渐接近0，这就导致gradient descent异常缓慢甚至无法converge。例如sigmoid 和tanh function，他们都是bounded below and above，当我们采用这两种activation function的时候，我们必须谨慎小心的让初始值尽量在function的接近liner的部分来避免上面问题的产生，因此，unbounded above是一个很好的优点，例如ReLU及其变种都采用了这一原则。 Bounded below &amp; non-monotonicityBounded below其实也是一种很好的方法，并且也有activation function已经采用了，采用该方法后，所有负数input都会得到相差无几的activation value，也就是说，-1000和-1的值几乎没有区别，按照author的话来讲，就是我们将 make large negative input “fogotten” 这其实也是regularzation的一种思想，这种方法在ReLU等方法中也有体现，但是，swish可以通过自身的非单调性质，将比较小的negative input仍然以negative value输出，non-monotonicity提供了更好的gradient flow. Smothness关于smoothness的优点，我们来看一张图： 总而言之，个人感觉swish应该算是一个不错的activation，本人由于时间原因，还没有来得及自己测试它，但是据我所看到的讨论，swish的实际效果貌似不是十分稳定，所以我们可以持保留意见，进一步观察它的表现。 Reference Ramachandran P, Zoph B, Le Q V. Swish：a Self-Gated Activation Function[J]. arXiv preprint arXiv:1710.05941, 2017.","link":"/2017/10/22/paper-swish/"},{"title":"Catalyst Optimization in Spark SQL","text":"Spark SQL is one of the most important components of Apache Spark and has become the fundation of Structure Streaming, ML Pipeline, GraphFrames and so on since Spark 2.0. Also, Spark SQL provides SQL queries and DataFrame/Dataset API, both of which are optimized by Catalyst. Let’s talk about Catalyst today.Catalyst Optimizer is a great optimization framework to improve Spark SQL performance, especially query capabilities of Spark SQL and it is written by functional programming construct in Scala. RBO(Rule Based Optimization) and CBO(Cost Based Optimization) are the optimization principles of Catalyst. The concept of Catalyst is familiar to many other query optimizers, so understanding Catalyst may help you learn the working pipeline of other query optimizers. Trees And RulesWe will have a quick review of trees and rules. You can learn more about them by the references. TreesThe tree is the basic datatype in Catalyst, which is consisted of node objects. Each node has a node type, and some children, which could be none. Let’s have an example, the tree for expression x+(1+2) could be translated in Scala as: 1Add(Attribute(x),Add(Literal(1),Literal(2))) Actually, most of the SQL query optimization, like Catalyst, would transform the SQL to a huge tree structure as the first step of the optimization. Tree structure could be modified, tailored and optimized easily, also it represents the query plan briefly. What’s more, the data can be thrown to every node of the tree by the query plan iteratively. That’s why tree datatype is used and introduced firstly in Catalyst. RulesTrees can be manipulated by the rules. The optimization can be treated as some transformations from one tree to another under some rules we provide. With the help of rules, we can run arbitrary code and many other operations on the input tree. And the Catalyst will know which part of the tree that rules could match or apply, and will automatically skip over the tree that does not match. The rules are all applied by Scala case class, in other words, one rule can match multiple patterns. Let’s see an example.12345tree.transform &#123; case Add(Literal(c1),Literal(c2)) =&gt; Literal(c1+c2) case Add(left, Literal(0)) =&gt; left case Add(Literal(0), right) =&gt; right&#125; CatalystFrom this section, we will know how Catalyst optimizes Spark SQL, which might improve your understanding of how Spark SQL works and how to manipulate it better. The figure below presents the workflow of Spark SQL, and the Analysis, Logical Optimization and Physical Planning are all working under the Catalyst, and we will go over one by one. ParserThe first stage in the figure above is called Parser. Despite not a part of Catalyst, Parser is also very crucial in the whole optimization. Spark SQL transform SQL queries to an AST(Abstract Syntax Tree), also called Unresolved Logical Plan in Catalyst, by ANTLR, which is also used in the Hive, presto and so on. However, DataFrame/Dataset object can be transformed to Unresolved Logical Plan by the API. AnalysisReturned by the Parser, Unresolved Logical Plan contains many unresolved attribute reference of relation. For example, you have no idea whether the column name provided is correct or type of the column, either. Spark SQL use Catalyst and Catalog object that tracks the data all the time to resolve the attributes. Looking up relations by name from Catalog, mapping all the named attributes to the input, checking the attributes which match to the same value and giving the unique ID and pushing the type information of the attributes, the Unresolved Logical Plan is transformed to Logical Plan by Catalyst. Logcial OptimizationLogical Optimization is mainly based on the rules, which is also called RBO(Rules Based Optimization). Lots of rules are provided in this step to accomplish RBO, including constant folding, predicate pushdown, project pruning and so on. As results, the Logical Plan is returned by Logical Optimization from Unresolved Logical Plan. Some figures below describe these ROB mentioned.Predicate pushdown can reduce the computation of join operation by filtering unnecessary data before join. Constant folding avoids calculating the same operation between constants for each record. Column Pruning makes Spark SQL only load data which would be used in the table. Physical PlanningSince we get the Logical Plan, Spark still doesn’t know how to execute the Logical Plan. Transformed from Logical Plan by Physical Planning, Physical Plan would guide the Spark on how to handle the data. In Physical Planning, several Physical Plans are generated from Logical Plan, using physical operator matches the Spark execution engine. Using CBO(Cost Based Optimization), Catalyst will select the best performance one as the Physical Plan. However, the CBO is now only used for join algorithms selection. Also, RBO is used in Physical Planning to pipelining projections or filters into single Spark map() transformation. Code GenerationGetting the Physical Plan from the Physical Planning stage, Spark will translate the AST into Java bytecode to run on each machine. Thanks to quasiquotes, a greate feature of Scala, ASTs would be built by Scala language based on the Physical Plan. And the AST will be send to Scala Compile at runtime to generateava bytecode. We use Catalyst to transform a tree representing an expression in SQL to an AST for Scala code to evaluate that expression, and then compile and run the generated code. In Spark 2.x, the WholeStageCodeGen has been used in code generation stage, you can read more from here. At last, if you want to see the Logical Plan and Physical Plan of your own Spark SQL, you may do as follows:1234// for Logical Planspark.sql(\"your SQL\").queryExecution// for Physical Planspark.sql(\"your SQL\").explain References Deep Dive into Spark SQL’s Catalyst Optimizer Spark SQL Optimization – Understanding the Catalyst Optimizer Catalyst Source Code Quasiquotes Introduction","link":"/2018/09/25/spark-catalyst-optimization/"},{"title":"From Spark RDD to DataFrame/Dataset","text":"This article presents the relationship between Spark RDD, DataFrame and Dataset, and talks about both the advantages and disadvantages of them. RDD is the fundamental API since the inception of Spark and DataFrame/Dataset API is also pretty popular since Spark 2.0. What’s the differences between them and how to decide which API to be imported, let’s have a quick look. RDDRDD (aka Resilient Distributed Dataset) is the most fundamental API, it’s so critical that all the computation in Spark are based on it. The RDD is distributed, immutable, type-safed, unstructured, partitioned and low-leveled API, which offers transformations and actions. Let’s have a quick review to the attributes of RDD. RDD has some awesome characteristics which make it the foundation of the Apache Spark. Let’s take a look and learn about the details one by one. Distributed data abstractionThe first attribute of RDD is the logical distributed abstraction, which makes RDD can run over the entire clusters. The RDD can be set across the storage and divided into many partitions so that the lambda function or any computation function you provide will execute on each partition separately. That’s really awesome as RDD is the logical distributed abstraction, the computation will be much faster as data is divided and run in parallel on several executors. Resilient and immutableRDD is also resilient and immutable. When an RDD is transformed to the next RDD, and then to the third one, and all the RDDs get recorded as a lineage of where they come from. The lineage can also be recorded as the acyclic graph of the RDDs, from which we can recreate any RDD in it when something goes wrong, and that’s why RDD is resilient.As for the immutability, when you make a transformation, the original RDD remains unaltered so that you can go back through the acyclic graph and recreate it at any time and point during the execution. Remember, the transformation operation creates a new RDD from the previous one instead of altering the previous one. Compile-time type-safeRDD is compiled type-safe and you can name the RDD with particular type briefly. Spark application could be complicated and debugging in the distributed environment could be cumbersome, the compiled type-safe really save time as it could find the type error in the compile time. Unstructured/Structured dataThe fourth attribute is that data can be unstructured, streaming data from the media for instance, or semi-structured, log files with some particular date, time or url information for example. Since RDD would not care about the structure or schema of the data, it’s good for those data without structures. Also, RDD can manipulate structured data, though it doesn’t understand the different kinds of types and all depends on how you parse the data. Lazy evaluationLazy evaluation in Apache Spark means the execution will not start until an action is triggered. In another word, the RDD will not be loaded and computed until it is necessary. And there are some benefits of lazy evaluation. Lazy evaluation really reduces the time and space complexities and speeds up the whole execution. DataFrame/DatasetDataFrame/Dataset, unlike RDD, in high-level API dealing with structured data. Data is organized into named columns in DataFrame and can be manipulated type-safely. In Scala, DataFrame is just an alias for Dataset[Row], and in Java, there is only Dataset API, as for Python and R, there is only DataFrame API provided since Python and R have no compile-time type-safety.There are several benefits of DataFrame/Dataset API. I just want to talk about two of them, which I think are pretty awesome. Static-typing and runtime type-safetyDataFrame/Dataset presents static-typing and runtime type-safety. You may have a spelling error when you are typing a SQL such as typing form rather than from, and you would not find the syntax errors until the runtime, however, you will catch these errors at compile time in the DataFrame/Dataset API. Also, as for some analysis errors, the column you queried is not in the schema for example, you can catch these errors when compiling in Dataset while until running in SQL and DataFrame. Nice performanceDataFrame/Dataset API can make the execution more intelligent and efficient. You are telling Spark how-to-do a operation when using RDD, while what-to-do using DataFrame/Dataset. Let’s have a look at the example.12345rdd.filter&#123;case(project, page, numRequests) =&gt; project=='en'&#125;. map&#123;case(_,page,numRequests) =&gt; (page, numRequests)&#125;. reduceByKey(_+_). filter&#123;case(page,_) =&gt; !isSpecialPage(page)&#125;. take(100).foreach &#123;case (project, requests) =&gt; println(s\"projec:$requests\"\")&#125; The code above can be run perfectly without any bug. But think about it, the RDD execute a filter followed by reduceByKey transformation, which means we filter some data after shuffling the entire data. That’s really a waste because shuffle operation is very expensive. However, this problem will be solved in DataFrame/Dataset. Based on the Catalyst, DataFrame/Dataset will optimize the query operation by rules and cost, thus the execution is much smarter than the raw RDD. When to UseSince we take a look at the RDD and DataFrame/Dataset, I make a list about when to use RDD and when to use DataFrame/Dataset. When to use RDD When you want more about the low-level control of dataset When you are dealing with some unstructred data When you prefer manipulate data with lambda function When you don’t care about schema or structure of data When to use DataFrame/Dataset When you are dealing with structured data When you want more code optimization and better performance All in all, I do recommend you to use DataFrame/Dataset API as you can for their easy-using and better optimization. Supporting by Catalyst and Tungsten, DataFrame/Dataset can reduce your time of optimization, thus you can pay more attention to the data itself. References A Tale of Three Apache Spark APIs: RDDs, DataFrames, and Datasets Apache Spark RDD vs DataFrame vs DataSet","link":"/2018/09/22/spark-from-rdd-to-dataframe-dataset/"},{"title":"Second Generation Tungsten Engine in Spark 2.x","text":"This article is about the 2nd generation Tungsten engine, which is the core project to optimize Spark performance. Compared with the 1st generation Tungsten engine, the 2nd one mainly focuses on optimizing query plan and speeding up query execution, which is a pretty aggressive goal to get orders of magnitude faster performance. Let’s take a look! Project TungstenIn the past several years, both storage and network IO bandwidth have been largely improved, while CPU efficiency bound has not. As results, CPU now becomes the new bottleneck and we have to substantially try to improve the efficiency of memory and CPU and push the performance of Spark closer to the limits of modern hardware, which is the main propose of Tungsten. To achieve it, three initiatives in 1st generation Tungsten engine are proposed, including: Memory Management and Binary Processing: leveraging application semantics to manage memory explicitly and eliminate the overhead of JVM object model and garbage collection Cache-aware computation: algorithms and data structures to exploit memory hierarchy Code generation: using code generation to exploit modern compilers and CPUs As we can see, the first two initiatives mainly foucus on memory, and the last one are for CPUs. In the 2nd generation Tungsten engine, rather than code generation, WholeStageCodeGen and Vectorization are proposed for the order of magnitude faster. WholeStageCodeGenAs the name presents, WholeStageCodeGen, aka whole-stage-code-generation, collapses the entire query into a single stage, or maybe a single function. So why combining all the query into a single stage could significantly improve the CPU efficiency and gain performance? We may take a quick look at what it looks like in 1st Tungsten engine. Volcano Iterator ModelWhat a vividly name! Volcano iterator model, as presents in the figure, would generate an interface for each operator, and the each operator would get the results from its parent one by one, just like the volcano eruption from bottom to top.Although Volcano Model can combine arbitrary operators together without worry about the data type of each operator provides, which makes it a popular classic query evaluation strategy in the past 20 years, there are still many downsides and we will take about it later. Bottom-up ModelIn this blog, a hand-written code is proposed to implement the query in the figure above, it’s just a so simple for-loop that even a college freshman can complete, which is:123456var count = 0for (ss_item_sk in store_sales) &#123; if (ss_item_sk == 1000) &#123; count += 1 &#125;&#125; Even though the code is pretty simple, the comparison of performance between Volcano Iterator Model and Bottom-up Model will do shake you.But why is that? Seems it turn out it to be caused by following downsides of Volcano Iterator Model: Too many virtual functions calls:In Volcano Iterator Model, when one operator call for the next operator, a virtual function next() would be called. But in Bottom-up Model, there is no virtual function called because all the operations are combined in a single loop function. Intermediate data of Volcano Iterator Model are in memory while of Bottom-up Model are in CPU registers:As one operator transforms data to another one in Volcano Iterator Model, the intermediate data can only be cached in Memory. But for Bottom-up Model, as there is no need to be transformed, data are always in CPU registers. Volcano Iterator Model don’t take advantage of modern techniques, which Bottom-up Model do, such as loop-pipelining, loop-unrolling and so on:As Bottom-up Model evaluates the whole query into a single loop function, some modern technique can be used in the Bottom-up Model, I will show you two of them, which are loop-pipelining and loop-unrolling. Loop-pipeliningIn a loop iteration function, one iteration of loop usually begins when the previous one is complete, which means the iteration of the loop should be executed sequentially one by one. While loop-pipelining can make some differences. The figure below could learn about it clearly. Loop-pipelining increases the parallelism of the loop iteration by implementing a concurrent manner. Loop-unrollingLoop-unrolling is another technique to exploit parallelism between loop iterations. Let’s learn about it by the code:1234567891011// without loop-unrollingint sum=0;for (int i=0; i&lt;10; i++) &#123; sum+=a[i];&#125;// with loop-unrollingint sum = 0;for (int i=0; i&lt;10; i+=2) &#123; sum += a[i]; sum += a[i+1];&#125; As shown above, loop-unrolling creates multiple copies of the loop body and also changes the loop iteration counter. Reducing loop iteration number, loop-unrolling also create more operation in each loop iteration and, as results, more parallisim could be exploited. Whole Stage Code GenerationFusing operators together to make the generated code looks like the hand-writing bottom-up model, WholeStageCodeGen makes chains of operators as a single stage, and it has been the alternatives for the Code Generation in Catalyst Optimization. It takes advantages of hand-writing and significantly optimizes the query evaluation and can be easily found in the DAG of your Spark application. VectorizationAlthough the WholeStageCodeGen makes a huge optimization of the query plan, there are still some problems. For example, when we import some external integrations, such as tensorflow, scikit-learn, and some python packages, these code cannot be optimized by the WholeStageCodeGen cause they cannot be merged in our code. What’s more, the complicated IO cannot be fused, reading Parquet or ORC for instance. How to speed up this excution? The answer is improving the in-core parallelism for an operation of data, so Vector Processing and Column Format are used in 2nd generation Tungsten engine. Vector Processing In computing, a vector processor or array processor is a central processing unit (CPU) that implements an instruction set containing instructions that operate on one-dimensional arrays of data called vectors, compared to scalar processors, whose instructions operate on single data items. The following figure presents the differences between Scalar and Vector Processing. And we find out that Vector Processing really fasten the operation. How does Vector Processing be implemented? We may need to learn something about SIMD(Single Instruction, Multiple Data) Single instruction, multiple data (SIMD) is a class of parallel computers in Flynn’s taxonomy. It describes computers with multiple processing elements that perform the same operation on multiple data points simultaneously. Let me show one figure to show what’s SIMD breifly.As presented above, SIMD can process multuple data via single instruction, and the data are all in an one-dimesional vector. As results, take advantage of SMID, Vector Processing can improve the in-core parallelism and thus make the operation faster. For the sake of the data are all in an one-dimesonal vector in SIMD, Colunm Format could be a better choice for Spark. Column FormatColumn Format has been widely used in many fields, such as disk storage. The following figure shows the differences between Row Format and Column Format. SummaryWholeStageCodeGen and Vectorization in 2nd generation Tungsten engine really optimize the query plan and speed up the query execution. Compared with 1st generation Tungsten engine, the 2nd one mainly foucses on improving the CPU parallelism to take advtange of some modern techniques. There are many terms and konwledges about CPU exection, which I learn so little that may make some mistakes in this blog, so it would be very nice of you to figure out any single mistake. References Project Tungsten: Bringing Apache Spark Closer to Bare Metal Apache Spark as a Compiler: Joining a Billion Rows per Second on a LaptopDeep dive into the new Tungsten execution engine Spark 2.x - 2nd generation Tungsten Engine Loop Pipelining and Loop Unrolling Vectorization: Ranger to Stampede Transition","link":"/2018/11/14/spark-second-generation-tungsten-in-spark/"},{"title":"Spark工作流程简析","text":"Hello，有一个月没写blog了感觉很自责，必须整起来！最近由于工作上遇到的一些调优困难，让我对Spark有些敬畏，所以集中的研究了下鬼魅玄学Spark，和大家分享一下。首先先来看看spark的基本工作流程。 Work Flow和hadoop一样，spark也是master-slave机制，Spark通过driver进程，将task分发到多个executors上并发进行计算。整个driver和所有的executors组成了一个spark application，每一个application是运行在cluster manager上的，Spark本身集成了standalone cluster，当然，Spark还可以运行在赫赫有名的YARN和Mesos上。我平时使用的公司集群都是基于YARN cluster manager的，因此本文重点探讨基于YARN的spark。 下图就是spark在cluster manager下的整体工作流程。 The DriverDriver是整个application最核心的部分，他运行的是application的main方法，它伴随这整个application的生命周期，driver进程的结束就会带来整个application的结束。 对于所有的Spark任务，他们其实都是实现RDD的transformation和action操作，而这些操作，最后是需要driver将他们转化和分发成tasks，然后才可以去执行。所有的user program都会被driver通过DAG(directed acyclic graph)转化成实际的tasks执行计划，除此之外，driver还会在tasks执行的期间，监控executor上的tasks，并且保证他们拥有健康而合理的资源。 ExecutorsExecutors是Spark application的执行者，他们也是伴随着application的生命周期而存在的，值得注意的是，Spark job在executors执行失败的情况下依然可以继续进行。Executors会对具体的tasks的执行结果返回给driver，同时给缓存的RDD提供存储空间。 Some terms Job: Job是executor层面最大的执行单元，job通过RDD的action操作来分割，每一个action操作就会进行一次job的划分； Stage: Stage是包含在job中的执行单元，stage通过RDD的shuffle操作来分割，每进行一次shuffle操作，就会进行一次stage的划分； Task: Task是executor执行中最细的执行单元，task的数目取和parent RDD的partition数目是一一对应的。 Spark on Yarn-cluster下面，我们一起看看整个Spark application中，driver和executors的都会起到什么作用。我以基于yarn-cluster的YARN的Spark作为例子来简述整个流程，先看一张图：首先我们要明确一些YARN的概念，YARN是与master-slaver的一个Cluster Manager， 在YARN中，RM(ResourseManager)负责整个调度分发，即我们常说的master；而NM(NodeManager)任务分发的接受者，负责执行具体的任务，也就是我们所说的worker。这些概念后续我专门介绍YARN的时候会详细的说明，他们的作用都是实现spark和YARN之间诸如资源申请等操作。 首先Client向ResourceManager发出提交application的请求，ResourseManager会在某一个NodeManager上启动AppManager进程，AppManager会随后启动driver，并将driver申请containers资源的信息发给ResourceManager，申请完成后，ResourceManager将资源分配消息传递给AppManager并由它启动container，每一个container中只运行一个spark executor，由此完成了资源的申请和分配。 然后整个application开始执行，在这个过程中，根据RDD的transformation或者action，driver把这些任务以tasks的形式，源源不断的传送给executors，于是executors不停地进行计算和存储的任务。当driver结束的时候，他会结束掉executors并且释放掉资源。这就是yarn-cluster上spark的整体工作流程。 除了yarn-cluster，还有一种yarn-client的方法，这种方法唯一的区别在于，他的driver并非运行在某个NodeManager上，而是一直运行在client中。这样的问题就是client一旦关闭，那么整个任务也就随之停止执行。因此相较而言，yarn-cluster更适合线上任务，而yarn-client更适合调试模式。 Reference Karau, Holden, et al. Learning spark: lightning-fast big data analysis. “ O’Reilly Media, Inc.”, 2015. Spark:Yarn-cluster和Yarn-client区别与联系","link":"/2018/01/07/spark-spark-workflow/"},{"title":"Spark Tuning","text":"Hi, all, 最近一直在研究spark tuning方面的问题，深感这是一个经验活，也是一个技术活，查阅和很多资料，在这里mark一下。上次我们review了一下spark的work-flow，主要是基于spark on yarn的，同样的，我们在这里探讨的也主要是基于spark on yarn。 Resource AllocationSome ConfigurationResource allocation是spark中一个非常重要的环节，给予一个application过少的resource会带了执行效率的低下和执行速度的缓慢；相反，过多的resource则会带来资源浪费，影响cluster上其他appllication的运行，因此，一个合适的resource allocation是非常非常重要的，我们来看看几个比较重要的parameter： num-executors: 表明spark申请executors的数目，我们可以通过设置spark.dynamicAllocation.enabled来让spark根据数据动态的分配executors，这样可以有效的提高资源利用率； executor-cores: 指定每一个executor的core数目，core数目决定了每个executor的最大并行task数目 executor-memory: 指定分配给每一个executor的内存大小。 Some Tips 对于executor来说，在过于大的memory上运行可能会带来比较高的GC(gabage collection) time，对于一个executor来说，建议给出的上限memory是64G； 由于HDFS在并行读写的时候存在一些瓶颈，因此每一个executor中最好不要超过5个并行任务，即cores数不要超过5个，有实验可以证明，spark在多executor少core的配置下执行效率更高； 相反的，对于executor来说，过分少的core，例如1个，将会使得executors数目变多，例如某个broadcast过程，需要传播到所有的executors上，那么过分多的executors会降低执行的效率。 Memory Mangement关于spark中的memory management，我们先来看一张图：在图中我们可以看到，spark把memory分成了三部分，即spark memory、user memory和reserved memory，我们顺次来看看： Reserved Memory所谓reserved memory，它就是系统预留下的一部分memory，用于存储spark的内部对象，默认大小为300m，绝大部分情况下，我们都不会修改这些参数。值得注意的是，当executor被分配的memory小于1.5倍的reserved memory时，将会抛出“please use larger heap size”的错误。 User MemoryUser memory用于储存spark的transfermation的一些信息，比如RDD之间的依赖信息等等，这部分内存默认大小为(Java Heap - 300M)*0.25，其中的300M其实就是上面提到的reserved memory.具体的大小要依赖于spark.memory.fraction参数，这个参数决定了user 和 下面要讲到的spark memory的分配比例。 Spark Memory上文已经到了，spark memory主要是spark自己使用的memory部分，这部分的大小依赖于spark.memory.fraction参数，即(Java Heap - 300M)*spark.memory.fraction，其中fraction的default为0.75。 Spark memory主要有两个用途，一是用于spark的shuffle等操作，而是用来cache spark中的RDD，因此spark memory也自然而然的分成了两部分，即负责shuffle操作的execution memory和负责cache的storage memory，两者的大小通过spark.memory.storageFraction参数来分割，默认值是0.5。 在spark memory中，还有一个重要的性质，那就是storage 和 execution memory的共享机制，说的简单一些就是，当一边内存空闲而另一方内存紧张的时候，可以借用对方的内存，我们下面看看在内存出现冲突的时候，spark怎么协调： 当storage占用execution memory的时候，发生execution memory使用紧张的情况时，强制将storage占有的内存释放并归还execution，丢失的数据将会后续重新计算； 当execution占用storage memory的时候，发生storage memory紧张的情况，被占用的内存不会被强制释放，因为这会带来任务丢失，storage会耐心等待知道execution执行完释放出内存。 Data Serialization在整个spark任务中，数据传输都是经过序列化后(serialization)之后传输的，因此数据的序列化是很重要的，冗余的序列化过程会让整个spark任务变慢，spark提供两种序列化方式： Java serialization：这是spark默认的序列化方式，java序列化是一种很经典和稳定的序列化方法，但是最大的缺点就是——慢！ Kryo serialization：Kryo 序列化可以让spark任务更加快速，甚至10倍于java序列化；但是它不支持所有的Serializable类型，同时需要为用户自己开发的class进行注册后，才可以使用Kyo. 关于Kryo的详细信息，可以查看spark documentation，或者Kryo documentation Summary关于spark调优的问题，有很多因素，我也是简单的做了一些了解并分享给大家，除了我提到的，还有诸如GC等等因素，大家可以根据我给出的references做进一步的了解。 References How-to: Tune Your Apache Spark Jobs (Part 2) Spark Documentation-Tuning Karau, Holden, et al. Learning spark: lightning-fast big data analysis. “ O’Reilly Media, Inc.”, 2015.","link":"/2018/02/23/spark-spark-tune/"},{"title":"Spark Tips Sum-up Part-2","text":"This article presents some tips of Apache Spark, which is part 2 of the series. All the tips below are based on the real problems which I met. Despite the background, the tips below are of valuable reference. I’ve tried a lot to learn about Apache Spark but can’t know the detail of every part of it. I’d appreciate it if you figure out the mistakes in this article. CoalesceChanging the number of partitions could benefit the performance of your Spark application. A large number of partitions may increase the parallelism of the process while too many tasks would be executed on a single executor, which could cost more running time. In contrast, a small number of partition might improve the complexity of each task and slow down the whole process. To solve this trade-off problem, repartition() and coalesce() is proposed in Apache Spark.Before talking about the detail about coalesce(), let’s review the concept of transformation with wide-dependencies and narrow-dependencies. Wide-dependencies: each partition of the parent RDD is used by at most one partition of the child RDD. Narrow-dependencies: multiple child partitions may depend on each partition in the parent RDD. According to the definition above, repartition() is a typical transformation with wide-dependencies and thus can bring in shuffle operation when triggered. But what about coalesce()? To find out more about it, let’s see the definition first.12345678910// coalesce() for RDD is defined in org.apache.spark.rdd.RDDdef coalesce(numPartitions: Int, shuffle: Boolean = false, partitionCoalescer: Option[PartitionCoalescer] = Option.empty) (implicit ord: Ordering[T] = null) : RDD[T] = withScope &#123;...&#125;// coalesce() for Dataset is defined in org.apache.spark.sql.Datasetdef coalesce(numPartitions: Int): Dataset[T] = withTypedPlan &#123; Repartition(numPartitions, shuffle = false, logicalPlan)&#125; For RDDs, coalesce() has a boolean typed parameter called shuffle. The coalesce() can be treated as repartition() as shuffle is set to True, which is a transformation with wide-dependencies. In contrast, when shuffle is False, coalesce() is a transformation with narrow-dependencies and only can reduce the number of partitions, which means you cannot increase the number of partitions by setting the parameter numPartitions. As for DataFrame/Dataset API, coalesce() is a transformation with narrow-dependencies as there is no shuffle operation at all. As results, coalesce() cannot increase the number of DataFrame/Dataset’s partitions and can only be used to reduce DataFrame/Dataset’s partitions.After understanding the above, there is a crucial tip for you. When you use coalesce() and reduce the number of partitions, which may cause a problem that the partition of the whole stage would be decreased and the computation could be even slower than you expect. since no shuffle operation performed, the stage executes with the level of parallelism assigned by coalesce(). To avoid this problem, you can set shuffle=True for RDDs or use repartition() instead for DataFrame/Dataset to split the whole stage by a shuffle. Read ORC TableReading data from and writing data to HIVE tables is quite often when I use Apache Spark. And we are now using ROC format to store HIVE table. ORC is a self-describing type-aware columnar file format designed for Hadoop workloads. It is optimized for large streaming reads, but with integrated support for finding required rows quickly. Generally, when Spark reads data from HDFS, the initial number of partitions are determined by the number of blocks of data store in HDFS, with one block represents one partition in Spark. However, when I read the ORC format HIVE table, the partition number is equal to the number of files stored on HDFS, not number block. That’s caused by ORC split strategy set by hive.exec.orc.split.strategy, which determines what strategy ORC should use to create splits for execution. The available option includes “BI”, “ETL” and “HYBRID” The HYBRID mode reads the footers for all files if there are fewer files than expected mapper count, switching over to generating 1 split per file if the average file sizes are smaller than the default HDFS block size. ETL strategy always reads the ORC footers before generating splits, while the BI strategy generates per-file splits fast without reading any data from HDFS. As results, when the split strategy is set to ETL, Spark would take more time to generate tasks and the number of partitions is based on the size of HDFS blocks. In contrast, BI strategy would make Spark generate tasks immediately and the number of tasks is equal to the number of files of HIVE table. These two strategies seem to be a trade-off for us and it’s better for us to decide by the actual situation. Reference Managing Spark Partitions with Coalesce and Repartition High Performence Spark Apache ORC Apache ORC Configuration Properties","link":"/2018/10/13/spark-sumup-part-2/"},{"title":"Spark Tips Sum-up Part-1","text":"This article is about things I learned about Apache Spark recently. I’ve been struggling with Spark tuning for more than one month, including shuffle tuning, GC time tuning and so on. Honestly speaking, Apache Spark is just like an untamed horse, it could be a beauty if you tune it well while a nightmare if not. So let me show you some tips I’ve learned from my work. This article is part 1 and here we go. RDD vs DataFrame Partition Number in ShuffleShuffle plays a pretty crucial role in MapReduce and Spark, which can influence a lot on the performance of the whole job. The shuffle would not only separate one stage but bring much more storage and I/O delay. However, is usually hard to avoid shuffle occurring, so how to make shuffle faster could be the key point.During my work in tuning the Spark SQL project, the join operation is unavoidable because it is the fundamental process in all the SQL program. While when I try to run the Spark SQL saving the joined DataFrame into a Hive table, I found the number of table partition is constant no matter how many partitions of two joining DataFrame, oh-gee! it would never happen in RDD join process. That makes the size of each file of Hive table on the HDFS very large, and I have to fix it up.After searching from the Internet, I found that the shuffle of DataFrame in Spark, join included, would have a partition number according to the configuration spark.sql.shuffle.partitions with 200 as default. And the problem above can be solved as this configuration changed. So, when you join two DataFrames or other process causing shuffle, remember the configuration spark.sql.shuffle.partitions when you want to modify the DataFrame partition number.What if I want to modify the partition number of a RDD? Actually, the configuration spark.default.parallelism, which seems to only working for raw RDD and is ignored when working with DataFrames in Spark SQL, may be a good choice. Alternatively, you can set the partition number by calling repartition( ) or coalese( ), which is effective for both DataFrames and RDDs. Smart ActionAs we know, RDDs support two types of operation, which are action and transformation. Transformation creates a new RDD from an existing one, and action returns a value to the Spark driver from computing on a RDD. Based on the lazy evaluation, all the transformations would run as a lineage when an action is triggered by the Spark. A lineage of transformation followed by an action consist of one job in Spark, and one wide-dependency between two transformations separate the job into two stages, which is aka shuffle.When I’m tuning a Spark project, I found that different action following the same lineage of transformation takes different period of time. For example, the action show( ) takes shorter time than createOrReplaceTempView( ), which makes me confuse a lot. After a long time thinking and searching, the answer finally comes out. Spark would draw a DAGSchedule when the program is submitted, the data would run through all the DAGSchedual and the result is sent to the Spark driver. Different action may generate different DAGSchedule even the the transformations are same, Spark is smart enough to know whether it needs to run everything in the RDD. Showed above, Spark may run less data for the show( ) action than those for createOrReplaceTempView( ). Broadcast JoinsBroadcast joins (aka map-side joins) is a good way to abort shuffle and reduce cost. Spark SQL provides two ways for developers, you can not only write SQL but also use DataFrame/Dataset API. When a large table joins a smaller one, a threshold defined by spark.sql.autoBroadcastJoinThreshold, with 10M as default value, determines whether the smaller one will be broadcast or not. If you use SQL in Spark SQL, as the smaller table size below the threshold, Spark would automatically broadcast it to all executors. However, if you use DataFrame/Dataset API, the broadcast function must be imported or Spark wouldn’t broadcast data even if the size is below the threshold.1val df = largeDF.join(broadcast(smallDF),Seq(\"col1\",\"col2\"),\"left\") Also, you can enlarge the value of spark.sql.autoBroadcastJoinThreshold so that larger table can also be broadcast, but the memory of your application should be paid attention.Broadcast joins is really an awesome solution to optimize Spark SQL joins, after using broadcast joins instead of default SortMerge joins, my application runs more than 10 times faster. Avoiding shuffle is quite important for Spark tuning, and broadcast is born to kill shuffle. You will fall in love with her as long as you have a try! References Spark SQL Programming guide Mastering Spark SQL","link":"/2018/09/15/spark-sumup-part-1/"},{"title":"Spark Tips Sum-up Part-3","text":"This blog is the third part of Apache Spark tips sum-up learnt from my programing and debugging. Have been busy for such a long time, I have some time to carry on my personal blog. Today I’ll show you some tips about some functions in Spark SQL, and there may be some mistakes caused by my misunderstanding. Anyway, thanks if you figure out anything incorrect! ExplodeSpark SQL provides a varority of functions in org.apache.spark.sql.functions for you to restruct your data, one of which is the explode() function. Since Spark 2.3, explode() function has been optimized and it has been much more faster than it in the previous Spark versions. For more details about this optimization, this issue may help you a lot.However, what I want to share about is the number of partitions when you use explode(). It’s easy to understand that the number of rows grows several times after the exploding process, as a result, the size of data for each partition has become larger than ever, which may cause the jobs taking more time and more memory. So, it could be a wise choice to enlarge the number of partitions by repartition(), especially when the rows explode more than 10 times than previous, each task would process much more data and that’s possible to get an OOM error, or high GC time. Foreach vs ForeachPartition, Map vs MapPartitionYeah, foreach() vs foreachPartition() and map() vs mapPartition(), these four method do confuse me for a long time and let me share you my understanding about them.First of all, foreach() and foreachPartition() are actiona in Spark, while map() and mapPartition() are transformations. If you have no ideas about the defination of action and transformation, it’s better to read about my previous blog or just ask help for dear google. foreach() and foreachPartition() are often used for writing data to external database while map() and mapPartition() are used to modify the data of each row in the RDD, also DataFrame or DataSet.Seondly, foreachPartition() and mapPartitionn() are respectively based on foreach() and map(). Instead of invoking function for each element, foreachPartition() and mapPartition() calls for each partition and provide an iterator to invoke the function. So what’s the advantages of invoking function for each partition? For example, when you invoke a function connecting to your database, redis for example, foreach() will build a connection for each row of your data, which means, number of connection could be pretty huge and you may get connection failed errors. Instead of invokeing for each element, foreachPartition() could invoke the function building connection to redis for each partition, and data in each partition would be written to redis iteratively. Amazing, isn’t it! Reading ORC TableActually speaking, I have already talked about reading data from ORC table in Apache Spark in the previous blog, but I find something about reading from ORC table pretty awesome since Spark 2.3. Let’s have a look. Spark supports a vectorized ORC reader with a new ORC file format for ORC files since 2.3 version, which means reading from and writing to ORC fromat file could be much faster!To enbale the vectotized ORC reader, you just need to set these configuration: –conf spark.sql.orc.impl=native –conf spark.sql.orc.enableVectorizedReader=true –conf spark.sql.hive.convertMetastoreOrc=true For more information, you can read the Spark Doc. References Apache Spark - foreach Vs foreachPartitions When to use What? Spark SQL Guide - ORC File","link":"/2019/03/06/spark-sumup-part-3/"}],"tags":[{"name":"regularization","slug":"regularization","link":"/tags/regularization/"},{"name":"gradient descent","slug":"gradient-descent","link":"/tags/gradient-descent/"},{"name":"hyperparameter","slug":"hyperparameter","link":"/tags/hyperparameter/"},{"name":"batch norm","slug":"batch-norm","link":"/tags/batch-norm/"},{"name":"covariate shift","slug":"covariate-shift","link":"/tags/covariate-shift/"},{"name":"moving averages","slug":"moving-averages","link":"/tags/moving-averages/"},{"name":"learning strategy","slug":"learning-strategy","link":"/tags/learning-strategy/"},{"name":"orthogonalization","slug":"orthogonalization","link":"/tags/orthogonalization/"},{"name":"transfer learning","slug":"transfer-learning","link":"/tags/transfer-learning/"},{"name":"multi-task learning","slug":"multi-task-learning","link":"/tags/multi-task-learning/"},{"name":"CNN","slug":"CNN","link":"/tags/CNN/"},{"name":"convex optimization","slug":"convex-optimization","link":"/tags/convex-optimization/"},{"name":"newton's method","slug":"newton-s-method","link":"/tags/newton-s-method/"},{"name":"unconstrained optimization","slug":"unconstrained-optimization","link":"/tags/unconstrained-optimization/"},{"name":"MAP","slug":"MAP","link":"/tags/MAP/"},{"name":"ridge regression","slug":"ridge-regression","link":"/tags/ridge-regression/"},{"name":"lasso regression","slug":"lasso-regression","link":"/tags/lasso-regression/"},{"name":"life","slug":"life","link":"/tags/life/"},{"name":"imbalanced data","slug":"imbalanced-data","link":"/tags/imbalanced-data/"},{"name":"gbt","slug":"gbt","link":"/tags/gbt/"},{"name":"logistic regression","slug":"logistic-regression","link":"/tags/logistic-regression/"},{"name":"undersampling","slug":"undersampling","link":"/tags/undersampling/"},{"name":"bagging","slug":"bagging","link":"/tags/bagging/"},{"name":"activtion function","slug":"activtion-function","link":"/tags/activtion-function/"},{"name":"spark","slug":"spark","link":"/tags/spark/"}],"categories":[{"name":"learning notes","slug":"learning-notes","link":"/categories/learning-notes/"},{"name":"machine learning","slug":"machine-learning","link":"/categories/machine-learning/"},{"name":"others","slug":"others","link":"/categories/others/"},{"name":"reading notes","slug":"reading-notes","link":"/categories/reading-notes/"},{"name":"spark","slug":"spark","link":"/categories/spark/"}]}